title,description,tags,accepted_answer
Can older spaCy models be ported to future spaCy versions?,"<p>The latest spaCy versions have better performance and compatibility for GPU acceleration on Apple devices, but I have an existing project that depends on spaCy 3.1.4 and some of the specific behavior of the 3.1.0 models (web lg, web trf).</p>
<p>Would it be possible to port the old models from source to work with newer versions of spaCy (e.g. 3.5) so I could get the same behavior and results, or do I need to use the model that comes with the respective spaCy version?</p>
<p>(This would be on a mac M1, and the advantage is that newer versions of spaCy started supporting Metal Performance Shaders.)</p>
","python, nlp, model, gpu, spacy",
How to download punkt tokenizer in nltk?,"<p>I installed the NLTK library using</p>
<pre><code>pip install nltk
</code></pre>
<p>and while using the lib</p>
<pre><code>from nltk.tokenize import sent_tokenize 
sent_tokenize(text)
</code></pre>
<p>I am getting this error</p>
<pre><code>LookupError: 
**********************************************************************
  Resource punkt not found.
  Please use the NLTK Downloader to obtain the resource:

  &gt;&gt;&gt; import nltk
  &gt;&gt;&gt; nltk.download('punkt')
  
  For more information see: https://www.nltk.org/data.html

  Attempted to load tokenizers/punkt/english.pickle

  Searched in:
    - 'C:\\Users\\adars/nltk_data'
    - 'C:\\Users\\adars\\AppData\\Local\\Programs\\Python\\Python310\\nltk_data'
    - 'C:\\Users\\adars\\AppData\\Local\\Programs\\Python\\Python310\\share\\nltk_data'
    - 'C:\\Users\\adars\\AppData\\Local\\Programs\\Python\\Python310\\lib\\nltk_data'
    - 'C:\\Users\\adars\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
    - ''
</code></pre>
<p>So in order to solve this error i tried</p>
<pre><code>import nltk
nltk.download('punkt')
</code></pre>
<p>but then i am unable to download this package because everytime i run this i get error that says</p>
<pre><code>[nltk_data] Error loading punkt: &lt;urlopen error [WinError 10060] A
[nltk_data]     connection attempt failed because the connected party
[nltk_data]     did not properly respond after a period of time, or
[nltk_data]     established connection failed because connected host
[nltk_data]     has failed to respond&gt;
</code></pre>
<p>please help me out here</p>
","python, machine-learning, nlp, nltk",
How to get bag of words and term frequency in text format using Sklearn?,"<p>I would like to print out the list of words (i.e., bag of words) for each document in a coprus and their respective term frequency (in text format), using Sklearn's <code>CountVectorizer</code>. How could I achieve that?</p>
<p>Here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import CountVectorizer  

#instantiate vectorizer
vectorizer=CountVectorizer()   

#Document creation 
document1='this is a sunny day';document2= 'today is a very very very pleasant day and we have fun fun fun';document3= 'this is an amazin experience'

#list 
list_of_words= [document1,document2,document3]

#bag of words
bag_of_words = vectorizer.fit(list_of_words)

#verify vocabulary of repeated word 
print (vectorizer.vocabulary_.get('very')) 

print (vectorizer.vocabulary_.get('fun'))

#transform
bag_of_words=vectorizer.transform(list_of_words)

print(bag_of_words)&gt;&gt;&gt;&gt;
(0, 3) 1 (0, 7) 1 (0, 9) 1 (0, 10) 1 (1, 2) 1 (1, 3) 1 (1, 5) 3 (1, 6) 1 (1, 7) 1 (1, 8) 1 (1, 11) 1 (1, 12) 3 (1, 13) 1 (2, 0) 1 (2, 1) 1 (2, 4) 1 (2, 7) 1 (2, 10) 1
</code></pre>
","python, scikit-learn, nlp, corpus, countvectorizer",
Low score and wrong answer for Flan-T5-XXL &quot;question-answering&quot; task,"<p>I'm trying to run Flan-T5-XXL model for a &quot;question-answering&quot; task.
Here's how I loaded and executed the model:</p>
<pre><code>model_id = &quot;~/Downloads/test_LLM/flan-t5-xxl&quot;
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForQuestionAnswering.from_pretrained(model_id, return_dict=False).to(DEVICE)

qa_T5XXL = pipeline(&quot;question-answering&quot;, model=model, tokenizer=tokenizer)

question = &quot;What is 42?&quot;
context = &quot;42 is the answer to life, the universe and everything&quot;

result = qa_T5XXL({
    &quot;question&quot;: question,
    &quot;context&quot;: context
})
</code></pre>
<p>However, I get a low score and a wrong answer:</p>
<pre><code>{'score': 0.03840925544500351, 'start': 0, 'end': 2, 'answer': '42'}
</code></pre>
<p>Could you please help me make changes to achieve the correct answer?
Thanks in advance.</p>
","python, tensorflow, nlp, huggingface-transformers, huggingface",
Unsupervised Topic Modeling for Short Event Descriptions,"<p>I have a dataset of approximately 750 lines containing quite short texts (less than 150 words each). These are all event descriptions related to a single broad topic (which I cannot specify for anonymity reasons).</p>
<p>I want to apply unsupervised topic modeling methods to get more precise and non-exclusive categories/tags for each text. My goal is to implement a multi-label approach where each text can belong to multiple categories simultaneously. Ideally, I'd like to identify around 10-20 relevant topics.</p>
<p>So far, I've tried:</p>
<ul>
<li>BERTopic: Since it relies on clustering, I end up with each text belonging to a single cluster, which is not what I want.</li>
<li>KeyBERT + BERTopic: I first used KeyBERT to extract keywords from each text and then applied BERTopic to get non-exclusive categories. This gave me the best results so far, but I'm still not completely convinced.</li>
</ul>
<p>I haven't tried LDA as I consider it a bit outdated and expected disappointing results for such short texts.</p>
","machine-learning, nlp, topic-modeling",
Save and reuse TfidfVectorizer in scikit learn,"<p>I am using TfidfVectorizer in scikit learn to create a matrix from text data. Now I need to save this object to reuse it later. I tried to use pickle, but it gave the following error.</p>
<pre class=""lang-none prettyprint-override""><code>loc=open('vectorizer.obj','w')
pickle.dump(self.vectorizer,loc)
*** TypeError: can't pickle instancemethod objects
</code></pre>
<p>I tried using <code>joblib</code> in sklearn.externals, which again gave similar error. Is there any way to save this object so that I can reuse it later?</p>
<p>Here is my full object:</p>
<pre class=""lang-py prettyprint-override""><code>class changeToMatrix(object):
    def __init__(self,ngram_range=(1,1),tokenizer=StemTokenizer()):
        from sklearn.feature_extraction.text import TfidfVectorizer
        self.vectorizer = TfidfVectorizer(ngram_range=ngram_range,analyzer='word',lowercase=True,
                                          token_pattern='[a-zA-Z0-9]+',strip_accents='unicode',
                                          tokenizer=tokenizer)

    def load_ref_text(self,text_file):
        textfile = open(text_file,'r')
        lines = textfile.readlines()
        textfile.close()
        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
        sentences = [item.strip().strip('.') for item in sent_tokenizer.tokenize(' '.join(lines).strip())]
        #vectorizer is transformed in this step
        chk2 = pd.DataFrame(self.vectorizer.fit_transform(sentences1).toarray())
        return sentences, [chk2]

    def get_processed_data(self,data_loc):
        ref_sentences,ref_dataframes=self.load_ref_text(data_loc)
        loc = open(&quot;indexedData/vectorizer.obj&quot;,&quot;w&quot;)
        pickle.dump(self.vectorizer,loc) #getting error here
        loc.close()
        return ref_sentences, ref_dataframes
</code></pre>
","python, scikit-learn, file-io, nlp, pickle","<p>Firstly, it's better to leave the import at the top of your code instead of within your class:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
class changeToMatrix(object):
  def __init__(self,ngram_range=(1,1),tokenizer=StemTokenizer()):
    ...
</code></pre>

<p>Next <code>StemTokenizer</code> don't seem to be a canonical class. Possibly you've got it from <a href=""http://sahandsaba.com/visualizing-philosophers-and-scientists-by-the-words-they-used-with-d3js-and-python.html"" rel=""noreferrer"">http://sahandsaba.com/visualizing-philosophers-and-scientists-by-the-words-they-used-with-d3js-and-python.html</a> or maybe somewhere else so <strong><em>we'll assume it returns a list of strings</em></strong>.</p>

<pre><code>class StemTokenizer(object):
    def __init__(self):
        self.ignore_set = {'footnote', 'nietzsche', 'plato', 'mr.'}

    def __call__(self, doc):
        words = []
        for word in word_tokenize(doc):
            word = word.lower()
            w = wn.morphy(word)
            if w and len(w) &gt; 1 and w not in self.ignore_set:
                words.append(w)
        return words
</code></pre>

<p>Now to answer your actual question, it's possible that you need to open a file in byte mode before dumping a pickle, i.e.:</p>

<pre><code>&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer
&gt;&gt;&gt; from nltk import word_tokenize
&gt;&gt;&gt; import cPickle as pickle
&gt;&gt;&gt; vectorizer = TfidfVectorizer(ngram_range=(0,2),analyzer='word',lowercase=True, token_pattern='[a-zA-Z0-9]+',strip_accents='unicode',tokenizer=word_tokenize)
&gt;&gt;&gt; vectorizer
TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',
        dtype=&lt;type 'numpy.int64'&gt;, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(0, 2), norm=u'l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents='unicode', sublinear_tf=False,
        token_pattern='[a-zA-Z0-9]+',
        tokenizer=&lt;function word_tokenize at 0x7f5ea68e88c0&gt;, use_idf=True,
        vocabulary=None)
&gt;&gt;&gt; with open('vectorizer.pk', 'wb') as fin:
...     pickle.dump(vectorizer, fin)
... 
&gt;&gt;&gt; exit()
alvas@ubi:~$ ls -lah vectorizer.pk 
-rw-rw-r-- 1 alvas alvas 763 Jun 15 14:18 vectorizer.pk
</code></pre>

<p><strong>Note</strong>: Using the <code>with</code> idiom for i/o file access automatically closes the file once you get out of the <code>with</code> scope.</p>

<p>Regarding the issue with <code>SnowballStemmer()</code>, note that <code>SnowballStemmer('english')</code> is an object while the stemming function is <code>SnowballStemmer('english').stem</code>. </p>

<p><strong>IMPORTANT</strong>:</p>

<ul>
<li><code>TfidfVectorizer</code>'s tokenizer parameter expects to take a string and return a list of string</li>
<li>But Snowball stemmer does not take a string as input and return a list of string.</li>
</ul>

<p>So you will need to do this:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import SnowballStemmer
&gt;&gt;&gt; from nltk import word_tokenize
&gt;&gt;&gt; stemmer = SnowballStemmer('english').stem
&gt;&gt;&gt; def stem_tokenize(text):
...     return [stemmer(i) for i in word_tokenize(text)]
... 
&gt;&gt;&gt; vectorizer = TfidfVectorizer(ngram_range=(0,2),analyzer='word',lowercase=True, token_pattern='[a-zA-Z0-9]+',strip_accents='unicode',tokenizer=stem_tokenize)
&gt;&gt;&gt; with open('vectorizer.pk', 'wb') as fin:
...     pickle.dump(vectorizer, fin)
...
&gt;&gt;&gt; exit()
alvas@ubi:~$ ls -lah vectorizer.pk 
-rw-rw-r-- 1 alvas alvas 758 Jun 15 15:55 vectorizer.pk
</code></pre>
"
semantic similarity for mix of languages,"<p>I have a database of several thousands of utterances. Each record (utterance) is a text representing a problem description, which a user has submitted to a service desk. Sometimes also the service desk agent's response is included. The language is highly technical, and it contains three types of tokens:</p>
<ol>
<li>words and phrases in Language 1 (e.g. English)</li>
<li>words and phrases in Language 2 (e.g. French, Norwegian, or Italian)</li>
<li>machine-generated output (e.g. listing of files using unix command ls -la)</li>
</ol>
<p>These languages are densely mixed. I often see that in one conversation, a sentence in Language 1 is followed by Language 2. So it is impossible to divide the data into two separate sets, corresponding to utterances in two languages.</p>
<p>The task is to find similarities between the records (problem descriptions). The purpose of this exercise is to understand whether some bugs submitted by users are similar to each other.</p>
<p><strong>Q: What is the most effective way to proceed in such a situation?</strong></p>
<p>In particular, the problem lies in the fact that the words come from two different corpora (corpuses), while in addition, some technical words (like filenames, OS paths, or application names) will not be found in any of those.</p>
<p>So the difficulty is in finding similarities between words and utterances in situation when the mix of words does not correspond to known corpus, but rather a mix of corpora.</p>
","nlp, spacy, word2vec, bert-language-model, sentence-similarity",
NameError: name &#39;init_empty_weights&#39; is not defined while using hugging face models,"<p>I am trying to set up hugging face locally and im running into this issue.</p>
<pre><code>NameError: name 'init_empty_weights' is not defined
</code></pre>
<p>Here is the code I have tested my installation with</p>
<pre><code>from transformers import pipeline
classifier = pipeline(&quot;sentiment-analysis&quot;)
text = &quot;I love using Hugging Face Transformers!&quot;
result = classifier(text)
print(result)


</code></pre>
<p>transformers: 4.51.0 <br>
tokenizers: 0.21.1 <br>
accelerate: 1.6.0 <br>
sentence-transformers: 4.0.2 <br>
huggingface_hub: 0.30.1 <br>
I am currently using  pytorch-metal mac M3 pro.<br></p>
<p>What causes this, and how can I fix it?</p>
","nlp, huggingface-transformers, huggingface","<p>Try using this version, it should resolve the issue.</p>
<pre><code>transformers==4.50.3
</code></pre>
"
Is there a way to reuse a heavy service across tasks in Airflow?,"<p>I'm building an Airflow DAG where some of the steps should do ML/NLP processing.</p>
<p>I have a service class that loads NLP model in constructor. E.g.:</p>
<pre><code>class SentenceService:
    def __init__(self, model: str = &quot;abc&quot;):
        self.nlp = spacy.load(model)
</code></pre>
<p>I'd like to reuse this service instance so that the tasks execute faster. Some of the models are very heavy and take disproportional amount of time to load.</p>
<p>I'm using <code>processed_texts = process_text.expand(texts=texts)</code> to create task for each text to process so that I can distribute the processing horizontally across multiple workers.</p>
<p>What I tried is to instantiate the service globally in the DAG - as a module-level/top-level variable)
According to the docs, this seems not advisable anyway (<a href=""https://airflow.apache.org/docs/apache-airflow/2.6.0/best-practices.html#top-level-python-code"" rel=""nofollow noreferrer"">https://airflow.apache.org/docs/apache-airflow/2.6.0/best-practices.html#top-level-python-code</a>).</p>
<p>I also tried to implement typical Python-ish singleton for the service, so that whenever it's used in the tasks, same instance would be returned.</p>
<p>I also understand that if I <strong>won't</strong> run the processing in parallel (<code>.expand</code> above), I can have one service per batch - but only on one worker.</p>
<p>None of that led to what I want - quite obviously to me now :), tasks execute separately so DAG &quot;runs fresh again&quot;, so it makes sense. Neither the global variable, nor Singleton is shared across processes.</p>
<p>Is there any technique to handle this in Airflow?</p>
","nlp, airflow, microservices",
How can I link tasks using machine learning / ai based on historical task sequences?,"<p>I'm working on an AI model to predict dependency links between tasks for industrial plannifications, based on historical project data. I have two tables:</p>
<p>Tasks Table:
TaskID, TaskName, EquipmentType
(I'm considering adding StartDate and EndDate)</p>
<p>Dependencies Table:
PredecessorTaskID, SuccessorTaskID, LinkType</p>
<p><strong>Goal:</strong>
Given a new list of tasks (typically filtered by EquipmentType), I want the model to suggest likely dependencies between them (and eventually, the LinkType) — learned from historical patterns in the existing data.</p>
<p><strong>What I’ve tried:</strong></p>
<p>Decision Trees</p>
<p>Basic Neural Networks</p>
<p><strong>Problems encountered:</strong></p>
<p>Random or irrelevant links</p>
<p>Models predicting dependencies between all tasks</p>
<p>Lack of logical flow learned from historical data</p>
<p>I'm pretty sure i am not pre-processing the data correctly as i'm not sur how to treat the tasks name for it to recognize the &quot;pattern&quot;</p>
<p><strong>My Question:</strong>
Would it make sense to frame this as a graph problem and use Graph Neural Networks (GNNs)? Or is there a better ML or statistical approach for modeling and predicting dependencies between tasks in this kind of scenario?</p>
<p>I'm open to advice on model architecture or data pre-processing strategies that might improve performance.</p>
","python, machine-learning, nlp, artificial-intelligence",
Why are weight matrices shared between embedding layers in &#39;Attention is All You Need&#39; paper?,"<p>I am using the Transformer module in pytorch from the paper &quot;Attention is All You Need&quot;. On page 5, the authors state that</p>
<blockquote>
<p>In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. (page 5)</p>
</blockquote>
<p>The embedding layer, at least in pytorch, is a learnable tensor whose columns are the embedding vectors corresponding to each word. My confusion stems from the fact that in the paper, the Transformer learns a translation task between languages (i.e. English to German). <strong>Thus, how could the embedding weights be shared for the English and German embedding vectors?</strong></p>
<p><strong>In addition, how could the weights be shared between the output embedding (which goes from word index to embedding vector) and the linear layer (which goes from embedding vector to word probabilities)?</strong> As far as I can tell there is no constraint requiring the embedding tensor must be orthogonal (so that its inverse is its transpose).</p>
","nlp, pytorch, word-embedding, transformer-model",
When is entity replacement necessary for relation extraction?,"<p>In this <a href=""https://www.microsoft.com/developerblog/2016/09/13/training-a-classifier-for-relation-extraction-from-medical-literature/#Reuse"" rel=""nofollow noreferrer"">tutorial</a> for ""Training a Machine Learning Classifier for Relation Extraction from Medical Literature"" the author does Entity replacement because that ""we don’t want the model to learn according to a specific entity name, but we want it to learn according to the structure of the text"".</p>

<p>Is this generally true or does it depend on the dataset or the used models?</p>
","machine-learning, nlp",
Natural Language Processing Tag definitions,"<p>When parsing and tagging the words with OpenNLP i was wondering if the tags (eg S, NP, VP, ADJP) actually mean, i found a few by researching the web, but some of them are still missing,
which i am unable to find, currently my code outputs this:</p>
<pre><code>The movie was really good

\-S - S
 |-NP - {Unknown}
 |  |-DT - Determiner
 |  | \- The - The
 |  \- NN - Noun, Singular or mass
 |    \- movie - movie
 \-VP - {Unknown}
   |-VBD - Verb, past tense
   | \- was - was
   \- ADJP - {Unknown}
      |-RB - Adverb
      | \-really - really
      \-JJ - Adjective
        \- good - good
</code></pre>
<p>As you can see I have managed to map some of them such as NN as &quot;Noun, Singular or mass&quot; but i am unable to find any references to S, NP, VP, ADJP</p>
","nlp, artificial-intelligence, opennlp","<p>The tags are part-of-speech tags or syntactic categories. </p>

<ul>
<li>S : sentence</li>
<li>NP : noun phrase</li>
<li>VP : verb phrase</li>
<li>ADJP : adjective phrase</li>
</ul>

<p>Here is a <a href=""http://cs.nyu.edu/grishman/jet/guide/PennPOS.html"" rel=""nofollow noreferrer"">list</a> of tags used in the Penn Treebank which is the corpus OpenNLP uses. Different projects use different abbreviations for parts of speech. Some projects use NP for a noun phrase, others NNP.</p>
"
Natural Language Processing Model,"<p>I'm making a project to parse, and understand the intentions of input lines by a user in english.</p>
<p>Here is what I think I should do:</p>
<ol>
<li>Create a text of sentences with POS tagging &amp; marked intentions for every sentence by hand.</li>
<li>Create a model say: decision tree and train it on the above sentences.</li>
<li>Try the model on user input:</li>
<li>Do basic tokenizing and POS tagging on user input sentence and testing it on the above model for knowing the intention of this sentence.</li>
</ol>
<p>It all may be completely wrong or silly but I'm determined to learn how to do it. I don't want to use ready-made solutions and the programming language is not a concern.</p>
<p>How would you guys do this task? Which model to choose and why? Normally to make NLP parsers, what steps are done.</p>
","nlp, machine-learning",
How extract text from PDF including images and text,"<p>I am going to extract text from multiple PDF files. The PDF files include text and some images and even some pages are scanned pages (I assumed the scanned pages are like images). I followed the below commands to extract text from PDF files.</p>
<p>How can I edit my commands with a condition to check if each page contains any images, then extract text from images?</p>
<pre><code>lst_all_text = []

for foldername,subfolders,files in os.walk(r&quot;C:/MY PATH&quot;):
    for file in files:
        # open the pdf file
        object = PyPDF2.PdfFileReader(os.path.join(foldername,file))
        # get number of pages
        NumPages = object.getNumPages()
        text =  &quot;&quot;
        # extract text and do the search
        for i in range(0, NumPages):         
            PageObj = object.getPage(i)
            text += PageObj.extractText() 
            
        lst_all_text.append(text)
</code></pre>
","python, nlp, text-mining, text-extraction",
Extracting Values from text using Python,"<p>I have long list of text from a pandas dataframe. However I am going to explain what I want to do just with one example. I have two pre-defined lists. These two-predefined list can be used for extracting the values I need from the text: </p>

<pre><code>ingredient_name_list = ['butter', 'butter oil', 'cheese', 'cheese food', 'cheese spread', 'cream', 'eggnog', 'sour dressing', 'milk', 'yogurt', 'cream substitute', 'dessert topping', 'sour cream', 'milk substitutes', 'milk shakes', 'whey', 'egg', 'egg substitute', 'cheese substitute', 'cheese sauce', 'parmesan cheese topping', 'reddi wip fat free whipped topping', 'cheese product', 'protein supplement', 'dulce de leche', 'ice cream', 'ice cream sandwich', 'ice cream cookie sandwich', 'ice cream cone', 'fat free ice cream', 'milk dessert bar', 'nutritional supplement for people with diabetes', 'ice cream bar', 'kefir', 'ice cream sundae cone', 'light ice cream', 'spices', 'basil', 'dill weed', 'mustard', 'salt', 'vinegar', 'thyme', 'vanilla extract', 'capers', 'horseradish', 'rosemary', 'peppermint', 'spearmint', 'seasoning mix', 'clif z bar', 'babyfood', 'zwieback', 'infant formula', 'toddler formula', 'toddler drink', 'child formula', 'fat', 'lard', 'salad dressing', 'sandwich spread', 'shortening', 'oil', 'margarine', 'vegetable oil', 'soybean', 'margarine-like', 'fish oil', 'animal fat', 'margarine-like spread with yogurt', 'margarine spread', 'margarine-like shortening', 'margarine-like spread', 'margarine-like vegetable-oil spread', 'dressing', 'mayonnaise', 'chicken', 'canada goose', 'duck', 'goose', 'guinea hen', 'pheasant', 'quail', 'squab', 'turkey', 'pate de foie gras', 'turkey and gravy', 'turkey breast', 'turkey thigh', 'turkey roast', 'turkey sticks', 'poultry', 'chicken patty', 'chicken breast tenders', 'ruffed grouse', 'emu', 'ostrich', 'turkey from whole', 'soup', ""campbell's"", 'sauce', 'gravy', 'split pea soup', 'split pea with ham soup', ""campbell's chunky"", 'smart soup', 'fish broth', 'potato soup', 'barbecue loaf', 'beerwurst', 'sausage', 'blood sausage', 'bockwurst', 'bologna', 'bratwurst', 'braunschweiger (a liver sausage)', 'cheesefurter', 'chicken spread', 'corned beef loaf', 'dutch brand loaf', 'frankfurter', 'ham', 'ham salad spread', 'headcheese', 'knackwurst', 'lebanon bologna', 'liver cheese', 'liver sausage', 'roast beef', 'luncheon meat', 'mortadella', 'olive loaf', 'pastrami', 'pate', 'peppered loaf', 'pepperoni', 'pickle and pimiento loaf', 'polish sausage', 'luxury loaf', ""mother's loaf"", 'picnic loaf', 'pork sausage', 'poultry salad sandwich spread', 'salami', 'thuringer', 'honey roll sausage', 'luncheon sausage', 'oscar mayer', 'bacon', 'liverwurst spread', 'roast beef spread', 'swisswurst', 'bacon and beef sticks', 'yachtwurst', 'chicken breast', 'kielbasa', 'macaroni and cheese loaf', 'scrapple', 'meatballs', 'cereals', 'cereals ready-to-eat', 'milk and cereal bar', 'rice and wheat cereal bar', 'incaparina', 'acerola', 'acerola juice', 'apples', 'apple juice', 'applesauce', 'apricots', 'apricot nectar', 'avocados', 'bananas', 'blackberries', 'blackberry juice', 'cherries', 'blueberries', 'boysenberries', 'breadfruit', 'carambola', 'carissa', 'cherimoya', 'crabapples', 'cranberries', 'cranberry sauce', 'cranberry-orange relish', 'currants', 'custard-apple', 'dates', 'elderberries', 'figs', 'fruit cocktail', 'fruit salad', 'gooseberries', 'goji berries', 'grapefruit', 'grapefruit juice', 'grapes', 'grape juice', 'groundcherries', 'guavas', 'guava sauce', 'jackfruit', 'java-plum', 'jujube', 'kiwifruit', 'kumquats', 'lemons', 'lemon juice', 'lemon juice from concentrate', 'lemon peel', 'limes', 'lime juice', 'litchis', 'loganberries', 'longans', 'loquats', 'mammy-apple', 'mangos', 'mangosteen', 'mango', 'melons', 'melon balls', 'mulberries', 'nectarines', 'oheloberries', 'olives', 'oranges', 'orange juice', 'orange peel', 'orange-grapefruit juice', 'tangerines', 'tangerine juice', 'papayas', 'papaya', 'papaya nectar', 'passion-fruit', 'passion-fruit juice', 'peaches', 'peach nectar', 'pears', 'pear nectar', 'persimmons', 'pineapple', 'pineapple juice', 'pitanga', 'plantains', 'plums', 'pomegranates', 'prickly pears', 'prunes', 'prune juice', 'pummelo', 'quinces', 'raisins', 'rambutan', 'raspberries', 'rhubarb', 'roselle', 'rose-apples', 'sapodilla', 'sapote', 'soursop', 'strawberries', 'sugar-apples', 'tamarinds', 'watermelon', 'maraschino cherries', 'feijoa', 'durian', 'prune puree', 'candied fruit', 'abiyuch', 'rowal', 'clementines', 'guanabana nectar', 'guava nectar', 'mango nectar', 'tamarind nectar', 'pomegranate juice', 'juice', 'nance', 'naranjilla (lulo) pulp', 'horned melon ', 'orange pineapple juice blend', 'fruit juice smoothie', 'cranberry juice blend', 'ruby red grapefruit juice blend (grapefruit', 'baobab powder', 'cherry juice', 'raspberry juice concentrate', 'pork', 'canadian bacon', 'hormel', 'hormel always tender', 'hormel canadian style bacon', 'pork loin', 'alfalfa seeds', 'amaranth leaves', 'arrowhead', 'artichokes', 'asparagus', 'balsam-pear (bitter gourd)', 'bamboo shoots', 'beans', 'lima beans', 'mung beans', 'beets', 'beet greens', 'broadbeans', 'broccoli', 'broccoli raab', 'brussels sprouts', 'burdock root', 'butterbur', 'cabbage', 'cardoon', 'carrots', 'cassava', 'cauliflower', 'celeriac', 'celery', 'celtuce', 'chard', 'chayote', 'chicory', 'chicory greens', 'chicory roots', 'chives', 'chrysanthemum', 'collards', 'coriander (cilantro) leaves', 'corn', 'corn with red and green peppers', 'cornsalad', 'cowpeas (blackeyes)', 'cowpeas', 'yardlong bean', 'cress', 'cucumber', 'dandelion greens', 'eggplant', 'edamame', 'endive', 'escarole', 'garlic', 'ginger root', 'gourd', 'drumstick leaves', 'hyacinth-beans', 'jerusalem-artichokes', ""jew's ear"", 'pepeao', 'jute', 'kale', 'kanpyo', 'mushrooms', 'kohlrabi', 'lambsquarters', 'leeks', 'lentils', 'lettuce', 'lotus root', 'mountain yam', 'mustard greens', 'mustard spinach', 'new zealand spinach', 'okra', 'onions', 'onion rings', 'parsley', 'parsnips', 'peas', 'peas and carrots', 'peas and onions', 'peppers', 'pigeonpeas', 'pokeberry shoots', 'potatoes', 'potato puffs', 'potato wedges', 'potato flour', 'potato salad', 'pumpkin flowers', 'pumpkin leaves', 'pumpkin', 'pumpkin pie mix', 'purslane', 'radishes', 'rutabagas', 'salsify', 'sauerkraut', 'seaweed', 'sesbania flower', 'soybeans', 'spinach', 'squash', 'succotash', 'water convolvulus', 'sweet potato leaves', 'sweet potato', 'taro', 'taro leaves', 'taro shoots', 'tomatoes', 'tomato juice', 'tomato products', 'tomato powder', 'tree fern', 'turnips', 'turnip greens', 'turnip greens and turnips', 'vegetable juice cocktail', 'vegetables', 'vinespinach', 'waterchestnuts', 'watercress', 'waxgourd', 'winged beans', 'winged bean leaves', 'winged bean tuber', 'yam', 'yambean (jicama)', 'borage', 'dock', 'eppaw', 'drumstick pods', 'shallots', 'carrot juice', 'corn pudding', 'spinach souffle', 'potato pancakes', 'radish seeds', 'carrot', 'arrowroot', 'chrysanthemum leaves', 'winged bean', 'ketchup', 'pickles', 'mushroom', 'pimento', 'pickle relish', 'catsup', 'radicchio', 'tomatillos', 'fennel', 'arugula', 'hearts of palm', 'nopales', 'lemon grass (citronella)', 'grape leaves', 'pepper', 'epazote', 'fireweed', 'malabar spinach', 'fungi', 'wasabi', 'yautia (tannier)', 'fiddlehead ferns', 'seeds', 'nuts', 'beef', 'alcoholic beverage', 'beverages', 'carbonated beverage', 'cocoa mix', 'cranberry juice cocktail', 'alcoholic beverages', 'lemonade', 'limeade', 'malt beverage', 'shake', 'strawberry-flavor beverage mix', 'water', 'whiskey sour mix', 'fish', 'crustaceans', 'mollusks', 'salmon nuggets', 'salmon', 'yokan', 'broadbeans (fava beans)', 'carob flour', 'chickpeas (garbanzo beans', 'chili with beans', 'hyacinth beans', 'lupins', 'mothbeans', 'noodles', 'mungo beans', 'peanuts', 'peanut butter', 'peanut flour', 'pigeon peas (red gram)', 'refried beans', 'meat extender', 'soy flour', 'soy meal', 'soymilk', 'soy protein ', 'tofu', 'okara', 'yardlong beans', 'hummus', 'falafel', 'veggie burgers or soyburgers', 'peanut spread', 'chickpea flour', 'mori-nu', 'frijoles rojos volteados (refried beans', 'tempeh', 'vitasoy usa', 'soymilk (all flavors)', 'silk plain', 'silk vanilla', 'silk chocolate', 'silk light plain', 'silk light vanilla', 'silk light chocolate', 'silk plus omega-3 dha', 'silk plus for bone health', 'silk plus fiber', 'silk unsweetened', 'silk very vanilla', 'silk nog', 'silk chai', 'silk mocha', 'silk coffee', 'silk vanilla soy yogurt', 'vitasoy usa organic nasoya', 'vitasoy usa nasoya', 'vitasoy usa organic nasoya sprouted', 'vitasoy usa azumaya', 'peanut butter with omega-3', 'soy protein concentrate', 'soy protein isolate', 'soy sauce made from soy and wheat (shoyu)', 'soy sauce', 'veal', 'lamb', 'game meat', 'bison', 'game meat ', 'bagels', 'biscuits', 'bread', 'cake', 'cookies', 'puff pastry', 'crackers', 'cracker', 'cream puff shell', 'croissants', 'croutons', 'danish pastry', 'doughnuts', 'muffins', 'french toast', 'hush puppies', 'ice cream cones', 'pancakes plain', 'pancakes', 'pie', 'pie crust', 'phyllo dough', 'popovers', 'rolls', 'strudel', 'sweet rolls', 'taco shells', 'toaster pastries', 'tortillas', 'waffles', 'wonton wrappers ', 'leavening agents', 'english muffins', 'tart', 'archway home style cookies', 'artificial blueberry muffin mix', 'kraft', 'george weston bakeries', 'keebler', 'continental mills', 'mckee baking', 'martha white foods', 'mission foods', 'nabisco', 'pillsbury', 'pillsbury grands', 'pillsbury golden layer buttermilk biscuits', 'kraft foods', 'heinz', 'interstate brands corp', 'waffle', 'muffin', 'tostada shells', 'keikitos (muffins)', 'pan dulce', 'pastry', 'garlic bread', 'cinnamon buns', 'cream puff', 'focaccia', 'schiff', 'candies', 'snacks', 'fruit syrup', 'topping', 'syrup', 'baking chocolate', 'ice creams', 'desserts', 'sherbet', 'syrups', 'puddings', 'chocolate-flavored hazelnut spread~^~chocolate-flavored hazelnut sprd~^^^~y~^^0^^6.25^^^', 'toppings', 'chewing gum~^~chewing gum~^~bubble gum~^^~y~^^0^^6.25^1.82^8.37^3.70', 'cocoa', 'egg custards', 'gelatin desserts', 'gelatins', 'flan', 'rennin', 'frozen novelties', 'frostings', 'frozen yogurts', 'fruit butters', 'honey', 'jams and preserves', 'jellies', 'marmalade', 'molasses', 'pectin', 'pie fillings', 'pudding', 'sugars', 'sweeteners', 'snack', 'tortilla chips', 'cheese puffs and twists', 'popcorn', 'potato chips', 'chocolate', 'sugar', 'sweetener', 'jams', 'amaranth grain', 'arrowroot flour', 'barley', 'buckwheat', 'buckwheat groats', 'buckwheat flour', 'bulgur', 'corn grain', 'corn bran', 'corn flour', 'cornmeal', 'cornstarch', 'couscous', 'hominy', 'millet', 'oat bran', 'quinoa', 'rice', 'oats', 'rice bran', 'rice flour', 'rye grain', 'rye flour', 'semolina', 'sorghum grain', 'tapioca', 'triticale', 'triticale flour', 'wheat', 'wheat bran', 'wheat germ', 'wheat flour', 'wild rice', 'pasta', 'macaroni', 'spaghetti', 'wheat flours', 'barley flour or meal', 'barley malt flour', 'oat flour', 'rice noodles', 'spelt', 'teff', 'millet flour', 'sorghum flour', 'fast foods', 'burger king', 'fast food', 'chick-fil-a', 'school lunch', 'subway', 'pizza', ""mcdonald's"", ""wendy's"", 'taco bell', 'bacon ranch salad with crispy chicken', 'popeyes', 'kfc', 'yogurt parfait', 'digiorno pizza', ""wend'ys"", 'pizza hut', ""arby's"", 'macaroni and cheese', 'spaghetti with meat sauce', 'beef macaroni with tomato sauce', 'pasta with sliced franks in tomato sauce', 'turkey pot pie', 'beef pot pie', 'hot pockets', ""hot pockets ham 'n cheese stuffed sandwich"", 'ravioli', 'tortellini', 'chili con carne with beans', 'beef stew', 'chicken pot pie', 'lasagna', 'chili', 'pasta with tomato sauce', 'lasagna with meat &amp; sauce', 'burrito', 'egg rolls', 'rice bowl with chicken', 'macaroni and cheese dinner with dry sauce mix', 'lean pockets', 'potato salad with egg~^~potato salad w/ egg~^^^~y~^^0^^^^^', 'pulled pork in barbecue sauce~^~pulled pork in barbecue sau~^^^^^0^^^^^', 'corn dogs', 'lasagna with meat sauce', 'chicken tenders', 'rice-a-roni', 'rice and vermicelli mix', 'beef composite', 'formulated bar', 'pretzels', 'rice crackers', 'dip', 'cookie', ""andrea's"", 'crunchmaster', 'glutino', ""mary's gone crackers"", 'pepperidge farm', ""rudi's"", 'sage valley', 'schar', ""udi's"", ""van's"", 'sweet potatoes', 'sweet potato puffs', 'vegetable juice', 'taquitos', 'pasta mix', 'yellow rice with seasoning', 'pizza rolls', 'potsticker or wonton', 'macaroni or noodles with cheese', 'turnover', 'spanish rice mix', 'rice mix', 'dumpling', 'salisbury steak with gravy', 'hungry man', 'banquet', 'jimmy dean', 'agutuk', 'ascidians ', 'bear', 'whale', 'caribou', 'stew/soup', 'chiton', 'cloudberries', 'cockles', 'cranberry', 'huckleberries', 'stew', 'moose', 'mashu roots', 'mouse nuts', 'octopus', 'seal', 'oopah (tunicate)', 'owl', 'sea cucumber', 'sourdock', 'squirrel', 'tea', 'walrus', 'deer (venison)', 'willow', 'mush', 'melon', 'chilchen ', 'mutton', 'frybread', 'tortilla', 'tamales', 'salmonberries', 'elk', 'buffalo', 'chokecherries', 'steelhead trout', 'acorn stew ', 'smelt', 'corned beef and potatoes in tortilla', 'tennis bread', 'agave', 'cattail', 'prairie turnips', 'rose hips', 'stinging nettles', 'pinon nuts', 'sea lion', 'wocas', 'hazelnuts', 'piki bread', ""applebee's"", ""t.g.i. friday's"", 'restaurant', 'cracker barrel', ""denny's"", 'olive garden', ""carrabba's italian grill"", 'on the border', 'creamy dressing', 'imitation cheese', 'milk dessert', 'whipped topping', 'granola bar', 'vegetable oil-butter spread', 'pork sausage rice links', 'papad', 'rice cake', 'gums', 'chewing gum', 'fluid replacement', 'breakfast bars', 'vermicelli', 'luncheon slices', 'vegetarian fillets', 'vegetarian meatloaf or patties', 'beverage', 'bacon bits', 'butter replacement', 'tomato sauce', 'whipped cream substitute', 'eggs', 'dove', 'tomato and vegetable juice', 'cranberry juice', 'yeast extract spread', 'tofu yogurt', 'jellyfish', 'breakfast bar', 'mayonnaise dressing', 'vital wheat gluten', 'frog legs', 'turtle', 'daikon', 'jicama', 'beatgreens', 'bokchoy', 'collard greens', 'microgreens', 'almonds', 'cashews', 'pecans', 'walnuts', 'blueberry', 'strawberry', 'barberries', 'raspberry', 'blackberry', 'boysenberry', 'lingonberries', 'lingonberry', 'elderberry', 'cloudberry', 'dewberry', 'goji berry', 'gooseberry']
quantity_list = ['cup','cups','teaspoon','teaspoons','tablespoon','tablespoons','lb','lbs']
</code></pre>

<p>The following is a text from that dataframe of a single row:</p>

<pre><code> 1 1/2  lbs    ground beef (a mixture of 1 pound ground beef and 1/2 pound ground pork will work well also)
1   large    egg
1/2  cup   grated parmesan cheese
1/3  cup    breadcrumbs (or use enough to hold the meat together (no dry breadcrumbs? just soak 3 slices of bread in the 1/3 )
1 -2   tablespoon   fresh minced garlic (or use 1 teaspoon garlic powder or to taste)
1 -2   teaspoon    salt (or to taste
1   teaspoon    fresh ground black pepper
1/3  cup    milk (can use up to 1/2 cup milk)
1/2  teaspoon    dried oregano (optional
1/4  cup   chopped fresh parsley (or 2 tablespoons dried parsley)
</code></pre>

<p>I obtained the above text using the following code:</p>

<pre><code>    import re
    text_value = df['ingredients']
    for i in text_value[0:1]:
        each_quantity_list = []
        each_unit_list = []
        each_ingradient_list = []
        my_list = i.split(',')
        rx = re.compile(r'\d+(?:/\d+)?')
        filtered = [item for item in my_list if rx.match(item)]
        for j in filtered:
            print (j)
            // Do stuff here
</code></pre>

<p>How can I extract the quantity, unit (cup,lb, etc.) and the name of the ingredient from the text so that I have the following:</p>

<pre><code>each_quantity_list  = ['1 1/2','1','1/2','1/3','1-2','1-2','1','1/3','1/2','1/4']
each_unit_list = ['lbs','cup','cup','cup','tablespoon','teaspoon','teaspoon','cup','teaspoon','cup']
each_ingradient_list = ['beef','egg','parmesan cheese','breadcrumbs','garlic','salt','black pepper','milk','oregano','parsley']
</code></pre>

<p>For clarity the below is the full code I used. But the problem of this code is it is not recognizing the quantity when it is something like 1 1/2, and also at times it is not getting the quantity like cup, cups, etc :</p>

<pre><code>for i in range(len(df)):
try:
    ingredient_list = []
    qunait_list = []
    value = df['ingredients'][i].split(',')
    rx = re.compile(r'\d+(?:/\d+)?')
    filtered = [item for item in value if rx.match(item)]
    for j in filtered:
        #print (j)
        each_ingredient = j.split()
        #print (each_ingredient)\\n
        for k in ingredient_name_list:
            if k in each_ingredient and each_ingredient[1] in quantity_list:
                quantity = each_ingredient[0]
                #print (quantity)
                #print (each_ingredient[1])
                #print (k)
                ingredient_list.append(k)
                qunait_list.append(str(quantity)+' '+str(each_ingredient[1]))
            elif k in each_ingredient and each_ingredient[1] not in quantity_list:
                int_quantity = each_ingredient[0]
                #print (int_quantity)\\n#print (each_ingredient[1])\\n#print (k)\\n
                ingredient_list.append(k)
                qunait_list.append(' '+str(quantity))
                #print (\\'----------------------------------------------------\\')\\n
    id_list.append(df['id'][i])
    #print(\""id\""+str(df[\\'id\\'][i]))\\n
    #print (\\'----------------------------------------------------\\')\\n
    ingre_list.append(ingredient_list)
    quan_list.append(qunait_list)
except:
    pass
</code></pre>

<p>I want to do this for all of them. Any help is really appreciated.</p>
","python, nlp",
Multi-Label emotion classification,"<p>i'm working on an Multi-Label Emotion Classification problem to be solved by word2vec. this is my code that i've learned from a couple of tutorials. now the accuracy is very low. about 0.02 which is telling me something is wrong in my code. but i cannot find it. can anyone find any mistake? i tried this code for TF-IDF and BOW (obviously except word2vec part) and i got much better accuracy scores such as 0.28, but it seems this one is somehow wrong:</p>

<pre><code>#Pre-Processor Function
pre_processor = TextPreProcessor(
    omit=['url', 'email', 'percent', 'money', 'phone', 'user',
        'time', 'url', 'date', 'number'],

    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',
        'time', 'url', 'date', 'number'],

    segmenter=""twitter"", 

    corrector=""twitter"", 

    unpack_hashtags=True,
    unpack_contractions=True,

    tokenizer=SocialTokenizer(lowercase=True).tokenize,

    dicts=[emoticons]
)

#Averaging Words Vectors to Create Sentence Embedding
class AverageEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim =300 # because we use 100 embedding points 

    def fit(self, X, y):
        return self

    def transform(self, X):
        return numpy.array([
            numpy.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [numpy.zeros(self.dim)], axis=0)
            for words in X
        ])

#Loading data
raw_train_tweets = pandas.read_excel('E:\\train.xlsx').iloc[:,1] #Loading all train tweets
train_labels = pandas.read_excel('E:\\train.xlsx').iloc[:,2:13] #Loading corresponding train labels (11 emotions)

raw_test_tweets = pandas.read_excel('E:\\test.xlsx').iloc[:300,1] #Loading 300 test tweets
test_gold_labels = pandas.read_excel('E:\\test.xlsx').iloc[:300,2:13] #Loading corresponding test labels (11 emotions)
print(""please wait"")

#Pre-Processing
train_tweets=[]
test_tweets=[]
for tweets in raw_train_tweets:
    train_tweets.append(pre_processor.pre_process_doc(tweets))
train_tweets=pandas.Series(train_tweets)

for tweets in raw_test_tweets:
    test_tweets.append(pre_processor.pre_process_doc(tweets))
test_tweets=pandas.Series(test_tweets)
all_tweets = train_tweets.append(test_tweets)
all_tweets=(all_tweets.apply(lambda x:str(x).strip().split()))

#Vectorizing 
vector_maker = Word2Vec(all_tweets, size=300, min_count=2, workers=6) #Vectorizer
words_vectors=dict(zip(vector_maker.wv.index2word, vector_maker.wv.vectors))

pipe1=Pipeline([(""wordVectz"",AverageEmbeddingVectorizer(words_vectors)),(""multilabel"",tree.DecisionTreeClassifier())])
pipe1.fit(train_tweets,train_labels)

predicted=pipe1.predict(test_tweets)
print(""Accuracy = "",accuracy_score(test_gold_labels,predicted))
</code></pre>
","python, nlp, classification, word2vec, emotion",
Why does Presidio with spacy nlp engine not recognize organizations and PESEL while spaCy does?,"<p>I'm using spaCy with the pl_core_news_lg model to extract named entities from Polish text. It correctly detects both organizations (ORG) and people's names (PER):</p>
<pre><code>import spacy

nlp = spacy.load(&quot;pl_core_news_lg&quot;)
text = &quot;Jan Kowalski pracuje w IBM i współpracuje z Microsoft oraz Google.&quot;

doc = nlp(text)
entities = [(ent.text, ent.label_) for ent in doc.ents]

print(entities)
</code></pre>
<p>Output:</p>
<pre><code>[('Jan Kowalski', 'persName'), ('IBM', 'orgName'), ('Microsoft', 'orgName'), ('Google', 'orgName')]
</code></pre>
<p>However, when I use Presidio with the pl_core_news_lg model and a configuration file, the recognizers do not correctly detect organizations (ORG) or PESEL numbers, even though they appear in the list of supported entities.</p>
<pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry
from presidio_analyzer.nlp_engine import NlpEngineProvider

provider = NlpEngineProvider(conf_file=&quot;path_to_my_file/nlp_config.yaml&quot;) 
nlp_engine = provider.create_engine()

print(f&quot;Supported recognizers (from NLP engine): {nlp_engine.get_supported_entities()}&quot;)

supported_languages = list(nlp_engine.get_supported_languages())
registry = RecognizerRegistry(supported_languages=[&quot;pl&quot;])
registry.load_predefined_recognizers([&quot;pl&quot;])

print(f&quot;Supported recognizers (from registry): {registry.get_supported_entities(['pl'])}&quot;)

analyzer = AnalyzerEngine(
    registry=registry, supported_languages=supported_languages, nlp_engine=nlp_engine
)

results = analyzer.analyze(text, &quot;pl&quot;)

for entity in results:
    print(f&quot;Found entity: {entity.entity_type} with score {entity.score}&quot;)
</code></pre>
<p>Output:</p>
<pre><code>Supported recognizers (from NLP engine): ['ID', 'NRP', 'DATE_TIME', 'PERSON', 'LOCATION']
Supported recognizers (from registry): ['IN_VOTER', 'URL', 'IBAN_CODE', 'CREDIT_CARD', 'DATE_TIME', 'NRP', 'PHONE_NUMBER', 'MEDICAL_LICENSE', 'PERSON', 'IP_ADDRESS', 'ORGANIZATION', 'CRYPTO', 'LOCATION', 'PL_PESEL', 'EMAIL_ADDRESS']
</code></pre>
<p>Even though 'ORGANIZATION' and 'PL_PESEL' are listed (org should be listed in from NLP engine) as supported recognizers, Presidio does not detect them correctly in the text.</p>
<p>My config file:</p>
<pre><code>nlp_engine_name: spacy
models:
  - lang_code: pl
    model_name: pl_core_news_lg

ner_model_configuration:
  model_to_presidio_entity_mapping:
    persName: PERSON
    orgName: ORGANIZATION
#    orgName: ORG
    placeName: LOCATION
    geogName: LOCATION
    LOC: LOCATION
    GPE: LOCATION
    FAC: LOCATION
    DATE: DATE_TIME
    TIME: DATE_TIME
    NORP: NRP
    ID: ID
</code></pre>
<p>Why does Presidio fail to detect organizations (ORG) and PESEL numbers (PL_PESEL), while spaCy correctly detects them?</p>
","python, nlp, spacy, presidio","<p>The configuration file is missing the 'labels_to_ignore' field, stating that no entities should be ignored in the nlp engine :</p>
<pre><code>  labels_to_ignore:
    - O
</code></pre>
<p>On your configuration it would look like this:</p>
<pre><code>nlp_engine_name: spacy
models:
  - lang_code: pl
    model_name: pl_core_news_lg

ner_model_configuration:
  labels_to_ignore:
    - O
  model_to_presidio_entity_mapping:
    persName: PERSON
    orgName: ORGANIZATION
#    orgName: ORG
    placeName: LOCATION
    geogName: LOCATION
    LOC: LOCATION
    GPE: LOCATION
    FAC: LOCATION
    DATE: DATE_TIME
    TIME: DATE_TIME
    NORP: NRP
    ID: ID
</code></pre>
<p><strong>Edit:</strong> This was fixed to be the default if 'labels_to_ignore' is not specified</p>
<p>Will be part of version 2.2.359 release</p>
"
Sentencepiece not generating models after preprocessing (SOLVED),"<p>So this is the log that I see on the terminal:</p>
<pre><code>sentencepiece_trainer.cc(78) LOG(INFO) Starts training with :  
trainer_spec {  
  input: C:\Users\xxxx\OneDrive\Documents\Projects\py\xxxxx\data\tokenizer\final_text_corpus.txt  
  input_format:  
  model_prefix: C:\Users\xxxx\OneDrive\Documents\Projects\py\xxxxxxxx\tokenizer\multilingual_unigram  
  model_type: UNIGRAM  
  vocab_size: 50000  
  self_test_sample_size: 0  
  character_coverage: 1  
  input_sentence_size: 10000000  
  shuffle_input_sentence: 1  
  seed_sentencepiece_size: 1000000  
  shrinking_factor: 0.75  
  max_sentence_length: 16384  
  num_threads: 16  
  num_sub_iterations: 2  
  max_sentencepiece_length: 16  
  split_by_unicode_script: 1  
  split_by_number: 1  
  split_by_whitespace: 1  
  split_digits: 0  
  pretokenization_delimiter:  
  treat_whitespace_as_suffix: 0  
  allow_whitespace_only_pieces: 0  
  user_defined_symbols: &lt;newline&gt;  
  required_chars:  
  byte_fallback: 0  
  vocabulary_output_piece_score: 1  
  train_extremely_large_corpus: 1  
  seed_sentencepieces_file:  
  hard_vocab_limit: 1  
  use_all_vocab: 0  
  unk_id: 1  
  bos_id: 2  
  eos_id: 3  
  pad_id: 0  
  unk_piece: &lt;unk&gt;  
  bos_piece: &lt;s&gt;  
  eos_piece: &lt;/s&gt;  
  pad_piece: &lt;pad&gt;  
  unk_surface:  Γüç  
  enable_differential_privacy: 0  
  differential_privacy_noise_level: 0  
  differential_privacy_clipping_threshold: 0  
}  
normalizer_spec {  
  name: nmt_nfkc  
  add_dummy_prefix: 1  
  remove_extra_whitespaces: 1  
  escape_whitespaces: 1  
  normalization_rule_tsv:  
}  
denormalizer_spec {}  
trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.  
trainer_interface.cc(185) LOG(INFO) Loading corpus: C:\Users\xxxxxxx\OneDrive\Documents\Projects\py\xxxxxxx\data\tokenizer\final_text_corpus.txt  
trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines  
trainer_interface.cc(147) LOG(INFO) Loaded 2000000 lines  
trainer_interface.cc(147) LOG(INFO) Loaded 3000000 lines  
trainer_interface.cc(147) LOG(INFO) Loaded 4000000 lines  
trainer_interface.cc(147) LOG(INFO) Loaded 5000000 lines  
trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (5816781), which may slow down training.  
trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=&lt;size&gt; and --shuffle_input_sentence=true.  
trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample &lt;size&gt; sentences from the entire corpus.  
trainer_interface.cc(409) LOG(INFO) Loaded all 5816781 sentences  
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;pad&gt;  
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;unk&gt;  
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;s&gt;  
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;/s&gt;  
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;newline&gt;  
trainer_interface.cc(430) LOG(INFO) Normalizing sentences...  
trainer_interface.cc(539) LOG(INFO) all chars count=731130164  
trainer_interface.cc(550) LOG(INFO) Done: 100% characters are covered.  
trainer_interface.cc(560) LOG(INFO) Alphabet size=1280  
trainer_interface.cc(561) LOG(INFO) Final character coverage=1  
trainer_interface.cc(592) LOG(INFO) Done! preprocessed 5816741 sentences.
</code></pre>
<p>After this the terminal closes.</p>
<p>This is the part of the code that trains:</p>
<pre><code>import sys
import os
from pathlib import Path
import sentencepiece as spm

# === Paths ===
root_dir = Path(__file__).resolve().parent.parent  
input_path = root_dir / &quot;data&quot; / &quot;tokenizer&quot; / &quot;final_text_corpus.txt&quot;
output_dir = root_dir / &quot;tokenizer&quot;
output_dir.mkdir(parents=True, exist_ok=True)
model_prefix = &quot;spm_tokenizer&quot;
log_path = output_dir / &quot;training.log&quot;

# === Logging setup ===
with open(log_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as log_file:
    sys.stdout = log_file
    sys.stderr = log_file

    print(&quot;Starting tokenizer training...&quot;)
    print(f&quot;Input corpus: {input_path}&quot;)
    print(f&quot;Output prefix: {model_prefix}&quot;)
    print(f&quot;Vocab size: {50000}&quot;)

    # === Train Tokenizer ===
    spm.SentencePieceTrainer.train(
        input=str(input_path),
        model_prefix=model_prefix,
        vocab_size=50000,
        model_type=&quot;unigram&quot;,
        character_coverage=1.0, 
        pad_id=0,
        unk_id=1,
        bos_id=2,
        eos_id=3,
        user_defined_symbols=[&quot;&lt;newline&gt;&quot;],
        train_extremely_large_corpus=True,
        input_sentence_size=10_000_000,
        shuffle_input_sentence=True,
        max_sentence_length=16384
    )

    print(f&quot;Tokenizer trained! Model saved to: {model_prefix}.model / .vocab&quot;)

</code></pre>
<p>I have added the <code>train_extremely_large_corpus</code> flag to True and the <code>input_sentence_size</code> and <code>max_sentence_length</code> flags in case it's because of memory bottlenecks, but still no help.I tried finding this issue all over the internet, still no luck.
Can anyone explain what's causing the problem?</p>
","python, nlp, sentencepiece",
How to understand hidden_states of the returns in BertModel?,"<blockquote>
<p>Returns last_hidden_state (torch.FloatTensor of shape (batch_size,
sequence_length, hidden_size)): Sequence of hidden-states at the
output of the last layer of the model.</p>
<p>pooler_output (torch.FloatTensor: of shape (batch_size, hidden_size)):
Last layer hidden-state of the first token of the sequence
(classification token) further processed by a Linear layer and a Tanh
activation function. The Linear layer weights are trained from the
next sentence prediction (classification) objective during
pre-training.</p>
<p>This output is usually not a good summary of the semantic content of
the input, you’re often better with averaging or pooling the sequence
of hidden-states for the whole input sequence.</p>
<p>hidden_states (tuple(torch.FloatTensor), optional, returned when
config.output_hidden_states=True): Tuple of torch.FloatTensor (one for
the output of the embeddings + one for the output of each layer) of
shape (batch_size, sequence_length, hidden_size).</p>
<p>Hidden-states of the model at the output of each layer plus the
initial embedding outputs.</p>
<p>attentions (tuple(torch.FloatTensor), optional, returned when
config.output_attentions=True): Tuple of torch.FloatTensor (one for
each layer) of shape (batch_size, num_heads, sequence_length,
sequence_length).</p>
<p>Attentions weights after the attention softmax, used to compute the
weighted average in the self-attention heads.</p>
</blockquote>
<p>This is from <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertmodel"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bert.html#bertmodel</a>. Although the description in the document is clear, I still don't understand the <strong>hidden_states</strong> of returns. There is a tuple, one for the output of the embeddings, and the other for the output of each layer.</p>
<p>Please tell me how to distinguish them, or what is the meaning of them.</p>
","nlp, pytorch, huggingface-transformers, bert-language-model, electrate",
No attention output in jinaai/jina-embeddings-v3 embedding model,"<p>When I use this model like so -</p>
<pre><code>from transformers import AutoModel, AutoTokenizer

model_id = &quot;jinaai/jina-embeddings-v3&quot;
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModel.from_pretrained(model_id, trust_remote_code=True)

inputs = tokenizer([
    &quot;The weather is lovely today.&quot;,
    &quot;It's so sunny outside!&quot;,
    &quot;He drove to the stadium.&quot;
], return_tensors=&quot;pt&quot;, padding=True, truncation=True)

outputs = model(**inputs, output_attentions=True)

attentions = outputs.attentions
</code></pre>
<p>I get this warning which seems contradictory -</p>
<pre><code>flash_attn is not installed. Using PyTorch native attention implementation.
Flash attention implementation does not support kwargs: output_attentions
</code></pre>
<p>attentions is None</p>
<p>I tried it with other models and it works as expected.</p>
","machine-learning, nlp, artificial-intelligence, sentence-transformers",
How can I use label studio to annotate text data in google colab?,"<p>How can I use label studio to annotate text data in google colab? And please also suggest good alternatives to label studio which I can easily used in google colab to annotate/label text data?</p>
<p>I tried installing label studio in google colab but unable to launch the server.</p>
<p><a href=""https://colab.research.google.com/drive/1Ac6B9qSGsBpQC1IYdcLZ96ffEy4MJ8hC?usp=sharing"" rel=""nofollow noreferrer"">Here</a> is my colab notebook.</p>
","nlp, label-studio",
Zero Accuracy in Sentiment Analysis FFNN Model (Pytorch),"<p>I'm constructing a feedforward neural net (FFNN) for binary sentiment classification using movie review dataset. It's a simplified task with only two class labels, i.e., positive (1) and negative (0). It FFNN makes word embeddings, averages them, runs them through two layers and activation later, and then applies a sigmoid too get a probability for prediction.</p>
<p>The model runs, but I keep getting zeroes. I am in desperate need of help.
<a href=""https://colab.research.google.com/drive/1nbK_G7snlw0Jxtxzv5ro_qPOHBvDnsfV?usp=sharing"" rel=""nofollow noreferrer"">Here's the link to the colab</a></p>
<p>I tried the following:</p>
<ul>
<li>Changing the loss function</li>
<li>Rearranging the model order</li>
<li>Changing the embedding dimensions</li>
</ul>
<h1>Below is the following:</h1>
<ol>
<li>The model</li>
<li>The assignment of the FFNN</li>
<li>The training</li>
</ol>
<h2>Here's the code:</h2>
<h3><em><strong>THE MODEL</strong></em></h3>
<pre class=""lang-py prettyprint-override""><code>from collections import OrderedDict

import torch
from typing import List

import torch.nn as nn

class FeedForwardNeuralNetClassifier(nn.Module):
    &quot;&quot;&quot;
    The Feed-Forward Neural Net sentiment classifier.
    &quot;&quot;&quot;
    def __init__(self, vocab_size, emb_dim, n_hidden_units):
        &quot;&quot;&quot;
        In the __init__ function, you will define modules in FFNN.
        :param vocab_size: size of vocabulary
        :param emb_dim: dimension of the embedding vectors
        :param n_hidden_units: dimension of the hidden units
        &quot;&quot;&quot;
        super(FeedForwardNeuralNetClassifier, self).__init__()
        self.vocab_size = vocab_size
        self.emb_dim = emb_dim
        self.n_hidden_units = n_hidden_units
       
        # TODO: implement a randomly initialized word embedding matrix using nn.Embedding
        # It should have a size of (vocab_size x emb_dim)
        self.word_embeddings = nn.Embedding(self.vocab_size, self.emb_dim) # replace me

        # Define the rest of the model
        self.layer1 = nn.Linear(self.emb_dim, self.n_hidden_units)
        self.layer2 = nn.Linear(self.n_hidden_units, 1)
        self.activation = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, batch_inputs: torch.Tensor, batch_lengths: torch.Tensor) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        The forward function, which defines how FFNN should work when given a batch of inputs and their actual sent lengths (i.e., before PAD)
        :param batch_inputs: a torch.Tensor object of size (n_examples, max_sent_length_in_this_batch), which is the *indexed* inputs
        :param batch_lengths: a torch.Tensor object of size (n_examples), which describes the actual sentence length of each example (i.e., before PAD)
        :return the logits of FFNN (i.e., the unnormalized hidden units before sigmoid) of shape (n_examples)
        &quot;&quot;&quot;

        # Lookup word embeddings
        embeddings = self.word_embeddings(batch_inputs)

        # Element-wise averaging layer
        averaged = torch.mean(embeddings, dim=1)

        # Hidden layer ReLU
        hidden = self.activation(self.layer1(averaged))

        # Output layer
        return self.layer2(hidden)

        

    def batch_predict(self, batch_inputs: torch.Tensor, batch_lengths: torch.Tensor) -&gt; List[int]:
        &quot;&quot;&quot;
        Make predictions for a batch of inputs. This function may directly invoke forward (which passes the input through FFNN and returns the output logits)

        :param batch_inputs: a torch.Tensor object of size (n_examples, max_sent_length_in_this_batch), which is the *indexed* inputs
        :param batch_lengths: a torch.Tensor object of size (n_examples), which describes the actual sentence length of each example (i.e., before PAD)
        :return: a list of predicted classes for this batch of data, either 0 for negative class or 1 for positive class
        &quot;&quot;&quot;
        output = self.forward(batch_inputs, batch_lengths)
         
        # Sigmoid
        probability = self.sigmoid(output)

        # Convert logits to predicted labels
        predicted_labels = (probability &gt; 0.5).int().tolist()

        return predicted_labels
</code></pre>
<h3><strong>THE ASSIGNMENT</strong></h3>
<pre class=""lang-py prettyprint-override""><code>
model = FeedForwardNeuralNetClassifier(vocab_size=len(vocab), emb_dim=300, n_hidden_units=300)

# FeedForwardNeuralNetClassifier(
# (word_embeddings): Embedding(4818, 300)
# (layer1): Linear(in_features=300, out_features=300, bias=True)
# (layer2): Linear(in_features=300, out_features=1, bias=True)
#  (activation): ReLU()
#  (sigmoid): Sigmoid()
# )



device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
print(device)
model = model.to(device)



optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # replace me
</code></pre>
<h3><strong>THE TRAINING</strong></h3>
<pre class=""lang-py prettyprint-override""><code>
import time

BATCH_SIZE=32
N_EPOCHS=20

# create a batch iterator for the training data
batch_iterator = SentimentExampleBatchIterator(
    train_exs, batch_size=BATCH_SIZE, PAD_idx=PAD_IDX, shuffle=True)

#loss function
loss_func = nn.BCEWithLogitsLoss()

# training
best_epoch = -1
best_acc = -1
start_time = time.time()
for epoch in range(N_EPOCHS):
    print(&quot;Epoch %i&quot; % epoch)

    batch_iterator.refresh() # initiate a new iterator for this epoch

    model.train() # turn on the &quot;training mode&quot;
    batch_loss = 0.0
    batch_example_count = 0
    batch_data = batch_iterator.get_next_batch()
    while batch_data is not None:
        batch_inputs, batch_lengths, batch_labels = batch_data
        # project to the device
        batch_inputs = batch_inputs.to(device)
        batch_lengths = batch_lengths.to(device)
        batch_labels = batch_labels.unsqueeze(1)
        batch_labels = batch_labels.to(device)
        

        
        # TODO: clean up the gradients for this batch
        optimizer.zero_grad()
        
        # TODO: call the model and get the loss
        outputs = model(batch_inputs, batch_lengths).float()
        
        batch_labels = batch_labels.float()
        loss = loss_func(outputs, batch_labels)
        # record the loss and number of examples, so we could report some stats
        batch_example_count += len(batch_labels)
        batch_loss += loss.item() * len(batch_labels)

        # TODO: backpropagation using loss
        # backpropagation using loss
        loss.backward()

        # update the model parameters
        optimizer.step()

        # get another batch
        batch_data = batch_iterator.get_next_batch()

    print(&quot;Avg loss: %.5f&quot; % (batch_loss / batch_example_count))

    # evaluate on dev set
    model.eval() # turn on the &quot;evaluation mode&quot;
    acc, _, _, _ = evaluate(model, dev_exs, return_metrics=True)
    if acc &gt; best_acc:
        best_acc = acc
        best_epoch = epoch
        print(&quot;Secure a new best accuracy %.3f in epoch %d!&quot; % (best_acc, best_epoch))
        
        # Save the current best model parameters
        print(&quot;Save the best model checkpoint as best_model.ckpt!&quot;)
        torch.save(model.state_dict(), &quot;best_model.ckpt&quot;)
    
    print(&quot;Time elapsed: %s&quot; % time.strftime(&quot;%Hh%Mm%Ss&quot;, time.gmtime(time.time()-start_time)))
    print(&quot;-&quot; * 10)

print(&quot;End of training! The best accuracy %.3f was obtained in epoch %d.&quot; % (best_acc, best_epoch))

# Load back the best checkpoint on dev set
model.load_state_dict(torch.load(&quot;best_model.ckpt&quot;))


</code></pre>
","python, deep-learning, pytorch, nlp, sentiment-analysis",
Spacy rules matching entities before text,"<p>I'm trying to write a spacy parser to extract the names and terms of a contract.
To do that, I've written a rule to extract the sellers and buyers, except it's extracting multiple times over a simple sentence.</p>
<p>Here's my rule</p>
<pre><code>[{&quot;label&quot;: &quot;seller&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;:  &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;seller&quot;}]},
{&quot;label&quot;: &quot;buyer&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;: &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;buyer&quot;}]},]
</code></pre>
<p>Which results in spans like this:</p>
<pre><code>span seller Text: john e. smith and wife judy c. smith, seller
span seller Text: e. smith and wife judy c. smith, seller
span seller Text: smith and wife judy c. smith, seller
span seller Text: judy c. smith, seller
span seller Text: c. smith, seller
span seller Text: smith, seller
</code></pre>
<p>It seems that spacy is chunking the person entities. How can I produce a rule that matches multiple sellers (or buyers), but doesn't cut them up like this example?</p>
<p>My code is below.</p>
<pre><code>#!/usr/bin/env python3

import spacy
from spacy.tokens import SpanGroup, DocBin, Span
from spacy import displacy
import bodytext
import sys
rules  = [{&quot;label&quot;: &quot;seller&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;: &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;seller&quot;}]},
        {&quot;label&quot;: &quot;buyer&quot;, &quot;pattern&quot;: [{&quot;ENT_TYPE&quot;: &quot;PERSON&quot;, &quot;OP&quot;: &quot;{1,2}&quot;}, { &quot;OP&quot;: &quot;*&quot;}, {&quot;TEXT&quot;: &quot;buyer&quot;}]},]
nlp = spacy.load(&quot;en_core_web_lg&quot;)
ruler = nlp.add_pipe(&quot;span_ruler&quot;)
ruler.add_patterns(rules)

text = &quot;THIS AGREEMENT made on this 12 day of December, 2008, between John E. Smith and wife Judy C. Smith, Seller (whether one or more), whose address is: 1234 CRD 5000, midland, Texas, 79221-2016, and real estate investors, LLC, Buyer, whose address is: 4321 Harvard Ave, Midland, Texas 79701.  &quot;
doc = nlp(text.lower())

doc.spans[&quot;test&quot;] = SpanGroup(doc)
db = DocBin()

for sentence in doc.sents:
    for span in doc.spans[&quot;ruler&quot;]:
        print(&quot;span &quot;+ span.label_+&quot; Text: &quot;+span.text)
        if span.start &gt;= sentence.start and span.end &lt;= sentence.end:
            doc.spans[&quot;test&quot;] += [
                Span(doc, start=sentence.start, end=sentence.end, label=span.label_)
            ]
            doc.set_ents(entities=[span], default=&quot;unmodified&quot;)
</code></pre>
","python, nlp, spacy",
Store images instead of showing in a server,"<p>I am running the code found on this <a href=""https://captum.ai/tutorials/Llama2_LLM_Attribution"" rel=""nofollow noreferrer"">site</a> in my server and I would like to store images instead of showing them since I have connected remotely with an ssh connection to my <code>server</code> via an <code>SSH</code> connection.</p>
<p>The code is for instance this one:</p>
<pre><code>skip_tokens = [1]  # skip the special token for the start of the text &lt;s&gt;
inp = TextTokenInput(
  eval_prompt, 
  tokenizer,
  skip_tokens=skip_tokens,
)

target = &quot;playing guitar, hiking, and spending time with his family.&quot;
attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)
attr_res.plot_token_attr(show=True)
</code></pre>
<p>How to store the files locally instead of showing them?</p>
","python, nlp, large-language-model","<p>I can't test it but ...</p>
<p>I checked <a href=""https://github.com/pytorch/captum/blob/4ca5c2c11b199f84544bdb09a0081443fc71f109/captum/attr/_core/llm_attr.py#L70"" rel=""nofollow noreferrer"">source code</a> and it uses <code>matplotlib</code> for this.</p>
<p>If you remove <code>show=True</code> then it shouldn't show it but it should only get <code>fig, ax</code>.</p>
<p>I think you could use <a href=""https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html"" rel=""nofollow noreferrer"">matplotlib.pyplot.savefig(filename)</a> to save it in file.</p>
<pre><code>import matplotlib.pyplot as plt

# ... code  ...

attr_res.plot_token_attr()  # without `show=True
plt.savefig(&quot;output.png&quot;)
#plt.show()  # eventually show it after saving
</code></pre>
<hr />
<p>Probably you can also use <code>fig</code> for this</p>
<pre><code>fig, ax = attr_res.plot_token_attr()  # without `show=True
fig.savefig(&quot;output.png&quot;)
</code></pre>
"
"GPT-2 and other models from huggingface -100 label index for training, instead of pad token","<p>I understand the -100 label id is used so that the predictions for these are not included when calculating the loss.</p>
<p>However on <a href=""https://huggingface.co/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16#bert2gpt2-summarization-with-%F0%9F%A4%97-encoderdecoder-framework"" rel=""nofollow noreferrer"">huggingface</a>, they state
&quot;complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not&quot;, when replacing pad tokens. In their implementation, they use nn.CrossEntropyLoss(), which has an argument &quot;ignore_index&quot;.</p>
<p>Is there any benefit to changing the id to -100 as opposed to adding the argument ignore_index in the loss and setting it as the pad token id? Or are the results the same?</p>
<p>The way it is written makes me think there is some benefit, but the description of &quot;ignore_index&quot; appears to achieve what is wanted.</p>
","nlp, huggingface-transformers, pre-trained-model","<p>The author of the tutorial you mentioned sets it to <code>-100</code> <strong>and</strong> uses <code>ignore_index</code> to save a few lines of code. You don't see the line where the author pass something to <code>ignore_index</code> because it has a default value. The default value of <code>ignore_index</code> for <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" rel=""nofollow noreferrer"">nn.CrossEntropyLoss</a> is <code>-100</code>. Using this value instead of the respective pad token id allows you to write some model indepent training code and you don't have to pass the pad token id from tokenizer down to the loss function.</p>
"
SFTTrainer Error : prepare_model_for_kbit_training() got an unexpected keyword argument &#39;gradient_checkpointing_kwargs&#39;,"<p>I'm trying to fine-tune a model using SFTTrainer from trl.</p>
<p>This is how my SFTConfig arguments look like,</p>
<pre><code>from trl import SFTConfig
training_arguments = SFTConfig(
       output_dir=output_dir,
       num_train_epochs=num_train_epochs,
       per_device_train_batch_size=per_device_train_batch_size,
       gradient_accumulation_steps=gradient_accumulation_steps,
       optim=optim,
       save_steps=save_steps,
       logging_steps=logging_steps,
       learning_rate=learning_rate,
       weight_decay=weight_decay,
       fp16=fp16,
       bf16=bf16,
       max_grad_norm=max_grad_norm,
       max_steps=max_steps,
       warmup_ratio=warmup_ratio,
       group_by_length=group_by_length,
       lr_scheduler_type=lr_scheduler_type,
       report_to=&quot;tensorboard&quot;,
       dataset_text_field=&quot;instruction&quot;,
       max_seq_length=None,
       packing=False,
       gradient_checkpointing=False,
   )

</code></pre>
<p>and this is my SFTTrainer block.</p>
<pre><code>trainer = SFTTrainer(
   model=model,
   train_dataset=dataset,
   peft_config=peft_config,
   tokenizer=tokenizer,
   args=training_arguments,
)
</code></pre>
<p>The error comes from internal function <code>SFTTrainer._prepare_model_for_kbit_training</code>.</p>
<pre><code> &quot;&quot;&quot;Prepares a quantized model for kbit training.&quot;&quot;&quot;
    330 prepare_model_kwargs = {
    331     &quot;use_gradient_checkpointing&quot;: args.gradient_checkpointing,
    332     &quot;gradient_checkpointing_kwargs&quot;: args.gradient_checkpointing_kwargs or {},
    333 }

</code></pre>
<p>I tried passing <code>gradient_checkpointing</code> as False and <code>gradient_checkpointing_kwargs</code> as an empty dictionary, but no luck.</p>
<p>How can I avoid this error?</p>
","python, nlp, large-language-model, transformer-model",
how to load deberta-v3 properly,"<p>I am trying to fine tune a DeBERTa model for a regression task,
the problem is that when I load the model using this code</p>
<pre><code>from transformers import AutoConfig, AutoTokenizer, AutoModel
## Model Configurations
MODEL_NAME = 'microsoft/deberta-v3-base'

config = AutoConfig.from_pretrained(MODEL_NAME) ## Configuration loaded from AutoConfig
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) ## Tokenizer loaded from AutoTokenizer
</code></pre>
<p>I get a KeyError</p>
<pre><code>Traceback (most recent call last)
&lt;ipython-input-23-62561c3f4e7b&gt; in &lt;module&gt;
      3 MODEL_NAME = 'microsoft/deberta-v3-base'
      4 
----&gt; 5 config = AutoConfig.from_pretrained(MODEL_NAME) ## Configuration loaded from AutoConfig
      6 tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) ## Tokenizer loaded from AutoTokenizer

/usr/lib/python3.8/dist-packages/transformers/models/auto/configuration_auto.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    350 
    351         if &quot;model_type&quot; in config_dict:
--&gt; 352             config_class = CONFIG_MAPPING[config_dict[&quot;model_type&quot;]]
    353             return config_class.from_dict(config_dict, **kwargs)
    354         else:

KeyError: 'deberta-v2'
</code></pre>
<p>what can be the problem? I am using transformers version 4.31.0</p>
","python, nlp, huggingface-transformers","<p>It seems V3 config structure is the same with V2 config?</p>
<pre><code>DebertaV2Config {
  &quot;_name_or_path&quot;: &quot;microsoft/deberta-v3-base&quot;,
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
...
</code></pre>
<p>I successfully ran <a href=""https://colab.research.google.com/drive/1GABUCj34h3OOjsC8vZ7ScOsYeYuMr7qR#scrollTo=HtXvH_u0mhgH"" rel=""nofollow noreferrer"">the following code</a>.</p>
<pre><code>from transformers import AutoModel, AutoTokenizer, AutoConfig
MODEL_NAME = 'microsoft/deberta-v3-base'
model = AutoModel.from_pretrained(MODEL_NAME)
config = AutoConfig.from_pretrained(MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
</code></pre>
<p>Output:</p>
<pre><code>Downloading (…)lve/main/config.json: 100%
579/579 [00:00&lt;00:00, 8.75kB/s]
Downloading pytorch_model.bin: 100%
371M/371M [00:02&lt;00:00, 152MB/s]
Downloading (…)okenizer_config.json: 100%
52.0/52.0 [00:00&lt;00:00, 1.43kB/s]
Downloading spm.model: 100%
2.46M/2.46M [00:00&lt;00:00, 42.9MB/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</code></pre>
<p>In Google Colab (or any Linux OS), the model files would be stored here.<br />
<code>/root/.cache/huggingface/hub/models--microsoft--deberta-v3-base</code></p>
<br>
<p>It also can be downloaded via V2 modules without error message.<br />
In this case, <code>sentencepiece</code> needs to be imported.<br />
Not sure whether the model will be usable.</p>
<pre><code>import sentencepiece
from transformers import DebertaV2Model, DebertaV2Config, DebertaV2Tokenizer
MODEL_NAME = 'microsoft/deberta-v3-base'
model = DebertaV2Model.from_pretrained(MODEL_NAME)
config = DebertaV2Config.from_pretrained(MODEL_NAME)
tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)
</code></pre>
"
dl4j does not contain text and models modules,"<p>In my project, I need to use the Word2Vec model. I decide to use dl4j for this. I load all dependencies, but there are no modules such as deeplearning4j.text and deeplearning4j.models </p>

<p>I tried to change the version of dl4j from beta5 to 4 and 3 but the problem still here
I even tried download jar files of dl4j but it didn't help too</p>
","java, neural-network, nlp, deeplearning4j","<p>It's deeplearning4j.nlp module and corresponding submodules like deeplearning4j.nlp.uima for example</p>
"
"Neither PyTorch nor TensorFlow &gt;= 2.0 have been found.Models won&#39;t be available and only tokenizers, configuration and file/data utilities can be used","<p>I am trying to install transformers using pip</p>
<pre><code>pip install transformers
</code></pre>
<p>after import transformers</p>
<p>this error show</p>
<pre><code>Neither PyTorch nor TensorFlow &gt;= 2.0 have been found.Models won't be available and only tokenizers, configuration, and file/data utilities can be used.
</code></pre>
<p>although I install TensorFlow-GPU= 2.3.1 and using conda</p>
<p>system info</p>
<pre><code>Windows 10 
python 3.6
cuda 10.1
tensorflow-gpu= 2.3.1
</code></pre>
","python, tensorflow, nlp","<p>I found the problem after investigate for 10 hours</p>
<p>I installed tensorflow  by using <code>conda install tensorflow-gpu </code></p>
<p>and transformers by using pip after remove tensorflow-gpu and install it by using pip</p>
<p>it works fine</p>
"
How to detect homophone,"<p>I am wondering how <a href=""http://en.wikipedia.org/wiki/Homonym"" rel=""nofollow noreferrer"">homophones</a> are detected. I am in search for an API which gives similarity between two words on the basis of how they are pronounced.</p>
<p><strong>for example:</strong> &quot;to&quot; and &quot;two&quot; are highly similar in terms of how they sound with respect to say &quot;to&quot; and &quot;from&quot;.</p>
","nlp, speech-recognition, voice-recognition",
simpler gmail Filter syntax for &quot;word family&quot; [verif +(y/ied/ification] + similar loanwords [term +(s/es/a)]?,"<p>Is there simpler filter that I can use for below cases?
Google has a very smart AI gemini, I hope there is a shortcut for this as I am receiving bilingual emails and loan words in Malay/Indonesia are quite similar to English.</p>
<p>a. variations of the same word?</p>
<pre><code>subject:{+updates +updated +updating +update}
subject:{+verify +verification +verified}
</code></pre>
<p>b. similar transliteration of words</p>
<pre><code>subject:{+terms +termes +terma}
subject:{+invois +invoice}
subject:{+privacy +privasi +privacidad}
</code></pre>
<p>c. words with different character(s)/prefix/suffix</p>
<pre><code>subject:{+e-statement +estatement +statement}
subject:{+&quot;log in&quot; +login +&quot;log-ins&quot; &quot;logged in&quot;}
</code></pre>
","filter, nlp, gmail, lemmatization, transliteration",
ChatGPT Token Limit,"<p>I want ChatGPT to remember past conversations and have a consistent (stateful) conversation.</p>
<p>I have seen several code of ChatGPT prompt engineering.</p>
<p>There were two ways to design the prompt shown below (pseudo code):</p>
<ol>
<li><p><strong>Use a single input</strong> (Cheap) &lt;- Better if possible</p>
</li>
<li><p><strong>Stack all of previous history</strong> (Expensive, Token Limitation)</p>
</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>def openai_chat(prompt):
    completions = openai.Completion.create(
        engine = &quot;text-davinci-003&quot;,
        prompt = prompt,
        max_tokens = 1024,
        n = 1,
        temperature = 0.8,
    )
    response = completions.choices[0].text.strip()
    return response

# 1. Use a single input
while True:
    prompt = input(&quot;User: &quot;)
    completion = openai_chat(prompt)

# 2. Stack all of previous history (prompt + completion)
prompt = &quot;&quot;
while True:
    cur_prompt = input(&quot;User: &quot;)
    prompt += cur_prompt  # pseudo code
    completion = openai_chat(prompt)
    prompt += completion  # pseudo code
</code></pre>
<p>Is it possible to choose the first way (the cheap one) to have a consistent conversation?</p>
<p>In other words, does ChatGPT remember past history even if the prompt only has the current input?</p>
","text, nlp, prompt, openai-api, gpt-3",
How to replace an Embedding layer with a Time Distributed Dense after training?,"<p>I have the following problem:</p>

<ol>
<li><p>I want to use a LSTM network for text classification. In order to speed up training and make code more clear I want to use an <code>Embedding</code> layer along <code>keras.Tokenizer</code> in order to train my model.</p></li>
<li><p>Once I trained my model - I want to compute a saliency map of output w.r.t. the input. To do that I decided to replace an <code>Embedding</code> layer with a <code>TimeDistributedDense</code>.</p></li>
</ol>

<p>Do you have any idea what is the best way to do that. For a simple model I'm able to simply rebuild model with a known weights - but I want to make it as generic as possible - e.g. to replace the model structure the future and make my framework as model agnostic as possible.</p>
","machine-learning, nlp, neural-network, embedding, keras",
Designing a System that would detect typos and suggestions,"<p>This was asked in an interview.</p>
<p>I think the answer can be done by constructing a trie of all valid words and then suggestions can be made based on a possible valid path which was otherwise given as incorrect.</p>
<p>Say if user types apfle, and system would detect that after ap a possible valid path was app, which would then satisfy apple.</p>
<p>Is there any better solution than this? Perhaps the one implemented by spell checkers?</p>
","nlp, spell-checking","<p>See:</p>

<p><a href=""https://stackoverflow.com/questions/307291/how-does-the-google-did-you-mean-algorithm-work"">How does the Google &quot;Did you mean?&quot; Algorithm work?</a></p>

<p><a href=""https://stackoverflow.com/questions/5265416/how-do-i-approximate-did-you-mean-without-using-google"">How do I approximate &quot;Did you mean?&quot; without using Google?</a></p>

<p><a href=""http://norvig.com/spell-correct.html"" rel=""nofollow noreferrer"">How to write a spelling corrector</a></p>

<p><a href=""http://www.youtube.com/watch?v=syKY8CrHkck#t=22m03s"" rel=""nofollow noreferrer"">Youtube Video: Search 101</a></p>
"
How to implement a &quot;related&quot; degree measure algorithm?,"<p>I was going to Ask a Question earlier today when I was presented to a surprising functionality in Stackoverflow. When I wrote my question title stackoverflow suggested me several related questions and I found out that there was already two similar questions. That was stunning! </p>

<p>Then I started thinking how I would implement such function. How I would order questions by relatedness:</p>

<ol>
<li>Question that have higher number of
words matchs with the new question</li>
<li>If the number of matchs are the
same, the order of words is considered</li>
<li>Words that appears in the title has
higher relevancy</li>
</ol>

<p>That would be a simple workflow or a complex score algortithm?
Some stemming to increase the recall, maybe?
Is there some library the implements this function?
What other aspects would you consider?
Maybe Jeff could answer himself! How did you implemented this in Stackoverflow? :)</p>
","algorithm, machine-learning, indexing, nlp, full-text-search","<p>One such way to implement such an algorithm would involve ranking the questions as per a heuristic function which assigns a 'relevance' weight factor using the following steps:</p>

<ol>
<li>Apply a noise filter to the 'New' question to remove words that are common across a large number of objects such as: 'the', 'and', 'or', etc.</li>
<li>Get the number of words contained in the 'New' question which match the words the set of questions already posted on the website. [A]</li>
<li>Get the number of tag matches between the words in the 'New' question and the available. [B]</li>
<li>Compute the 'relevance weight' based on [A] and [B] as 'x[A] + y[B]', where x and y are weight multipliers (Assign a higher weight multiplier to [B] as tagging is more relevant than simple word search)</li>
<li>Get the top 5 questions which have the highest 'relevance weight'.</li>
</ol>

<p>The heuristic might require tweaking to get optimal results, but it should work.</p>
"
label_smoothing in &quot;The Annotated Transformer&quot;,"<p>In <a href=""http://nlp.seas.harvard.edu/2018/04/03/attention.html"" rel=""nofollow noreferrer"">&quot;The Annotated Transformer&quot;</a>, label smoothing is implemented as the following:</p>
<pre><code>class LabelSmoothing(nn.Module):
    &quot;Implement label smoothing.&quot;
    def __init__(self, size, padding_idx, smoothing=0.0):
        super(LabelSmoothing, self).__init__()
        self.criterion = nn.KLDivLoss(size_average=False)
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
        self.true_dist = None

    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() &gt; 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        self.true_dist = true_dist
        return self.criterion(x, Variable(true_dist, requires_grad=False))

</code></pre>
<p>In particular, why it is</p>
<pre><code>true_dist.fill_(self.smoothing / (self.size - 2))
</code></pre>
<p>Where does the 2 in <code>self.size - 2</code> come from?</p>
<p>And what is the purpose of <code>true_dist[:, self.padding_idx] = 0</code></p>
","nlp, transformer-model",
Gensim on Google Colab : ModuleNotFoundError: No module named &#39;numpy.strings&#39;,"<p>I've had some problems using GENSIM.</p>
<p>After running:</p>
<pre><code>pip install --upgrade gensim
</code></pre>
<p>i execute:</p>
<pre><code>import gensim.downloader as api
</code></pre>
<p>I get the following error:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-7-bfa495c1e338&gt; in &lt;cell line: 0&gt;()
----&gt; 1 import gensim.downloader as api

9 frames
/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)
    374     _sanity_check()
    375     del _sanity_check
--&gt; 376 
    377     def _mac_os_check():
    378         &quot;&quot;&quot;

ModuleNotFoundError: No module named 'numpy.strings'

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
&quot;Open Examples&quot; button below.
---------------------------------------------------------------------------
</code></pre>
<p>I understand that the problem is in the versions of some packages that gensim uses.</p>
<pre><code>Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)
Requirement already satisfied: numpy&lt;2.0,&gt;=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)
Requirement already satisfied: scipy&lt;1.14.0,&gt;=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)
Requirement already satisfied: smart-open&gt;=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)
Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open&gt;=1.8.1-&gt;gensim) (1.17.2)
</code></pre>
<p>How to resolve this issue?</p>
","numpy, nlp, google-colaboratory, gensim",
AllenNLP all models about ccg_supertagger are unavailable. How to fix or download it?,"<p>I am trying to use AllenNLP models to parse a file to create a CCG dataset, because as a student I can't afford the CCGBank dataset, However I have to, cuz I need a dataset to help me to train a model to resolve syntactic ambiguities, parsing the sentence to ccg format is an inevitable step. I really need the model like
predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/ccg_supertagger-2020.02.10.tar.gz&quot;)
or if you have better option , I am willing to have a try!
It's my code below</p>
<pre><code>import pandas as pd
from allennlp.predictors.predictor import Predictor
import allennlp_models.tagging

# 读取原始 CSV 文件
input_path = &quot;validation.csv&quot;  # 替换为你的本地路径
df = pd.read_csv(input_path)
sentences = df[&quot;sentence&quot;].tolist()

# 加载 AllenNLP 的预训练 CCG Supertagger 模型
predictor = Predictor.from_path(&quot;https://storage.googleapis.com/allennlp-public-models/ccg_supertagger-2020.02.10.tar.gz&quot;)

# 定义预测函数：输入句子，输出 “词/范畴” 序列
def get_ccg_tags(sentence):
    output = predictor.predict(sentence=sentence)
    tokens = output[&quot;words&quot;]
    tags = output[&quot;ccg_tags&quot;]
    tagged = [f&quot;{w}/{t}&quot; for w, t in zip(tokens, tags)]
    return &quot; &quot;.join(tagged)

# 批量处理每个句子，添加 ccg_tags 列
df[&quot;ccg_tags&quot;] = df[&quot;sentence&quot;].apply(get_ccg_tags)

# 保存结果到新文件
output_path = &quot;validation_with_allennlp_ccg.csv&quot;
df.to_csv(output_path, index=False)

print(f&quot; AllenNLP CCG ：{output_path}&quot;)
</code></pre>
","nlp, allennlp",
How to Fine-Tune Projection Layer in CLIP Model Using LoRA?,"<p>I'm trying to fine-tune the projection layers in the CLIP model using LoRA.</p>
<p>I need help identifying the exact projection layers to modify for my fine-tuning and how I can apply LoRA to them.</p>
<p>Model loading:</p>
<pre class=""lang-py prettyprint-override""><code>import clip

device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
model, preprocess = clip.load(&quot;ViT-B/32&quot;, device=device)
</code></pre>
<p>Model structure when printed</p>
<pre><code>CLIP(
  (visual): VisionTransformer()
  (transformer): Transformer()
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
</code></pre>
<p>I need help identifying the exact projection layers to modify for my fine-tuning and how I can apply LoRA to them.</p>
","nlp, artificial-intelligence, large-language-model, image-text",
Creating regular expression(s) which finds capitalization errors,"<blockquote>
<p>This is a Sentence which contains<br/>
Some capitalization errors.</p>
</blockquote>
<p>So far I have this: <code>(?&lt;![.!?]\s)(?&lt;!^)(?&lt;!\sI\s)(?!I['’][a-z])(?!\b(?:Dr|Mr|Mrs)\.[\s\r\n])\b(?!I\b)[A-Z]\w*</code></p>
<p>It will find &quot;Sentence&quot; in the above. It avoids hitting on I and I' contractions, and Dr. / Mr. / Mrs.</p>
<p>What I can't get it to do is find &quot;Some&quot; in the above.</p>
<p>I feel maybe a second expression might be better for document scanning, as the first expression is quite long and probably not optimized.</p>
<p>I need the expression to be PCRE compliant that avoids non fixed width errors and such.</p>
<p>Just can't solve this on my own unfortunately. As expected AI is no help here... the best models struggle with regular expressions unless they are more simple.</p>
<p>Tried many different RegEx's to match the word &quot;Some&quot; in the above. It should NOT match a preceding line that ends with a period, question mark, or exclamation point. It should also NOT match on I and I' contractions, or on Dr. / Mr. / Mrs.</p>
","regex, nlp, capitalization",
Unable to get the tokenizer of Gemma-3,"<p>I am trying to get the tokenizer using huggingface AutoTokenizer library, but I am unable to fetch, is there any other way to get it?
Where I am doing wrong?</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer
# Load Gemma 3‑27B‑IT’s tokenizer
MODEL_GEMMA = &quot;google/gemma-3-27b-it&quot;
gemma_tokenizer = AutoTokenizer.from_pretrained(MODEL_GEMMA, trust_remote_code=True)
</code></pre>
<p>I am getting the following error on running it</p>
<pre><code>ValueError: Tokenizer class GemmaTokenizer does not exist or is not 
currently imported.
</code></pre>
","deep-learning, nlp, huggingface-transformers, huggingface-tokenizers, gemma",
Using Stanford CoreNLP,"<p>I am trying to get around using the Stanford CoreNLP. I used some code from the web to understand what is going on with the coreference tool. I tried running the project in Eclipse but keep encountering an out of memory exception. I tried increasing the heap size but there isnt any difference.</p>
<p>Why this keeps happening? Is this a code specific problem?</p>
<p>Here is my code:</p>
<pre><code>import edu.stanford.nlp.dcoref.CorefChain;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;


import java.util.Iterator;
import java.util.Map;
import java.util.Properties;


public class testmain {

    public static void main(String[] args) {

        String text = &quot;Viki is a smart boy. He knows a lot of things.&quot;;
        Annotation document = new Annotation(text);
        Properties props = new Properties();
        props.put(&quot;annotators&quot;, &quot;tokenize, ssplit, pos, parse, dcoref&quot;);
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        pipeline.annotate(document);


        Map&lt;Integer, CorefChain&gt; graph = document.get(CorefCoreAnnotations.CorefChainAnnotation.class);



        Iterator&lt;Integer&gt; itr = graph.keySet().iterator();
    
        while (itr.hasNext()) {
        
             String key = itr.next().toString();
        
             String value = graph.get(key).toString();
        
             System.out.println(key + &quot; &quot; + value);      
        }

   }
}
</code></pre>
","java, eclipse, nlp, stanford-nlp","<p>I found similar problem when building small application using Stanford CoreNLP in Eclipse.<br>
Increasing Eclipse's heap size will not solve your problem.<br>
After doing search, it is <strong>ant build tool</strong> heap size that should be increased, but I have no idea how to do that.<br>
So I give up Eclipse and use Netbeans instead.</p>

<p><strong>PS:</strong> You will eventually get out of memory exception with default setting in Netbeans. But it can easily solved by adjust setting <strong>-Xms</strong> per application basis.</p>
"
Save only best weights with huggingface transformers,"<p>Currently, I'm building a new transformer-based model with huggingface-transformers, where attention layer is different from the original one. I used <code>run_glue.py</code> to check performance of my model on GLUE benchmark. However, I found that Trainer class of huggingface-transformers saves all the checkpoints that I set, where I can set the maximum number of checkpoints to save. However, I want to save only the weight (or other stuff like optimizers) with <strong>best</strong> performance on validation dataset, and current Trainer class doesn't seem to provide such thing. (If we set the maximum number of checkpoints, then it removes older checkpoints, not ones with worse performances). <a href=""https://github.com/huggingface/transformers/issues/2675"" rel=""noreferrer"">Someone already asked about same question on Github</a>, but I can't figure out how to modify the script and do what I want. Currently, I'm thinking about making a custom Trainer class that inherits original one and change the <code>train()</code> method, and it would be great if there's an easy and simple way to do this. Thanks in advance.</p>
","deep-learning, nlp, pytorch, huggingface-transformers","<p>You may try the following parameters from trainer in the huggingface</p>
<pre><code>training_args = TrainingArguments(
    output_dir='/content/drive/results',          # output directory
    do_predict= True, 
    num_train_epochs=3,              # total number of training epochs
    **per_device_train_batch_size=4,  # batch size per device during training
    per_device_eval_batch_size=2**,   # batch size for evaluation
    warmup_steps=1000,                # number of warmup steps for learning rate  
    save_steps=1000,
    save_total_limit=10,
    load_best_model_at_end= True,
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=0, evaluate_during_training=True)
</code></pre>
<p>There may be better ways to avoid too many checkpoints and selecting the best model.
So far you can not save only the best model, but you check when the evaluation yields better results than the previous one.</p>
"
Building Q&amp;A-like application using Stanford NLP,"<p>I am working on building Question and Answer module using Stanford NLP.</p>
<p>Are there any java api from Stanford to develop Q&amp;A related application?</p>
<p>Something like where i first feed my data like as
<strong>Tiger killed dog.</strong>
After this if i ask questions like<br></p>
<pre><code>who killed dog ?
Whom Tiger killed ?
Is dog alive ? 
</code></pre>
<p>It return answers as <code>Tiger</code>, <code>dog</code>,<code>no</code>.</p>
","java, nlp, stanford-nlp",
sentiment analysis for tweets,"<pre><code>tweets = [
    &quot;Wow, what a great day today!! #sunshine&quot;,
    &quot;I feel sad about the things going on around us. #covid19&quot;,
    &quot;I'm really excited to learn Python with @JovianML #zerotopandas&quot;,
    &quot;This is a really nice song. #linkinpark&quot;,
    &quot;The python programming language is useful for data science&quot;,
    &quot;Why do bad things happen to me?&quot;,
    &quot;Apple announces the release of the new iPhone 12. Fans are excited.&quot;,
    &quot;Spent my day with family!! #happy&quot;,
    &quot;Check out my blog post on common string operations in Python. #zerotopandas&quot;,
    &quot;Freecodecamp has great coding tutorials. #skillup&quot;
]


happy_words = ['great', 'excited', 'happy', 'nice', 'wonderful', 'amazing', 'good', 'best']

sad_words = ['sad', 'bad', 'tragic', 'unhappy', 'worst']
</code></pre>
<p>I am not able to find the neutral tweets using loops, can anyone help me out?</p>
","python, nlp, sentiment-analysis",
How is polarity calculated for a sentence in sentiment analysis?,"<p>How is polarity of words in a statement are calculated? Like</p>
<pre><code>&quot;i am successful in accomplishing the task,but in vain&quot; 
</code></pre>
<p>how each word is scored? (like - successful- 0.7 accomplishing- 0.8 but - -0.5
vain - - 0.8)</p>
<p>how is it calculated? how is each word given a value or score? what is the thing that's going on behind?</p>
","nlp, text-mining, sentiment-analysis",
NLP with Racket,"<p>I am studying NLP with Racket and Dr. Racket.</p>

<p>I am working with this code:</p>

<pre><code>#lang racket

(define english-1
  '((Initial (1))
    (Final (9))
    (From 1 to 3 by NP)
    (From 1 to 2 by DET)
    (From 2 to 3 by N)
    (From 3 to 4 by BV)
    (From 4 to 5 by ADV)
    (From 4 to 5 by |#|)
    (From 5 to 6 by DET)
    (From 5 to 7 by DET)
    (From 5 to 8 by |#|)
    (From 6 to 7 by ADJ)    
    (From 6 to 6 by MOD)
    (From 7 to 9 by N)
    (From 8 to 8 by MOD)
    (From 8 to 9 by ADJ)
    (From 9 to 4 by CNJ)
    (From 9 to 1 by CNJ)))

(define (getf x y)
  (if (eq? (car x) y)
      (cadr x)
      (getf (cdr x) y)))

(define (initial-nodes network)
  (list-ref (assoc 'Initial network) 1))

(define (final-nodes network)
  (list-ref  (assoc 'Final network) 1))

(define (transitions network)
  (filter (lambda (x) (eq? (car x) 'From)) network))

(define (trans-node transition)
  (getf transition 'From))

(define(trans-newnode transition)
  (getf transition 'to))

(define (trans-label transition)
  (getf transition 'by))

(define abbreviations
  '((NP kim sandy lee)
    (DET a the her)
    (N consumer man woman)
    (BV is was)
    (CNJ and or)
    (ADJ happy stupid)
    (MOD very)
  (ADV often always sometimes)))

(define (recognize network tape)
  ;; returns t if sucessfully recognizes tape - nil otherwise
  (call/cc (lambda (return)
             (define (recognize-next node tape network)
               (if (and (null? tape) (member node (final-nodes network)))
                   (return #t) ; success
                   (for ([transition (transitions network)])
                           ;; try each transition of the network
                           (when (equal? node (trans-node transition)) ; if it starts at the right node
                               (for ([newtape (recognize-move (trans-label transition) tape)])
                                       ;; try each possible new value of tape
                                 (recognize-next (trans-newnode transition) newtape network))))))
             (for ([initialnode (initial-nodes network)])
               (recognize-next initialnode tape network))
             null))) ; failed to recognize

(define (recognize-move label tape)
  (if (or (eq? label (car tape))
          (member (car tape) (assoc label abbreviations)))
      (list (cdr tape))
      (if (eq? label '|#|)
          (list tape)
          null)))

(require racket/trace)
(trace recognize-move)
(recognize-move english-1 '(hahaha))
</code></pre>

<p>The code seems to be mostly fine. However, I keep getting a error messaging related to the recognize-move function:</p>

<pre><code>member: not a proper list: #f
</code></pre>

<p>And I thought I was dealing with lists... How can I solve this?</p>
","nlp, lisp, racket","<p>The problem is with this form:</p>

<pre><code>(member (car tape) (assoc label abbreviations))
</code></pre>

<p>If <code>assoc</code> does not find anything the result is <code>#f</code>. <code>(member 'anything #f)</code> will not work. In Common Lisp false is the same as an empty list so <code>member</code> on false will work, but not in Scheme. You can perhaps make sure it's a list like this:</p>

<pre><code>(member (car tape) (or (assoc label abbreviations) '()))
</code></pre>
"
How to Identify Similar Code Parts Using CodeBERT Embeddings?,"<p>I'm using CodeBERT to compare how similar two pieces of code are. For example:</p>
<pre><code># Code 1
def calculate_area(radius):
return 3.14 * radius * radius
</code></pre>
<pre><code># Code 2
def compute_circle_area(r):
return 3.14159 * r * r
</code></pre>
<p>CodeBERT creates &quot;embeddings,&quot; which are like detailed descriptions of the code as numbers. I then compare these numerical descriptions to see how similar the codes are. This works well for telling me how much the codes are alike.</p>
<p>However, I can't tell which parts of the code CodeBERT thinks are similar. Because the &quot;embeddings&quot; are complex, I can't easily see what CodeBERT is focusing on. Comparing the code word-by-word doesn't work here.</p>
<p>My question is: How can I figure out which specific parts of two code snippets CodeBERT considers similar, beyond just getting a general similarity score?</p>
<p>I tried simple diff methods but that defeats the purpose of purely using CodeBERT.
I want to know if it's possible using CodeBERT alone.</p>
","machine-learning, nlp, bert-language-model, plagiarism-detection, multihead-attention",
Training a model to identify names appearing in a sentence,"<p>I have a dataset containing the names of about 238583 people. The names can contain more than one word for example:
 <code>Willie Enriquez , James J Johnson, D.J. Khaled</code>.
 My problem is to identify these names when it appears in a sentence. I am trying to create a machine learning model that can identify if the input is a name or not. My trouble is figuring the input and output of this model. Since I have a bunch of names I can train a model which can recognise a name when the input is a name, but what about the other words that are part of this sentence. The model should also be able to identify words that are not names. Assuming the sentences can have any other words in it, what would be the ideal dataset for this purpose? Does it make sense to train a model on a random bunch of words and tag it as NonNames?<br>
(The entire sentences in which the names appear is not available. The user can type absolutely anything he/she wants)   </p>

<p>Thankyou.</p>
","machine-learning, nlp, named-entity-recognition","<p>The specifics of the answer may vary according to which model you are using, but the general idea is more or less the following:</p>

<p>You are trying to solve a classification task, precisely a binary classification task where you want to distinguish between proper names (assuming from your example) from other expressions. </p>

<p>The input to your model, in the most general case, are the features of the example that you want to classify: you should decide what features you think are useful to distinguish such names (e.g., number of words, contains capital letter, every word is capitalized, contains dotted letters, contains any word that you already have in your dataset, etc...). The output is the class, that is 0/1 for non-names/names. </p>

<p>You then train your model with positive examples from the dataset that you have and negative examples (i.e. non-names) taken from random words for non-names.</p>

<p>If the use can enter full sentences then you will need to do a preprocessing step where you extract all sequences of length N (word n-grams) and classify each of them individually with your previously trained model.</p>
"
Changing examples value in langchain chain,"<p>Assuming we have langchain chain <code>my_chain</code> created  using <code>my_schema</code> via:</p>
<pre><code>from langchain.chat_models import ChatOpenAI
from kor.extraction import create_extraction_chain
from kor.nodes import Object, Text, Number
from langchain.chat.models import ChatOpenAI
from langchain.llms import OpenAI

schema = Object(
    id=&quot;bank_statement_info&quot;,
    description=&quot;bank statement information about a given person.&quot;,
    attributes=[
        Text(
            id=&quot;first_name&quot;,
            description=&quot;The first name of the person&quot;,
            examples=[(&quot;John Smith&quot;, &quot;John&quot;)],
        ),
        Text(
            id=&quot;last_name&quot;,
            description=&quot;The last name of the person&quot;,
            examples=[(&quot;John Smith&quot;, &quot;Smith&quot;)],
        ),
        Text(
            id=&quot;account_number&quot;,
            description=&quot;Account Number of the person in Bank statement.&quot;,
            examples=[(&quot;Account Number: 122-233-566-800&quot;, &quot;122-233-566-800&quot;)],
        ),
        Text(
            id=&quot;address&quot;,
            description=&quot;address of the person in Bank statement.&quot;,
        ),
        Text(
            id=&quot;opening_balance&quot;,
            description=&quot;opening blance of the person in Bank statement.&quot;,
            examples=[(&quot;opening Balance: 245,800.00&quot;,&quot;245,800.00&quot;)]
        ),
        Text(
            id=&quot;closing_balance&quot;,
            description=&quot;closing blance of the person in Bank statement.&quot;,
            examples=[(&quot;Closing Balance: 591,800.00&quot;,&quot;591,800.00&quot;)]
        ),
    ],
    examples=[
        (
            &quot;&quot;&quot;ya 231 Valley Farms Street
              FIRST Santa Monica, CA 90403 STATEMENT OF ACCOUNT
              CITIZENS __firstcitizensbank@domain.com
              BANK
              Account Number: 122-233-566-800
              Statement Date: 11/22/2019 Page 1 of 1
              Period Covered: 05/22/2019 to 11/22/2019
              Eric Nam Opening Balance: 175,800.00
              240 st, apt 15, hill road, Total Credit Amount: 510,000.00
              Baverly Hills, LA, 90209 Total Debit Amount: 94,000.00
              Closing Balance: 591,800.00
              Branch - Baverly Hills Account Type: Saving Account&quot;&quot;&quot;,
            [
                {&quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;account_number&quot;: '122-233-566-800',&quot;address&quot;:&quot;240 st, apt 15, hill road,Baverly Hills, LA, 90209&quot;,
                 &quot;Closing Balance&quot;: &quot;458,589.00&quot;,&quot;opening Balance&quot;: &quot;800.00&quot;},
                {&quot;first_name&quot;: &quot;Jane&quot;, &quot;last_name&quot;: &quot;Doe&quot;, &quot;age&quot;: '923-533-256-205',&quot;address&quot;:&quot;850 st, apt 82, hill road,Baverly Hills, New york, 82044&quot;,
                 &quot;Closing Balance&quot;: &quot;1000.00&quot;,&quot;opening Balance&quot;: &quot;125,987.00&quot;},
            ],
        )
    ],
    many=True,
)
llm = ChatOpenAI(temperature=0.0, openai_api_key='', open_api_base='', model_kwargs={'engine': 'openai_gpt_4'}
my_chain = create_extraction_chain(llm, my_schema, encoder_or_encoder_class='json')
</code></pre>
<p>I want to have access to <code>examples</code> of <code>my_chain</code> after creating it. I tried to take access by accesing <code>my_chain.prompt</code> but afterwards could not get to examples. In particular, I would like to be able to  dynamically change the <code>examples</code> of <code>my_chain</code>. Is that possible?</p>
","python, nlp, langchain, large-language-model, chatgpt-api",
How is teacher-forcing implemented for the Transformer training?,"<p>In <a href=""https://www.tensorflow.org/beta/tutorials/text/transformer#training_and_checkpointing"" rel=""nofollow noreferrer"">this part</a> of Tensorflow's tutorial, they mentioned that they are training with teacher-forcing. To my knowledge, teacher-forcing involves feeding the target output into the model so that it converges faster. So I'm curious as to how this is done here? The real target is <code>tar_real</code>, and as far as I can see, it is only used to calculate loss and accuracy.</p>
<p>How is this code implementing teacher-forcing?</p>
","tensorflow, machine-learning, nlp, transformer-model",
how to modify a step or a prompt of an existing langchain chain (customize SelfQueryRetriever)?,"<p>I need to customize a SelfQueryRetriever(the reason is: the generated target queries in OpenSearch are being generated incorrrectly so we need to tune prompts + we need to add some custom behavior such as multi-tenancy) but we don't want to re-write the whole chain, just the parts what we need to customize. How can we customize specific steps of a chain, is there  a way to modify it by position, let's say something like this (pseudo-code):</p>
<pre><code>retriever = SelfQueryRetriever(**config)
retriever[2] = create_custom_module1()
retriever[4] = create_custom_module2()
</code></pre>
<p>In this example we preserve the majority of the chain but  customize only the third and fifth elements.</p>
<p>Is it possible to do?</p>
","python, nlp, artificial-intelligence, langchain, large-language-model",
Trouble getting importing gensim to work in colab,"<p>I am trying to import gensim into colab.</p>
<pre><code>!pip install gensim
</code></pre>
<p>I get the following error:</p>
<pre><code>/usr/local/lib/python3.11/dist-packages/numpy/__init__.py in __getattr__(attr)
    365                 raise AssertionError()
    366         except AssertionError:
--&gt; 367             msg = (&quot;The current Numpy installation ({!r}) fails to &quot;
    368                    &quot;pass simple sanity checks. This can be caused for example &quot;
    369                    &quot;by incorrect BLAS library being linked in, or by mixing &quot;

ModuleNotFoundError: No module named 'numpy.char'
</code></pre>
<p>my numpy version is 2.02. If I downgrade numpy to another version like say 1.26.4 I get a different error but always a numpy string related issue. Thanks</p>
","numpy, nlp, dependencies, google-colaboratory, gensim","<p>You have to restart the session for the underlying runtime to notice the package changes. See: <a href=""https://stackoverflow.com/a/79518359/130288"">https://stackoverflow.com/a/79518359/130288</a></p>
<p>I recall in the past Colab offering a warning when you had to do this. And possibly also, in the past, Colab hadn't yet loaded <code>numpy</code>/etc in a fresh environment – and so it was OK for them to downgrade behind the scenes without a problem - the 1st import was only after the downgrade.</p>
<p>But something changed in Colab recently – maybe some fast-start optimization? – with a bunch of reports of problems like this in just the last day or two.</p>
<p>Explicitly restarting after the Gensim-install &amp; <code>numpy</code>/<code>scipy</code> downgrades resolves the errors.</p>
"
Converting data into spacy format &quot;convert_to_spacy_format&quot; in Name entity recognition Model,"<p><a href=""https://i.sstatic.net/TMDd85MJ.png"" rel=""nofollow noreferrer"">Dataset structure</a>Can somebody help me with the NER model in converting the data into spacy format.
The dataset format is shown in the screenshot here (<a href=""https://www.kaggle.com/datasets/naseralqaydeh/named-entity-recognition-ner-corpus"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/naseralqaydeh/named-entity-recognition-ner-corpus</a>)</p>
<p>Though i build but the model is not giving any output during test.</p>
<pre><code>#Convert data to spaCy format
def convert_to_spacy_format(data):
    nlp = spacy.blank(&quot;en&quot;)  # Creating blank English language model
    db = DocBin()  # document bin object
    
    for _, row in tqdm(data.iterrows(), total=len(data)):
        sentence = row[&quot;CleanSentence&quot;]
        pos_tags = row[&quot;POS&quot;]
        ner_tags = row[&quot;Tag&quot;]
        
        # Create a doc object
        doc = nlp.make_doc(sentence)
        
        # Split the sentence into words (tokens)
        words = sentence.split()
        
        # Check if lengths match
        if len(words) != len(ner_tags) or len(words) != len(pos_tags):
            print(f&quot;Warning: Length mismatch: Words: {len(words)}, NER tags: {len(ner_tags)}, POS tags: {len(pos_tags)}&quot;)
            continue
            
        ents = []
        current_ent = None
        current_ent_start = None
        
        # Process each token
        for idx, (token, tag) in enumerate(zip(doc, ner_tags)):
            # If it's the beginning of an entity
            if tag.startswith(&quot;B-&quot;):
                # If we were tracking an entity, add it to our list
                if current_ent is not None:
                    ents.append((current_ent_start, token.idx + len(token), current_ent))
                
                # Start tracking a new entity
                current_ent = tag[2:]  # Remove &quot;B-&quot; prefix
                current_ent_start = token.idx
            
            # If it's inside an entity
            elif tag.startswith(&quot;I-&quot;):
                # Continue tracking the current entity
                pass
            
            # If it's outside any entity
            elif tag == &quot;O&quot;:
                # If we were tracking an entity, add it to the list
                if current_ent is not None:
                    ents.append((current_ent_start, token.idx, current_ent))
                    current_ent = None
                    current_ent_start = None
        
        # Add the last entity if we were tracking one
        if current_ent is not None:
            ents.append((current_ent_start, len(sentence), current_ent))
        
        # Create spans for each entity
        spans = []
        for start, end, label in ents:
            span = doc.char_span(start, end, label=label)
            if span is not None:
                spans.append(span)
        
        # Filter overlapping spans
        filtered_spans = filter_spans(spans)
        
        # Add entities to the doc
        doc.ents = filtered_spans
        
        # Add the doc to the DocBin
        db.add(doc)
    
    return db

</code></pre>
<p>I tried to build an NER model but didn't got the expected output. I need help in the function</p>
<pre><code>convert_to_spacy_format(data)
</code></pre>
","python, string, nlp, spacy, named-entity-recognition",
How to use Hugging Face model with 512 max tokens on longer text (for Named Entity Recognition),"<p>I have been using the Named Entity Recognition (NER) model <a href=""https://huggingface.co/cahya/bert-base-indonesian-NER"" rel=""nofollow noreferrer"">https://huggingface.co/cahya/bert-base-indonesian-NER</a> on Indonesian text as follows:</p>
<pre class=""lang-py prettyprint-override""><code>text = &quot;...&quot;
model_name = &quot;cahya/bert-base-indonesian-NER&quot;
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForTokenClassification.from_pretrained(model_name)
nlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer, aggregation_strategy=&quot;simple&quot;)
entities = nlp(text)
</code></pre>
<p>This works great, but when <code>text</code> contains more than 512 tokens I get the error:</p>
<blockquote>
<p>The size of tensor a (1098) must match the size of tensor b (512) at non-singleton dimension 1</p>
</blockquote>
<p>What is the best way to check if <code>text</code> contains more than 512 tokens, and then split it into manageable chunks that I can use for NER?</p>
<hr />
<p>Counting the number of tokens seems straightforward:</p>
<pre><code>n_tokens = len(tokenizer.encode(text, add_special_tokens=True, truncation=False))
</code></pre>
<p>However, it is unclear what is the best way to split this. Surely there should be a suite of functions for this?</p>
","python-3.x, nlp, bert-language-model, named-entity-recognition, huggingface",
_batch_encode_plus() got an unexpected keyword argument &#39;return_attention_masks&#39;,"<p>I am studying RoBERTA model to detect emotions in tweets.
On Google colab. Following this Noteboook file from Kaggle - <a href=""https://www.kaggle.com/ishivinal/tweet-emotions-analysis-using-lstm-glove-roberta?scriptVersionId=38608295"" rel=""nofollow noreferrer"">https://www.kaggle.com/ishivinal/tweet-emotions-analysis-using-lstm-glove-roberta?scriptVersionId=38608295</a></p>
<p>Code snippet:</p>
<pre><code>def regular_encode(texts, tokenizer, maxlen=512):
     enc_di = tokenizer.batch_encode_plus(
         texts, 
         return_attention_masks=True, 
         return_token_type_ids=False,
         pad_to_max_length=True,
         #padding=True,
         max_length=maxlen
     )
    
     return np.array(enc_di['input_ids'])

 def build_model(transformer, max_len=160):
     input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;)
     sequence_output = transformer(input_word_ids)[0]
     cls_token = sequence_output[:, 0, :]
     out = Dense(13, activation='softmax')(cls_token)
    
     model = Model(inputs=input_word_ids, outputs=out)
     model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    
     return model


AUTO = tf.data.experimental.AUTOTUNE
MODEL = 'roberta-base'
tokenizer = AutoTokenizer.from_pretrained(MODEL)


X_train_t = regular_encode(X_train, tokenizer, maxlen= max_len)
X_test_t = regular_encode(X_test, tokenizer, maxlen=max_len)
</code></pre>
<p>In regular_encode part I am getting the following error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-101-4e1e74c2ea8f&gt; in &lt;module&gt;()
----&gt; 1 X_train_t = regular_encode(X_train, tokenizer, maxlen= max_len)
      2 X_test_t = regular_encode(X_test, tokenizer, maxlen=max_len)

2 frames
/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/tokenization_gpt2_fast.py in _batch_encode_plus(self, *args, **kwargs)
    161         )
    162 
--&gt; 163         return super()._batch_encode_plus(*args, **kwargs)
    164 
    165     def _encode_plus(self, *args, **kwargs) -&gt; BatchEncoding:

TypeError: _batch_encode_plus() got an unexpected keyword argument 'return_attention_masks'
</code></pre>
","python, nlp, google-colaboratory, bert-language-model, roberta-language-model",
"How can I deploy and run a Flask web application using heavy NLP libraries (pandas, numpy, sklearn) on a SiteGround shared hosting plan?","<p>I have a Flask-based web application that performs NLP tasks using libraries like pandas, numpy, sklearn, and nltk. I've tried deploying it to my current hosting (SiteGround shared hosting plan), but encountered multiple issues, such as:</p>
<p>Installation issues (pyahocorasick and other dependency errors).
Resource limitations (KeyboardInterrupt when importing heavy libraries).
Difficulty running continuously in the background.
My current setup:
Hosting provider: SiteGround Shared Hosting
Python version: 3.13.2
Flask app with dependencies: pandas, numpy, sklearn, nltk, contractions, etc.
SSH access available, but no root access.
Tried using virtual environment (venv), encountering build issues.
My questions are:</p>
<ol>
<li>Is it possible to run resource-intensive NLP applications like this on SiteGround’s shared hosting plan at all?</li>
<li>If yes, how? What steps or configurations are required to overcome these errors?</li>
<li>If no, what are the simplest and most cost-effective alternatives to deploy such a Flask NLP application smoothly (PythonAnywhere, Render.com, Heroku, AWS, DigitalOcean, or others)?</li>
</ol>
<p>Thanks in advance for any guidance or advice!</p>
","python, flask, nlp",
Named Entity Recognition using Python spaCy,"<p>I want to code a Named Entity Recognition system using Python <code>spaCy</code> package. However, I couldn't install my local language inside <code>spaCy</code> package. Is there anyone who can tell me how to install or otherwise use my local language?</p>
<p>I tried:</p>
<pre><code>python -m spacy download xx_ent_wiki_sm
</code></pre>
","python, nlp, spacy, named-entity-recognition",
Re-Training spaCy&#39;s NER v1.8.2 - Training Volume and Mix of Entity Types,"<p>I'm in the process of (re-) training spaCy's Named Entity Recognizer and have a couple of doubts that I hope a more experienced researcher/practitioner can help me figure out:</p>
<ol>
<li>If a few hundred examples are considered 'a good starting point', then what would be a reasonable number to aim for? Is 100 000 entity/label excessive?</li>
<li>If I introduce a new label, is it best if the number of the entities of that labeled are roughly the same (balanced) during training?</li>
<li>Regarding the mixing in 'examples of other entity types':
<ul>
<li><p>do I just add random known categories/labels to my training set eg: <code>('The Business Standard published in its recent issue on crude oil and natural gas ...', [(4,21, 'ORG')], )</code>?</p>
</li>
<li><p>can I use the same text for various labels? e.g. <code>('The Business Standard published in its recent issue on crude oil and natural gas ...', [(55,64, 'COMMODITY')], )</code>?</p>
</li>
</ul>
</li>
</ol>
<ul>
<li><p>on a similar note let's assume I want spaCyto also recognize a second <code>COMMODITY</code> could I then just use the same sentence and label a different region e.g. <code>('The Business Standard published in its recent issue on crude oil and natural gas ...', [(69,80, 'COMMODITY')], )</code>? Is that how it's supposed to be done?</p>
<ul>
<li>what ratio between new and other (old) labels is considered reasonable</li>
</ul>
</li>
</ul>
<p>I'm working with Python2.7 in Ubuntu 16.04 using spaCy 1.8.2</p>
","python-2.7, nlp, named-entity-recognition, spacy","<p>For a full answer by <a href=""https://github.com/explosion/spaCy/issues/1054"" rel=""nofollow noreferrer"">Matthew Honnibal check out issue 1054 on spaCy's github page</a>. Below are the most important points as they relate to my questions:</p>
<blockquote>
<p><strong>Question(Q) 1:</strong> If a few hundred examples are considered 'a good starting point', then what would be a reasonable number to aim for? Is 100 000 entity/label excessive?</p>
<blockquote>
<p><strong>Answer(A):</strong> Every machine learning problem will have a different examples/accuracy curve. You can get an idea for this by training with less data than you have, and seeing what the curve looks like. If you have 1,000 examples, then try training with 500, 750, etc, and see how that affects your accuracy.</p>
</blockquote>
<p><strong>Q 2:</strong> If I introduce a new label, is it best if the number of the entities of that label are roughly the same (balanced) during training?</p>
<blockquote>
<p><strong>A:</strong> There's trade-off between making the gradients too sparse, and making the learning problem too unrepresentative of what the actual examples will look like.</p>
</blockquote>
<p><strong>Q 3:</strong> Regarding the mixing in 'examples of other entity types':</p>
<ul>
<li>do I just add random known categories/labels to my training set:</li>
</ul>
</blockquote>
<p><strong>A:</strong> No, one should annotate all the entities in that text, so the example above: <code>('The Business Standard published in its recent issue on crude oil and natural gas ...', [(4,21, 'ORG')], )</code> should be <code>('The Business Standard published in its recent issue on crude oil and natural gas ...', [(4,21, 'ORG'), (55,64, 'COMMODITY'), (69,80, 'COMMODITY')], )</code></p>
<blockquote>
<ul>
<li>can I use the same text for various labels?:</li>
</ul>
</blockquote>
<p><strong>A:</strong> Not in the way the examples were given. See previous answer.</p>
<blockquote>
<ul>
<li>what ratio between new and other (old) labels is considered reasonable?:</li>
</ul>
</blockquote>
<p><strong>A:</strong> See answer <strong>Q 2</strong>.</p>
<hr />
<p><em>PS: Double citations are direct quotes from the github issue answer.</em></p>
"
Training a new entity type with spacy,"<p>Need help to try adding new entity and train my own model with spacy named entity recognition. I wanted first to try the example already done here:</p>
<p><a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py</a></p>
<p>but i'am getting this error :</p>
<pre><code>ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-c46f384e-5989-4902-a775-7618ffadd54e.json
An exception has occurred, use %tb to see the full traceback.

SystemExit: 2
/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.
  warn(&quot;To exit: use 'exit', 'quit', or Ctrl-D.&quot;, stacklevel=1)
</code></pre>
<p>How to resolve this?</p>
","python, nlp, spacy, named-entity-recognition",
Presidio with Langchain Experimental does not detect Polish names,"<p>I am using presidio/langchain_experimental to anonymize text in Polish, but it does not detect names (e.g., &quot;Jan Kowalski&quot;). Here is my code:</p>
<pre><code>from presidio_anonymizer import PresidioAnonymizer
from presidio_reversible_anonymizer import PresidioReversibleAnonymizer

config = {
    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,
    &quot;models&quot;: [{&quot;lang_code&quot;: &quot;pl&quot;, &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],
}

anonymizer = PresidioAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],
                                languages_config=config)

anonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;],
                                               languages_config=config)

text = &quot;Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.&quot;

anonymized_result = anonymizer_tool.anonymize(text)
anon_result = anonymizer.anonymize(text)
deanonymized_result = anonymizer_tool.deanonymize(anonymized_result)

print(&quot;Anonymized text:&quot;, anonymized_result)
print(&quot;Deanonymized text:&quot;, deanonymized_result)
print(&quot;Map:&quot;, anonymizer_tool.deanonymizer_mapping)
print(&quot;Anonymized text:&quot;, anon_result)
</code></pre>
<p>Output:</p>
<pre><code>Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.
Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.
Map: {}
Anonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com.
</code></pre>
<p>I expected the name &quot;Jan Kowalski&quot; and the email address to be anonymized, but the output remains unchanged.
I have installed the pl_core_news_lg model using:</p>
<pre><code>python -m spacy download pl_core_news_lg
</code></pre>
<p>Am I missing something in the configuration, or does Presidio not support Polish entity recognition properly?
Any suggestions on how to make it detect names in Polish?</p>
<p>The interesting thing is that when I use only</p>
<pre><code>anonymizer_tool = PresidioReversibleAnonymizer()
</code></pre>
<p>Then the output look like this:</p>
<pre><code>Anonymized text: Elizabeth Tate mieszka w Warszawie i ma e-mail christinemurray@example.net. 
Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. 
Map: {'PERSON': {'Elizabeth Tate': 'Jan Kowalski'}, 'EMAIL_ADDRESS': {'christinemurray@example.net': 'jan.kowalski@example.com'}}
</code></pre>
<p><strong>As mentioned below if I use only spaCy:</strong></p>
<pre><code>nlp = spacy.load(&quot;pl_core_news_lg&quot;)
doc = nlp(text)
</code></pre>
<p>Then the output is correct so I guess that it's the problem with presidio itself. Output from spaCy:</p>
<pre><code>Jan Kowalski persName
Warszawie placeName
</code></pre>
<p>So I would not like to create custom analyzer for that but use spaCy in  Presidio as it works as expected.</p>
","python, nlp, spacy, langchain, presidio","<p>After some test I was able to find the solution:</p>
<pre><code>config = {
    &quot;nlp_engine_name&quot;: &quot;spacy&quot;,
    &quot;models&quot;: [{&quot;lang_code&quot;: 'pl', &quot;model_name&quot;: &quot;pl_core_news_lg&quot;}],
}
spacy_recognizer = SpacyRecognizer(
    supported_language=&quot;pl&quot;,
    supported_entities=[&quot;persName&quot;]
)
anonymizer.add_recognizer(spacy_recognizer)

anonymizer_tool = PresidioReversibleAnonymizer(analyzed_fields=[&quot;PERSON&quot;, &quot;PHONE_NUMBER&quot;, &quot;EMAIL_ADDRESS&quot;, &quot;CREDIT_CARD&quot;], languages_config=config)
</code></pre>
<p>The output look like this:<br />
<code>Anonymized text: &lt;persName&gt; mieszka w Warszawie i ma e-mail glenn58@example.org. </code></p>
<p><code>Deanonymized text: Jan Kowalski mieszka w Warszawie i ma e-mail jan.kowalski@example.com. </code></p>
<p><code>Map: {'persName': {'&lt;persName&gt;': 'Jan Kowalski', '&lt;persName_2&gt;': 'Jana Kowalskiego'}, 'EMAIL_ADDRESS': {'glenn58@example.org': 'jan.kowalski@example.com'}}</code></p>
<p>You need to directly add <code>SpacyRecognizer</code> with <code>supported_entities</code> formatted according to spaCy's requirements. I believe there's something missing or unclear in the documentation, which is causing the misunderstanding.</p>
"
spaCy: Can&#39;t find model &#39;en_core_web_sm&#39; on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit),"<p>What is the difference between <code>spacy.load('en_core_web_sm')</code> and <code>spacy.load('en')</code>? <a href=""https://stackoverflow.com/questions/50487495/what-is-difference-between-en-core-web-sm-en-core-web-mdand-en-core-web-lg-mod"">This link</a> explains different model sizes. But I am still not clear how <code>spacy.load('en_core_web_sm')</code> and <code>spacy.load('en')</code> differ</p>
<p><code>spacy.load('en')</code> runs fine for me. But the <code>spacy.load('en_core_web_sm')</code> throws error</p>
<p>I have installed <code>spacy</code>as below. when I go to Jupyter notebook and run command <code>nlp = spacy.load('en_core_web_sm')</code> I get the below error</p>
<pre><code>---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
&lt;ipython-input-4-b472bef03043&gt; in &lt;module&gt;()
      1 # Import spaCy and load the language library
      2 import spacy
----&gt; 3 nlp = spacy.load('en_core_web_sm')
      4 
      5 # Create a Doc object

C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder\lib\site-packages\spacy\__init__.py in load(name, **overrides)
     13     if depr_path not in (True, False, None):
     14         deprecation_warning(Warnings.W001.format(path=depr_path))
---&gt; 15     return util.load_model(name, **overrides)
     16 
     17 

C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder\lib\site-packages\spacy\util.py in load_model(name, **overrides)
    117     elif hasattr(name, 'exists'):  # Path or Path-like to model data
    118         return load_model_from_path(name, **overrides)
--&gt; 119     raise IOError(Errors.E050.format(name=name))
    120 
    121 

OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
</code></pre>
<p>how I installed Spacy ---</p>
<pre><code>(C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder) C:\Users\nikhizzz&gt;conda install -c conda-forge spacy
Fetching package metadata .............
Solving package specifications: .

Package plan for installation in environment C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder:

The following NEW packages will be INSTALLED:

    blas:           1.0-mkl
    cymem:          1.31.2-py35h6538335_0    conda-forge
    dill:           0.2.8.2-py35_0           conda-forge
    msgpack-numpy:  0.4.4.2-py_0             conda-forge
    murmurhash:     0.28.0-py35h6538335_1000 conda-forge
    plac:           0.9.6-py_1               conda-forge
    preshed:        1.0.0-py35h6538335_0     conda-forge
    pyreadline:     2.1-py35_1000            conda-forge
    regex:          2017.11.09-py35_0        conda-forge
    spacy:          2.0.12-py35h830ac7b_0    conda-forge
    termcolor:      1.1.0-py_2               conda-forge
    thinc:          6.10.3-py35h830ac7b_2    conda-forge
    tqdm:           4.29.1-py_0              conda-forge
    ujson:          1.35-py35hfa6e2cd_1001   conda-forge

The following packages will be UPDATED:

    msgpack-python: 0.4.8-py35_0                         --&gt; 0.5.6-py35he980bc4_3 conda-forge

The following packages will be DOWNGRADED:

    freetype:       2.7-vc14_2               conda-forge --&gt; 2.5.5-vc14_2

Proceed ([y]/n)? y

blas-1.0-mkl.t 100% |###############################| Time: 0:00:00   0.00  B/s
cymem-1.31.2-p 100% |###############################| Time: 0:00:00   1.65 MB/s
msgpack-python 100% |###############################| Time: 0:00:00   5.37 MB/s
murmurhash-0.2 100% |###############################| Time: 0:00:00   1.49 MB/s
plac-0.9.6-py_ 100% |###############################| Time: 0:00:00   0.00  B/s
pyreadline-2.1 100% |###############################| Time: 0:00:00   4.62 MB/s
regex-2017.11. 100% |###############################| Time: 0:00:00   3.31 MB/s
termcolor-1.1. 100% |###############################| Time: 0:00:00 187.81 kB/s
tqdm-4.29.1-py 100% |###############################| Time: 0:00:00   2.51 MB/s
ujson-1.35-py3 100% |###############################| Time: 0:00:00   1.66 MB/s
dill-0.2.8.2-p 100% |###############################| Time: 0:00:00   4.34 MB/s
msgpack-numpy- 100% |###############################| Time: 0:00:00   0.00  B/s
preshed-1.0.0- 100% |###############################| Time: 0:00:00   0.00  B/s
thinc-6.10.3-p 100% |###############################| Time: 0:00:00   5.49 MB/s
spacy-2.0.12-p 100% |###############################| Time: 0:00:10   7.42 MB/s

(C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder) C:\Users\nikhizzz&gt;python -V
Python 3.5.3 :: Anaconda custom (64-bit)

(C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder) C:\Users\nikhizzz&gt;python -m spacy download en
Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0
  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)
    100% |################################| 37.4MB ...
Installing collected packages: en-core-web-sm
  Running setup.py install for en-core-web-sm ... done
Successfully installed en-core-web-sm-2.0.0

    Linking successful
    C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder\lib\site-packages\en_core_web_sm
    --&gt;
    C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder\lib\site-packages\spacy\data\en

    You can now load the model via spacy.load('en')


(C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder) C:\Users\nikhizzz&gt;
</code></pre>
","python, python-3.x, nlp, spacy","<p>The answer to your misunderstanding is a Unix concept, <strong>softlinks</strong> which we could say that in Windows are similar to shortcuts. Let's explain this.</p>
<p>When you <code>spacy download en</code>, spaCy tries to find the best <strong>small</strong> model that matches your spaCy distribution. The small model that I am talking about defaults to <code>en_core_web_sm</code> which can be found in different variations which correspond to the different spaCy versions (for example <code>spacy</code>, <code>spacy-nightly</code> have <code>en_core_web_sm</code> of different sizes).</p>
<p>When spaCy finds the best model for you, it downloads it and then <strong>links</strong> the name <code>en</code> to the package it downloaded, e.g. <code>en_core_web_sm</code>. That basically means that whenever you refer to <code>en</code> you will be referring to <code>en_core_web_sm</code>. In other words, <code>en</code> after linking is not a &quot;real&quot; package, is just a name for <code>en_core_web_sm</code>.</p>
<p>However, it doesn't work the other way. You can't refer directly to <code>en_core_web_sm</code> because your system doesn't know you have it installed. When you did <code>spacy download en</code> you basically did a pip install. So pip knows that you have a package named <code>en</code> installed for your python distribution, but knows nothing about the package <code>en_core_web_sm</code>. This package is just replacing package <code>en</code> when you import it, which means that package <code>en</code> is just a softlink to <code>en_core_web_sm</code>.</p>
<p>Of course, you can directly download <code>en_core_web_sm</code>, using the command: <code>python -m spacy download en_core_web_sm</code>, or you can even link the name <code>en</code> to other models as well. For example, you could do <code>python -m spacy download en_core_web_lg</code> and then <code>python -m spacy link en_core_web_lg en</code>. That would make
<code>en</code> a name for <code>en_core_web_lg</code>, which is a large spaCy model for the English language.</p>
"
Error when installing Constituent Tree Lib in Python,"<p>I am trying to use the so visually attractive Constituent Tree Lib in Python : <a href=""https://github.com/Halvani/Constituent-Treelib"" rel=""nofollow noreferrer"">https://github.com/Halvani/Constituent-Treelib</a>. I have followed the steps in the installation (pip install, ...), but I get this error :</p>
<pre><code>Collecting constituent-treelib
Downloading constituent_treelib-0.0.6-py3-none-any.whl.metadata (17 kB)
Requirement already satisfied: benepar in 
c:\users\... \python311\lib\site-packages (from constituent- 
treelib) (0.2.0)
Collecting contractions (from constituent-treelib)
Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)
Collecting fasttext (from constituent-treelib)
Using cached fasttext-0.9.2.tar.gz (68 kB)
Installing build dependencies ... done
Getting requirements to build wheel ... error
error: subprocess-exited-with-error

× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─&gt; [28 lines of output]
  C:\Users\...\Python311\python.exe: No module named pip
  Traceback (most recent call last):
    File &quot;&lt;string&gt;&quot;, line 38, in __init__
  ModuleNotFoundError: No module named 'pybind11'
 
  During handling of the above exception, another exception occurred:
 
  Traceback (most recent call last):
    File &quot;C:\Users\...\Python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 353, in &lt;module&gt;
      main()
    File &quot;C:\Users\...\Python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 335, in main
      json_out['return_val'] = hook(**hook_input['kwargs'])
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File &quot;C:\Users\...\Python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 118, in get_requires_for_build_wheel
      return hook(config_settings)
             ^^^^^^^^^^^^^^^^^^^^^
    File &quot;C:\Users\...\AppData\Local\Temp\pip-build-env-8f9djh2i\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 325, in get_requires_for_build_wheel
      return self._get_build_requires(config_settings, requirements=['wheel'])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File &quot;C:\Users\...\AppData\Local\Temp\pip-build-env-8f9djh2i\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 295, in _get_build_requires
      self.run_setup()
    File &quot;C:\Users\...\AppData\Local\Temp\pip-build-env-8f9djh2i\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 487, in run_setup
      super().run_setup(setup_script=setup_script)
    File &quot;C:\Users\...\AppData\Local\Temp\pip-build-env-8f9djh2i\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 311, in run_setup
      exec(code, locals())
    File &quot;&lt;string&gt;&quot;, line 72, in &lt;module&gt;
    File &quot;&lt;string&gt;&quot;, line 41, in __init__
  RuntimeError: pybind11 install failed.
  [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  error: subprocess-exited-with-error

  × Getting requirements to build wheel did not run successfully.
 │ exit code: 1
 ╰─&gt; See above for output.
</code></pre>
<p>Does anyone know about how to fix the problem? Thanks so much in advance.</p>
","python, nlp, libraries",
Difference between constituency parser and dependency parser,"<p>What is the difference between a <em>constituency parser</em> and a <em>dependency parser</em>? What are the different usages of the two?</p>
","parsing, nlp, terminology","<p>A constituency parse tree breaks a text into sub-phrases.  Non-terminals in the tree are types of phrases, the terminals are the words in the sentence, and the edges are unlabeled.  For a simple sentence &quot;John sees Bill&quot;, a constituency parse would be:</p>
<pre><code>                  Sentence
                     |
       +-------------+------------+
       |                          |
  Noun Phrase                Verb Phrase
       |                          |
     John                 +-------+--------+
                          |                |
                        Verb          Noun Phrase
                          |                |
                        sees              Bill
</code></pre>
<p>A dependency parse connects words according to their relationships.  Each vertex in the tree represents a word, child nodes are words that are dependent on the parent, and edges are labeled by the relationship.  A dependency parse of &quot;John sees Bill&quot;, would be:</p>
<pre><code>              sees
                |
        +--------------+
subject |              | object
        |              |
      John            Bill
</code></pre>
<p>You should use the parser type that gets you closest to your goal.  If you are interested in sub-phrases within the sentence, you probably want the constituency parse.  If you are interested in the dependency relationships between words, then you probably want the dependency parse.</p>
<p>The Stanford parser can give you either (<a href=""https://corenlp.run/"" rel=""noreferrer"">online demo</a>).  In fact, the way it really works is to always parse the sentence with the constituency parser, and then, if needed, it performs a deterministic (rule-based) transformation on the constituency parse tree to convert it into a dependency tree.</p>
<p>More can be found here:</p>
<p><a href=""http://en.wikipedia.org/wiki/Phrase_structure_grammar"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Phrase_structure_grammar</a></p>
<p><a href=""http://en.wikipedia.org/wiki/Dependency_grammar"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Dependency_grammar</a></p>
"
How to compute the similarity between two text documents?,"<p>I want to take two documents and determine how similar they are. Any programming language if fine but I prefer Python.</p>
","python, nlp","<p>The common way of doing this is to transform the documents into TF-IDF vectors and then compute the cosine similarity between them. Any textbook on information retrieval (IR) covers this. See esp. <a href=""http://www-nlp.stanford.edu/IR-book/"" rel=""noreferrer""><em>Introduction to Information Retrieval</em></a>, which is free and available online.</p>
<h3 id=""computing-pairwise-similarities-m9ec"">Computing Pairwise Similarities</h3>
<p>TF-IDF (and similar text transformations) are implemented in the Python packages <a href=""http://radimrehurek.com/gensim/"" rel=""noreferrer"">Gensim</a> and <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer"" rel=""noreferrer"">scikit-learn</a>. In the latter package, computing cosine similarities is as easy as</p>
<pre class=""lang-python prettyprint-override""><code>from sklearn.feature_extraction.text import TfidfVectorizer

documents = [open(f).read() for f in text_files]
tfidf = TfidfVectorizer().fit_transform(documents)
# no need to normalize, since Vectorizer will return normalized tf-idf
pairwise_similarity = tfidf * tfidf.T
</code></pre>
<p>or, if the documents are plain strings,</p>
<pre class=""lang-python prettyprint-override""><code>&gt;&gt;&gt; corpus = [&quot;I'd like an apple&quot;, 
...           &quot;An apple a day keeps the doctor away&quot;, 
...           &quot;Never compare an apple to an orange&quot;, 
...           &quot;I prefer scikit-learn to Orange&quot;, 
...           &quot;The scikit-learn docs are Orange and Blue&quot;]                                                                                                                                                                                                   
&gt;&gt;&gt; vect = TfidfVectorizer(min_df=1, stop_words=&quot;english&quot;)                                                                                                                                                                                                   
&gt;&gt;&gt; tfidf = vect.fit_transform(corpus)                                                                                                                                                                                                                       
&gt;&gt;&gt; pairwise_similarity = tfidf * tfidf.T 
</code></pre>
<p>though Gensim may have more options for this kind of task.</p>
<p>See also <a href=""https://stackoverflow.com/questions/2380394/simple-implementation-of-n-gram-tf-idf-and-cosine-similarity-in-python"">this question</a>.</p>
<p>[Disclaimer: I was involved in the scikit-learn TF-IDF implementation.]</p>
<h3 id=""interpreting-the-results-1v7i"">Interpreting the Results</h3>
<p>From above, <code>pairwise_similarity</code> is a Scipy <a href=""https://docs.scipy.org/doc/scipy/reference/sparse.html"" rel=""noreferrer"">sparse matrix</a> that is square in shape, with the number of rows and columns equal to the number of documents in the corpus.</p>
<pre class=""lang-python prettyprint-override""><code>&gt;&gt;&gt; pairwise_similarity                                                                                                                                                                                                                                      
&lt;5x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
    with 17 stored elements in Compressed Sparse Row format&gt;
</code></pre>
<p>You can convert the sparse array to a NumPy array via <code>.toarray()</code> or <code>.A</code>:</p>
<pre class=""lang-python prettyprint-override""><code>&gt;&gt;&gt; pairwise_similarity.toarray()                                                                                                                                                                                                                            
array([[1.        , 0.17668795, 0.27056873, 0.        , 0.        ],
       [0.17668795, 1.        , 0.15439436, 0.        , 0.        ],
       [0.27056873, 0.15439436, 1.        , 0.19635649, 0.16815247],
       [0.        , 0.        , 0.19635649, 1.        , 0.54499756],
       [0.        , 0.        , 0.16815247, 0.54499756, 1.        ]])
</code></pre>
<p>Let's say we want to find the document most similar to the final document, &quot;The scikit-learn docs are Orange and Blue&quot;.  This document has index 4 in <code>corpus</code>.  You can find the index of the most similar document by <strong>taking the argmax of that row, but first you'll need to mask the 1's, which represent the similarity of each document to itself</strong>.  You can do the latter through <code>np.fill_diagonal()</code>, and the former through <code>np.nanargmax()</code>:</p>
<pre class=""lang-python prettyprint-override""><code>&gt;&gt;&gt; import numpy as np     
                                                                                                                                                                                                                                  
&gt;&gt;&gt; arr = pairwise_similarity.toarray()     
&gt;&gt;&gt; np.fill_diagonal(arr, np.nan)                                                                                                                                                                                                                            
                                                                                                                                                                                                                 
&gt;&gt;&gt; input_doc = &quot;The scikit-learn docs are Orange and Blue&quot;                                                                                                                                                                                                  
&gt;&gt;&gt; input_idx = corpus.index(input_doc)                                                                                                                                                                                                                      
&gt;&gt;&gt; input_idx                                                                                                                                                                                                                                                
4

&gt;&gt;&gt; result_idx = np.nanargmax(arr[input_idx])                                                                                                                                                                                                                
&gt;&gt;&gt; corpus[result_idx]                                                                                                                                                                                                                                       
'I prefer scikit-learn to Orange'
</code></pre>
<p>Note: the purpose of using a sparse matrix is to save (a substantial amount of space) for a large corpus &amp; vocabulary.  Instead of converting to a NumPy array, you could do:</p>
<pre><code>&gt;&gt;&gt; n, _ = pairwise_similarity.shape                                                                                                                                                                                                                         
&gt;&gt;&gt; pairwise_similarity[np.arange(n), np.arange(n)] = -1.0
&gt;&gt;&gt; pairwise_similarity[input_idx].argmax()                                                                                                                                                                                                                  
3
</code></pre>
"
Getting word-level encodings from sub-word tokens encodings,"<p>I'm looking into using a pretrained BERT ('bert-base-uncased') model to extract contextualised word-level encodings from a bunch sentences.</p>

<p>Wordpiece tokenisation breaks down some of the words in my input into subword units. Possibly a trivial question, but I was wondering what would be the most sensible way to combine output encodings for subword tokens into word-level encodings.</p>

<p>Is averaging subword encodings a reasonable way to go? If not, is there any better alternative?</p>
","nlp, tokenize, bert-language-model, huggingface-transformers","<p>Intuitively, your problem seems similar to ""how to get a good sentence representation"", with the exception that these days you could also use a classification token of a sentence to get a sentence representation in most transformer-based models. Such token is not available for token-level representations, though.</p>

<p>In your case, I think there are a few options but from what I've seen, people most often use either an average or a max value. In other words: take the average of your subword units, or take the max values. Averaging is the most intuitive place to start, in my opinion.</p>

<p>Note that averages are only just that, an average over a sequence. This implies that it is not super accurate (one high and one low value will have the same mean as two medium values), but it's probably the most straightforward.</p>
"
How to normalize ingredient names in a recipe dataset and handle NOUN + NOUN cases using spaCy in python?,"<p>I'm working on normalizing ingredient names from a recipe dataset using Python and spaCy. My goal is to extract only the relevant ingredients and ignore measurement units, fractions, and other unnecessary details. For instance, if I have a string like: &quot;5 tablespoons butter, divided&quot;. I want to extract &quot;butter&quot; as the normalized ingredient. However, i struggle to parse the string &quot;8 cups broccoli florets&quot; - it loses &quot;broccoli&quot; and outputs only &quot;floret&quot;. Other strings, like &quot;3 cups chicken broth&quot;, work fine and output &quot;chicken broth&quot;.</p>
<p><strong>My final version of the function is:</strong></p>
<pre><code>`def normalize_ingredient(ingredient_text):
    doc = nlp(ingredient_text)

    # Set of measurement units to exclude
    measurement_units = {
        &quot;cup&quot;, &quot;teaspoon&quot;, &quot;tablespoon&quot;, &quot;tablespoons&quot;, &quot;gram&quot;, &quot;ounce&quot;, &quot;pound&quot;, &quot;can&quot;,
        &quot;clove&quot;, &quot;pinch&quot;, &quot;dash&quot;, &quot;quart&quot;, &quot;liter&quot;, &quot;milliliter&quot;, &quot;gallon&quot;,
        &quot;stick&quot;, &quot;rib&quot;, &quot;head&quot;, &quot;package&quot;, &quot;inch&quot;, &quot;piece&quot;, &quot;fluid&quot;, &quot;container&quot;,
        &quot;jar&quot;, &quot;loaf&quot;, &quot;bottle&quot;, &quot;pack&quot;, &quot;pint&quot;, &quot;cube&quot;, &quot;stalk&quot;, &quot;slice&quot;, &quot;bulb&quot;,
        &quot;strip&quot;, &quot;packet&quot;, &quot;envelope&quot;, &quot;box&quot;, &quot;bag&quot;, &quot;carton&quot;, &quot;sprig&quot;, &quot;leaf&quot;,
        &quot;half&quot;, &quot;purpose&quot;, &quot;pound&quot;, &quot;ounce&quot;, &quot;gram&quot;, &quot;milliliter&quot;, &quot;liter&quot;, &quot;gallon&quot;,
        &quot;quart&quot;, &quot;pint&quot;, &quot;dash&quot;, &quot;pinch&quot;, &quot;clove&quot;, &quot;can&quot;, &quot;package&quot;, &quot;container&quot;,
        &quot;jar&quot;, &quot;loaf&quot;, &quot;bottle&quot;, &quot;pack&quot;, &quot;cube&quot;, &quot;stalk&quot;, &quot;bulb&quot;, &quot;strip&quot;, &quot;packet&quot;,
        &quot;envelope&quot;, &quot;box&quot;, &quot;bag&quot;, &quot;carton&quot;, &quot;sprig&quot;, &quot;leaf&quot;, &quot;fluid&quot;, &quot;inch&quot;, &quot;piece&quot;, &quot;cup&quot;,
        &quot;bite&quot;, &quot;size&quot;, &quot;bunch&quot;, &quot;cups&quot;,&quot;all&quot;, &quot;sized&quot;, &quot;chunks&quot;
    }

    # Set of fractions to exclude
    fractions = {'½', '¼', '¾', '⅓', '⅔', '⅛', '⅜', '⅝', '⅞', '⅙', '⅚', '®'}

    # List to store relevant terms
    relevant_terms = []

    for token in doc:
        # Skip numbers, fractions, and measurement units
        if (token.like_num or
                token.text in fractions or
                token.lemma_.lower() in measurement_units):
            continue

            # Focus on nouns, proper nouns, and adjectives that modify nouns
        if token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;, &quot;ADJ&quot;}:
            if token.pos_ == &quot;ADJ&quot; and token.head.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:
                relevant_terms.append(token.lemma_.lower())
            elif token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:
                relevant_terms.append(token.lemma_.lower())

    return &quot; &quot;.join(relevant_terms)`
</code></pre>
<p><strong>This skips &quot;broccoli&quot; for unknown for me reasons (this is the one and only incorrectly parsed string for now).
I also tried this approach with compounds:</strong></p>
<pre><code>`if token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;, &quot;ADJ&quot;}:
            
            if token.pos_ == &quot;ADJ&quot; and token.head.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:
                relevant_terms.append(f&quot;{token.lemma_.lower()} {token.head.lemma_.lower()}&quot;)
            elif token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:
                compound = [t for t in token.children if t.dep_ == &quot;compound&quot;]
                if compound:
                    combined = &quot; &quot;.join([t.lemma_.lower() for t in compound] + [token.lemma_.lower()])
                    relevant_terms.append(combined)
                else:
                    relevant_terms.append(token.lemma_.lower())`
</code></pre>
<p><strong>This messed up all the logic and didn't exclude some measurement units.</strong></p>
<p><strong>Finally, i would like to avoid adding special case with broccoli like this:</strong></p>
<pre><code>        if token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;, &quot;ADJ&quot;} or token.lemma_.lower() == &quot;broccoli&quot;:
            if token.pos_ == &quot;ADJ&quot; and token.head.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;}:
                relevant_terms.append(token.lemma_.lower())
            elif token.pos_ in {&quot;NOUN&quot;, &quot;PROPN&quot;} or token.lemma_.lower() == &quot;broccoli&quot;:
                relevant_terms.append(token.lemma_.lower())
</code></pre>
","python, nlp, spacy",
Comparing the similarity of spoken and written form text,"<p>I'm converting spoken form text to its written form. For example, &quot;he owes me two-thousand dollars&quot; should be converted to &quot;he owes me $2,000&quot; . I want an automatic check, to judge if the conversion was right or not. Can i use sentence transformers to compare the embeddings of &quot;two-thousand dollars&quot; to &quot;$2,000&quot; to check if the spoken to written conversion was right? For example, if the cosine similarity of the embeddings is close to 1, that would mean right conversion. Is there any other better way to do this?</p>
","nlp, large-language-model",
My model&#39;s loss value decreases slowly .how to reduce my loss faster while training?,"<p>when I train the model the loss decrease from 0.9 to 0.5 in 2500 epochs. Is it normal? </p>

<p>my model:</p>

<pre><code>    model = Sequential()

    model.add(Embedding(vocab_size , emd_dim, weights=[emd_matrix], input_length=maxLen,trainable=False))

    model.add(LSTM(256,return_sequences=True,activation=""relu"",kernel_regularizer=regularizers.l2(0.01),kernel_initializer=keras.initializers.glorot_normal(seed=None)))
    model.add(LSTM(256,return_sequences=True,activation=""relu"",kernel_regularizer=regularizers.l2(0.01),kernel_initializer=keras.initializers.glorot_normal(seed=None)))

    model.add(LSTM(256,return_sequences=False,activation=""relu"",kernel_regularizer=regularizers.l2(0.01),kernel_initializer=keras.initializers.glorot_normal(seed=None)))
    model.add(Dense(l_h2i,activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=""adam"", metrics=['accuracy'])
    filepath = ""F:/checkpoints/""+modelname+""/lstm-{epoch:02d}-{loss:0.3f}-{acc:0.3f}-{val_loss:0.3f}-{val_acc:0.3f}.hdf5""
    checkpoint = ModelCheckpoint(filepath, monitor=""loss"", verbose=1, save_best_only=True, mode='min')
    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=2, min_lr=0.000001)
    print(model.summary())
    history=model.fit(X_train_indices, Y_train_oh, batch_size=batch_size ,
                      epochs=epochs , validation_split=0.1, shuffle=True,
                      callbacks=[checkpoint, reduce_lr])
</code></pre>

<p>some part of the results are as shown here : </p>

<pre><code>loss improved from 0.54275 to 0.54272
loss: 0.5427 - acc: 0.8524 - val_loss: 1.1198 - val_acc: 0.7610

loss improved from 0.54272 to 0.54268
loss: 0.5427 - acc: 0.8525 - val_loss: 1.1195 - val_acc: 0.7311

loss improved from 0.54268 to 0.54251
loss: 0.5425 - acc: 0.8519 - val_loss: 1.1218 - val_acc: 0.7420

loss improved from 0.54251 to 0.54249
loss: 0.5425 - acc: 0.8517 - val_loss: 1.1210 - val_acc: 0.7518
</code></pre>
","tensorflow, keras, neural-network, deep-learning, nlp",
How can I transform verbs from present tense to past tense with using NLP library?,"<h1>What I would like to do</h1>
<p>I would like to transform verbs from  present tense to past tense with using NLP library like below.</p>
<pre><code>As she leaves the kitchen, his voice follows her.

#output
As she left the kitchen, his voice followed her.
</code></pre>
<h1>Problem</h1>
<p>There is no way to transform from present tense to past tense.</p>
<p>I've checked the following similar question, but they only introduced the way to transform from
past tense to present tense.</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/3753021/using-nltk-and-wordnet-how-do-i-convert-simple-tense-verb-into-its-present-pas"">Using NLTK and WordNet; how do I convert simple tense verb into its present, past or past participle form?</a></li>
</ul>
<h1>What I tried to do</h1>
<p>I was able to transform verbs from past tense to present tense using <a href=""https://spacy.io/usage/linguistic-features"" rel=""nofollow noreferrer"">spaCy</a>.
However, there is no way to do the same thing from present tense to past tense.</p>
<pre class=""lang-py prettyprint-override""><code>text = &quot;As she left the kitchen, his voice followed her.&quot;
doc_dep = nlp(text)
for i in range(len(doc_dep)):
    token = doc_dep[i]
    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_) 
    if token.pos_== 'VERB':
        print(token.text)
        print(token.lemma_)
        text = text.replace(token.text, token.lemma_)
print(text)

#output
'As she leave the kitchen, his voice follow her.'
</code></pre>
<h1>Development Environment</h1>
<p>Python 3.7.0</p>
<p>spaCy version 2.3.1</p>
","python, python-3.x, nlp, stanford-nlp, spacy","<p>As far as I know spaCy does not have any built-in function for this type of transformation, but you can use an extension where you map present/past tense pairs, and where you don't have the appropriate pairs 'ed' suffix for the past participle of weak verbs as below:</p>
<pre><code>verb_map = {'leave': 'left'}

def make_past(token):
    return verb_map.get(token.text, token.lemma_ + 'ed')

spacy.tokens.Token.set_extension('make_past', getter=make_past, force=True)

text = &quot;As she leave the kitchen, his voice follows her.&quot;
doc_dep = nlp(text)
for i in range(len(doc_dep)):
    token = doc_dep[i]
    if token.tag_ in ['VBP', 'VBZ']:
        print(token.text, token.lemma_, token.pos_, token.tag_) 
        text = text.replace(token.text, token._.make_past)
print(text)
</code></pre>
<p>Output:</p>
<pre><code>leave leave VERB VBP
follows follow VERB VBZ
As she left the kitchen, his voice followed her.
</code></pre>
"
"How can I make sentence-BERT throw an exception if the text exceeds max_seq_length, and what is the max possible max_seq_length for all-MiniLM-L6-v2?","<p>I'm using sentence-BERT from Huggingface in the following way:</p>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
model.max_seq_length = 512
model.encode(text)
</code></pre>
<p>When <code>text</code> is long and contains more than 512 tokens, it does not throw an exception. I assume it automatically truncates the input to 512 tokens.</p>
<p>How can I make it throw an exception when the input length is larger than <code>max_seq_length</code>?</p>
<p>Further, what is the maximum possible <code>max_seq_length</code> for <code>all-MiniLM-L6-v2</code>?</p>
","nlp, huggingface-transformers, bert-language-model, huggingface-tokenizers, sentence-transformers",
Upserting in Pinecone takes too long,"<p>I'm trying to upsert reviews that i've scraped into pinecone. For the embedding model im using <code>jina-embedding-v3</code>. For 204 reviews this takes around <strong>2.5 hours!</strong> in Colab. Tried using GPU but the embeddings arent using GPU.
Am i doing something wrong? Is there a way that i can speed up the process? The code is below:</p>
<p>Initialising DB:</p>
<pre><code>if index_name not in pc.list_indexes().names():
  pc.create_index(
    name=index_name,
    dimension=1024,
    metric=&quot;cosine&quot;,
    spec=ServerlessSpec(
        cloud=&quot;aws&quot;,
        region=&quot;us-east-1&quot;
    )
)
</code></pre>
<p>Embedding &amp; Upserting:</p>
<pre><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Load the Jina embedding model and tokenizer from Hugging Face
model_name = &quot;jinaai/jina-embeddings-v3&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, trust_remote_code=True)

from langchain.text_splitter import SpacyTextSplitter
text_splitter = SpacyTextSplitter(chunk_size=500)

# Function to generate embeddings
def generate_embeddings(text, task='retrieval.passage'):
    return model.encode(text, convert_to_tensor=True, task=task).numpy()

for review_id, review in enumerate(all_reviews[:2]):
    chunks = text_splitter.split_text(review)

    for chunk_index, chunk in enumerate(chunks):
        embedding = generate_embeddings(chunk)

        unique_id = f&quot;{review_id}_{chunk_index}&quot;

        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}

        index.upsert([(unique_id, embedding, metadata)])

# Generate and store embeddings
for review_id, review in enumerate(all_reviews):
    chunks = text_splitter.split_text(review)

    for chunk_index, chunk in enumerate(chunks):
        embedding = generate_embeddings(chunk)

        unique_id = f&quot;{review_id}_{chunk_index}&quot;

        metadata = {&quot;review_id&quot;: review_id, &quot;chunk_index&quot;: chunk_index, &quot;text&quot;: chunk}

        index.upsert([(unique_id, embedding, metadata)])
</code></pre>
","python, nlp, rag, pinecone",
How can I run an input file on the porter stemmer java code?,"<p>How can I run an input file
on the porter stemmer java code?</p>
","java, nlp, porter-stemmer",
Calculate the gradient with respect to attention but also the FFN layers for a pre-trained LLMs,"<p>I would like to return the gradient with respect to specific layers and the FFN layer in the Transformer architecture of pre-trained LLMs from the hugging-face model. Is that even possible?</p>
<p>I am working with the code of this <a href=""https://github.com/kristosh/xAI/blob/main/attn_vizualizations.py"" rel=""nofollow noreferrer"">repo</a> which is the following:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/Phi-3-medium-4k-instruct&quot;)
model = AutoModelForCausalLM.from_pretrained(
    &quot;microsoft/Phi-3-medium-4k-instruct&quot;,  # note: check spelling if you get error
    device_map=&quot;auto&quot;,
    torch_dtype=torch.float16,            # or torch.float32 if preferred
    trust_remote_code=True
)

# Create a pipeline
generator = pipeline(
    &quot;text-generation&quot;,
    model = model,
    tokenizer = tokenizer,
    return_full_text= False,
    max_new_tokens = 100,
    do_sample = False
)

# Prepare a prompt
prompt = &quot;Whats is the co-capital of Greece according to the country's public opinion?&quot;
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)
inputs = inputs.to(&quot;cuda:0&quot;)  # send inputs to cuda

# Run the model with attention outputs enabled
# Make sure to pass output_attentions=True
outputs = model(input_ids=inputs.input_ids, output_attentions=True)

# outputs.attentions is a tuple with one element per layer
# Each element is a tensor of shape (batch_size, num_heads, seq_len, seq_len)
attentions = outputs.attentions

# Generate output
output = generator(prompt)
print(output[0][&quot;generated_text&quot;])
</code></pre>
<p>How to return the gradient concerning the input or a specific attention layer (in a similar fashion with <code>grad-CAM</code> in <code>CNN</code>). Is it possible to do that in <code>transformers</code>?</p>
","python, pytorch, nlp, large-language-model",
Difference between TfIdf vectorizer and kwx,"<p>I am looking at querying a text database based on keywords and getting the relevant chunks. I want to know what is the difference between python's <a href=""https://kwx.readthedocs.io/en/latest/index.html"" rel=""nofollow noreferrer"">kwx package</a> and sklearn's <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">tfidfvectoriser</a> in implementing this. Even though kwx extracts keywords and topics, looks like I can't perform a keyword based search using it, For which I anyway have to use sklearn's tfidf vectorizer. Can you please help me understand the difference and implement an elegant solution? TIA :)</p>
","nlp, large-language-model, tf-idf, tfidfvectorizer, keyword-search",
How to Download webpage as .mhtml,"<p>I am able to successfully open a URL and save the resultant page as a .html file. However, I am unable to determine how to download and save a .mhtml (Web Page, Single File).</p>

<p>My code is: </p>

<pre><code>import urllib.parse, time
from urllib.parse import urlparse
import urllib.request

url = ('https://www.example.com')

encoded_url = urllib.parse.quote(url, safe='')

print(encoded_url)

base_url = (""https://translate.google.co.uk/translate?sl=auto&amp;tl=en&amp;u="")

translation_url = base_url+encoded_url

print(translation_url)

req = urllib.request.Request(translation_url, headers={'User-Agent': 'Mozilla/6.0'})

print(req)

response = urllib.request.urlopen(req)

time.sleep(15)

print(response)

webContent = response.read()

print(webContent)

f = open('GoogleTranslated.html', 'wb')

f.write(webContent)

print(f)

f.close
</code></pre>

<p>I have tried to use wget using the details captured in this question:
<a href=""https://stackoverflow.com/questions/42966245/how-to-download-a-webpage-mhtml-format-using-wget-in-python"">How to download a webpage (mhtml format) using wget in python</a> but the details are incomplete (or I am simply unabl eto understand).</p>

<p>Any suggestions would be helpful at this stage.</p>
","python-3.x, nlp, arabic","<p>Did you try using Selenium with a Chrome Webdriver to save page?</p>

<pre><code>import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.expected_conditions import visibility_of_element_located
from selenium.webdriver.support.ui import WebDriverWait
import pyautogui

URL = 'https://en.wikipedia.org/wiki/Python_(programming_language)'
FILE_NAME = ''

# open page with selenium
# (first need to download Chrome webdriver, or a firefox webdriver, etc)
driver = webdriver.Chrome()
driver.get(URL)


# wait until body is loaded
WebDriverWait(driver, 60).until(visibility_of_element_located((By.TAG_NAME, 'body')))
time.sleep(1)
# open 'Save as...' to save html and assets
pyautogui.hotkey('ctrl', 's')
time.sleep(1)
if FILE_NAME != '':
    pyautogui.typewrite(FILE_NAME)
pyautogui.hotkey('enter')
</code></pre>
"
Word frequency algorithm for natural language processing,"<p>Without getting a degree in information retrieval, I'd like to know if there exists any algorithms for counting the frequency that words occur in a given body of text.  The goal is to get a ""general feel"" of what people are saying over a set of textual comments.  Along the lines of <a href=""http://wordle.net/"" rel=""noreferrer"">Wordle</a>.</p>

<p>What I'd like:</p>

<ul>
<li>ignore articles, pronouns, etc ('a', 'an', 'the', 'him', 'them' etc)</li>
<li>preserve proper nouns</li>
<li>ignore hyphenation, except for soft kind</li>
</ul>

<p>Reaching for the stars, these would be peachy:</p>

<ul>
<li>handling stemming &amp; plurals (e.g. like, likes, liked, liking match the same result)</li>
<li>grouping of adjectives (adverbs, etc) with their subjects (""great service"" as opposed to ""great"", ""service"")</li>
</ul>

<p>I've attempted some basic stuff using Wordnet but I'm just tweaking things blindly and hoping it works for my specific data.  Something more generic would be great.</p>
","algorithm, nlp, word-frequency","<p>You'll need not one, but several nice algorithms, along the lines of the following.</p>

<ul>
<li>ignoring pronouns is done via a <a href=""http://en.wikipedia.org/wiki/Stoplist"" rel=""noreferrer"">stoplist</a>.</li>
<li>preserving proper nouns? You mean, detecting named entities, like <em>Hoover</em> <em>Dam</em> and saying ""it's one word"" or compound nouns, like <em>programming</em> <em>language</em>? I'll give you a hint: that's tough one, but there exist libraries for both. Look for NER (Named entitiy recognition) and lexical chunking. <a href=""http://opennlp.sf.net"" rel=""noreferrer"">OpenNLP</a> is a Java-Toolkit that does both.</li>
<li>ignoring hyphenation? You mean, like at line breaks? Use regular expressions and verify the resulting word via dictionary lookup.</li>
<li>handling plurals/stemming: you can look into the <a href=""http://snowball.tartarus.org"" rel=""noreferrer"">Snowball stemmer</a>. It does the trick nicely.</li>
<li>""grouping"" adjectives with their nouns is generally a task of <a href=""http://en.wikipedia.org/wiki/Shallow_parsing"" rel=""noreferrer"">shallow parsing</a>. But if you are looking specifically for qualitative adjectives (good, bad, shitty, amazing...) you may be interested in <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Sentiment_analysis"" rel=""noreferrer"">sentiment analysis</a>. <a href=""http://alias-i.com/lingpipe"" rel=""noreferrer"">LingPipe</a> does this, and a lot more.</li>
</ul>

<p>I'm sorry, I know you said you wanted to KISS, but unfortunately, your demands aren't that easy to meet. Nevertheless, there exist tools for all of this, and you should be able to just tie them together and not have to perform any task yourself, if you don't want to. If you want to perform a task yourself, I suggest you look at stemming, it's the easiest of all.</p>

<p>If you go with Java, combine <a href=""http://lucene.apache.org"" rel=""noreferrer"">Lucene</a> with the <a href=""http://opennlp.sf.net"" rel=""noreferrer"">OpenNLP</a> toolkit. You will get very good results, as Lucene already has a stemmer built in and a lot of tutorial. The OpenNLP toolkit on the other hand is poorly documented, but you won't need too much out of it. You might also be interested in <a href=""http://nltk.sf.net"" rel=""noreferrer"">NLTK</a>, written in Python.</p>

<p>I would say you drop your last requirement, as it involves shallow parsing and will definetly not impove your results.</p>

<p>Ah, btw. the exact term of that document-term-frequency-thing you were looking for is called <a href=""http://en.wikipedia.org/wiki/Tf-idf"" rel=""noreferrer"">tf-idf</a>. It's pretty much the best way to look for document frequency for terms. In order to do it properly, you won't get around using multidimenional vector matrices.</p>

<p>... Yes, I know. After taking a seminar on IR, my respect for Google was even greater. After doing some stuff in IR, my respect for them fell just as quick, though.</p>
"
Unable to use nltk functions,"<p>I was trying to run some nltk functions on the UCI spam message dataset but ran into this problem of word_tokenize not working even after downloading dependencies.</p>
<pre><code>import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

df['text'].apply(lambda x: len(nltk.word_tokenize(x)))
</code></pre>
<p>following is the error:</p>
<pre><code>{
    &quot;name&quot;: &quot;LookupError&quot;,
    &quot;message&quot;: &quot;
**********************************************************************
  Resource punkt_tab not found.
  Please use the NLTK Downloader to obtain the resource:

  &gt;&gt;&gt; import nltk
  &gt;&gt;&gt; nltk.download('punkt_tab')
  
  For more information see: https://www.nltk.org/data.html

  Attempted to load tokenizers/punkt_tab/english/

  Searched in:
    - 'C:\\\\Users\\\\user/nltk_data'
    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\
ltk_data'
    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\share\\\
ltk_data'
    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\lib\\\
ltk_data'
    - 'C:\\\\Users\\\\user\\\AppData\\\\Roaming\\\
ltk_data'
    - 'C:\\\
ltk_data'
    - 'D:\\\
ltk_data'
    - 'E:\\\
ltk_data'
**********************************************************************
&quot;,
    &quot;stack&quot;: &quot;---------------------------------------------------------------------------
LookupError                               Traceback (most recent call last)
Cell In[1024], line 3
      1 #finding no. of words
----&gt; 3 df['text'].apply(lambda x: len(nltk.word_tokenize(x)))

File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\series.py:4915, in Series.apply(self, func, convert_dtype, args, by_row, **kwargs)
   4780 def apply(
   4781     self,
   4782     func: AggFuncType,
   (...)
   4787     **kwargs,
   4788 ) -&gt; DataFrame | Series:
   4789     \&quot;\&quot;\&quot;
   4790     Invoke function on values of Series.
   4791 
   (...)
   4906     dtype: float64
   4907     \&quot;\&quot;\&quot;
   4908     return SeriesApply(
   4909         self,
   4910         func,
   4911         convert_dtype=convert_dtype,
   4912         by_row=by_row,
   4913         args=args,
   4914         kwargs=kwargs,
-&gt; 4915     ).apply()

File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\apply.py:1427, in SeriesApply.apply(self)
   1424     return self.apply_compat()
   1426 # self.func is Callable
-&gt; 1427 return self.apply_standard()

File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\apply.py:1507, in SeriesApply.apply_standard(self)
   1501 # row-wise access
   1502 # apply doesn't have a `na_action` keyword and for backward compat reasons
   1503 # we need to give `na_action=\&quot;ignore\&quot;` for categorical data.
   1504 # TODO: remove the `na_action=\&quot;ignore\&quot;` when that default has been changed in
   1505 #  Categorical (GH51645).
   1506 action = \&quot;ignore\&quot; if isinstance(obj.dtype, CategoricalDtype) else None
-&gt; 1507 mapped = obj._map_values(
   1508     mapper=curried, na_action=action, convert=self.convert_dtype
   1509 )
   1511 if len(mapped) and isinstance(mapped[0], ABCSeries):
   1512     # GH#43986 Need to do list(mapped) in order to get treated as nested
   1513     #  See also GH#25959 regarding EA support
   1514     return obj._constructor_expanddim(list(mapped), index=obj.index)

File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\base.py:921, in IndexOpsMixin._map_values(self, mapper, na_action, convert)
    918 if isinstance(arr, ExtensionArray):
    919     return arr.map(mapper, na_action=na_action)
--&gt; 921 return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)

File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\algorithms.py:1743, in map_array(arr, mapper, na_action, convert)
   1741 values = arr.astype(object, copy=False)
   1742 if na_action is None:
-&gt; 1743     return lib.map_infer(values, mapper, convert=convert)
   1744 else:
   1745     return lib.map_infer_mask(
   1746         values, mapper, mask=isna(values).view(np.uint8), convert=convert
   1747     )

File lib.pyx:2972, in pandas._libs.lib.map_infer()

Cell In[1024], line 3, in &lt;lambda&gt;(x)
      1 #finding no. of words
----&gt; 3 df['text'].apply(lambda x: len(nltk.word_tokenize(x)))

File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\
ltk\\tokenize\\__init__.py:129, in word_tokenize(text, language, preserve_line)
    114 def word_tokenize(text, language=\&quot;english\&quot;, preserve_line=False):
    115     \&quot;\&quot;\&quot;
    116     Return a tokenized copy of *text*,
    117     using NLTK's recommended word tokenizer
   (...)
    127     :type preserve_line: bool
    128     \&quot;\&quot;\&quot;
--&gt; 129     sentences = [text] if preserve_line else sent_tokenize(text, language)
    130     return [
    131         token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)
    132     ]

File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\
ltk\\tokenize\\__init__.py:106, in sent_tokenize(text, language)
     96 def sent_tokenize(text, language=\&quot;english\&quot;):
     97     \&quot;\&quot;\&quot;
     98     Return a sentence-tokenized copy of *text*,
     99     using NLTK's recommended sentence tokenizer
   (...)
    104     :param language: the model name in the Punkt corpus
    105     \&quot;\&quot;\&quot;
--&gt; 106     tokenizer = PunktTokenizer(language)
    107     return tokenizer.tokenize(text)

File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\
ltk\\tokenize\\punkt.py:1744, in PunktTokenizer.__init__(self, lang)
   1742 def __init__(self, lang=\&quot;english\&quot;):
   1743     PunktSentenceTokenizer.__init__(self)
-&gt; 1744     self.load_lang(lang)

File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\
ltk\\tokenize\\punkt.py:1749, in PunktTokenizer.load_lang(self, lang)
   1746 def load_lang(self, lang=\&quot;english\&quot;):
   1747     from nltk.data import find
-&gt; 1749     lang_dir = find(f\&quot;tokenizers/punkt_tab/{lang}/\&quot;)
   1750     self._params = load_punkt_params(lang_dir)
   1751     self._lang = lang

File ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\
ltk\\data.py:582, in find(resource_name, paths)
    580 sep = \&quot;*\&quot; * 70
    581 resource_not_found = f\&quot;\
{sep}\
{msg}\
{sep}\
\&quot;
--&gt; 582 raise LookupError(resource_not_found)

LookupError: 
**********************************************************************
  Resource punkt_tab not found.
  Please use the NLTK Downloader to obtain the resource:

  &gt;&gt;&gt; import nltk
  &gt;&gt;&gt; nltk.download('punkt_tab')
  
  For more information see: https://www.nltk.org/data.html

  Attempted to load tokenizers/punkt_tab/english/

  Searched in:
    - 'C:\\\\Users\\\\user/nltk_data'
    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\
ltk_data'
    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\share\\\
ltk_data'
    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.1520.0_x64__qbz5n2kfra8p0\\\\lib\\\
ltk_data'
    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\
ltk_data'
    - 'C:\\\
ltk_data'
    - 'D:\\\
ltk_data'
    - 'E:\\\
ltk_data'
**********************************************************************
&quot;
}
</code></pre>
<p>I tried re installing nltk and try and download a few other dependency files but nothing works. What am I doing wrong?</p>
","python, machine-learning, nlp, nltk",
TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0] while using RF classifier?,"<p>I am learning about random forests in scikit learn and as an example I would like to use Random forest classifier for text classification, with my own dataset. So first I vectorized the text with tfidf and for classification:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier(n_estimators=10) 
classifier.fit(X_train, y_train)           
prediction = classifier.predict(X_test)
</code></pre>

<p>When I run the classification I got this:</p>

<pre><code>TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.
</code></pre>

<p>then I used the <code>.toarray()</code> for <code>X_train</code> and I got the following:</p>

<pre><code>TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]
</code></pre>

<p>From a previous <a href=""https://stackoverflow.com/questions/21689141/classifying-text-documents-with-random-forests"">question</a> as I understood I need to reduce the dimensionality of the numpy array so I do the same:</p>

<pre><code>from sklearn.decomposition.truncated_svd import TruncatedSVD        
pca = TruncatedSVD(n_components=300)                                
X_reduced_train = pca.fit_transform(X_train)               

from sklearn.ensemble import RandomForestClassifier                 
classifier=RandomForestClassifier(n_estimators=10)                  
classifier.fit(X_reduced_train, y_train)                            
prediction = classifier.predict(X_testing) 
</code></pre>

<p>Then I got this exception:</p>

<pre><code>  File ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 419, in predict
    n_samples = len(X)
  File ""/usr/local/lib/python2.7/site-packages/scipy/sparse/base.py"", line 192, in __len__
    raise TypeError(""sparse matrix length is ambiguous; use getnnz()""
TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]
</code></pre>

<p>The I tried the following:</p>

<pre><code>prediction = classifier.predict(X_train.getnnz()) 
</code></pre>

<p>And got this:</p>

<pre><code>  File ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 419, in predict
    n_samples = len(X)
TypeError: object of type 'int' has no len()
</code></pre>

<p>Two questions were raised from this: How can I use Random forests to classify correctly? and what's happening with <code>X_train</code>?. </p>

<p>Then I tried the following:</p>

<pre><code>df = pd.read_csv('/path/file.csv',
header=0, sep=',', names=['id', 'text', 'label'])



X = tfidf_vect.fit_transform(df['text'].values)
y = df['label'].values



from sklearn.decomposition.truncated_svd import TruncatedSVD
pca = TruncatedSVD(n_components=2)
X = pca.fit_transform(X)

a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.ensemble import RandomForestClassifier

classifier=RandomForestClassifier(n_estimators=10)
classifier.fit(a_train, b_train)
prediction = classifier.predict(a_test)

from sklearn.metrics.metrics import precision_score, recall_score, confusion_matrix, classification_report
print '\nscore:', classifier.score(a_train, b_test)
print '\nprecision:', precision_score(b_test, prediction)
print '\nrecall:', recall_score(b_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(b_test, prediction)
print '\n clasification report:\n', classification_report(b_test, prediction)
</code></pre>
","python, numpy, machine-learning, nlp, scikit-learn","<p>It is a bit unclear if you are passing the same data structure (type and shape) to the <code>fit</code> method and <code>predict</code> method of the classifier. Random forests will take a long time to run with a large number of features, hence the suggestion to reduce the dimensionality in the post you link to.</p>

<p>You should apply the SVD to both the training and test data so the classifier in trained on the same shaped input as the data you wish to predict for. Check the input to the fit, and the input to the predict method have the same number of features, and are both arrays rather than sparse matrices. </p>

<p><strong>updated with example:</strong>
<strong>updated to use dataframe</strong></p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vect= TfidfVectorizer(  use_idf=True, smooth_idf=True, sublinear_tf=False)
from sklearn.cross_validation import train_test_split

df= pd.DataFrame({'text':['cat on the','angel eyes has','blue red angel','one two blue','blue whales eat','hot tin roof','angel eyes has','have a cat']\
              ,'class': [0,0,0,1,1,1,0,3]})



X = tfidf_vect.fit_transform(df['text'].values)
y = df['class'].values

from sklearn.decomposition.truncated_svd import TruncatedSVD        
pca = TruncatedSVD(n_components=2)                                
X_reduced_train = pca.fit_transform(X)  

a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.ensemble import RandomForestClassifier 

classifier=RandomForestClassifier(n_estimators=10)                  
classifier.fit(a_train.toarray(), b_train)                            
prediction = classifier.predict(a_test.toarray()) 
</code></pre>

<p>Note the SVD happens before the split into training and test sets, so that the array passed to the predictor has the same <code>n</code> as the array the <code>fit</code> method is called on.</p>
"
How to Load a 4-bit Quantized VLM Model from Hugging Face with Transformers?,"<p>I’m new to quantization and working with visual language models (VLM).I’m trying to load a 4-bit quantized version of the Ovis1.6-Gemma model from Hugging Face using the transformers library. I downloaded the model from this link: <a href=""https://huggingface.co/ThetaCursed/Ovis1.6-Gemma2-9B-bnb-4bit"" rel=""nofollow noreferrer"">https://huggingface.co/ThetaCursed/Ovis1.6-Gemma2-9B-bnb-4bit</a>.</p>
<p>Here’s the code I’m using to load the model:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# Define the quantization configuration
kwargs = {
    &quot;quantization_config&quot;: BitsAndBytesConfig(
        load_in_4bit=True,
        load_in_8bit=False,
        bnb_4bit_compute_dtype=&quot;float32&quot;,
        bnb_4bit_quant_storage=&quot;uint8&quot;,
        bnb_4bit_quant_type=&quot;fp4&quot;,
        bnb_4bit_use_double_quant=False,
        llm_int8_enable_fp32_cpu_offload=False,
        llm_int8_has_fp16_weight=False,
        llm_int8_skip_modules=None,
        llm_int8_threshold=6.0
    )
}

model = AutoModelForCausalLM.from_pretrained(
    &quot;ThetaCursed/Ovis1.6-Gemma2-9B-bnb-4bit&quot;,
    trust_remote_code=True,
    **kwargs
).cuda()
</code></pre>
<p>However, I am encountering the following warnings:</p>
<pre class=""lang-bash prettyprint-override""><code>warnings.warn(_BETA_TRANSFORMS_WARNING)
Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method'].
Loading checkpoint shards: 100%|██████████| 2/2 [00:06&lt;00:00,  3.06s/it]
You shouldn't move a model that is dispatched using accelerate hooks.
</code></pre>
<p>Additionally, when I try to access the tokenizers:</p>
<pre class=""lang-py prettyprint-override""><code>text_tokenizer = model.get_text_tokenizer()
visual_tokenizer = model.get_visual_tokenizer()
</code></pre>
<p>I get the following error:</p>
<pre class=""lang-bash prettyprint-override""><code>AttributeError: 'NoneType' object has no attribute 'get_text_tokenizer'
</code></pre>
<p>How can I properly load the 4-bit quantized model without encountering these warnings?
Why am I receiving an AttributeError when trying to access the tokenizers? Does this model not support them?</p>
","python, nlp, huggingface-transformers, huggingface, quantization",
Text classification in python - (NLTK Sentence based),"<p>I need to classify text and i am using Text blob python module to achieve it.I can use either Naive Bayes classifier/Decision tree. I am concern about the below mentioned points.</p>
<ol>
<li><p>I Need to classify <strong>sentences</strong> as argument/ Not an argument. I am using two classifiers and training the model using apt data sets. My question is all about do i need to train the model with only keywords? or i can train the data set with all possible argument and non argument <strong>sample sentences</strong>? Which would be the best approach in terms of text classification accuracy and time to retrieve?</p>
</li>
<li><p>Since the classification would be either argument/not an argument, which classifier would fetch exact results? It is Naive Bayes /Decision tree/Positive Naive bayes?</p>
</li>
</ol>
","python, machine-learning, nlp, text-classification",
ValueError: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead,"<p>I am trying to run a GitHub project on my computer.<br>
<a href=""https://github.com/suryanshgupta9933/Law-GPT"" rel=""noreferrer"">GitHub Repo that I am trying to run</a>
<br>This is the code snippet that is causing errors.
<br>Steps I took for replicating the project are:</p>
<blockquote>
<ol>
<li>Cloned the repository.</li>
<li>Generated the Hugging Face access token</li>
<li>Added <code>offload_folder</code> and <code>offload_dict_state</code> after reading the Hugging Face guide to load huge models.</li>
</ol>
</blockquote>
<pre><code>def load_llm():
    &quot;&quot;&quot;
    Load the LLM
    &quot;&quot;&quot;
    # Model ID
    repo_id = 'meta-llama/Llama-2-7b-chat-hf'
    login(token=&quot;hf_xxxxxxxx&quot;)
    # Load the model
    model = AutoModelForCausalLM.from_pretrained(
        repo_id,
        device_map='auto',
        load_in_4bit=False,
        token = True,
        offload_folder = r&quot;C:\Users\DHRUV\Desktop\New folder\Law-GPT&quot;,
        offload_state_dict = True
    )

    # Load the tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        repo_id,
        use_fast=True
    )

    # Create pipeline
    pipe = pipeline(
        'text-generation',
        model=model,
        tokenizer=tokenizer,
        max_length=512
    )

    # Load the LLM
    llm = HuggingFacePipeline(pipeline=pipe)

    return llm
</code></pre>
<p>The Error I am facing, Please help:</p>
<pre><code>Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to C:\Users\DHRUV\.cache\huggingface\token
Login successful
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;C:\Users\DHRUV\Desktop\New folder\Law-GPT\app.py&quot;, line 5, in &lt;module&gt;
    chain = qa_pipeline()
  File &quot;C:\Users\DHRUV\Desktop\New folder\Law-GPT\utils.py&quot;, line 100, in qa_pipeline
    llm = load_llm()
  File &quot;C:\Users\DHRUV\Desktop\New folder\Law-GPT\utils.py&quot;, line 44, in load_llm
    model = AutoModelForCausalLM.from_pretrained(
  File &quot;C:\Users\DHRUV\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\transformers\models\auto\auto_factory.py&quot;, line 566, in from_pretrained
    return model_class.from_pretrained(
  File &quot;C:\Users\DHRUV\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\transformers\modeling_utils.py&quot;, line 3773, in from_pretrained
    dispatch_model(model, **device_map_kwargs)
  File &quot;C:\Users\DHRUV\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\accelerate\big_modeling.py&quot;, line 438, in dispatch_model
    raise ValueError(
ValueError: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.
</code></pre>
","python, nlp, large-language-model, pre-trained-model, llama",
Wordcloud is cropping text,"<p>I am using twitter API to generate sentiments. I am trying to generate a word-cloud based on tweets. </p>

<p>Here is my code to generate a wordcloud </p>

<pre><code>wordcloud(clean.tweets, random.order=F,max.words=80, col=rainbow(50), scale=c(3.5,1))
</code></pre>

<p>Result for this:</p>

<p><a href=""https://i.sstatic.net/BAfJE.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/BAfJE.png"" alt=""enter image description here""></a></p>

<p>I also tried this:</p>

<pre><code>pal &lt;- brewer.pal(8,""Dark2"")

wordcloud(clean.tweets,min.freq = 125,max.words = Inf,random.order  = TRUE,colors = pal)
</code></pre>

<p>Result for this:</p>

<p><a href=""https://i.sstatic.net/7d9bT.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/7d9bT.png"" alt=""enter image description here""></a></p>

<p>Am I missing something?</p>

<p>This is how I am getting and cleaning tweets:</p>

<pre><code>#downloading tweets
tweets &lt;- searchTwitter(""#hanshtag"",n = 5000, lang = ""en"",resultType = ""recent"")
# removing re tweets 
no_retweets &lt;- strip_retweets(tweets , strip_manual = TRUE)

#converts to data frame
df &lt;- do.call(""rbind"", lapply(no_retweets , as.data.frame))

#remove odd characters
df$text &lt;- sapply(df$text,function(row) iconv(row, ""latin1"", ""ASCII"", sub="""")) #remove emoticon
df$text = gsub(""(f|ht)tp(s?)://(.*)[.][a-z]+"", """", df$text) #remove URL
sample &lt;- df$text


    # Cleaning Tweets 
    sum_txt1 &lt;- gsub(""(RT|via)((?:\\b\\w*@\\w+)+)"","""",sample)
    sum_txt2 &lt;- gsub(""http[^[:blank:]]+"","""",sum_txt1)
    sum_tx3 &lt;- gsub(""@\\w+"","""",sum_txt2)
    sum_tx4 &lt;- gsub(""[[:punct:]]"","" "", sum_tx3)
    sum_tex5 &lt;- gsub(""[^[:alnum:]]"", "" "", sum_tx4)
    sum_tx6 &lt;- gsub(""RT  "","""", sum_tex5)

    # WordCloud

    # data frame is not good for text convert it corpus
    corpus &lt;- Corpus(VectorSource(sum_tx6))
    clean.tweets&lt;- tm_map(corpus , content_transformer(tolower)) #converting everything to lower cases
    clean.tweets&lt;- tm_map(guj_clean,removeWords, stopwords(""english"")) #stopword are words like of, the, a, as..
    clean.tweets&lt;- tm_map(guj_clean, removeNumbers)
    clean.tweets&lt;- tm_map(guj_clean, stripWhitespace)
</code></pre>

<p>Thanks in advance!</p>
","r, text-analysis, word-cloud, sttwitterapi",
How to &quot;Create environment variable file with: touch .env for configuration (in project root).&quot;,"<p>basically just don't understand what this line means, did everything else already <a href=""https://github.com/adityasarvaiya/Automatic_Question_Generation#environment-variables"" rel=""nofollow noreferrer"">https://github.com/adityasarvaiya/Automatic_Question_Generation#environment-variables</a></p>
","python, nlp, stanford-nlp",
What is nlp in spacy?,"<p>Usually we start from:</p>
<pre><code>nlp = spacy.load('en_encore_web_sm') # or medium, or large
</code></pre>
<p>or</p>
<pre><code>nlp = English()
</code></pre>
<p>then:</p>
<pre><code>doc = nlp('my text')
</code></pre>
<p>Then we can do a lot of fun with that even not knowing the nature of the first line.</p>
<p>But what exactly is 'nlp'? What is going on under the hood? Is &quot;nlp&quot; a pretrained model, as understood in machine learning, and therefore some big file located somewhere on the disc?</p>
<p>I met an explanation, that 'nlp' is an 'object, containing process pipeline', but that only explains a little.</p>
","python-3.x, machine-learning, nlp, spacy",
MITIE library for NLP,"<p>I am trying to understand how MITIE is integrated with Rasa. I wanted to know what exactly the MITIE file total_word_feature_extractor.dat contain? I dont find any good documentation about this.</p>
","python, neural-network, nlp, rasa-nlu","<p>If you poke around deep enough in the <a href=""https://github.com/mit-nlp/MITIE"" rel=""nofollow noreferrer"">MITIE repo</a>'s on Github you can find your answer. For example here is a <a href=""https://github.com/mit-nlp/MITIE/issues/31#issuecomment-168979001"" rel=""nofollow noreferrer"">bit of information</a> about what goes into that file.</p>

<blockquote>
  <p>As for what's inside, yes, it's a variant of word2vec based on the two step CCA method from this paper: <a href=""http://icml.cc/2012/papers/763.pdf"" rel=""nofollow noreferrer"">http://icml.cc/2012/papers/763.pdf</a>. I also upgraded it to include something that is similar to the CCA method but works on out of sample words by analyzing their morphology to produce a word vector. This significantly improved the results on datasets containing lots of words not in the original dictionary.</p>
</blockquote>

<p>As far as how MITIE integrates into Rasa, it is one of a <a href=""http://rasa-nlu.readthedocs.io/en/latest/installation.html#first-alternative-mitie"" rel=""nofollow noreferrer"">few backend choices</a> for Rasa. It provides a <a href=""http://rasa-nlu.readthedocs.io/en/latest/pipeline.html#pre-configured-pipelines"" rel=""nofollow noreferrer"">few pipeline components</a> that can do both intent classification and NER. Both of which use an SVM and use the <code>total_word_feature_extractor.dat</code> to provide the individual word vectors.</p>
"
Information extraction in Python,"<p>I am attempting to extract this type of information from the following paragraph structure:</p>
<pre><code> women_ran men_ran kids_ran walked
         1       2        1      3
         2       4        3      1
         3       6        5      2

text = [&quot;On Tuesday, one women ran on the street while 2 men ran and 1 child ran on the sidewalk. Also, there were 3 people walking.&quot;, &quot;One person was walking yesterday, but there were 2 women running as well as 4 men and 3 kids running.&quot;, &quot;The other day, there were three women running and also 6 men and 5 kids running on the sidewalk. Also, there were 2 people walking in the park.&quot;]
</code></pre>
<p>I am using Python's <code>spaCy</code> as my NLP library. What would be the best way to extract this tabular information from such sentences?</p>
<p>If it was simply a matter of identifying whether there were individuals running or walking, I would just use <code>sklearn</code> to fit a classification model, but the information that I need to extract is obviously more granular than that (I am trying to retrieve subcategories and values for each).</p>
","python, nlp, information-extraction, spacy","<p>You'll want to use the dependency parse for this. You can see a visualisation of your example sentence using <a href=""https://demos.explosion.ai/displacy/?text=On%20Tuesday%2C%20one%20women%20ran%20on%20the%20street%20while%202%20men%20ran%20and%201%20child%20ran%20on%20the%20sidewalk.%20Also%2C%20there%20were%203%20people%20walking.&amp;model=en&amp;cpu=1&amp;cph=0"" rel=""noreferrer"">the displaCy visualiser</a>.</p>

<p>You could implement the rules you need a few different ways — much like how there are always multiple ways to write an XPath query, DOM selector, etc.</p>

<p>Something like this should work:</p>

<pre><code>nlp = spacy.load('en')
docs = [nlp(t) for t in text]
for i, doc in enumerate(docs):
    for j, sent in enumerate(doc.sents):
        subjects = [w for w in sent if w.dep_ == 'nsubj']
        for subject in subjects:
            numbers = [w for w in subject.lefts if w.dep_ == 'nummod']
            if len(numbers) == 1:
                print('document.sentence: {}.{}, subject: {}, action: {}, numbers: {}'.format(i, j, subject.text, subject.head.text, numbers[0].text))
</code></pre>

<p>For your examples in <code>text</code> you should get:</p>

<pre><code>document.sentence: 0.0, subject: men, action: ran, numbers: 2
document.sentence: 0.0, subject: child, action: ran, numbers: 1
document.sentence: 0.1, subject: people, action: walking, numbers: 3
document.sentence: 1.0, subject: person, action: walking, numbers: One
</code></pre>
"
Compare two phrases using WordNet?,"<p>I am trying to compare the semantic of two phrases.
In Python I am using nltk and difflib.
First I am removing the stop words from the phrases, then I am using WordNetLemmatizer and PorterStemmer to normalise the words then I am comparing the rest with the SequenceMatcher of difflib.
I still think that there is a much better way than using difflib.
Any suggestion or propostion?
Is there any library that use Wordnet in the comparision between phrases?
Is the steps I am making are correct?</p>
","nlp, nltk, wordnet, difflib, semantic-analysis","<p>In short, no, you cannot do this sort of semantics with NLTK. And using Wordnet will simply not work because most sentences contain words that are not in the database. The current way to approximate sentential semantics involves distributional techniques (word space models).</p>

<p>If you are a python programmer, scikit-learn and Gensim give you the functionality you want by means of Latent Semantic Analysis (LSA, LSI) and Latent Dirichlet Allocation (LDA). See the answers to <a href=""https://stackoverflow.com/questions/6593030/what-are-some-good-ways-of-estimating-approximate-semantic-similarity-between"">this previous question</a>. In Java, I would suggest you to try the excellent <a href=""https://github.com/fozziethebeat/S-Space/"" rel=""nofollow noreferrer"">S-Space package</a>.</p>

<p>However, most models will give you a strictly word-based representation. Combining the semantics of words into larger structures is much more difficult, unless you assume that phrases and sentences are bags-of-words (and thus, missing the difference between e.g. <em>Mary loves Kate</em> and <em>Kate loves Mary</em>.</p>
"
semantic similarity between sentences,"<p>I'm doing a project. I need any opensource tool or technique to find the semantic similarity of two sentences, where I give two sentences as an input, and receive score (i.e.,semantic similarity) as an output. Any help?</p>
","java, machine-learning, nlp",
Understanding word2vec text representation,"<p>I would like to implement the <code>distance</code> part of word2vec in my program. Unfortunately it's not in C/C++ or Python, but first I did not understand the non-binary representation. This is how I get the file
<code>./word2vec -train text8-phrase -output vectorsphrase.txt -cbow 0 -size 300 -window 10 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 0</code></p>

<p>When I check vectorsphrase.txt file for france all I get is:</p>

<pre><code> france -0.062591 0.264201 0.236335 -0.072601 -0.094313 -0.202659 -0.373314 0.074684 -0.262307 0.139383 -0.053648 -0.154181 0.126962 0.432593 -0.039440 0.108096 0.083703 0.148991 0.062826 0.048151 0.005555 0.066885 0.004729 -0.013939 -0.043947 0.057280 -0.005259 -0.223302 0.065608 -0.013932 -0.199372 -0.054966 -0.026725 0.012510 0.076350 -0.027816 -0.187357 0.248191 -0.085087 0.172979 -0.116789 0.014136 0.131571 0.173892 0.316052 -0.045492 0.057584 0.028944 -0.193623 0.043965 -0.166696 0.111058 0.145268 -0.119645 0.091659 0.056593 0.417965 -0.002927 -0.081579 -0.021356 0.030447 0.052507 -0.109058 -0.011124 -0.136975 0.104396 0.069319 0.030266 -0.193283 -0.024614 -0.025636 -0.100761 0.032366 0.069175 0.200204 -0.042976 -0.045123 -0.090475 0.090071 -0.037075 0.182373 0.151529 0.080198 -0.024067 -0.196623 -0.204863 0.154429 -0.190242 -0.063265 -0.323000 -0.109863 0.102366 -0.085017 0.198042 -0.033342 0.119225 0.176891 0.214628 0.031771 0.168739 0.063246 -0.147353 -0.003526 0.138835 -0.172460 -0.133294 -0.369451 0.063572 0.076098 -0.116277 0.208374 0.015783 0.145032 0.090530 -0.090470 0.109325 0.119184 0.024838 0.101194 -0.184154 -0.161648 -0.039929 0.079321 0.029462 -0.016193 -0.005485 0.197576 -0.118860 0.019042 -0.137174 -0.047933 -0.008472 0.092360 0.165395 0.013253 -0.099013 -0.017355 -0.048332 -0.077228 0.034320 -0.067505 -0.050190 -0.320440 -0.040684 -0.106958 -0.169634 -0.014216 0.225693 0.345064 0.135220 -0.181518 -0.035400 -0.095907 -0.084446 0.025784 0.090736 -0.150824 -0.351817 0.174671 0.091944 -0.112423 -0.140281 0.059532 0.002152 0.127812 0.090834 -0.130366 -0.061899 -0.280557 0.076417 -0.065455 0.205525 0.081539 0.108110 0.013989 0.133481 -0.256035 -0.135460 0.127465 0.113008 0.176893 -0.018049 0.062527 0.093005 -0.078622 -0.109232 0.065856 0.138583 0.097186 -0.124459 0.011706 0.113376 0.024505 -0.147662 -0.118035 0.129616 0.114539 0.165050 -0.134871 -0.036298 -0.103552 -0.108726 0.025950 0.053895 -0.173731 0.201482 -0.198697 -0.339452 0.166154 -0.014059 0.022529 0.212491 -0.051978 0.057627 0.198319 0.092990 -0.171989 -0.060376 0.084172 -0.034411 -0.065443 0.054581 -0.024187 0.072550 0.113017 0.080476 -0.170499 0.148091 -0.010503 0.158095 0.111080 0.007598 0.042551 -0.161005 -0.078712 0.318305 -0.011473 0.065593 0.121385 0.087249 -0.011178 0.053639 -0.100713 0.168689 0.120121 -0.058025 -0.161788 -0.101135 -0.080533 0.120502 -0.099477 0.187640 -0.054496 0.180532 -0.097961 0.049633 -0.019596 0.145623 0.284261 0.039761 0.053866 0.089779 -0.000676 -0.081653 0.082819 0.263937 -0.141818 0.011605 -0.028248 -0.020573 0.091329 -0.080264 -0.358647 -0.134252 0.115414 -0.066107 0.150770 -0.018897 0.168325 0.111375 -0.091567 -0.152783 -0.034834 -0.418656 -0.091504 -0.134671 0.051754 -0.129495 0.230855 -0.339259 0.208410 0.191621 0.007837 -0.016602 -0.131502 -0.059481 -0.185196 0.303028 0.017646 -0.047340 
</code></pre>

<p>So apart from cosine values I don't get anything else, and when I run the distance and type france I get </p>

<h2>               Word       Cosine distance</h2>

<pre><code>            spain              0.678515
          belgium              0.665923
      netherlands              0.652428
            italy              0.633130
      switzerland              0.622323
       luxembourg              0.610033
         portugal              0.577154
           russia              0.571507
          germany              0.563291
        catalonia              0.534176
</code></pre>

<p>So, from the given probabilities, how do I link it to other words, and how do I know which one belongs to which?</p>
","nlp, word2vec",
Bertopic assign topics to data frame,"<p>I have build a topic model with Bertopic.
After getting topic how could I assign them to dataset.</p>
<p>My main aim is to convert unsupervised topic modelling to supervised multi label classification problem.</p>
","python, machine-learning, nlp, bert-language-model, topic-modeling",
What is the concept of negative-sampling in word2vec?,"<p>I'm reading the 2014 paper <em><a href=""https://arxiv.org/pdf/1402.3722v1.pdf"" rel=""nofollow noreferrer"">word2vec Explained: Deriving Mikolov et al.’s
Negative-Sampling Word-Embedding Method</a></em> (note: direct download link) and it references the concept of &quot;negative-sampling&quot;:</p>
<blockquote>
<p>Mikolov et al. present the negative-sampling approach as a more efficient
way of deriving word embeddings. While negative-sampling is based on the
skip-gram model, it is in fact optimizing a different objective.</p>
</blockquote>
<p>I have some issue understanding the concept of negative-sampling.</p>
<p><a href=""https://arxiv.org/pdf/1402.3722v1.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1402.3722v1.pdf</a></p>
<p>Can anyone explain in layman's terms what negative-sampling is?</p>
","machine-learning, nlp, word2vec","<p>The idea of <code>word2vec</code> is to maximise the similarity (dot product) between the vectors for words which appear close together (in the context of each other) in text, and minimise the similarity of words that do not. In equation (3) of the paper you link to, ignore the exponentiation for a moment. You have</p>
<pre><code>      v_c . v_w
 -------------------
   sum_i(v_ci . v_w)
</code></pre>
<p>The numerator is basically the similarity between words <code>c</code> (the context) and <code>w</code> (the target) word. The denominator computes the similarity of all other contexts <code>ci</code> and the target word <code>w</code>. Maximising this ratio ensures words that appear closer together in text have more similar vectors than words that do not. However, computing this can be very slow, because there are many contexts <code>ci</code>. Negative sampling is one of the ways of addressing this problem- just select a couple of contexts <code>ci</code> at random. The end result is that if <code>cat</code> appears in the context of <code>food</code>, then the vector of <code>food</code> is more similar to the vector of <code>cat</code> (as measures by their dot product) than the vectors of <strong>several other randomly chosen words</strong> (e.g. <code>democracy</code>, <code>greed</code>, <code>Freddy</code>), instead of <strong>all other words in language</strong>. This makes <code>word2vec</code> much much faster to train.</p>
"
Does Word2Vec has a hidden layer?,"<p>When I am reading one of papers of Tomas Mikolov: <a href=""http://arxiv.org/pdf/1301.3781.pdf"" rel=""nofollow noreferrer"">http://arxiv.org/pdf/1301.3781.pdf</a></p>

<p>I have one concern on the Continuous Bag-of-Words Model section：</p>

<blockquote>
  <p>The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged).</p>
</blockquote>

<p>I find some people mention that there is a hidden layer in Word2Vec model, but from my understanding, there is only one projection layer in that model. Does this projection layer do the same work as hidden layer?</p>

<p>The another question is that how to project input data into the projection layer? </p>

<p>""the projection layer is shared for all words (not just the projection matrix)"", what does that mean?</p>
","machine-learning, nlp, neural-network, word2vec","<p>From the <a href=""http://arxiv.org/pdf/1301.3781.pdf"" rel=""noreferrer"">original paper</a>, section 3.1, it is clear that there is no hidden layer:</p>

<blockquote>
  <p>""the first proposed architecture is similar to the feedforward NNLM
  where the non-linear hidden layer is removed and the projection layer is shared for all words"".</p>
</blockquote>

<p>With respect to your second question (what does sharing the projection layer means), it means that you consider only one single vector, which is the centroid of the vectors of all the words in context. Thus, instead of having <code>n-1</code> word vectors as input, you consider only one vector. This is why it is called Continuous <strong>Bag of Words</strong> (because word order is lost within the context of size <code>n-1</code>).</p>
"
Understanding Word2Vec&#39;s Skip-Gram Structure and Output,"<p>I have a two-fold question about the Skip-Gram model in Word2Vec:</p>
<ul>
<li><p>The first part is about structure: as far as I understand it, the Skip-Gram model is based on one neural network with one input weight matrix <strong>W</strong>, one hidden layer of size N, and C output weight matrices <strong>W'</strong> each used to produce one of the C output vectors. Is this correct?</p>
</li>
<li><p>The second part is about the output vectors: as far as I understand it, each output vector is of size V and is a result of a Softmax function. Each output vector <em>node</em> corresponds to the index of a word in the vocabulary, and the value of each node is the probability that the corresponding word occurs at that context location (for a given input word). The target output vectors are not, however, one-hot encoded, even if the training instances are. Is this correct?</p>
</li>
</ul>
<p>The way I imagine it is something along the following lines (made-up example):</p>
<p>Assuming the vocabulary ['quick', 'fox', 'jumped', 'lazy', 'dog'] and a context of C=1, and assuming that for the input word 'jumped' I see the two output vectors looking like this:</p>
<p>[0.2 <strong>0.6</strong> 0.01 0.1 0.09]</p>
<p>[0.2 0.2 0.01 0.16 <strong>0.43</strong>]</p>
<p>I would interpret this as 'fox' being the most likely word to show up before 'jumped' (p=0.6), and 'dog' being the most likely to show up after it (p=0.43).</p>
<p>Do I have this right? Or am I completely off?</p>
","vector, machine-learning, nlp, word2vec","<p>Your understanding in both parts seem to be correct, according to this paper :</p>
<p><a href=""http://arxiv.org/abs/1411.2738"" rel=""nofollow noreferrer"">http://arxiv.org/abs/1411.2738</a></p>
<p>The paper explains word2vec in detail and at the same time, keeps it very simple - it's worth a read for a thorough understanding of the neural net architecture used in word2vec.</p>
<ul>
<li>The structure of Skip Gram does use a single neural net, with input as one-hot encoded target-word and <strong>expected-output</strong> as one-hot encoded context words. After the neural-net is trained on the text-corpus, the input weight matrix <strong>W</strong>   is used as the input-vector representations of words in the corpus and the output weight matrix <strong>W'</strong> which is shared across all the <strong>C</strong> outputs (output-vectors in the terminology of the question, but avoiding that to prevent confusion with output-vector representations used next..), becomes the output-vector representations of words. Usually the output-vector representations are ignored, and the input-vector representations, <strong>W</strong> are used as the word embeddings. To get into the dimensionality of the matrices, if we assume a vocabulary size of <strong>V</strong>, size of hidden layer as <strong>N</strong>, we will have <strong>W</strong> as <strong>(V,N)</strong> matrix, with each row representing the input vector of the indexed word in the vocabulary. <strong>W'</strong> will be a <strong>(N,V)</strong> matrix, with each column representing the output vector of the indexed word. In this way we get N-dimensional vectors for words.</li>
<li>As you mentioned, each of the outputs(avoiding using the term output vector) is of size <strong>V</strong> and are the result of a softmax function, with each node in the output giving the probability of the word occurring as a context word for the given target word, resulting in the outputs not being one-hot encoded.But the expected outputs are indeed one-hot encoded, i.e in training phase, the error is computed by subtracting the one-hot encoded vector of the actual word occurring at that context position, from the neural-net output and then the weights are updated using gradient descent.</li>
</ul>
<p>Referring to the example you mentioned, with <code>C=1</code> and with a vocabulary of <code>['quick', 'fox', 'jumped', 'lazy', 'dog']</code>.</p>
<p>If the output from the skip-gram is <code>[0.2 0.6 0.01 0.1 0.09]</code>, where the correct target word is <code>'fox'</code> then error is calculated as:</p>
<pre><code>[0 1 0 0 0] - [0.2 0.6 0.01 0.1 0.09] = [-0.2 0.4 -0.01 -0.1 -0.09]
</code></pre>
<p>and the weight matrices are updated to minimize this error.</p>
"
what actually word embedding dimensions values represent?,"<p>I have a doubt in word2vec and word embedding, I have downloaded GloVe pre-trained word embedding (shape 40,000 x 50)  and using this function to extract information from that:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
def loadGloveModel(gloveFile):
    print (&quot;Loading Glove Model&quot;)
    f = open(gloveFile,'r')
    model = {}
    for line in f:
        splitLine = line.split()
        word = splitLine[0]
        embedding = np.array([float(val) for val in splitLine[1:]])
        model[word] = embedding
    print (&quot;Done.&quot;,len(model),&quot; words loaded!&quot;)
    return model
</code></pre>
<p>Now if I call this function for word <code>'python'</code>, something like:</p>
<pre class=""lang-py prettyprint-override""><code>print(loadGloveModel('glove.6B.100d.txt')['python'])
</code></pre>
<p>it gives me 1x50 shape vector like this:</p>
<pre><code>[ 0.24934    0.68318   -0.044711  -1.3842    -0.0073079  0.651
 -0.33958   -0.19785   -0.33925    0.26691   -0.033062   0.15915
  0.89547    0.53999   -0.55817    0.46245    0.36722    0.1889
  0.83189    0.81421   -0.11835   -0.53463    0.24158   -0.038864
  1.1907     0.79353   -0.12308    0.6642    -0.77619   -0.45713
 -1.054     -0.20557   -0.13296    0.12239    0.88458    1.024
  0.32288    0.82105   -0.069367   0.024211  -0.51418    0.8727
  0.25759    0.91526   -0.64221    0.041159  -0.60208    0.54631
  0.66076    0.19796   -1.1393     0.79514    0.45966   -0.18463
 -0.64131   -0.24929   -0.40194   -0.50786    0.80579    0.53365
  0.52732    0.39247   -0.29884    0.009585   0.99953   -0.061279
  0.71936    0.32901   -0.052772   0.67135   -0.80251   -0.25789
  0.49615    0.48081   -0.68403   -0.012239   0.048201   0.29461
  0.20614    0.33556   -0.64167   -0.64708    0.13377   -0.12574
 -0.46382    1.3878     0.95636   -0.067869  -0.0017411  0.52965
  0.45668    0.61041   -0.11514    0.42627    0.17342   -0.7995
 -0.24502   -0.60886   -0.38469   -0.4797   ]
</code></pre>
<p>I need help in understanding the output matrix. What does these value represent and there significance in generating new word?</p>
","machine-learning, nlp, word2vec, word-embedding",
Why does word2vec use 2 representations for each word?,"<p>I am trying to understand why word2vec's skipgram model has 2 representations for each word (the hidden representation which is the word embedding) and the output representation (also called context word embedding) . Is this just for generality where the context can be anything (not just words) or is there a more fundamental reason </p>
","machine-learning, nlp, word2vec",
how to pass additional query filters to a SelfQueryRetriever?,"<p>We are implementing a SelfQueryRetriever using OpenSearch as vectorstore, in general it works fine generating the metadata filters from the user query but we need some way to append other filters to the query, and I cannot find how to do that in the documentation, the use case is to make some things such as:</p>
<ul>
<li>The UI will have some filters that product owners want exposed as tradditional filters instead of filters to be extracted from user query.</li>
<li>There are roles and the space of possible search results for a query is restricted by the data available only for users of that role.</li>
</ul>
<p>A possible solution is to add those to the metadata fields of the SelfQueryRetriever and append a &quot;system&quot; component to the user query and let the self query retriever create the filters, but to me it does't sound so clean and intuitive.</p>
<p>How can additional filters be added to the query?</p>
","python, nlp, artificial-intelligence, langchain",
Fine-tuning TheBloke/Llama-2-13B-chat-GPTQ model with Hugging Face Transformers library throws Exllama error,"<p>I am trying to fine-tune the TheBloke/Llama-2-13B-chat-GPTQ model using the Hugging Face Transformers library. I am using a JSON file for the training and validation datasets. However, I am encountering an error related to Exllama backend when I try to run the script.</p>
<p>Here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
import torch

# Check GPU availability
print(&quot;Available GPU devices:&quot;, torch.cuda.device_count())
print(&quot;Name of the first available GPU:&quot;, torch.cuda.get_device_name(0))

# Load model and tokenizer
model_name = &quot;TheBloke/Llama-2-13B-chat-GPTQ&quot;

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Move the model to GPU
model.to('cuda')

# Load training and validation data
train_data = load_dataset('json', data_files='train_data.jsonl')
val_data = load_dataset('json', data_files='val_data.jsonl')

# Function to format the data
def formatting_func(example):
    return tokenizer(example['input'], example.get('output', ''), truncation=True, padding='max_length')

# Prepare training and validation data
train_data = train_data.map(formatting_func)
val_data = val_data.map(formatting_func)

# Set training arguments
training_args = TrainingArguments(
    output_dir=&quot;./output&quot;,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
)

# Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
)

# Start training
trainer.train()

# Save the model
model.save_pretrained(&quot;./output&quot;)

</code></pre>
<p>The error message I get is:</p>
<pre><code>ValueError: Found modules on cpu/disk. Using Exllama backend requires all the 
modules to be on GPU. You can deactivate exllama backend by setting 
`disable_exllama=True` in the quantization config object.
</code></pre>
<p>I have already moved the model to GPU using model.to('cuda'), but the error persists. Any help would be greatly appreciated.</p>
<p>I tried moving the model to the GPU using model.to('cuda') before initiating the training process, as suggested in the Hugging Face documentation. I also ensured that my environment has all the required packages and dependencies installed. I was expecting the model to fine-tune on my custom JSON dataset without any issues.</p>
<p>However, despite moving the model to the GPU, I still encounter the Exllama backend error. I am not sure why this is happening, as the model should be on the GPU as per my code. I am looking for a way to resolve this error and successfully fine-tune the model on my custom dataset.</p>
","nlp, huggingface-transformers, huggingface, llama, fine-tuning",
"How to handle German language specific characters like (&#228;, &#246;, &#252;, &#223;) while tokenizing using GPT2Tokenizer?","<p>I am working with German Texts, where I need to tokenize texts using GPT2Tokenizer.</p>
<p>To tokenize the text, I wrote the implementation as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2Tokenizer

text = &quot;zügiger Transport des ABCD stabilen Kindes in die Notaufnahme UKA&quot;
text = text.encode(&quot;utf-8&quot;).decode(&quot;utf-8&quot;)  # Re-encode to fix encoding issues

# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)

# Tokenize the text
tokens = tokenizer.tokenize(text)

print(tokens)  # Should properly tokenize &quot;zügiger&quot; instead of splitting &quot;ü&quot;
</code></pre>
<p>Now, when I execute this code snippet I get output as follows:</p>
<pre><code>['z', 'Ã¼', 'g', 'iger', 'ĠTransport', 'Ġdes', 'ĠABC', 'D', 'Ġstabil', 'en', 'ĠKind', 'es', 'Ġin', 'Ġdie', 'ĠNot', 'au', 'fn', 'ah', 'me', 'ĠUK', 'A']
</code></pre>
<p>After a bit of analysis, I have found that all German language specific characters are mis-decoded as Latin-1 see the table below.</p>
<pre class=""lang-markdown prettyprint-override""><code>| Character | UTF-8 Bytes | Misdecoded as Latin-1 | Resulting String |
|-----------|-------------|-----------------------|------------------|
| ä         | C3 A4       | Ã + ¤                 | Ã¤               |
| ö         | C3 B6       | Ã + ¶                 | Ã¶               |
| ü         | C3 BC       | Ã + ¼                 | Ã¼               |
| ß         | C3 9F       | Ã + Ÿ                 | ÃŸ               |
</code></pre>
<p>Now, how I can keep German language specific characters like (ä, ö, ü, ß) inside tokens after the tokenization process, avoiding unintentional misdecodeding, i.e. &quot;zügiger&quot; becomes something like ['z', 'ü', 'g', 'iger'].</p>
","python, nlp, tokenize, large-language-model, gpt-2",
BertScore giving me high results for unrelated sentences,"<p>I've been trying out bert_score to do some metrics. When testing it, I'm finding that it's giving me some very high scores for some totally unrelated sentences.</p>
<p>For example, I tried this:</p>
<pre class=""lang-py prettyprint-override""><code>refs = [&quot;Ladies in a kitchen laughing at the camera while one grates cheese.&quot;]
cands=[&quot;sky&quot;]
P,R,F = score(cands=cands,refs=refs,lang = 'en')
print(P,R,F)
</code></pre>
<p>The above execution returns:</p>
<pre class=""lang-none prettyprint-override""><code>tensor([0.8362]) tensor([0.8028]) tensor([0.8192])
</code></pre>
<p>I'm not sure why it's giving me these, but I would assume it's not what it should give me for those two sentences.</p>
","python, nlp, bert-language-model",
How do I remove escape characters from output of nltk.word_tokenize?,"<p>How do I get rid of non-printing (escaped) characters from the output of the nltk.word_tokenize method? I am working through the book 'Natural Language Processing with Python' and am following the code examples, which inform me that the output should consist only of words and punctuation, however I'm still getting escapes in the output.</p>
<p>Here's my code:</p>
<pre><code>from __future__ import division
import nltk, re, pprint
from urllib.request import urlopen

url = &quot;https://www.gutenberg.org/cache/epub/75394/pg75394.txt&quot;
raw = urlopen(url).read()
raw = raw.decode('utf-8')
tokens = nltk.word_tokenize(raw)
print(type(tokens))
print(len(tokens))
print(tokens[:10])
</code></pre>
<p>And the output, with the escapes visible in the first list item:
<a href=""https://i.sstatic.net/L1QJ1Mdr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/L1QJ1Mdr.png"" alt=""enter image description here"" /></a></p>
<p>I've poked around online and have a suspicion this may be to do with the fact that the book's sample code was written for Python 2, which has already caused me some encoding issues (I needed to add the line above to convert the output from bytes to a string). Am I on the right track? If not, what am I doing wrong?</p>
<p>I'm using Python 3.12.1 on Windows 11.</p>
<p>Thanks in advance - please do let me know if I can provide any further helpful information.</p>
","python, nlp, nltk, tokenize, text-processing",
Using sentence transformers with limited access to internet,"<p>I have access to the latest packages but I cannot access internet from my python enviroment.</p>
<p>Package versions that I have are as below</p>
<pre><code>huggingface-hub-0.4.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0
sentence-transformers-2.1.0 sentencepiece-0.1.96 torchvision-0.11.2

print (torch.__version__)
1.10.1+cu102
</code></pre>
<p>I went to the <a href=""https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1/tree/main"" rel=""nofollow noreferrer"">location</a> and copied all the files in a folder</p>
<pre><code>os.listdir('multi-qa-mpnet-base-dot-v1_Jan2022/')

['config_sentence_transformers.json',
 'config.json',
 'gitattributes',
 'modules.json',
 'data_config.json',
 'sentence_bert_config.json',
 'README.md',
 'special_tokens_map.json',
 'tokenizer_config.json',
 'train_script.py',
 'vocab.txt',
 'tokenizer.json',
 '1_Pooling',
 '.ipynb_checkpoints',
 '9e1e76b7a067f72e49c7f571cd8e811f7a1567bec49f17e5eaaea899e7bc2c9e']
</code></pre>
<p>Then I went to the <a href=""https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1"" rel=""nofollow noreferrer"">url</a> and tried to execute the code listed there</p>
<p>But I get below error</p>
<pre><code>model = SentenceTransformer('multi-qa-mpnet-base-dot-v1_Jan2022/')

OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory multi-qa-mpnet-base-dot-v1_Jan2022/ or `from_tf` and `from_flax` set to False.
</code></pre>
<p>Where could I get those 4 files (<code>'pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'</code>) or what else do I need to change? These files are not available at the first URL mentioned above</p>
","python, nlp, huggingface-transformers, sentence-transformers","<p>Based on the things you mentioned, I checked the source code of <code>sentence-transformers</code> on Google Colab. After running the model and getting the files, I check the directory and I saw the <code>pytorch_model.bin</code> there.</p>
<p><a href=""https://i.sstatic.net/FeiSi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FeiSi.png"" alt=""download directory"" /></a></p>
<p>And according to <code>sentence-transformers</code> code:
<a href=""https://github.com/UKPLab/sentence-transformers/blob/2158fff3aa96651b10fe367c41fdd5008a33c5c6/sentence_transformers/SentenceTransformer.py#L80"" rel=""nofollow noreferrer"">Link</a></p>
<p><a href=""https://i.sstatic.net/8VdJK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8VdJK.png"" alt=""code"" /></a></p>
<p>the <code>flax_model.msgpack</code> , <code>rust_model.ot</code>, <code>tf_model.h5</code> are getting ignored when the it is trying to download.</p>
<p>and these are the files that it downloads :</p>
<pre><code>['1_Pooling', 'config_sentence_transformers.json', 'tokenizer.json', 'tokenizer_config.json', 'modules.json', 'sentence_bert_config.json', 'pytorch_model.bin', 'special_tokens_map.json', 'config.json', 'train_script.py', 'data_config.json', 'README.md', '.gitattributes', 'vocab.txt']
</code></pre>
<p>The only thing that you have to have to load the model is <code>pytorch_model.bin</code> file. I tested with copying the modules to another directory and it worked. And according to your question, you haven't downloaded this file, so that is the problem.</p>
<p>All in all, you should download the model using its command and then move the files to another directory and initialize the <code>SentenceTransformer</code> class with that dir.</p>
<p>I wish it would be helpful.</p>
"
How to add index to python FAISS incrementally,"<p>I am using Faiss to index my huge dataset embeddings, embedding generated from bert model. I want to add the embeddings incrementally, it is working fine if I only add it with faiss.IndexFlatL2 , but the problem is while saving it the size of it is too large.
So I tried with faiss.IndexIVFPQ, but it needs to train embeddings before I add the data, so I can not add it incrementally, I have to compute all embeddings first and then train and add it, it is having issue because all the data should be kept in RAM till I write it. Is there any way to do this incrementally.
Here is my code:</p>
<pre><code>    # It is working fine when using with IndexFlatL2
    def __init__(self, sentences, model):
        self.sentences = sentences
        self.model = model
        self.index = faiss.IndexFlatL2(768)

    def process_sentences(self):
        result = self.model(self.sentences)
        self.sentence_ids = []
        self.token_ids = []
        self.all_tokens = []
        for i, (toks, embs) in enumerate(tqdm(result)):
            # initialize all_embeddings for every new sentence (INCREMENTALLY)
            all_embeddings = []
            for j, (tok, emb) in enumerate(zip(toks, embs)):
                self.sentence_ids.append(i)
                self.token_ids.append(j)
                self.all_tokens.append(tok)
                all_embeddings.append(emb)

            all_embeddings = np.stack(all_embeddings) # Add embeddings after every sentence
            self.index.add(all_embeddings)

        faiss.write_index(self.index, &quot;faiss_Model&quot;)
</code></pre>
<p>ANd when using with IndexIVFPQ:</p>
<pre><code>   def __init__(self, sentences, model):
       self.sentences = sentences
       self.model = model
       self.quantizer = faiss.IndexFlatL2(768)
       self.index = faiss.IndexIVFPQ(self.quantizer, 768, 1000, 16, 8)

   def process_sentences(self):
       result = self.model(self.sentences)
       self.sentence_ids = []
       self.token_ids = []
       self.all_tokens = []
       all_embeddings = []
       for i, (toks, embs) in enumerate(tqdm(result)):
           for j, (tok, emb) in enumerate(zip(toks, embs)):
               self.sentence_ids.append(i)
               self.token_ids.append(j)
               self.all_tokens.append(tok)
               all_embeddings.append(emb)

       all_embeddings = np.stack(all_embeddings)
       self.index.train(all_embeddings) # Train
       self.index.add(all_embeddings) # Add to index
       faiss.write_index(self.index, &quot;faiss_Model_mini&quot;)
</code></pre>
","python, python-3.x, nlp, bert-language-model, faiss",
Using an AWS service to execute a python script that will extract keywords from text using keyBERT?,"<p>I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords.</p>
<p>Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.</p>
","python, amazon-web-services, aws-lambda, nlp, large-language-model","<p>In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do:</p>
<ol>
<li>Package your KeyBERT script in a lambda and easily deploy it with a container.</li>
<li>Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so it’s cost-efficient for occasional tasks.</li>
</ol>
"
Count of Combination of bigrams,"<p>I have create a dataset as follows using bigrams</p>
<pre><code>index                   |   product_action
-------------------------------------------------------|
('customer', 'called')  |   action  
('customer', 'service') |   action
('blue', 'dress')       |   product
('the', 'service')      |   product
('to', 'complain')      |   action
('complain', 'about')   |   action
('service', 'received') |   action
('the', 'dress')        |   product
</code></pre>
<p>I want to know if in each sentence how many times the combination has occured for the entire dataset</p>
<p>I have tried to use a count on the bigram</p>
<pre class=""lang-py prettyprint-override""><code>def get_bigrams(text):
    tokens = nltk.word_tokenize(text.lower()) 
    return list(ngrams(tokens, 2))  

def count_bigrams(text, bigram):
    bigrams = get_bigrams(text)
    return bigrams.count(bigram)
</code></pre>
<p>The dataset I have in mind is as follows:</p>
<pre><code>product           |      action               |  count
---------------------------------------------------------|
('blue', 'dress') |  ('customer', 'called')   |   10
</code></pre>
","python, nlp, nltk",
OpenNLP POSTaggerME and ChunkerME synergy,"<p>I'm trying to use the OpenNLP chunking API to chunk a portuguese sentence. So, first I tokenized a sentence using <a href=""https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.tokenizer.api"" rel=""nofollow noreferrer"">TokenizerME</a>, then I tagged it with <a href=""https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.postagger.tagging.api"" rel=""nofollow noreferrer"">POSTaggerME</a>. For both I used the ready-made models provided by the project <a href=""https://opennlp.apache.org/models.html"" rel=""nofollow noreferrer"">here</a>.</p>
<p>For the sentence “Ivo viu a uva”, POSTaggerME returns the tags [PROPN, VERB, DET, NOUN]. The model seems to be using the <a href=""https://universaldependencies.org/u/pos/"" rel=""nofollow noreferrer"">UD POS Tags</a>.</p>
<p>As there is no ready-made model for ChunkerME in portuguese, I <a href=""https://opennlp.apache.org/docs/2.5.3/manual/opennlp.html#tools.corpora.arvores-deitadas"" rel=""nofollow noreferrer"">followed the instructions</a> and did the training first using the ChunkerConverter tool (to convert from &quot;arvore deitada&quot; to CoNLL2000) and then generating the model with ChunkerTrainerME tool. Everything worked well. For the sentence above, the chunker produced correct tags ([B-NP, B-VP, B-NP, I-NP]).</p>
<p>But, for more complex sentences, it hasn't produced such good results.</p>
<p>I was trying to identify what I could improve in chunker training, and one of the things I noticed is that there is a difference between the types of tags. The portuguese corpus (<a href=""https://www.linguateca.pt/Floresta/corpus.html#download"" rel=""nofollow noreferrer"">Bosque 8.0</a>) seems to be using portuguese tags. For example, instead of <strong>PROPN</strong>, the corpus uses <strong>prop</strong> and instead of <strong>DET</strong>, it uses <strong>art</strong>.</p>
<p>It seems to me that this could lead to problems, especially since one of the parameters the chunker receives is an array with UD tags, but it has been trained with another type of tag...</p>
<p>But before writing code creating a routine to convert from a portuguese notation to UD (or Penn) I wanted to ask, if</p>
<ol>
<li>this does indeed have an impact,</li>
<li>there is a tool that already does this translation and</li>
<li>there are any other suggestions for improving the chunker precision/recall.</li>
</ol>
","nlp, opennlp","<h2>Q1</h2>
<p>Yes, the chosen tag set (UD, Penn, custom) has an impact. Conversion is not possible in a bi-directional manner:</p>
<ul>
<li>Penn -&gt; UD should work well.</li>
<li>UD -&gt; Penn is not a good idea as it a lossy conversion. UD tag set are less detailed when compared to the &quot;classic' Penn tag set.</li>
</ul>
<p>Using a custom, language specific tag-set can work, but it is a matter of &quot;mapping&quot; from/to UD correctly. This might work for some tag sets and languages, for others it might be too complicated / lossy.</p>
<h2>Q2</h2>
<p>No, there isn't. The OpenNLP project takes code donations for upcoming releases, if you want to provide such a mapping/translation for PT lang.</p>
<h2>Q3</h2>
<p>This needs details/discussion on the Apache OpenNLP user and/or dev <a href=""https://opennlp.apache.org/mailing-lists.html"" rel=""nofollow noreferrer"">mailing lists</a>. Alternatively, feel free to open a <a href=""https://issues.apache.org/jira/projects/OPENNLP"" rel=""nofollow noreferrer"">Jira issue</a> if you can drill the topic down to a clear idea or proposed code addition.</p>
"
Where is the HuggingFace model saved in when loading a model on colab?,"<p>I have this code for loading a generative model. I'm not sure how to see model files in colab (i.e., config.json etc.).</p>
<pre><code>model_id = &quot;deepseek-ai/DeepSeek-R1-Distill-Llama-8B&quot;


pipeline = transformers.pipeline(
            &quot;text-generation&quot;,
            model=model_id,
            #model_kwargs={&quot;torch_dtype&quot;: torch.bfloat16, &quot;cache_dir&quot;: cache_dir},
            device_map=&quot;auto&quot;)
</code></pre>
","nlp, huggingface-transformers, huggingface",
Document Analysis and Tagging,"<p>Let's say I have a bunch of essays (thousands) that I want to tag, categorize, etc.  Ideally, I'd like to train <em>something</em> by manually categorizing/tagging a few hundred, and then let the thing loose.</p>
<p>What resources (books, blogs, languages) would you recommend for undertaking such a task?  Part of me thinks this would be a good fit for a <a href=""http://www.process.com/precisemail/bayesian_filtering.htm"" rel=""nofollow noreferrer"">Bayesian Classifier</a> or even <a href=""http://en.wikipedia.org/wiki/Latent_semantic_analysis"" rel=""nofollow noreferrer"">Latent Semantic Analysis</a>, but I'm not really familiar with either other than what I've found from a few <a href=""http://rubyforge.org/projects/bishop/"" rel=""nofollow noreferrer"">ruby</a> <a href=""http://classifier.rubyforge.org/"" rel=""nofollow noreferrer"">gems</a>.</p>
<p>Can something like this be solved by a bayesian classifier?  Should I be looking more at semantic analysis/natural language processing?  Or, should I just be looking for keyword density and mapping from there?</p>
","machine-learning, nlp, classification, bayesian, tagging","<p>Wow, that's a pretty huge topic you are venturing into :)
There is definitely a lot of books and articles you can read about it but I will try to provide a short introduction. I am not a big expert but I worked on some of this stuff.</p>

<p>First you need to decide whether you are want to classify essays into predefined topics/categories (classification problem) or you want the algorithm to decide on different groups on its own (clustering problem). From your description it appears you are interested in classification.</p>

<p>Now, when doing classification, you first need to create enough training data. You need to have a number of essays that are separated into different groups. For example 5 physics essays, 5 chemistry essays, 5 programming essays and so on. Generally you want as much training data as possible but how much is enough depends on specific algorithms. You also need verification data, which is basically similar to training data but completely separate. This data will be used to judge quality (or performance in math-speak) of your algorithm.</p>

<p>Finally, the algorithms themselves. The two I am familiar with are Bayes-based and TF-IDF based. For Bayes, I am currently developing something similar for myself in ruby, and I've documented my experiences in my blog. If you are interested, just read this - <a href=""http://arubyguy.com/2011/03/03/bayes-classification-update/"" rel=""noreferrer"">http://arubyguy.com/2011/03/03/bayes-classification-update/</a> and if you have any follow up questions I will try to answer.</p>

<p>The TF-IDF is a short for TermFrequence - InverseDocumentFrequency. Basically the idea is for any given document to find a number of documents in training set that are most similar to it, and then figure out it's category based on that. For example if document D is similar to T1 which is physics and T2 which is physics and T3 which is chemistry, you guess that D is most likely about physics and a little chemistry.</p>

<p>The way it's done is you apply the most importance to rare words and no importance to common words. For instance 'nuclei' is rare physics word, but 'work' is very common non-interesting word. (That's why it's called inverse term frequency). If you can work with Java, there is a very very good Lucene library which provides most of this stuff out of the box. Look for API for 'similar documents' and look into how it is implemented. Or just google for 'TF-IDF' if you want to implement your own</p>
"
How to use Chunker Class in OpenNLP?,"<p>The <code>ChunkerME</code> class in OpenNLP has a <code>chunk()</code> method which takes two <code>String[]</code>. The first one should be the tags (tags from part of speech tagging process) and the second one is the actual terms.</p>

<p>I'm having a tagged string in the format of <code>Sir_NNP Arthur_NNP Conan_NNP...</code> and I'd like to chunk it using the ChunkerME class. However the chunker does not accept this string as is. however the OpenNLP command line has a command (opennlp ChunkerME en-chunker.bin) which directly accepts a tagged sentence and return a chunked sentence. </p>

<p>How can I use something like the one in the command line.</p>
","nlp, opennlp, part-of-speech",
NoneType&#39; object is not iterable for Vectorizer sklearn,"<p>I have imported text data into pandas dataframe. I would like to implement Vectorizer. So i use sklearn to do tfidf and so on  </p>

<p>So the first step i did. clean the text. </p>

<pre><code>#Creating Function
from nltk.corpus import stopwords
def text_process(sms):  
nonpunc = [char for char in sms if char not in string.punctuation]
nonpunc = ''.join(nonpunc)
return[word for word in nonpunc.split() if word.lower() not in stopwords.words('english')]
</code></pre>

<p>Next</p>

<pre><code>data['sms'].head(5).apply(text_process)
</code></pre>

<p>Next</p>

<pre><code>from sklearn.feature_extraction.text import  CountVectorizer
bow_transformer = CountVectorizer(analyzer = text_process).fit(data['sms'])
</code></pre>

<p>I receive an error. </p>

<pre><code>  ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-84-f1812582c7e1&gt; in &lt;module&gt;
      1 #Step 1
      2 from sklearn.feature_extraction.text import  CountVectorizer
----&gt; 3 bow_transformer = CountVectorizer(analyzer = text_process).fit(data['sms'])

~\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py in fit(self, raw_documents, y)
    976         self
    977         """"""
--&gt; 978         self.fit_transform(raw_documents)
    979         return self
    980 

~\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py in fit_transform(self, raw_documents, y)
   1010 
   1011         vocabulary, X = self._count_vocab(raw_documents,
-&gt; 1012                                           self.fixed_vocabulary_)
   1013 
   1014         if self.binary:

~\Anaconda3\lib\site-packages\sklearn\feature_extraction\text.py in _count_vocab(self, raw_documents, fixed_vocab)
    920         for doc in raw_documents:
    921             feature_counter = {}
--&gt; 922             for feature in analyze(doc):
    923                 try:
    924                     feature_idx = vocabulary[feature]

&lt;ipython-input-82-4149ae75d7bf&gt; in text_process(sms)
      3 def text_process(sms):
      4 
----&gt; 5     nonpunc = [char for char in sms if char not in string.punctuation]
      6     nonpunc = ''.join(nonpunc)
      7     return[word for word in nonpunc.split() if word.lower() not in stopwords.words('english')]

TypeError: 'NoneType' object is not iterable
</code></pre>
","python, scikit-learn, nlp",
NLP: LayoutXLM (HF model) inference- index out of bounds: 0 &lt;= tmp30 &lt; 1L,"<p>I am getting this during inference:</p>
<pre><code>what():  index out of bounds: 0 &lt;= tmp30 &lt; 1L
</code></pre>
<p>The stack trace of the error:</p>
<pre><code>0   c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) 
</code></pre>
<p>I am training on</p>
<p>Ubuntu 22.04
nvidia-smi:
NVIDIA-SMI 545.23.08 Driver Version: 545.23.08 CUDA Version: 12.3
env:
cuda 12.1
cudnn 9.1
datasets 2.15.0
transformers 4.36.2
torch 2.4</p>
<p>Document question answering, custom dataset.</p>
<p>Model repo being trained: <a href=""https://huggingface.co/Sharka/CIVQA_DVQA_LayoutXLM"" rel=""nofollow noreferrer"">https://huggingface.co/Sharka/CIVQA_DVQA_LayoutXLM</a></p>
<p>Tokenizer:
copied from: microsoft/layoutxlm-base · Hugging Face</p>
<p>I am getting this error during evaluation (prior to training)(always with the same samle, as far as I can tell):</p>
<p>Code part:</p>
<pre><code>...
outputs = model(input_ids=input_ids, attention_mask=attention_mask,
                token_type_ids=token_type_ids, bbox=bbox, image=image, 
                start_positions=start_positions, end_positions=end_positions)
... 
</code></pre>
<p>Error message:</p>
<pre><code>5%|▌         | 5901/109877 [56:31&lt;16:12:41,  1.78it/s]terminate called after throwing an instance of 'c10::Error'
  what():  index out of bounds: 0 &lt;= tmp30 &lt; 1L
Exception raised from kernel at /tmp/torchinductor_aiteam/li/cliz2c63uoa3repoiaztoizrjecjxefsfbjltc6wzfp7p6brqesb.cpp:155 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f486a6d0f86 in /home/aiteam/miniconda3/envs/hf_layoutLM_test/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x68 (0x7f486a67fdd9 in /home/aiteam/miniconda3/envs/hf_layoutLM_test/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: &lt;unknown function&gt; + 0x432b (0x7f47aa16032b in /tmp/torchinductor_aiteam/li/cliz2c63uoa3repoiaztoizrjecjxefsfbjltc6wzfp7p6brqesb.so)
frame #3: &lt;unknown function&gt; + 0x16405 (0x7f48b92aa405 in /home/aiteam/miniconda3/envs/hf_layoutLM_test/lib/python3.9/site-packages/torch/lib/libgomp-a34b3233.so.1)
frame #4: &lt;unknown function&gt; + 0x8609 (0x7f48ba6e4609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #5: clone + 0x43 (0x7f48ba4af353 in /lib/x86_64-linux-gnu/libc.so.6)
terminate called recursively
--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*)
----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1725357799 (unix time) try &quot;date -d @1725357799&quot; if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3ea0001d000) received by PID 118784 (TID 0x7f47ff7ab700) from PID 118784 ***]
</code></pre>
<p>I heavily assume it is something with the data, with that particular sample.
I cannot see a systematic difference (obviously I am just missing it) between that sample and any other.
My feature:
“input_ids” → in range [0, 250002], which is the tokenizers’ vocab
“attention_mask” → in {0, 1}
“start_positions” 0 for this sample (subfinder didnt find the answer in the context)
“end_positions” 0 or this samples
“bbox” → normalized, all in [0, 1000]
“image” → uint8, all in range [0, 255]</p>
<p>Some (naive) direct question(s):</p>
<ol>
<li><p>Can the problem stem from start and end positions predictions being 0?</p>
</li>
<li><p>For some reason, the tokenizer is using token id==6 to tokenize the context of that sample, which, for this tokenizer, corresponds to an empty string, e.g. <code>''</code> : (from <code>tokenizer.json</code>):</p>
<p>&quot;vocab&quot;:[...,[&quot;▁&quot;,-3.9299705028533936], ...]</p>
</li>
</ol>
<p>This being token id==6 (Why is the empty string being represented by this symbol?)
However, there is no empty string the original context to begin with.</p>
<ol start=""3"">
<li>Can the problem stem from using the fast tokenizer, while the model config says &quot;LayoutXLMTokenizer&quot; ?</li>
</ol>
","string, nlp, huggingface-transformers, large-language-model, huggingface-tokenizers",
Fine-tuning a Pretrained Model with Quantization and AMP: Scaler Error &quot;Attempting to Unscale FP16 Gradients&quot;,"<p>I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem?</p>
<p>Here is a minimal example:</p>
<pre class=""lang-none prettyprint-override""><code>import os
from transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast
import torch
from torch.cuda.amp import GradScaler, autocast

model_name = &quot;facebook/opt-1.3b&quot;
cache_dir = './models'
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;7&quot;

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.float16
)

pretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, 
                                                    cache_dir=cache_dir,                                                     
                                                    quantization_config=quantization_config)
tokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,
                                                    cache_dir=cache_dir)
optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)
scaler = GradScaler()
input_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)
labels = torch.LongTensor([[1, 2, 3, 4]]).to(0)
with torch.autocast(device_type='cuda'):
    out = pretrained_model(input_ids=input_ids, labels=labels)
    loss = out.loss
scaler.scale(out.loss).backward()
scaler.step(optimizer) 
scaler.update()
optimizer.zero_grad()

print(f'End')
</code></pre>
<p>At the line <code>scaler.step(optimizer)</code>, an error occurs:</p>
<pre><code>Exception has occurred: ValueError: Attempting to unscale FP16 gradients.

</code></pre>
","python, pytorch, nlp, huggingface-transformers, fine-tuning","<p>You can't fine-tune a fp16/uint8 model with AMP. AMP uses fp32 parameters. The params are autocast to fp16 for the forward pass, but AMP expects the master set of parameters to be FP32.</p>
<p>You also shouldn't fine-tune a quantized model in the first place. The quantization causes all sorts of numerical issues and instability during training.</p>
<p>What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model. You can find more details <a href=""https://huggingface.co/docs/peft/en/developer_guides/quantization"" rel=""nofollow noreferrer"">here</a></p>
"
Understanding the Difference Between Entropy and Cross-Entropy in Language Models: Practical Example with Character-Level Unigram Model,"<p>I'm trying to understand the difference between entropy and cross-entropy, as I often hear about the entropy of a language and the cross-entropy of a language model, and I want to understand the link between the two.</p>
<p>To simplify things, let's consider a language (with a vocabulary) and a language model trained on that language.</p>
<p>We'll work at the character level (which gives us 26 characters), and a limited number of words (let's take the 20 names below).</p>
<pre><code>prenoms = [
    &quot;Alice&quot;, &quot;Alfred&quot;, &quot;Alina&quot;, &quot;Aline&quot;, &quot;Alexandre&quot;, 
    &quot;Alicia&quot;, &quot;Alison&quot;, &quot;Alma&quot;, &quot;Alva&quot;, &quot;Elise&quot;, 
    &quot;Elisa&quot;, &quot;Eliane&quot;, &quot;Alain&quot;, &quot;Amélie&quot;, &quot;Arline&quot;, 
    &quot;Olivier&quot;, &quot;Oline&quot;, &quot;Alva&quot;, &quot;Eliott&quot;, &quot;Julien&quot;
]
</code></pre>
<p>How do we calculate the entropy over these 20 names (i.e., the entropy of our language) and the cross-entropy for our language model (let's take a unigram model or any language model you prefer to help me to understand)?</p>
<p>If you have a more relevant example, I’m open to it.</p>
<p>PS: My confusion comes from the fact that, in general definitions, we talk about a (language) distribution P when calculating entropy (without quite knowing how to calculate it), and about two distributions P and Q when calculating cross-entropy (where P is a one-hot encoding vector in this case, when calculating cross-entropy-loss).</p>
<p>PS2:  A python code could help me to well understand, here is my understanding. I based it on y understanding of Jurasky Book (and <a href=""https://huggingface.co/docs/transformers/perplexity"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/perplexity</a>)</p>
<pre class=""lang-py prettyprint-override""><code>def distribution_ngrams(text, n=4):
    &quot;&quot;&quot;
    &quot;&quot;&quot;
    import math
    from collections import Counter 

    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]
    counts = Counter(ngrams)
    total = len(ngrams)
    
    # Calculate the distribution of n-grams
    distribution = {ngram: count/total for ngram, count in counts.items()}
    return distribution

def language_entropy_ngrams(text, n_approx=4):
    &quot;&quot;&quot;
    Calculate an estimate of the entropy of a text using n-grams (normally, we take a very large n and consider an infinite sequence L)
    &quot;&quot;&quot;
    import math
    distribution = distribution_ngrams(text, n_approx)
    # Calculate entropy
    entropy = -sum((p * math.log2(p)) for ngram,p in distribution.items())
    entropy_rate = entropy / n_approx  # normalize by the size of the n-gram
    return entropy_rate  

def model_cross_entropy(text,n_approx=4):
    &quot;&quot;&quot;
    Calculate the cross-entropy between the true text and the model's predictions
    &quot;&quot;&quot;
    import math
    unigram_model_distribution =  distribution_ngrams(text, 1)
    language_model_distribution_approximation = distribution_ngrams(text, n_approx)

    q = {}
    cross_entropy = 0
    for ngram,p in language_model_distribution_approximation.items():
        q[ngram] = 1
        for c in ngram:
            q[ngram] = q[ngram]*unigram_model_distribution[c]
        cross_entropy -= p*math.log2(q[ngram])
        
    return cross_entropy/n_approx

if __name__ == &quot;__main__&quot;:
    prenoms = [&quot;Alice&quot;, &quot;Alfred&quot;, &quot;Alina&quot;, &quot;Aline&quot;, &quot;Alexandre&quot;, &quot;Alicia&quot;, 
            &quot;Alison&quot;, &quot;Alma&quot;, &quot;Alva&quot;, &quot;Elise&quot;, &quot;Elisa&quot;, &quot;Eliane&quot;, &quot;Alain&quot;, 
            &quot;Amélie&quot;, &quot;Arline&quot;, &quot;Olivier&quot;, &quot;Oline&quot;, &quot;Alva&quot;, &quot;Eliott&quot;, &quot;Julien&quot;] #each prenonm can be seen as a sequence of characters


    L = ''.join(prenoms).lower() #the corpus/language L can be seen as the concatenation of the sequences
    print(language_entropy_ngrams(L))
    print(model_cross_entropy(L))



</code></pre>
",nlp,
How to transcribe multiple audio files at once using Whisper finetuned model?,"<p><strong>TL;DR: I'm trying to transcribe multiple files together using Hugging face fine-tuned whisper ai model and extract the output as a single text file</strong></p>
<p>I have this code which works and transcribes an audio and shows its output as a string of text. But I want to improve upon this by making this transcribe multiple files together, and exporting its output to a text file with each line representing a single audio file.</p>
<h2>What did I try?</h2>
<p>Im not a coder but I asked bing to generate a code and it came up with this which has errors.</p>
<pre><code>audio_files = [&quot;/content/audio1&quot;, &quot;/content/audio2&quot;, ..., &quot;/content/audioN&quot;]
transcriptions = []

for audio_file in audio_files:
    transcription = pipe(audio_file, chunk_length_s=10, stride_length_s=(4, 2))
    transcriptions.append(transcription)

with open(&quot;transcriptions.txt&quot;, &quot;w&quot;) as f:
    for transcription in transcriptions:
        f.write(transcription + &quot;\n&quot;)
</code></pre>
<h2>What I want?</h2>
<p>I need a code which transcribes <strong>all the audio that I have into a single text file</strong> on which each line represents an audio file(preferably starting with the file name). If I <strong>can specify a folder which has all the files</strong> for transcription instead of entering each file manually, that would be AWESOME.</p>
<h2>Whats my workspace?</h2>
<p>I'm using hugging face open ai whisper(fine-tuned) to transcribe my files on google colab.</p>
<p>Any of your help is deeply appreciated.</p>
","nlp, scripting, artificial-intelligence, google-colaboratory, openai-whisper",
word/ sentence similarities,"<p>I am trying to find if a given word/ set of words are similar to a definition.</p>
<p>Example - Definition - &quot;vegetarian User&quot;</p>
<p>Now, if I want to check a set of sentences like below</p>
<pre><code>sentences = ['vegetarian User',
            'user sometimes eats chicken',
            'user is vegetarian',
            'user only eats fruits',
            'user likes fish']
</code></pre>
<p>I tried using some sentence transformer like below</p>
<pre><code>model = SentenceTransformer(&quot;all-mpnet-base-v2&quot;)
embeddings = model.encode(sentences)
similarities = model.similarity(embeddings,embeddings)
print(similarities)
</code></pre>
<p>But this is not giving me expected results.</p>
<p>What is the best approach to achieve results like below?</p>
<pre><code>[False,True,True,False]
</code></pre>
<p>Is it doable with nlp/ some other technique?</p>
","python, python-3.x, nlp",
how to get the target generated query on a self-query retriever(langchain),"<p>I'm implementing a <a href=""https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html"" rel=""nofollow noreferrer"">self-query retriever</a>  using langchain with OpenSearch as the target vectore store, so far everything is good but we need to capture the generated query in DSL, for debugging and auditing purposes, after some testing I cannot find how to do it, I found how to return thye <a href=""https://api.python.langchain.com/en/latest/chains/langchain.chains.query_constructor.ir.StructuredQuery.html"" rel=""nofollow noreferrer"">StructuredQuery</a>, and how to use the StructuredQuery and <a href=""https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.self_query.opensearch.OpenSearchTranslator.html"" rel=""nofollow noreferrer"">OpenSearchTranslator</a> to  get a step closer to the final query, however it is not the final query sent to OpenSearch. Question is, how to get the query? This is my current code(that returns something close to it but not the final version):</p>
<pre><code>opensearch_translator = OpenSearchTranslator()
def show_translated_query(query):
    chain_structured_query = retriever.llm_chain.invoke(query)
    print(&quot;langchain structured query:&quot;)
    print(chain_structured_query)
    os_structured_query = opensearch_translator.visit_structured_query(chain_structured_query)
    print(&quot;OS query(semantic, filter):&quot;)
    print(os_structured_query)

show_translated_query(&quot;a fire ocurring before 2023&quot;)
&gt;&gt;langchain structured query:
&gt;&gt;query='fire' filter=Comparison(comparator=&lt;Comparator.LT: 'lt'&gt;, attribute='year', value=2023) limit=None
&gt;&gt;OS query(semantic, filter):
&gt;&gt;('fire', {'filter': {'range': {'metadata.year': {'lt': 2023}}}})
</code></pre>
","python, nlp, artificial-intelligence, langchain",
Where is GENIA corpus with annotated uncertainity?,"<p>I am looking for the well known GENIA corpus, which annotates events and uncertainity of the events (doubtful, etc.) The old webpage seems not to be available any more. I cannot find a Genia corpus version on hugging face, which would contain the uncertainity annotation. Does anybody know? I am usually quite good in finding data I want :-( MAybe I am blind this time :-(</p>
","nlp, uncertainty, relation-extraction",
MiniBatchKMeans BERTopic not returning topics for half of data,"<p>I am trying to topic a dataset of tweets. I have around 50 million tweets. Unfortunately, such a large dataset will not fit in ram (even 128GB) due to the embeddings. Therefore, I have been working on making an incremental BERTopic as per the <a href=""https://maartengr.github.io/BERTopic/getting_started/online/online.html"" rel=""nofollow noreferrer"">docs</a></p>
<p>As such:</p>
<pre class=""lang-none prettyprint-override""><code>from bertopic.vectorizers import OnlineCountVectorizer
from bertopic.vectorizers import ClassTfidfTransformer
from sklearn.cluster import MiniBatchKMeans
import numpy as np


class SafeIncrementalPCA(IncrementalPCA):
    def partial_fit(self, X, y=None):
        # Ensure the input is contiguous and in float64
        X = np.ascontiguousarray(X, dtype=np.float64)
        return super().partial_fit(X, y)
    
    def transform(self, X):
        result = super().transform(X)
        # Force the output to be float64 and contiguous
        return np.ascontiguousarray(result, dtype=np.float64)


vectorizer_model = OnlineCountVectorizer(stop_words=&quot;english&quot;)
ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)
umap_model = SafeIncrementalPCA(n_components=100)
cluster_model = MiniBatchKMeans(n_clusters=1000, random_state=0)

from bertopic import BERTopic

topic_model = BERTopic(umap_model=umap_model,
                       hdbscan_model=cluster_model,

for docs_delayed, emb_delayed in tqdm(zip(docs_partitions, embeddings_partitions), total=len(docs_partitions)):

    docs_pdf = docs_delayed.compute()
    emb_pdf = emb_delayed.compute()

    docs = docs_pdf[&quot;text&quot;].tolist()
    embeddings = np.vstack(emb_pdf['embeddings'].tolist())
    
    # Partial fit your model (make sure your model supports partial_fit, like many scikit-learn estimators do)
    topic_model.partial_fit(docs, embeddings)

</code></pre>
<p>and then transforming the dataset into a SQL database:</p>
<pre class=""lang-none prettyprint-override""><code>
for docs_delayed, emb_delayed in tqdm(zip(docs_partitions, embeddings_partitions), total=len(docs_partitions)):

    docs_pdf = docs_delayed.compute()
    emb_pdf = emb_delayed.compute()
    docs = docs_pdf[&quot;text&quot;].tolist()
    embeddings = np.vstack(emb_pdf['embeddings'].tolist())

    # 3) Apply BERTopic on this shard
    topics, probs = topic_model.transform(docs, embeddings)

    # Save topics to DataFrame
    df_topics = pd.DataFrame({
        &quot;tweet_id&quot;: docs_pdf[&quot;id&quot;].tolist(),
        &quot;topic&quot;: topics,
        &quot;probability&quot;: probs
    })

    ## Merge &amp; store in DB
    docs_pdf[&quot;topic&quot;] = df_topics[&quot;topic&quot;]
    docs_pdf[&quot;probability&quot;] = df_topics[&quot;probability&quot;]
    docs_pdf.to_sql(&quot;tweets&quot;, engine, if_exists=&quot;append&quot;, index=False)
</code></pre>
<p>I've been trying to do this for a quite a while and this is the closest working example I have gotten. The only issue is half of the dataset has null topics in the database at the end. From what I understand of the theory, MiniBatchKMeans should not have any outliers and therefore all tweets should be assigned to at least one topic, right? I've checked out the unclassified tweets in question and there is nothing in their doc that should suggest it would be hard to classify (relative to others that are classified).</p>
<p>I would be very happy to hear any sort of suggestion on what could be going wrong and how I could fix this!</p>
<p>Thanks!</p>
","python, machine-learning, nlp, data-science, topic-modeling",
Python Farm-haystack Dependencies,"<p>i am trying to implement a model using farm-haystack, however am having a dependency mismatch for the following libraries : transformers farm-haystack langchain pydantic fastapi uvicorn elasticsearch python-multipart, currently i have 2 versions of python installed on my machine (3.12 and 3.11.10), all facing the same challenges. I need help on the proper version for both dependencies and python version which works better for these</p>
<p>from this implementation:</p>
<pre><code>import os
from typing import List
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import PreProcessor, BM25Retriever, FARMReader

# Initialize an in-memory document store (replaceable with Elasticsearch)
document_store = InMemoryDocumentStore()

# Folder where uploaded documents are stored
UPLOAD_FOLDER = &quot;uploaded_docs&quot;

# Ensure the upload folder exists
os.makedirs(UPLOAD_FOLDER, exist_ok=True)


def list_documents() -&gt; List[str]:
    &quot;&quot;&quot;List all uploaded documents.&quot;&quot;&quot;
    try:
        return os.listdir(UPLOAD_FOLDER)
    except FileNotFoundError:
        raise RuntimeError(f&quot;Upload folder '{UPLOAD_FOLDER}' not found. Please create it.&quot;)


def read_document(file_path: str) -&gt; str:
    &quot;&quot;&quot;Read the content of a document.&quot;&quot;&quot;
    try:
        with open(file_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
            return f.read()
    except Exception as e:
        raise RuntimeError(f&quot;Error reading file '{file_path}': {str(e)}&quot;)


def preprocess_document(content: str) -&gt; List[dict]:
    &quot;&quot;&quot;Preprocess the document content into smaller chunks for indexing.&quot;&quot;&quot;
    preprocessor = PreProcessor(
        split_by=&quot;word&quot;,  # Split the content into chunks by word count
        split_length=200,  # Chunk size
        split_overlap=20,  # Overlap between chunks
        split_respect_sentence_boundary=True,
    )
    return preprocessor.process({&quot;content&quot;: content})


def index_document(file_name: str):
    &quot;&quot;&quot;Read, preprocess, and index a document.&quot;&quot;&quot;
    file_path = os.path.join(UPLOAD_FOLDER, file_name)
    if not os.path.isfile(file_path):
        raise RuntimeError(f&quot;File '{file_name}' not found in '{UPLOAD_FOLDER}'.&quot;)

    content = read_document(file_path)
    chunks = preprocess_document(content)

    # Prepare chunks in Haystack-compatible format
    formatted_chunks = [{&quot;content&quot;: chunk[&quot;content&quot;]} for chunk in chunks]
    document_store.write_documents(formatted_chunks)

    return {
        &quot;message&quot;: f&quot;Document '{file_name}' indexed successfully.&quot;,
        &quot;chunks_count&quot;: len(formatted_chunks),
    }


def search_documents(query: str):
    &quot;&quot;&quot;Search indexed documents using a query.&quot;&quot;&quot;
    retriever = BM25Retriever(document_store=document_store)
    reader = FARMReader(model_name_or_path=&quot;deepset/roberta-base-squad2&quot;, use_gpu=False)
    
    # Retrieve documents
    retrieved_docs = retriever.retrieve(query)
    if not retrieved_docs:
        return {&quot;message&quot;: &quot;No relevant documents found.&quot;}

    # Reader to predict answers from retrieved documents
    answers = reader.predict(query=query, documents=retrieved_docs, top_k=3)

    # Serialize the results to avoid unsupported types
    results = [
        {
            &quot;answer&quot;: ans.answer,
            &quot;score&quot;: ans.score,
            &quot;context&quot;: ans.context,
            &quot;document_id&quot;: ans.document_id,
        }
        for ans in answers[&quot;answers&quot;]
    ]

    return {&quot;results&quot;: results}
</code></pre>
<p>But i keep getting this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 131, in _main
    prepare(preparation_data)
  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 244, in prepare
    _fixup_main_from_name(data['init_main_from_name'])
  File &quot;/usr/lib/python3.11/multiprocessing/spawn.py&quot;, line 268, in _fixup_main_from_name
    main_content = runpy.run_module(mod_name,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;&lt;frozen runpy&gt;&quot;, line 226, in run_module
  File &quot;&lt;frozen runpy&gt;&quot;, line 98, in _run_module_code
  File &quot;&lt;frozen runpy&gt;&quot;, line 88, in _run_code
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/main.py&quot;, line 7, in &lt;module&gt;
    from app.views.routes import router
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/views/routes.py&quot;, line 2, in &lt;module&gt;
    from app.services.document_service import list_documents, index_document, search_documents
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/app/services/document_service.py&quot;, line 3, in &lt;module&gt;
    from haystack.document_stores import InMemoryDocumentStore
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/__init__.py&quot;, line 8, in &lt;module&gt;
    from haystack.schema import Document, Answer, Label, MultiLabel, Span, EvaluationResult, TableCell
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/haystack/schema.py&quot;, line 42, in &lt;module&gt;
    @dataclass
     ^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py&quot;, line 250, in dataclass
    return create_dataclass(_cls)
           ^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/dataclasses.py&quot;, line 241, in create_dataclass
    pydantic_complete = _pydantic_dataclasses.complete_dataclass(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py&quot;, line 159, in complete_dataclass
    schema = gen_schema.generate_schema(cls, from_dunder_get_core_schema=False)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 502, in generate_schema
    schema = self._generate_schema_inner(obj)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner
    return self.match_type(obj)
           ^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 832, in match_type
    return self._dataclass_schema(obj, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1561, in _dataclass_schema
    args = sorted(
           ^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1562, in &lt;genexpr&gt;
    (self._generate_dc_field_schema(k, v, decorators) for k, v in fields.items()),
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 933, in _generate_dc_field_schema
    common_field = self._common_field_schema(name, field_info, decorators)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1081, in _common_field_schema
    schema = self._apply_annotations(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1825, in _apply_annotations
    schema = get_inner_schema(source_type)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py&quot;, line 82, in __call__
    schema = self._handler(source_type)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1806, in inner_handler
    schema = self._generate_schema_inner(obj)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner
    return self.match_type(obj)
           ^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 840, in match_type
    return self._match_generic_type(obj, origin)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 864, in _match_generic_type
    return self._union_schema(obj)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 1152, in _union_schema
    choices.append(self.generate_schema(arg))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 502, in generate_schema
    schema = self._generate_schema_inner(obj)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 758, in _generate_schema_inner
    return self.match_type(obj)
           ^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 844, in match_type
    return self._unknown_type_schema(obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/devoop/Documents/Python Projects/mohcc-ai-tools/mohcc-ai-tools-env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 405, in _unknown_type_schema
    raise PydanticSchemaGenerationError(
pydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for &lt;class 'pandas.core.frame.DataFrame'&gt;. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.

If you got this error by calling handler(&lt;some type&gt;) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(&lt;some type&gt;)` since we do not call `__get_pydantic_core_schema__` on `&lt;some type&gt;` otherwise to avoid infinite recursion.

For further information visit https://errors.pydantic.dev/2.7/u/schema-for-unknown-type
</code></pre>
","python-3.x, nlp, artificial-intelligence, fastapi, haystack",
Training llm for Query Generation in a Graph Database,"<p>If I have developed a graph database which has its own query language. I have to find a way to feed llm the graph and then llm should be able to generate the queries of our database.</p>
<p>I have found something similar in langchain that we can feed it the rdf file and then it will generate the sparql queries.</p>
<p>So I have many doubts regarding this as I am very new to this:</p>
<p>Is it possible to train a llm on an entirely new technology like here it is our database. If it is possible then how.</p>
<p>I know that we have to provide the training data to the llm. So in this case, will it be the dataset with our database queries. If yes , then how many queries we have to provide in a dataset.</p>
<p>Sorry if the question is not detailed , its only my second time asking here.</p>
","machine-learning, nlp, sparql, langchain, large-language-model",
is there a method to detect person and associate a text?,"<p>I have a text like :</p>
<blockquote>
<p>Take a loot at some of the first confirmed Forum speakers:  <strong>John
Sequiera</strong>  Graduated in Biology at Facultad de Ciencias Exactas y
Naturales,University of Buenos Aires, Argentina. In 2004 obtained a
PhD in Biology (Molecular Neuroscience), at University of Buenos
Aires, mentored by Prof. Marcelo Rubinstein. Between 2005 and 2008
pursued postdoctoral training at Pasteur Institute (Paris) mentored by
Prof Jean-Pierre Changeux, to investigate the role of nicotinic
receptors in executive behaviors. Motivated by a deep interest in
investigating human neurological diseases, in 2009 joined the
Institute of Psychiatry at King’s College London where she performed
basic research with a translational perspective in the field of
neurodegeneration.
Since 2016 has been chief of instructors / Adjunct professor at University of Buenos Aires, Facultad de Ciencias Exactas y Naturales.
<strong>Tom Gonzalez</strong> is a professor of Neuroscience at the Sussex Neuroscience, School of Life Sciences, University of Sussex. Prof.
Baden studies how neurons and networks compute, using the beautiful
collection of circuits that make up the vertebrate retina as a model.</p>
</blockquote>
<p>I want to have in output :
<code>[{&quot;person&quot; : &quot;John Sequiera&quot; , &quot;content&quot;: &quot;Graduated in Biology at Facultad....&quot;},{&quot;person&quot; : &quot;Tom Gonzalez&quot; , &quot;content&quot;: &quot;is a professor of Neuroscience at the Sussex...&quot;}]</code></p>
<p>so we want to get NER : PER for person and in content we put all contents after detecting person until we found a new person in the text ...</p>
<p>it is possible ?</p>
<p>i try to use spacy to extract NER , but i found a difficulty to get content :</p>
<pre><code>import spacy
​
nlp = spacy.load(&quot;en_core_web_lg&quot;)
doc = nlp(text)
​
for ent in doc.ents:
    print(ent.text,ent.label_)
</code></pre>
","python-3.x, nlp, spacy",
"How to solve ERROR: Could not build wheels for sparse-dot-topn-for-blocks, which is required to install pyproject.toml-based projects?","<p>I want to perform a string grouping operation on a dataset of ~7000 variants. After much research, I've found the <a href=""https://github.com/Bergvca/string_grouper"" rel=""nofollow noreferrer"">string-grouper</a> library to be matching my use case. This is an explainer <a href=""https://bergvca.github.io/2017/10/14/super-fast-string-matching.html"" rel=""nofollow noreferrer"">blog post</a> by van den Berg.</p>
<p>However, I am unable to install the library. Here's the error that I am receiving:</p>
<pre><code>Building wheels for collected packages: sparse-dot-topn-for-blocks
  Building wheel for sparse-dot-topn-for-blocks (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for sparse-dot-topn-for-blocks (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─&gt; [245 lines of output]
      C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\dist.py:314: InformationOnly: Normalizing '0.3.1-3' to '0.3.1.post3'
        self.metadata.version = self._normalize_version(self.metadata.version)
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build\lib.win-amd64-cpython-39
      creating build\lib.win-amd64-cpython-39\sparse_dot_topn_for_blocks
      copying sparse_dot_topn_for_blocks\awesome_cossim_topn.py -&gt; build\lib.win-amd64-cpython-39\sparse_dot_topn_for_blocks
      copying sparse_dot_topn_for_blocks\__init__.py -&gt; build\lib.win-amd64-cpython-39\sparse_dot_topn_for_blocks
      running egg_info
      writing sparse_dot_topn_for_blocks.egg-info\PKG-INFO
      writing dependency_links to sparse_dot_topn_for_blocks.egg-info\dependency_links.txt
      writing requirements to sparse_dot_topn_for_blocks.egg-info\requires.txt
      writing top-level names to sparse_dot_topn_for_blocks.egg-info\top_level.txt
      reading manifest file 'sparse_dot_topn_for_blocks.egg-info\SOURCES.txt'
      reading manifest template 'MANIFEST.in'
      adding license file 'LICENSE'
      writing manifest file 'sparse_dot_topn_for_blocks.egg-info\SOURCES.txt'
      copying sparse_dot_topn_for_blocks\array_wrappers.pxd -&gt; build\lib.win-amd64-cpython-39\sparse_dot_topn_for_blocks
      copying sparse_dot_topn_for_blocks\array_wrappers.pyx -&gt; build\lib.win-amd64-cpython-39\sparse_dot_topn_for_blocks
      copying sparse_dot_topn_for_blocks\sparse_dot_topn.pyx -&gt; build\lib.win-amd64-cpython-39\sparse_dot_topn_for_blocks
      copying sparse_dot_topn_for_blocks\sparse_dot_topn_parallel.cpp -&gt; build\lib.win-amd64-cpython-39\sparse_dot_topn_for_blocks
      copying sparse_dot_topn_for_blocks\sparse_dot_topn_parallel.h -&gt; build\lib.win-amd64-cpython-39\sparse_dot_topn_for_blocks
      copying sparse_dot_topn_for_blocks\sparse_dot_topn_source.cpp -&gt; build\lib.win-amd64-cpython-39\sparse_dot_topn_for_blocks
      copying sparse_dot_topn_for_blocks\sparse_dot_topn_source.h -&gt; build\lib.win-amd64-cpython-39\sparse_dot_topn_for_blocks
      copying sparse_dot_topn_for_blocks\sparse_dot_topn_threaded.pyx -&gt; build\lib.win-amd64-cpython-39\sparse_dot_topn_for_blocks
      running build_ext
      C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\normal\Lib\site-packages\Cython\Compiler\Main.py:381: FutureWarning: Cython directive 'language_le
vel' not set, using '3str' for now (Py3). This has changed from earlier releases! File: C:\Users\zaida\AppData\Local\Temp\pip-install-kbeq40ku\sparse-dot-topn-for-blocks_0e7bc776223c4685b4535c096adeb9be\sparse_dot_topn_for_blocks\array_wrappers.pxd
        tree = Parsing.p_module(s, pxd, full_module_name)
      Compiling ./sparse_dot_topn_for_blocks/array_wrappers.pyx because it changed.
      [1/1] Cythonizing ./sparse_dot_topn_for_blocks/array_wrappers.pyx
      building 'sparse_dot_topn_for_blocks.array_wrappers' extension
      creating build\temp.win-amd64-cpython-39
      creating build\temp.win-amd64-cpython-39\Release
      creating build\temp.win-amd64-cpython-39\Release\sparse_dot_topn_for_blocks
      &quot;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.38.33130\bin\HostX86\x64\cl.exe&quot; /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\
Users\zaida\DataspellProjects\test_string_grouper_2\venv\include -IC:\Users\zaida\AppData\Local\Programs\Python\Python39\include -IC:\Users\zaida\AppData\Local\P
rograms\Python\Python39\Include -IC:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\normal\Lib\site-packages\numpy\core\include &quot;-IC:\Program Files (x86)\
Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.38.33130\include&quot; &quot;-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\incl
ude&quot; &quot;-IC:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt&quot; &quot;-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um&quot; &quot;-IC:\Program File
s (x86)\Windows Kits\10\\include\10.0.22621.0\\shared&quot; &quot;-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt&quot; &quot;-IC:\Program Files (x86)\Windows 
Kits\10\\include\10.0.22621.0\\cppwinrt&quot; &quot;-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um&quot; /EHsc /Tp./sparse_dot_topn_for_blocks/array_wrappers.cpp /Fobuild\temp.win-amd64-cpython-39\Release\./sparse_dot_topn_for_blocks/array_wrappers.obj -Ox
      array_wrappers.cpp
      &quot;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.38.33130\bin\HostX86\x64\link.exe&quot; /nologo /INCREMENTAL:NO /LTCG /DLL /MAN
IFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\Users\zaida\DataspellProjects\test_string_grouper_2\venv\libs /LIBPATH:C:\Users\zaida\AppData\Local\Programs\Python\
Python39\libs /LIBPATH:C:\Users\zaida\AppData\Local\Programs\Python\Python39 /LIBPATH:C:\Users\zaida\DataspellProjects\test_string_grouper_2\venv\PCbuild\amd64 &quot;
/LIBPATH:C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.38.33130\lib\x64&quot; &quot;/LIBPATH:C:\Program Files (x86)\Windows Kits\NETFXSDK
\4.8\lib\um\x64&quot; &quot;/LIBPATH:C:\Program Files (x86)\Windows Kits\10\lib\10.0.22621.0\ucrt\x64&quot; &quot;/LIBPATH:C:\Program Files (x86)\Windows Kits\10\\lib\10.0.22621.0\\
um\x64&quot; /EXPORT:PyInit_array_wrappers build\temp.win-amd64-cpython-39\Release\./sparse_dot_topn_for_blocks/array_wrappers.obj /OUT:build\lib.win-amd64-cpython-39
\sparse_dot_topn_for_blocks\array_wrappers.cp39-win_amd64.pyd /IMPLIB:build\temp.win-amd64-cpython-39\Release\./sparse_dot_topn_for_blocks\array_wrappers.cp39-win_amd64.lib
         Creating library build\temp.win-amd64-cpython-39\Release\./sparse_dot_topn_for_blocks\array_wrappers.cp39-win_amd64.lib and object build\temp.win-amd64-cpython-39\Release\./sparse_dot_topn_for_blocks\array_wrappers.cp39-win_amd64.exp
      Generating code
      Finished generating code
      C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\normal\Lib\site-packages\Cython\Compiler\Main.py:381: FutureWarning: Cython directive 'language_le
vel' not set, using '3str' for now (Py3). This has changed from earlier releases! File: C:\Users\zaida\AppData\Local\Temp\pip-install-kbeq40ku\sparse-dot-topn-for-blocks_0e7bc776223c4685b4535c096adeb9be\sparse_dot_topn_for_blocks\sparse_dot_topn.pyx
        tree = Parsing.p_module(s, pxd, full_module_name)

      Error compiling Cython file:
      ------------------------------------------------------------
      ...
      # April 14, 2021

      # distutils: language = c++

      from libcpp.vector cimport vector
      from array_wrappers cimport ArrayWrapper_int, ArrayWrapper_float, ArrayWrapper_double
      ^
      ------------------------------------------------------------

      sparse_dot_topn_for_blocks\sparse_dot_topn.pyx:23:0: 'array_wrappers.pxd' not found

      Error compiling Cython file:
      ------------------------------------------------------------
      ...
      # April 14, 2021

      # distutils: language = c++

      from libcpp.vector cimport vector
      from array_wrappers cimport ArrayWrapper_int, ArrayWrapper_float, ArrayWrapper_double
      ^
      ------------------------------------------------------------

      sparse_dot_topn_for_blocks\sparse_dot_topn.pyx:23:0: 'array_wrappers\ArrayWrapper_int.pxd' not found

      Error compiling Cython file:
      ------------------------------------------------------------
      ...
      # April 14, 2021

      # distutils: language = c++

      from libcpp.vector cimport vector
      from array_wrappers cimport ArrayWrapper_int, ArrayWrapper_float, ArrayWrapper_double
      ^
      ------------------------------------------------------------

      sparse_dot_topn_for_blocks\sparse_dot_topn.pyx:23:0: 'array_wrappers\ArrayWrapper_float.pxd' not found

      Error compiling Cython file:
      ------------------------------------------------------------
      ...
      # April 14, 2021

      # distutils: language = c++

      from libcpp.vector cimport vector
      from array_wrappers cimport ArrayWrapper_int, ArrayWrapper_float, ArrayWrapper_double
      ^
      ------------------------------------------------------------

      sparse_dot_topn_for_blocks\sparse_dot_topn.pyx:23:0: 'array_wrappers\ArrayWrapper_double.pxd' not found

      Error compiling Cython file:
      ------------------------------------------------------------
      ...
          ) except +;

      cpdef ArrayWrapper_template(vector[float_ft] vCx):
          # raise Exception(&quot;In sparse_dot_topn.pyx&quot;)
          if float_ft is float:
                  return ArrayWrapper_float(vCx)
               ^
      ------------------------------------------------------------

      sparse_dot_topn_for_blocks\sparse_dot_topn.pyx:116:9: 'ArrayWrapper_float' is not a constant, variable or function identifier

      Error compiling Cython file:
      ------------------------------------------------------------
      ...
      cpdef ArrayWrapper_template(vector[float_ft] vCx):
          # raise Exception(&quot;In sparse_dot_topn.pyx&quot;)
          if float_ft is float:
                  return ArrayWrapper_float(vCx)
          elif float_ft is double:
                  return ArrayWrapper_double(vCx)
               ^
      ------------------------------------------------------------

      sparse_dot_topn_for_blocks\sparse_dot_topn.pyx:118:9: 'ArrayWrapper_double' is not a constant, variable or function identifier

      Error compiling Cython file:
      ------------------------------------------------------------
      ...

          if nnz_max_is_too_small:

                  # raise Exception(&quot;In sparse_dot_topn.pyx&quot;)

                  c_indices = np.asarray(ArrayWrapper_int(vCj)).squeeze(axis=0)
                               ^
      ------------------------------------------------------------

      sparse_dot_topn_for_blocks\sparse_dot_topn.pyx:209:25: 'ArrayWrapper_int' is not a constant, variable or function identifier

      Error compiling Cython file:
      ------------------------------------------------------------
      ...

          if nnz_max_is_too_small:

                  # raise Exception(&quot;In sparse_dot_topn.pyx&quot;)

                  c_indices = np.asarray(ArrayWrapper_int(vCj)).squeeze(axis=0)
                               ^
      ------------------------------------------------------------

      sparse_dot_topn_for_blocks\sparse_dot_topn.pyx:209:25: 'ArrayWrapper_int' is not a constant, variable or function identifier

      Error compiling Cython file:
      ------------------------------------------------------------
      ...

          if nnz_max_is_too_small:

                  # raise Exception(&quot;In sparse_dot_topn.pyx&quot;)

                  c_indices = np.asarray(ArrayWrapper_int(vCj)).squeeze(axis=0)
                               ^
      ------------------------------------------------------------

      sparse_dot_topn_for_blocks\sparse_dot_topn.pyx:296:25: 'ArrayWrapper_int' is not a constant, variable or function identifier

      Error compiling Cython file:
      ------------------------------------------------------------
      ...

          if nnz_max_is_too_small:

                  # raise Exception(&quot;In sparse_dot_topn.pyx&quot;)

                  c_indices = np.asarray(ArrayWrapper_int(vCj)).squeeze(axis=0)
                               ^
      ------------------------------------------------------------

      sparse_dot_topn_for_blocks\sparse_dot_topn.pyx:296:25: 'ArrayWrapper_int' is not a constant, variable or function identifier
      Compiling ./sparse_dot_topn_for_blocks/sparse_dot_topn.pyx because it changed.
      [1/1] Cythonizing ./sparse_dot_topn_for_blocks/sparse_dot_topn.pyx
      Traceback (most recent call last):
        File &quot;C:\Users\zaida\DataspellProjects\test_string_grouper_2\venv\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 353, in &lt;module&gt;
          main()
        File &quot;C:\Users\zaida\DataspellProjects\test_string_grouper_2\venv\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 335, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
        File &quot;C:\Users\zaida\DataspellProjects\test_string_grouper_2\venv\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 251, in build_wheel
          return _build_backend().build_wheel(wheel_directory, config_settings,
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 404, in build_wheel
          return self._build_with_temp_dir(
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 389, in _build_with_temp_dir    
          self.run_setup()
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 311, in run_setup
          exec(code, locals())
        File &quot;&lt;string&gt;&quot;, line 65, in &lt;module&gt;
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\__init__.py&quot;, line 103, in setup
          return distutils.core.setup(**attrs)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\_distutils\core.py&quot;, line 185, in setup
          return run_commands(dist)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\_distutils\core.py&quot;, line 201, in run_commands       
          dist.run_commands()
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\_distutils\dist.py&quot;, line 969, in run_commands       
          self.run_command(cmd)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\dist.py&quot;, line 963, in run_command
          super().run_command(command)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\_distutils\dist.py&quot;, line 988, in run_command
          cmd_obj.run()
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\wheel\bdist_wheel.py&quot;, line 368, in run
          self.run_command(&quot;build&quot;)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\_distutils\cmd.py&quot;, line 318, in run_command
          self.distribution.run_command(command)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\dist.py&quot;, line 963, in run_command
          super().run_command(command)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\_distutils\dist.py&quot;, line 988, in run_command        
          cmd_obj.run()
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\_distutils\command\build.py&quot;, line 131, in run       
          self.run_command(cmd_name)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\_distutils\cmd.py&quot;, line 318, in run_command
          self.distribution.run_command(command)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\dist.py&quot;, line 963, in run_command
          super().run_command(command)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\_distutils\dist.py&quot;, line 988, in run_command        
          cmd_obj.run()
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\command\build_ext.py&quot;, line 88, in run
          _build_ext.run(self)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\_distutils\command\build_ext.py&quot;, line 345, in run   
          self.build_extensions()
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\_distutils\command\build_ext.py&quot;, line 467, in build_extensions
          self._build_extensions_serial()
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\_distutils\command\build_ext.py&quot;, line 493, in _build_extensions_serial
          self.build_extension(ext)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\overlay\Lib\site-packages\setuptools\command\build_ext.py&quot;, line 249, in build_extension  
          _build_ext.build_extension(self, ext)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\normal\Lib\site-packages\Cython\Distutils\build_ext.py&quot;, line 130, in build_extension     
          new_ext = cythonize(
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\normal\Lib\site-packages\Cython\Build\Dependencies.py&quot;, line 1154, in cythonize
          cythonize_one(*args)
        File &quot;C:\Users\zaida\AppData\Local\Temp\pip-build-env-w3s5dhxf\normal\Lib\site-packages\Cython\Build\Dependencies.py&quot;, line 1321, in cythonize_one       
          raise CompileError(None, pyx_file)
      Cython.Compiler.Errors.CompileError: ./sparse_dot_topn_for_blocks/sparse_dot_topn.pyx
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for sparse-dot-topn-for-blocks
Failed to build sparse-dot-topn-for-blocks
ERROR: Could not build wheels for sparse-dot-topn-for-blocks, which is required to install pyproject.toml-based projects

</code></pre>
<p>It was earlier asking me to upgrade Microsoft Build tools which I did. Now it is giving the above error.</p>
<p>I've read earlier posts of users facing a similar error but didn't find a solution.</p>
","python, nlp",
How many obs per class are necessary? - transfer learning w. BERT fine-tuning,"<p>I seek advice on a classification problem in industry.</p>
<p>The rows in a dataset must be classified/labeled--it lacks a target/column (labels have dot-separated levels like 'x.x.x.x.x.x.x')--during every business cycle. For each cycle, a dataset is given as input to this exercise. The dataset includes some variables, most importantly 1. an ID variable and 2. a short text description. When the dataset is correctly classified, the ID will correspond perfectly to a class. At every iteration, most of the ID---class links as identities will carry over, but some won't: There will be new labels and some labels get redefined. Basically, <em>there will be unlabeled rows at every iteration</em>.</p>
<p>The classes are assigned using variable 2, the text description (besides a few others) compared with the content of a, let's say, scoring manual accompanying the cycle's new dataset. (Well, rather, the dataset and the key come from two independent business processes, but we can ignore that here.) As a de facto scoring manual, the classes are unique and are described only once using a handful of descriptions, which are short text fields about what identifies, =, and what contrasts, !=, a class. So, <em>in the manual (available as a second dataset), each class is described only once</em>.</p>
<p>The desire is to classify the datasets ad infinitum as automatically as possible. The dataset has already been classified at time T0 but will need more new classes at T1. It is possible to supervise a learning algorithm on the first batch. <strong>The question</strong> is whether subsequent batches require labeling by experts for training the model or, ideally, whether the 'scoring manual' with one row/obs/example per label may suffice for fine-tuning? (more loosely formulated:  <a href=""https://stats.stackexchange.com/q/310947/207649"">How much data is needed for transfer learning?</a>; and in the case of training from scratch: <a href=""https://stats.stackexchange.com/questions/446667/how-many-data-points-per-class-is-neccesary-to-train-a-multi-class-deep-learning"">how many data points per class is neccesary to train a multi-class deep learning</a>)</p>
","nlp, classification, bert-language-model, transfer-learning, fine-tuning",
Pytorch TypeError: only integer tensors of a single element can be converted to an index,"<p>I am trying to use pytorch in a case for nlp binary classification and i need help to finish the neural network training and validate. I am newbie and this is my first time using pytorch, see the code below and the error...</p>
<pre><code>X_train_tensor = torch.from_numpy(np.asarray(X_train)).type(torch.FloatTensor).to(device)
y_train_tensor = torch.from_numpy(np.asarray(y_train)).type(torch.FloatTensor).unsqueeze(1).to(device)

X_valid_tensor = torch.from_numpy(np.asarray(X_valid)).type(torch.FloatTensor).to(device)
y_valid_tensor = torch.from_numpy(np.asarray(y_valid)).type(torch.FloatTensor).unsqueeze(1).to(device)



X_train_tensor.size()
</code></pre>
<p>out: torch.Size([5438, 768])</p>
<pre><code>y_train_tensor.size()
</code></pre>
<p>out: torch.Size([5438, 1])</p>
<pre><code>criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)


def binary_acc(preds, y_valid):
    
    y_valid_tag = torch.round(preds)

    correct_results = (y_valid_tag == y_valid).float()
    acc = correct_results.sum() / len(correct_results)
    
    return acc


def train(model, *var):
    
    epoch_loss = 0
    epoch_acc = 0
    
    model.train()
    
    for x in range(X_train_tensor):
    
        optimizer.zero_grad() 
    
        predictions = model(X_train_tensor)
        loss = criterion(predictions, y_train_tensor) 
        acc = binary_acc(predictions, y_valid_tensor)
    
        loss.backward()
        optimizer.step()
    
        epoch_loss += loss.item()
        epoch_acc += acc.item()
    
    return epoch_loss / len(X_train_tensor), epoch_acc / len(X_train_tensor)




def evaluate(model, *var):

    epoch_acc = 0
    
    model.eval()
    
    with torch.no_grad():
        for X in range(X_valid_tensor):
            predictions = model(X_train_tensor)
            acc = binary_acc(predictions, y_valid_tensor)
        
            epoch_acc += acc.item()
        
    return epoch_acc / len(X_valid_tensor)




loss=[]
acc=[]
val_acc=[]

for epoch in range(10):
    
    train_loss, train_acc = train(model, X_train_tensor, y_train_tensor, y_valid_tensor)
    valid_acc = evaluate(model, X_valid_tensor, y_valid_tensor)
    
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
    print(f'\t Val. Acc: {valid_acc*100:.2f}%')
    
    loss.append(train_loss)
    acc.append(train_acc)
    val_acc.append(valid_acc)
</code></pre>
<p>THE OUTPUT ERROR: <strong>TypeError: only integer tensors of a single element can be converted to an index</strong></p>
<p><strong>Please help me to fix this</strong></p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-46-bfd7a45f13aa&gt; in &lt;module&gt;
      5 for epoch in range(10):
      6 
----&gt; 7     train_loss, train_acc = train(model, X_train_tensor, y_train_tensor, y_valid_tensor)
      8     valid_acc = evaluate(model, X_valid_tensor, y_valid_tensor)
      9 

&lt;ipython-input-44-689c66e0e9ed&gt; in train(model, *var)
      6     model.train()
      7 
----&gt; 8     for x in range(X_train_tensor):
      9 
     10         optimizer.zero_grad()

TypeError: only integer tensors of a single element can be converted to an index
</code></pre>
","python, neural-network, nlp, pytorch, tensor",
How to read gguf format file and print content in python such as in NLP meta&#39;&#39;codellama-7b-instruct.Q4_K_S.gguf&#39;&#39;,"<p>How to read gguf format file in python and print content in python such as in NLP  meta''codellama-7b-instruct.Q4_K_S.gguf''</p>
<p>I tried using Python's open() function to read the gguf format file, expecting to retrieve its content and print it to the console or store it in a variable. However, I encountered errors due to the unsupported file format, preventing me from accessing the content as intended.</p>
","nlp, artificial-intelligence, jnlp",
Calculating Minimum Edit Distance for unequal strings python,"<p>I am trying to implement Minimum Edit Distance with the substitution cost of 2. Following is the code I've so far. It works well for strings of equal length but generates error for the unequal strings. Kindly correct me where i am wrong</p>

<pre><code>def med(source, target):
#     if len(x) &gt; len(y):
#         print(""insode if"")
#         source, target = y, x
print(len(source), len(target))
cost = [[0 for inner in range(len(source)+1)] for outer in 
range(len(target)+1)]

global backtrace 
backtrace = [[0 for inner in range(len(source)+1)] for outer in 
range(len(target)+1)]
global SUB
global INS
global DEL

for i in range(0,len(target)+1):
    cost[i][0] = i

for j in range(0,len(source)+1):
    cost[0][j] = j

for i in range(1,len(target)+1):
    for j in range(1,len(source)+1):
        if source[i-1]==target[j-1]:
            cost[i][j] = cost[i-1][j-1] 
        else:
            deletion = cost[i-1][j]+1
            insertion = cost[i][j-1]+1
            substitution = cost[i-1][j-1]+2
            cost[i][j] = min(insertion,deletion,substitution)

            if cost[i][j] == substitution:
                backtrace[i][j] = SUB
            elif cost[i][j] == insertion:
                backtrace[i][j] = INS
            else:
                backtrace[i][j] = DEL


return cost[i][j]

med(""levenshtein"",""levels"")
</code></pre>

<p>The error i get is:</p>

<pre><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-26-86bf20ea27c7&gt; in &lt;module&gt;()
 49     return cost[i][j]
 50 
---&gt; 51 med(""levenshtein"",""levels"")

&lt;ipython-input-26-86bf20ea27c7&gt; in med(source, target)
 31     for i in range(1,len(target)+1):
 32         for j in range(1,len(source)+1):
---&gt; 33             if source[i-1]==target[j-1]:
 34                 cost[i][j] = cost[i-1][j-1]
 35             else:

IndexError: string index out of range
</code></pre>
","python, nlp, edit-distance","<p>For different length strings, <code>cost</code> and <code>backtrace</code> indices doesn't match.</p>
<p>Can be implemented minimum edit distance with 2 substitution cost by updating only one numpy <code>m</code> * <code>n</code> arr with cost at each step.</p>
<p><a href=""https://i.sstatic.net/GZ5Iq.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GZ5Iq.jpg"" alt=""enter image description here"" /></a></p>
<p>As per Algorithm,
Below code will do the job.</p>
<pre class=""lang-py prettyprint-override""><code>def minimumEditDistance(first, second): 
    
    #Creating numpy ndarray( initialized with 0 of dimension of size of both strings
    
    matrix = np.zeros((len(first)+1,len(second)+1), dtype=np.int)
    
    
    # Cross relation loop through each character of each string with each other and
    # fill the respective index of matrxi (row,column)
    
    for i in range(len(first)+1): 
        for j in range(len(second)+1): 
            
            #First doing the boundary value analysis, if first or second string is empty so directly adding insertion cost
            if i == 0:  
                matrix[i][j] = j  
            #Second case
            elif j == 0: 
                matrix[i][j] = i
            else: 
                matrix[i][j] = min(matrix[i][j-1] + 1,  
                                   matrix[i-1][j] + 1,        
                                   matrix[i-1][j-1] + 2 if first[i-1] != second[j-1] else matrix[i-1][j-1] + 0)     
                                   # Adjusted the cost accordinly, insertion = 1, deletion=1 and substitution=2
    return matrix[len(first)][len(second)]  # Returning the final
</code></pre>
<h1>Output:</h1>
<pre><code>&gt;&gt;&gt;print(minimumEditDistance('levenshtein','levels'))
7
&gt;&gt;&gt;print(minimumEditDistance('levenshtein','levenshtein'))
0
</code></pre>
"
How to determine the language of a piece of text?,"<p>I want to get this:</p>
<pre class=""lang-none prettyprint-override""><code>Input text: &quot;ру́сский язы́к&quot;
Output text: &quot;Russian&quot; 

Input text: &quot;中文&quot;
Output text: &quot;Chinese&quot; 

Input text: &quot;にほんご&quot;
Output text: &quot;Japanese&quot; 

Input text: &quot;العَرَبِيَّة&quot;
Output text: &quot;Arabic&quot;
</code></pre>
<p>How can I do it in python?</p>
","python, nlp","<p>Have you had a look at <a href=""https://pypi.python.org/pypi/langdetect?"" rel=""noreferrer"">langdetect</a>?</p>

<pre><code>from langdetect import detect

lang = detect(""Ein, zwei, drei, vier"")

print lang
#output: de
</code></pre>
"
Data Annotation for Machine Learning,"<p>I am going to develop a machine learning model. I have large data sets(Text). I need overall better accuracy F1 score etc. I am using data annotation tools(Dataturks). Which approach will be good to label the data as single label per entity or multiple label per entity (like there has been 5 times GUI so we have to label it 1 time or 5 times for better overall score). Your help will be highly appreciated.</p>
","machine-learning, nlp, data-annotations, spacy, named-entity-recognition","<p>If you have any duplicate examples where all the features are identical you need to remove them</p>
"
Clustering of news articles,"<p>My scenario is pretty straightforwrd: I have a bunch of news articles (~1k at the moment) for which I know that some cover the same story/topic. I now would like to group these articles based on shared story/topic, i.e., based on their similarity.</p>

<p>What I did so far is to apply basic NLP techniques including stopword removal and stemming. I also calculated the tf-idf vector for each article, and with this can also calculate the, e.g., cosine similarity based on these tf-idf-vectors. But now with the grouping of the articles I struggles a bit. I see two principle ways -- probably related -- to do it:</p>

<p>1) Machine Learning / Clustering: I already played a bit with existing clustering libraries, with more or less success; see <a href=""https://stackoverflow.com/questions/25217065/scikit-learn-clustering-text-documents-using-dbscan"">here</a>. On the one hand, algorithms such as k-means require the number of clusters as input, which I don't know. Other algorithms require parameters that are also not intuitive to specify (for me that is).</p>

<p>2) Graph algorithms: I can represent my data as a graph with the articles being the nodes and weighted adges representing the pairwise (cosine) similarity between the articles. With that, for example, I can first remove all edges that fall below a certain threshold and then might apply graph algorithms to look for strongly-connected subgraphs.</p>

<p>In short, I'm not sure where best to go from here -- I'm still pretty new in this area. I wonder if there some best practices for that, or some kind of guidelines which methods / algorithms can (not) be applied in certain scenarios.</p>

<p>(EDIT: forgot to link to related question of mine)</p>
","machine-learning, nlp, cluster-analysis, information-retrieval, unsupervised-learning",
Underfitting Pre-Trained Glove + LSTM Model: Accurcacy Unchanged,"<p>I am doing a sentiment classification using Pre-Trained Glove and LSTM model. I use google play review and scrap it by myself, resulting in 50k++ texts. I implement random over sampling on the minority classes.</p>
<p>However, when I train my LSTM model, the training accuracy is remain unchanged after several epoch, need insight how to fix the issue.</p>
<p>This is several information about the dataset:</p>
<p>Embedding size: (41151, 100)</p>
<p>Maximum sequence length: 731</p>
<p>Label distribution before random over sampling: {'positive': 58749, 'negative': 26643, 'neutral': 9106}</p>
<p>Label distribution after random over sampling: ('positive': 58749, 'negative': 26643, 'neutral': 9106}</p>
<p>Total x training set (padded): (140997, 200)</p>
<p>Total x validation set (padded): (17625, 200)</p>
<p>Total x testing set (padded): (17625, 200)</p>
<p>Total y training set (one hot): (140997, 3)</p>
<p>Total y validation set (one hot): (17625, 3)</p>
<p>Total y testing set (one hot): (17625, 2003</p>
<p>This is my full code:
<a href=""https://www.kaggle.com/code/mathiasyeremia/sentiment-analysis-model"" rel=""nofollow noreferrer"">enter link description here</a></p>
<p>This is my highlight code for this issue:</p>
<pre><code>lstm_model = Sequential()
lstm_model.add(Input(shape=(max_len,)))
lstm_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))
lstm_model.add(LSTM(256, return_sequences=True))
lstm_model.add(LSTM(128, return_sequences=True))
lstm_model.add(LSTM(64))
lstm_model.add(Dense(128, activation='relu'))
lstm_model.add(Dense(units=3, activation='softmax'))

lstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

lstm_model.summary()
</code></pre>
<p><a href=""https://i.sstatic.net/T6vCZ9Jj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/T6vCZ9Jj.png"" alt=""enter image description here"" /></a></p>
","keras, deep-learning, nlp, lstm, sentiment-analysis","<p>Based on extra information in the comments, I'm going to say the reason the LSTM model hits a wall at an (unspecified) lower accuracy than the 85% you are trying to reach is because it is not the best type of model for the problem. In which case tweaking parameters is likely to be wasted effort.</p>
<p>I'm fairly sure encoder transformers (e.g. BERT) surpassed them in sentiment analysis benchmarks a number of years back (but sorry, a quick search couldn't find a killer reference to insert here), and transformers have only got bigger and better since then.</p>
<p>Extra thought: building on top of GloVe embeddings presents you with the problem that they don't handle multiple meanings of the word. So &quot;queen&quot; might be a female king (as in embedding's party trick: king - male + female = queen) or it might be a pop group, or it might be a gay man, or it might be a chess piece.
This is going to put a limit on the accuracy of models built on them, whereas transformers don't have that limitation because they look at the whole string to see the words in context.
(It is possible to argue with that, of course, because bringing in the context is where the LSTM comes in. But transformers are still scaling strongly with 20+ layers, whereas LSTMs tend to choke after two layers.)</p>
"
Generating an n-gram dataset based on an LLM,"<p>I want a dataset of common n-grams and their log likelihoods. Normally I would download the <a href=""https://storage.googleapis.com/books/ngrams/books/datasetsv3.html"" rel=""nofollow noreferrer"">Google Books Ngram Exports</a>, but I wonder if I can generate a better dataset using a large language model. For example, this script uses <a href=""https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_completion"" rel=""nofollow noreferrer"">llama_cpp.Llama.create_completion</a> to find likely 3-grams starting with &quot;welcome to&quot;:</p>
<pre class=""lang-py prettyprint-override""><code>from llama_cpp import Llama # pip install llama-cpp-python

llm = Llama.from_pretrained(
    repo_id=&quot;unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF&quot;,
    filename=&quot;DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf&quot;,
    logits_all=True,
)
print(
    llm.create_completion(
        &quot;welcome to&quot;,
        max_tokens=1,
        logprobs=10,
    )[&quot;choices&quot;][0][&quot;logprobs&quot;][&quot;top_logprobs&quot;][0],
)
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>{' the': np.float32(-0.18572943), ' this': np.float32(-3.444591), ' our': np.float32(-4.0559974), ' python': np.float32(-4.3010955), ' a': np.float32(-4.571982), ' bc': np.float32(-5.036485), ' module': np.float32(-5.4879394), ' week': np.float32(-5.7402453), ' all': np.float32(-6.2308974), ' thread': np.float32(-6.272795)}
</code></pre>
<p>One issue is that the prompt gets prefixed with a <a href=""https://www.linkedin.com/pulse/what-special-tokens-tokenization-farhan-naqvi-uxxsf/"" rel=""nofollow noreferrer"">BOS token</a>, so I only get n-grams that appear at the beginning of a sentence. I can fix this by passing a list of tokens instead of a string. But this leads to a problem when generating 1-grams:</p>
<pre class=""lang-py prettyprint-override""><code>print(
    llm.create_completion(
        [],
        max_tokens=1,
        logprobs=10,
    )[&quot;choices&quot;][0][&quot;logprobs&quot;][&quot;top_logprobs&quot;][0],
)
</code></pre>
<p><code>AssertionError</code> at <a href=""https://github.com/abetlen/llama-cpp-python/blob/v0.3.7/llama_cpp/llama.py#L788"" rel=""nofollow noreferrer"">llama.py line 788</a>: <code>assert self.n_tokens &gt; 0</code></p>
<p>Apparently <a href=""https://github.com/ggerganov/llama.cpp"" rel=""nofollow noreferrer"">llama.cpp</a> is unable to generate text when no context is provided. I confirmed this by using llama.cpp directly without the Python wrapper. My question is, <strong>is this an arbitrary limitation of the library, or a fundamental limitation of the language model?</strong></p>
<p>I found a possible clue in the model's <a href=""https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/blob/6393b7559e403fd1d80bfead361586fd6f630a4d/config.json#L10"" rel=""nofollow noreferrer"">config.json</a> file:</p>
<pre class=""lang-json prettyprint-override""><code>  &quot;initializer_range&quot;: 0.02,
</code></pre>
<p>The <a href=""https://huggingface.co/docs/transformers/en/model_doc/qwen2#transformers.Qwen2Config.initializer_range"" rel=""nofollow noreferrer"">documentation</a> says that <code>initializer_range</code> is</p>
<blockquote>
<p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p>
</blockquote>
<p>I imagine that the model has a hidden state vector which is initialized with random values sampled from a normal distribution, and the values get updated as context is added. I wonder if it's possible to sample from the model in its initial random state, and get a list of the most common words by sampling multiple times with different random seeds.</p>
","nlp, large-language-model, n-gram, llama-cpp-python, llamacpp",
Cannot Keep My Datetime Data and &#39;No&#39; Word in My Pandas DataFrame,"<p>I have a pandas dataframe from csv and I want to clean it using Regex in Python. The data that I have look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Name</th>
<th>Date</th>
<th>Status</th>
<th>Number</th>
</tr>
</thead>
<tbody>
<tr>
<td>A/bCDef</td>
<td>2022-07-11</td>
<td>Yes</td>
<td>io123-07</td>
</tr>
<tr>
<td>GhIjK-l</td>
<td>2022-07-12</td>
<td>No</td>
<td>io456-08</td>
</tr>
</tbody>
</table>
</div>
<p>I'm trying to clean the dataframe so it will be easier to process, but the thing is, my code deletes the date, the word 'no', and the hyphen.</p>
<p>This the data that I got so far:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>name</th>
<th>date</th>
<th>status</th>
<th>number</th>
</tr>
</thead>
<tbody>
<tr>
<td>abcdef</td>
<td></td>
<td>yes</td>
<td>io</td>
</tr>
<tr>
<td>ghijkl</td>
<td></td>
<td>no</td>
<td>io</td>
</tr>
</tbody>
</table>
</div>
<p>This is the code that I found on the internet and tried on my dataframe:</p>
<pre><code>def regex_values(cols):
    nltk.download(&quot;stopwords&quot;)
    stemmer = nltk.SnowballStemmer('english')
    stopword = set(stopwords.words('english'))

    cols = str(cols).lower()
    cols = re.sub('\[.*?\]', '', cols)
    cols = re.sub('https?://\S+|www\.\S+', '', cols)
    cols = re.sub('&lt;.*?&gt;+/', '', cols)
    cols = re.sub('[%s]' % re.escape(string.punctuation), '', cols)
    cols = re.sub('\n', '', cols)
    cols = re.sub('\w*\d\w*', '', cols)
    cols = re.sub(r'^\s+|\s+$', '', cols)
    cols = re.sub(' +', ' ', cols)
    cols = re.sub(r'\b(\w+)(?:\W\1\b)+', 'r\1', cols, flags = re.IGNORECASE)
    cols = [word for word in cols.split(' ') if word not in stopword]
    cols = &quot; &quot;.join(cols)
    
    return cols
</code></pre>
<p>This is the pandas dataframe that I wish to have at the end:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>name</th>
<th>date</th>
<th>status</th>
<th>number</th>
</tr>
</thead>
<tbody>
<tr>
<td>abcdef</td>
<td>2022-07-11</td>
<td>yes</td>
<td>io123-07</td>
</tr>
<tr>
<td>ghijkl</td>
<td>2022-07-12</td>
<td>no</td>
<td>io456-08</td>
</tr>
</tbody>
</table>
</div>
<p>I'm new to Regex so I wish anyone can help me to code the right code. Or if there is a simpler way to clean my data I would much appreciate the help. Thanks in advance.</p>
","python, regex, datetime, nlp",
huggingface transformer models: KeyError: &#39;input_ids&#39; message at beginning of BERT model training,"<p>Using the Huggingface transformer library, I am encountering a bug in the final step when I go to fine tune the BERT language model for masked language modelling task.  I am looking to fine tune it for an in domain finance corpus that model has not been trained on yet.  However, I get the following error message when I call the model to train:  KeyError: 'input_ids'.  Provided below are the steps and code I took.  Any insights are appreciated!</p>
<p>First, I created a dataset object from a pandas dataframe that was in turn created from a csv file with 1 column of many rows of text:</p>
<pre><code>unlabelled_dataset = Dataset.from_pandas(unlabelled)    
</code></pre>
<p>Second, I then tokenized the dataset with the following code:</p>
<pre><code>tokenizerBERT = BertTokenizerFast.from_pretrained('bert-base-uncased')  #BERT model tokenization &amp; check
tokenizerBERT(unlabelled_dataset['paragraphs'], padding=True, truncation=True)
tokenizerBERT.save_pretrained('tokenizers/pytorch/labelled/BERT/')
</code></pre>
<p>Third, I created a data collator as instructed:</p>
<pre><code>data_collator_BERT = DataCollatorForLanguageModeling(tokenizer=tokenizerBERT, mlm=True, mlm_probability=0.15)
</code></pre>
<p>Next, I select my model from_pretrained to get the benefits of transfer learning:</p>
<pre><code>model_BERT = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)
</code></pre>
<p>Next, I passed my training args to the transformer trainer and initialize:</p>
<pre><code>training_args_BERT = TrainingArguments(
    output_dir=&quot;./BERT&quot;,
    num_train_epochs=10,
    evaluation_strategy='steps',
    warmup_steps=10000,
    weight_decay=0.01,
    per_gpu_train_batch_size=64,    
)

trainer_BERT = Trainer(
    model=model_BERT,
    args=training_args_BERT,
    data_collator=data_collator_BERT,
    train_dataset=unlabelled_dataset,
)
</code></pre>
<p>Last, I call the model to train and get the error KeyError: 'input_ids'</p>
<pre><code>trainer_BERT.train()
</code></pre>
<p>Any insights on how to debug this approach to training the model?</p>
<p>Provided below is the exact error message received:</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-9-83b7063dea0b&gt; in &lt;module&gt;
----&gt; 1 trainer_BERT.train()
      2 trainer.save_model(&quot;./models/royalBERT&quot;)

~/anaconda3/lib/python3.7/site-packages/transformers/trainer.py in train(self, model_path, trial)
    755             self.control = self.callback_handler.on_epoch_begin(self.args, self.state, self.control)
    756 
--&gt; 757             for step, inputs in enumerate(epoch_iterator):
    758 
    759                 # Skip past any already trained steps if resuming training

~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)
    361 
    362     def __next__(self):
--&gt; 363         data = self._next_data()
    364         self._num_yielded += 1
    365         if self._dataset_kind == _DatasetKind.Iterable and \

~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)
    401     def _next_data(self):
    402         index = self._next_index()  # may raise StopIteration
--&gt; 403         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    404         if self._pin_memory:
    405             data = _utils.pin_memory.pin_memory(data)

~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     45         else:
     46             data = self.dataset[possibly_batched_index]
---&gt; 47         return self.collate_fn(data)

~/anaconda3/lib/python3.7/site-packages/transformers/data/data_collator.py in __call__(self, examples)
    193     ) -&gt; Dict[str, torch.Tensor]:
    194         if isinstance(examples[0], (dict, BatchEncoding)):
--&gt; 195             examples = [e[&quot;input_ids&quot;] for e in examples]
    196         batch = self._tensorize_batch(examples)
    197         if self.mlm:

~/anaconda3/lib/python3.7/site-packages/transformers/data/data_collator.py in &lt;listcomp&gt;(.0)
    193     ) -&gt; Dict[str, torch.Tensor]:
    194         if isinstance(examples[0], (dict, BatchEncoding)):
--&gt; 195             examples = [e[&quot;input_ids&quot;] for e in examples]
    196         batch = self._tensorize_batch(examples)
    197         if self.mlm:

KeyError: 'input_ids'
</code></pre>
","python, nlp, bert-language-model",
RuntimeError: Failed to import transformers.integrations.bitsandbytes,"<p>I am trying to load an llm model in 4 bits precision. However, I got RuntimeError: Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):
[WinError 193] %1 is not a valid Win32 application after running the code below:</p>
<pre><code>#device_map = {&quot;&quot;: 0}

model = AutoModelForCausalLM.from_pretrained(model_id, 
                                             device_map= &quot;auto&quot;, 
                                             quantization_config=quantization_config,
                                             token=ACCESS_TOKEN)
model.eval()
device = 'cuda' if torch.cuda.is_available() else 'cpu'
</code></pre>
<p>I have installed accelerate, bitsandbytes, transformers, and all other packages but I am still getting the same error.</p>
<p>The only thing that I am seeing in the traceback is OSError.</p>
<p>I also restarted the kernel but it still did not work.</p>
","deep-learning, nlp, huggingface-transformers, large-language-model",
"How to pass AzureOpenAIEmbeddings in CrewAI, I don&#39;t have api keys I use azure_ad_token?","<p>How to pass AzureOpenAIEmbeddings, I don't have api keys I use azure_ad_token?</p>
<pre><code>from langchain_openai.embeddings import AzureOpenAIEmbeddings

azure_embeddings = AzureOpenAIEmbeddings(
openai_api_version=conf.pf_api_version,
azure_endpoint=conf.pf_oa_endpoint_embed,
azure_ad_token=tokens,
model=conf.pf_embedding_engine,
)

crew = Crew(
  agents=[support_agent, support_quality_assurance_agent],
  tasks=[inquiry_resolution, quality_assurance_review],
  verbose=2,
  memory=True,
  embedder=embedder_config,
)
</code></pre>
<pre><code>raise SchemaError([message] + x.autos, [e.format(data) if e else None] + x.errors)
schema.SchemaError: Key 'embedder' error:
Key 'config' error:
Wrong keys 'azure_ad_token', 'azure_endpoint', 'openai_api_version' in {'azure_endpoint
</code></pre>
","python-3.x, nlp, large-language-model, crewai",
How to load a huggingface pretrained transformer model directly to GPU?,"<p>I want to load a huggingface pretrained transformer model directly to GPU (not enough CPU space)
e.g. loading BERT</p>
<pre><code>from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(&quot;bert-base-uncased&quot;)
</code></pre>
<p>would be loaded to CPU until executing</p>
<pre><code>model.to('cuda')
</code></pre>
<p>now the model is loaded into GPU</p>
<p>I want to load the model directly into GPU when executing <code>from_pretrained</code>. Is this possible?</p>
","python, nlp, huggingface-transformers","<p>I'm answering my own question.</p>
<p>Hugging Face <code>accelerate</code> (add via <code>pip install accelerate</code>) could be helpful in moving the model to GPU before it's fully loaded in CPU. It's useful when:</p>
<p>GPU memory &gt; model size &gt; CPU memory</p>
<p>Also specify <code>device_map=&quot;cuda&quot;</code>:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(&quot;bert-base-uncased&quot;, device_map=&quot;cuda&quot;)
</code></pre>
"
Calculating Topic Correlations or Coocurrences for keyATM,"<p>I have been playing around with the keyATM package extensively, however unfortunately there is no approach how to calculate topic correlations and cooccurences, once the model is calculated. I already adjusted the the alpha prior to receive more topic correlation, however I am stuck at this point.</p>
<pre><code># Compute total topics
total_k &lt;- 25 + length(keywords)  
# Construct Gamma matrix for priors
gamma_matrix &lt;- matrix(as.numeric(1), nrow = total_k, ncol = 2)


for(al in 1:5){

print(paste0(&quot;Alpha set to &quot;,al))  
  
priors          &lt;-  list(alpha = rep(al, total_k),
                         beta = 0.01,
                         beta_s = 0.1,
                         gamma = gamma_matrix)

out_temp &lt;- keyATM(
  docs              = keyATM_docs,    # text input
  no_keyword_topics = 25,              # number of topics without keywords
  keywords          = keywords,       # keywords
  model             = &quot;base&quot;,         # select the model
  options           = list(seed = 250,
                           iterations = 1500,
                           verbose = TRUE),
  priors = priors
)

assign(paste0(&quot;out_alpha&quot;,al), out_temp)

}
</code></pre>
<p>I tried several approaches from other topic model packages, but nothing worked.
If anyone has any recommendations how to proceed from here, I would be very thankful.</p>
","r, nlp, topic-modeling",
How to resolve ValueError while training Seq2Seq using DataCollatorForSeq2Seq,"<p>I want to fine tune a VisionEncoderDecoderModel.from_pretrained(model_name)
I use a CustomOCRDataset from <a href=""https://learnopencv.com/fine-tuning-trocr-training-trocr-to-recognize-curved-text/"" rel=""nofollow noreferrer"">Learn Open CV</a>.
But the default_data_collator fails to stack the inputs because the samples have a different shape , so I decided to use DataCollatorForSeq2Seq and Resize in augmentation.</p>
<p>I get an error
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['pixel_values']</p>
<p>So I changed <strong>getitem</strong>  to get input_ids, but the error is still the same.</p>
<pre><code>def __getitem__(self, idx):
        file_name = self.df['file_name'][idx]
        text = self.df['text'][idx]

        assert text.strip() != &quot;&quot;, f&quot;ERROR Empty text in {idx}&quot;

        # Read the image, apply augmentations, and get the transformed pixels.
        image = Image.open(self.root_dir + file_name).convert('RGB')
        image = train_transforms(image)
        pixel_values = self.processor(image, return_tensors='pt').pixel_values
        # Pass the text through the tokenizer and get the labels,
        # i.e. tokenized labels.
        labels = self.processor.tokenizer(
            text,
            padding='max_length',
            max_length=self.max_target_length,
            return_tensors='pt'
        ).input_ids.squeeze(0)

        # We are using -100 as the padding token.
        labels = torch.where(labels == self.processor.tokenizer.pad_token_id, torch.tensor(-100), labels)
        encoding = {&quot;pixel_values&quot;: pixel_values.squeeze(0),
                    &quot;input_ids&quot;: labels}
        return encoding
</code></pre>
<pre><code>@dataclass(frozen=True)
class TrainingConfig:
    BATCH_SIZE:    int = 16
    EPOCHS:        int = 5
    LEARNING_RATE: float = 0.00005

@dataclass(frozen=True)
class DatasetConfig:
    DATA_ROOT:     str = image_dir

@dataclass(frozen=True)
class ModelConfig:
    MODEL_NAME: str = 'microsoft/trocr-base-handwritten'
</code></pre>
<pre><code># Augmentations.
train_transforms = transforms.Compose([
    transforms.Resize((1024, 880))
])
</code></pre>
<pre><code>processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)
train_dataset = CustomOCRDataset(
    root_dir=os.path.join(DatasetConfig.DATA_ROOT, train_destination),
    df=train_df,
    processor=processor
)
valid_dataset = CustomOCRDataset(
    root_dir=os.path.join(DatasetConfig.DATA_ROOT, test_destination),
    df=test_df,
    processor=processor
)
</code></pre>
<pre><code>training_args = Seq2SeqTrainingArguments(
    predict_with_generate=True,
    evaluation_strategy='epoch',
    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,
    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,
    fp16=True,
    output_dir='seq2seq_model_printed/',
    logging_strategy='epoch',
    save_strategy='epoch',
    save_total_limit=5,
    report_to='tensorboard',
    num_train_epochs=TrainingConfig.EPOCHS
)
</code></pre>
<pre><code>data_collator = DataCollatorForSeq2Seq(tokenizer=processor.tokenizer, model=model, padding=True)
# Initialize trainer.
trainer = Seq2SeqTrainer(
    model=model,
    tokenizer=processor.feature_extractor,
    args=training_args,
    compute_metrics=compute_cer,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    data_collator=data_collator
)
trainer.train()
</code></pre>
<p>The full ERROR :</p>
<pre><code>File \transformers\data\data_collator.py:599, in DataCollatorForSeq2Seq.__call__(self, features, return_tensors)
    596 non_labels_features = [{k: v for k, v in feature.items() if k != label_name} for feature in features]
    598 # run through tokenizer without labels to ensure no side effects
--&gt; 599 batch = pad_without_fast_tokenizer_warning(
    600     self.tokenizer,
    601     non_labels_features,
    602     padding=self.padding,
    603     max_length=self.max_length,
    604     pad_to_multiple_of=self.pad_to_multiple_of,
    605     return_tensors=return_tensors,
    606 )
    608 # we have to pad the labels manually as we cannot rely on `tokenizer.pad` and we need them to be of the same length to return tensors
    609 no_padding = self.padding is False or self.padding == PaddingStrategy.DO_NOT_PAD

File \transformers\data\data_collator.py:66, in pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs)
     63 tokenizer.deprecation_warnings[&quot;Asking-to-pad-a-fast-tokenizer&quot;] = True
     65 try:
---&gt; 66     padded = tokenizer.pad(*pad_args, **pad_kwargs)
     67 finally:
     68     # Restore the state of the warning.
     69     tokenizer.deprecation_warnings[&quot;Asking-to-pad-a-fast-tokenizer&quot;] = warning_state

File \transformers\tokenization_utils_base.py:3305, in PreTrainedTokenizerBase.pad(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)
   3303 # The model's main input name, usually `input_ids`, has been passed for padding
   3304 if self.model_input_names[0] not in encoded_inputs:
-&gt; 3305     raise ValueError(
   3306         &quot;You should supply an encoding or a list of encodings to this method &quot;
   3307         f&quot;that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}&quot;
   3308     )
   3310 required_input = encoded_inputs[self.model_input_names[0]]
   3312 if required_input is None or (isinstance(required_input, Sized) and len(required_input) == 0):

ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['pixel_values']
</code></pre>
<p>The solutions that I've already tried were:
-&gt; to do transforms.Resize((1024, 880)),
-&gt; to use a custom data collator
such as:
`def collate_fn(batch):
batch = list(filter(lambda x: x is not None, batch))</p>
<pre><code># Pack pixel vals und labels separately in lists
pixel_values = [item['pixel_values'] for item in batch]
labels = [item['labels'] for item in batch]

# Convert lists to tensors + padding
pixel_values = torch.stack(pixel_values)
labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)

return {
    &quot;pixel_values&quot;: pixel_values,
    &quot;labels&quot;: labels
}`
</code></pre>
<p>I got a TypeError: ViTModel.forward() got an unexpected keyword argument 'num_items_in_batch'. That's why I decided to use DataCollatorForSeq2Seq.</p>
<p>But I'm getting another error over and over: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['pixel_values'].
Thanks in advance!</p>
","nlp, huggingface-transformers, huggingface-tokenizers, seq2seq",
LLM Model Lacking Confidence and Changing Answers Based on User Input,"<p>I've trained a Large Language Model (LLM) using the RAG method to answer user queries. However, I'm facing an issue where the model lacks confidence in its answers and changes them based on user input, even when the initial response is correct.</p>
<p>For example, when asked &quot;What is the capital of France?&quot;, the model correctly responds with &quot;Paris.&quot; However, if the user replies &quot;No, it's Berlin,&quot; the model accepts this incorrect response and later provides &quot;Berlin&quot; as the capital of France when asked again.</p>
<p>I've tried using different prompt templates to reinforce answer consistency, but the issue persists. How can I improve the model’s robustness and prevent it from altering correct answers based on user responses? Any suggestions or guidance would be greatly appreciated.</p>
","nlp, langchain, large-language-model, aiml",
How to gradually train a model on transformers library?,"<p>I have seen <a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=o6sASa36Nf-N"" rel=""nofollow noreferrer"">this tutorial on how to train a BERT model</a> from scratch on Hugging Face Transformers library. </p>

<p>I'm trying to train a GPT-2 model on 1.5 GB data on Google Colab. I load up all the data using this code:</p>

<pre><code>dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=""./my-1.5gb-large-file.txt"",
    block_size=128,
)
</code></pre>

<p>The file gets too large, training fails because of limited memory in the GPU. Is there a way I could train my GPT-2 model gradually by splitting the dataset?</p>
","python, tensorflow, nlp, pytorch, huggingface-transformers",
PunktTokenizer does not work with Russian `я.`,"<p>When tokenizing paragraphs to sentences in the Russian language, I am observing the special case when the sequence is not treated as the end of the sentence. The case is with the <code>я.</code> at the end of the sentence. See the working example:</p>
<pre><code>import nltk

tok = nltk.tokenize.PunktTokenizer('russian')

print('-----------------')
line = 'Родилась заново, стал размышлять я. Она не застрелена, а это дело упрощает.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')

print('-----------------')
line = 'Родилась заново, стал размышлять я. - Она не застрелена, а это дело упрощает.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')
</code></pre>
<p><a href=""https://i.sstatic.net/yrj2ukn0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yrj2ukn0.png"" alt=""the output"" /></a></p>
<p>The second case works as expected. It differs only in adding the dash (kind of introduction of the speaker's note [sorry for my lack of terms]).</p>
<p>The <code>я</code> is not present in the Russian abbreviations (<code>c:\nltk_data\tokenizers\punkt_tab\russian\abbrev_types.txt</code>). However, even when added to the file with abbreviations, it does not make a difference.</p>
<p>How the situation should be fixed?</p>
<p>The nltk is of the version 3.9.1, the nltk_data are shared -- stored in c:\nltk_data; fresh download (30. 1. 2025). Python 3.12 on Windows 10 was used.</p>
<p><strong>Update:</strong></p>
<p>(... observing donwnvotes). Please, I am very new to nltk, and I tried my best to get the answer by myself. When down-voting, write the comment why. I am searching for the answer in various sources without success.</p>
<p>I am using UTF-8 encoding for both the test script here, and also in the file that is actually being processed (the image take from notepad++).
<a href=""https://i.sstatic.net/bmEd4NOU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bmEd4NOU.png"" alt=""UTF-8 encoding used"" /></a></p>
<p><strong>2nd Update:</strong>
As Joop Eggen suggested, I have tried with other single letters:</p>
<pre><code>import nltk

tok = nltk.tokenize.PunktTokenizer('russian')

print('\n================= я in the original question (я not in the abbrev_types.txt)\n')
line = 'Родилась заново, стал размышлять я. Она не застрелена, а это дело упрощает.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')

print('-----------------')
line = 'Родилась заново, стал размышлять я. - Она не застрелена, а это дело упрощает.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')

print('\n================= г is in the abbrev_types.txt\n')
line = 'Родилась заново, стал размышлять г. Она не застрелена, а это дело упрощает.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')

print('-----------------')
line = 'Родилась заново, стал размышлять г. - Она не застрелена, а это дело упрощает.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')

print('\n================= а IS NOT the abbrev_types.txt\n')
line = 'Родилась заново, стал размышлять а. Она не застрелена, а это дело упрощает.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')

print('-----------------')
line = 'Родилась заново, стал размышлять а. - Она не застрелена, а это дело упрощает.'
lst = tok.tokenize(line)
for n, s in enumerate(lst, 1):
    print(f'{n}: {s!r}')
</code></pre>
<p><a href=""https://i.sstatic.net/9dezvzKN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9dezvzKN.png"" alt=""other single letters"" /></a></p>
<p>The <code>а</code> is also <strong>not</strong> in the abbreviations; however, it is treated as if it was (differently from <code>я</code>). The <code>г</code> is in the abbreviations, so here the behavior is expected.</p>
","nlp, nltk, tokenize",
QuickUMLS Always Returns &quot;UNK&quot; for Any Input Text,"<p>I am using QuickUMLS to extract UMLS Concept Unique Identifiers (CUIs) from text, but no matter what word I input, it always returns &quot;UNK&quot;. Here is my code:</p>
<pre><code>from quickumls import QuickUMLS

quickumls_fp = &quot;med7_en/lib/python3.10/site-packages/quickumls&quot;
matcher = QuickUMLS(quickumls_fp)

def extract_umls_cuis(text):
    &quot;&quot;&quot;Extract UMLS CUIs using QuickUMLS.&quot;&quot;&quot;
    if isinstance(text, str):
        matches = matcher.match(text)
        if matches:
            return [match['cui'] for match in matches[0]]
        else:
            return &quot;UNK&quot;

sample_text = &quot;diclofenac.&quot;
print(extract_umls_cuis(sample_text))
</code></pre>
<p>What I Have Checked:</p>
<ul>
<li>QuickUMLS Installation: I have installed QuickUMLS correctly.</li>
<li>UMLS Data Availability: I have set the correct path to QuickUMLS.</li>
<li>Different Input Words: I tried various medical terms, but all return &quot;UNK&quot;.</li>
</ul>
","python, machine-learning, deep-learning, nlp",
How can I add citations in the response on Vectara? While testing the Multiple Corpora Query,"<p>How can I add citations in the response on Vectara? While testing the Multiple Corpora Query, I updated the citation in the payload. I followed the approach mentioned in the Vectara documentation.
I have tried all the models mentioned in the documentation and followed the instructions on how to provide the citation style, but it is still not working.</p>
<p>The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset:</p>
<pre><code>mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)
vectara-summary-ext-24-05-sml (gpt-3.5-turbo)
vectara-summary-ext-24-05-med-omni (gpt-4o)
vectara-summary-ext-24-05-med (gpt-4.0)
vectara-summary-ext-24-05-large (gpt-4.0-turbo)
</code></pre>
<p>I have used these models, but still, the citation is not showing in the response.</p>
<p>The documentation states that to use citations, one must specify one of the following summarizers in the generation_preset:</p>
<pre><code>mockingbird-1.0-2024-07-16 (Vectara's Mockingbird LLM)
vectara-summary-ext-24-05-sml (gpt-3.5-turbo)
vectara-summary-ext-24-05-med-omni (gpt-4o)
vectara-summary-ext-24-05-med (gpt-4.0)
vectara-summary-ext-24-05-large (gpt-4.0-turbo)
</code></pre>
<p>I have used these models, but still, the citation is not showing in the response.</p>
","nlp, large-language-model, rag, vectara, enterprise-rag",
llama-cpp-python not using NVIDIA GPU CUDA,"<p>I have been playing around with <a href=""https://github.com/oobabooga/text-generation-webui"" rel=""noreferrer"">oobabooga text-generation-webui </a> on my Ubuntu 20.04 with my NVIDIA GTX 1060 6GB for some weeks without problems. I have been using llama2-chat models sharing memory between my RAM and NVIDIA VRAM. I installed without much problems following the intructions on its repository.</p>
<p>So what I want now is to use the model loader <code>llama-cpp</code> with its package <code>llama-cpp-python</code> bindings to play around with it by myself. So using the same miniconda3 environment that oobabooga text-generation-webui uses I started a jupyter notebook and I could make inferences and everything is working well <em>BUT ONLY for CPU</em>.</p>
<p>A working example bellow,</p>
<pre><code>from llama_cpp import Llama

llm = Llama(model_path=&quot;/mnt/LxData/llama.cpp/models/meta-llama2/llama-2-7b-chat/ggml-model-q4_0.bin&quot;, 
            n_gpu_layers=32, n_threads=6, n_ctx=3584, n_batch=521, verbose=True), 

prompt = &quot;&quot;&quot;[INST] &lt;&lt;SYS&gt;&gt;
Name the planets in the solar system? 
&lt;&lt;/SYS&gt;&gt;
[/INST] 
&quot;&quot;&quot;
output = llm(prompt, max_tokens=350, echo=True)
print(output['choices'][0]['text'].split('[/INST]')[-1])
</code></pre>
<blockquote>
<p>Of course! Here are the eight planets in our solar system, listed in order from closest to farthest from the Sun:</p>
<ol>
<li>Mercury</li>
<li>Venus</li>
<li>Earth</li>
<li>Mars</li>
<li>Jupiter</li>
<li>Saturn</li>
<li>Uranus</li>
<li>Neptune</li>
</ol>
</blockquote>
<blockquote>
<p>Note that Pluto was previously considered a planet but is now classified as a dwarf planet due to its small size and unique orbit.</p>
</blockquote>
<p>I want to make inference using GPU as well. What is wrong?
Why can't I offload to gpu like the parameter <code>n_gpu_layers=32</code> specifies and also like <code>oobabooga text-generation-webui</code> already does on the same miniconda environment whithout any problems?</p>
","python, python-3.x, nlp, llama, llama-cpp-python","<p>After searching around and suffering quite for 3 weeks I found out this <a href=""https://github.com/abetlen/llama-cpp-python/issues/509"" rel=""nofollow noreferrer"">issue</a> on its repository.</p>
<p>The <code>llama-cpp-python</code> needs to known where is the <code>libllama.so</code> shared library. So exporting it before running my python interpreter, jupyter notebook etc. did the trick.</p>
<p>For using the miniconda3 installation used by <code>oobabooga text-generation-webui</code> I exported it like bellow:</p>
<pre><code>export LLAMA_CPP_LIB=/yourminicondapath/miniconda3/lib/python3.10/site-packages/llama_cpp_cuda/libllama.so
</code></pre>
<h3>Voilà!!!!</h3>
<p>On importing  <code>from llama_cpp import Llama</code> I get</p>
<blockquote>
<p>ggml_init_cublas: found 1 CUDA devices:
Device 0: NVIDIA GeForce GTX 1060, compute capability 6.1</p>
</blockquote>
<p>And on</p>
<pre><code>llm = Llama(model_path=&quot;/mnt/LxData/llama.cpp/models/meta-llama2/llama-2-7b-chat/ggml-model-q4_0.bin&quot;, 
            n_gpu_layers=28, n_threads=6, n_ctx=3584, n_batch=521, verbose=True), 
</code></pre>
<p>...</p>
<blockquote>
<p>llama_model_load_internal: using CUDA for GPU acceleration
llama_model_load_internal: mem required  = 2381.32 MB (+ 1026.00 MB per state)
llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 480 MB VRAM for the scratch buffer
llama_model_load_internal: offloading 28 repeating layers to GPU
llama_model_load_internal: offloaded 28/35 layers to GPU
llama_model_load_internal: total VRAM used: 3521 MB
...</p>
</blockquote>
"
Fuzzy Wuzzy: Obtain accurate scores,"<p>I have a case where users posts multiple messages daily to our work channel. These messages are an issue that can be related to multiple applications that we support (Okta, Slack, Cloudflare etc...). We have multiple solution documented that can help these users, and I want our bot to be able to send them the correct documentation automatically based on their messages.</p>
<p>I am using the string matching approach to achieve this with the <a href=""https://pypi.org/project/fuzzywuzzy/"" rel=""nofollow noreferrer"">FuzzyWuzzy</a> Python library.</p>
<p>I run the message posted by a user in the list of applications that we support, and whenever FuzzyWuzzy gives me a score of <code>&gt; 90</code>, I assume that this is the application that is being requested in the message. I am using the code below:</p>
<p><code>requests.py</code></p>
<pre><code>from enum import Enum
from fuzzywuzzy import fuzz

class RequestType(Enum):

    OKTA_ACCESS = &quot;okta access&quot;
    OKTA_UNLOCK = &quot;okta unlock&quot;
    CLOUDFLARE = &quot;cannot connect to cloudflare&quot;

class RequestsMatcher:

    def __init__(self, message: str) -&gt; None:
        
        scores = [
            (
                type,
                fuzz.partial_token_set_ratio(message, type.value)
            )
            for type in RequestType
        ]

        mvs = [score for score in scores if score[1] &gt; 75]

        hits = [score for score in scores if score[1] &gt; 90]

        print(scores)
        self.scores = scores
        self.highest = mvs
        self.hit = next(iter(hits), None)
</code></pre>
<p>The issue with this code is that, for a sample message like the one below, both <code>cloudflare</code> and <code>okta</code> will give a score of <code>&gt; 90</code> when it should've been just Cloudflare to return a score of <code>&gt; 90</code>:</p>
<pre><code>        '''
        Hello team, it looks like I am having an issue with Cloudflare. I have been added to the following Okta group.
        Can someone look into this for me, please?
        '''
</code></pre>
<p>Is there a better score calculation or better approach to obtain the necessary application that is being requested on support?</p>
","python, python-3.x, nlp, string-matching, fuzzy-search",
Transformers PaliGemma evaluate and compute_loss fail with tensors/device errors,"<p>I'm loading a PaliGemma2 model <code>google/paligemma2-3b-pt-224</code> and trying to fine-tune using Trainer/Seq2SeqTrainer. If I add evaluation, this fails. After doing some digging, I found that this only happens if the model is in evaluate mode.</p>
<pre><code>batch = [valid_dataset[i] for i in range(8)]
inputs = collate_fn(batch)
#generate_ids = model.generate(**inputs, max_length=286+30)
trainer.model.train()
trainer.compute_loss(model, inputs, return_outputs=False, num_items_in_batch=416)
print(&quot;works&quot;)
trainer.model.train(False)
trainer.compute_loss(model, inputs, return_outputs=False, num_items_in_batch=416)
print(&quot;fails.&quot;)
</code></pre>
<p>I've worked around it by mokey-patching compute_loss_context_manager as follows:</p>
<pre><code>orig_context_manager = trainer.compute_loss_context_manager
class TempTrainContext(object):
    def __init__(self, trainer):
        self.trainer = trainer
        self.orig_context_manager = trainer.compute_loss_context_manager
    def __enter__(self):
        self.orig_context_inst = self.orig_context_manager()
        self.orig_context_inst.__enter__()
        self.training_enter = self.trainer.model.training
        self.trainer.model.train()
    def __exit__(self, type, value, traceback):
        self.trainer.model.train(self.training_enter)
        self.orig_context_inst.__exit__(type, value, traceback)
    def __call__(self):
        return self

trainer.compute_loss_context_manager = TempTrainContext(trainer)
</code></pre>
<p>(Bonus question: Is this safe to do, or will I train on the test set?)</p>
<p>My versions are:</p>
<pre><code>Python Version: 3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 16:05:46) [GCC 13.3.0]
Torch Version: 2.5.1+cu124
CUDA Available: True
CUDA Device Count: 2
GPU Name: NVIDIA GeForce RTX 3090
Transformers Version: 4.48.1
Tokenizers Version: 0.21.0
Accelerate Version: 1.3.0
</code></pre>
<p>Error:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[13], line 8
      6 print(&quot;works&quot;)
      7 trainer.model.train(False)
----&gt; 8 trainer.compute_loss(model, inputs, return_outputs=False, num_items_in_batch=416)
      9 print(&quot;fails.&quot;)
     12 orig_context_manager = trainer.compute_loss_context_manager

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/trainer.py:3731, in Trainer.compute_loss(self, model, inputs, return_outputs, num_items_in_batch)
   3729         loss_kwargs[&quot;num_items_in_batch&quot;] = num_items_in_batch
   3730     inputs = {**inputs, **loss_kwargs}
-&gt; 3731 outputs = model(**inputs)
   3732 # Save past state if it exists
   3733 # TODO: this needs to be fixed and made cleaner later.
   3734 if self.args.past_index &gt;= 0:

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-&gt; 1736     return self._call_impl(*args, **kwargs)

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/accelerate/hooks.py:170, in add_hook_to_module.&lt;locals&gt;.new_forward(module, *args, **kwargs)
    168         output = module._old_forward(*args, **kwargs)
    169 else:
--&gt; 170     output = module._old_forward(*args, **kwargs)
    171 return module._hf_hook.post_forward(module, output)

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/paligemma/modeling_paligemma.py:530, in PaliGemmaForConditionalGeneration.forward(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep)
    525     labels = torch.where(input_ids == self.pad_token_id, self.config.ignore_index, labels)
    527 causal_mask = self._update_causal_mask(
    528     attention_mask, token_type_ids, past_key_values, cache_position, input_ids, inputs_embeds, is_training
    529 )
--&gt; 530 outputs = self.language_model(
    531     attention_mask=causal_mask,
    532     position_ids=position_ids,
    533     past_key_values=past_key_values,
    534     inputs_embeds=inputs_embeds,
    535     use_cache=use_cache,
    536     output_attentions=output_attentions,
    537     output_hidden_states=output_hidden_states,
    538     return_dict=return_dict,
    539     cache_position=cache_position,
    540     num_logits_to_keep=num_logits_to_keep,
    541 )
    543 logits = outputs.logits
    544 loss = None

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-&gt; 1736     return self._call_impl(*args, **kwargs)

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:842, in Gemma2ForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)
    840 return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    841 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
--&gt; 842 outputs = self.model(
    843     input_ids=input_ids,
    844     attention_mask=attention_mask,
    845     position_ids=position_ids,
    846     past_key_values=past_key_values,
    847     inputs_embeds=inputs_embeds,
    848     use_cache=use_cache,
    849     output_attentions=output_attentions,
    850     output_hidden_states=output_hidden_states,
    851     return_dict=return_dict,
    852     cache_position=cache_position,
    853 )
    855 hidden_states = outputs[0]
    856 # Only compute necessary logits, and do not upcast them to float if we are not computing the loss

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-&gt; 1736     return self._call_impl(*args, **kwargs)

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:629, in Gemma2Model.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)
    617     layer_outputs = self._gradient_checkpointing_func(
    618         decoder_layer.__call__,
    619         hidden_states,
   (...)
    626         cache_position,
    627     )
    628 else:
--&gt; 629     layer_outputs = decoder_layer(
    630         hidden_states,
    631         position_embeddings=position_embeddings,
    632         attention_mask=causal_mask,
    633         position_ids=position_ids,
    634         past_key_value=past_key_values,
    635         output_attentions=output_attentions,
    636         use_cache=use_cache,
    637         cache_position=cache_position,
    638         **flash_attn_kwargs,
    639     )
    641 hidden_states = layer_outputs[0]
    643 if output_attentions:

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-&gt; 1736     return self._call_impl(*args, **kwargs)

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/accelerate/hooks.py:170, in add_hook_to_module.&lt;locals&gt;.new_forward(module, *args, **kwargs)
    168         output = module._old_forward(*args, **kwargs)
    169 else:
--&gt; 170     output = module._old_forward(*args, **kwargs)
    171 return module._hf_hook.post_forward(module, output)

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:299, in Gemma2DecoderLayer.forward(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)
    296 hidden_states = self.input_layernorm(hidden_states)
    298 # Self Attention
--&gt; 299 hidden_states, self_attn_weights = self.self_attn(
    300     hidden_states=hidden_states,
    301     position_embeddings=position_embeddings,
    302     attention_mask=attention_mask,
    303     position_ids=position_ids,
    304     past_key_value=past_key_value,
    305     output_attentions=output_attentions,
    306     use_cache=use_cache,
    307     cache_position=cache_position,
    308 )
    309 hidden_states = self.post_attention_layernorm(hidden_states)
    310 hidden_states = residual + hidden_states

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)
   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735 else:
-&gt; 1736     return self._call_impl(*args, **kwargs)

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)
   1742 # If we don't have any hooks, we want to skip the rest of the logic in
   1743 # this function, and just call forward.
   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1745         or _global_backward_pre_hooks or _global_backward_hooks
   1746         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1747     return forward_call(*args, **kwargs)
   1749 result = None
   1750 called_always_called_hooks = set()

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/accelerate/hooks.py:170, in add_hook_to_module.&lt;locals&gt;.new_forward(module, *args, **kwargs)
    168         output = module._old_forward(*args, **kwargs)
    169 else:
--&gt; 170     output = module._old_forward(*args, **kwargs)
    171 return module._hf_hook.post_forward(module, output)

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:224, in Gemma2Attention.forward(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)
    221 if past_key_value is not None:
    222     # sin and cos are specific to RoPE models; cache_position needed for the static cache
    223     cache_kwargs = {&quot;sin&quot;: sin, &quot;cos&quot;: cos, &quot;cache_position&quot;: cache_position}
--&gt; 224     key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
    226 attention_interface: Callable = eager_attention_forward
    227 if self.config._attn_implementation != &quot;eager&quot;:

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/cache_utils.py:1717, in HybridCache.update(self, key_states, value_states, layer_idx, cache_kwargs)
   1714 else:
   1715     update_fn = self._static_update
-&gt; 1717 return update_fn(
   1718     cache_position,
   1719     layer_idx,
   1720     key_states,
   1721     value_states,
   1722     k_out,
   1723     v_out,
   1724     k_out.shape[2],
   1725 )

File ~/local/miniconda3/envs/paligemma/lib/python3.12/site-packages/transformers/cache_utils.py:1694, in HybridCache._static_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len)
   1693 def _static_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):
-&gt; 1694     k_out[:, :, cache_position] = key_states
   1695     v_out[:, :, cache_position] = value_states
   1697     self.key_cache[layer_idx] = k_out

RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!&quot;
</code></pre>
<p>Error of Evaluator (bottom half of file): <a href=""https://gist.github.com/BlGene/607c7bee450e03835aa2bf0d2fd2959a"" rel=""nofollow noreferrer"">https://gist.github.com/BlGene/607c7bee450e03835aa2bf0d2fd2959a</a></p>
","python, nlp, huggingface-transformers",
Relation Extraction Model returns only one entity instead of entity pairs,"<p>I'm working on a relation extraction model task using a transformer-based model. the `pipeline is expected to extract entity pairs along with their labelled relation labels. When I run the evaluation after training the model on labelled data, it works fine. However, when I run the model pipeline, the model sometimes returns only one entity instead of a complete entity pair, along with relation label. I don't quite understand how the model is making a prediction for relation label just by considering one entity.</p>
<pre><code>def entity_extract(text):
    doc_=MODEL_NER(text)
    for name, proc in MODEL_REL.pipeline:
            doc_ = proc(doc_)
    return doc_
</code></pre>
<pre><code>def relation_extraction(doc):
    relation_extraction_output = {}
    unique_entities = set()

    for span, rel_dict in doc._.rel.items():
        # Extract the relation with the highest confidence score
        most_probable_relation = max(rel_dict, key=rel_dict.get)
        score = rel_dict[most_probable_relation]
        
        # Skip if the score is below the cutoff (0.5 50%  in your case)
        if score &lt;= 0.5:
            continue

        start, end = span
        relation_span = doc[start:end]
        # Extract entities involved in the relation
        entities = [ent for ent in relation_span.ents if ent.text not in unique_entities]
        # Store the information if entities exist
        if entities:
            unique_entities.update(ent.text for ent in entities)  # Efficient update
            # Create a key for the entity pair
            entity_pair = tuple(sorted([ent.text for ent in entities]))
            if entity_pair not in relation_extraction_output:
                relation_extraction_output[entity_pair] = {
                    'relations': [],
                    'scores': []
                }
            # Append the relation and score
            relation_extraction_output[entity_pair]['relations'].append(most_probable_relation)
            relation_extraction_output[entity_pair]['scores'].append(score)
    # Convert the output to a list of dictionaries
    final_output = [
        {
            'entities': key,
            'relations': value['relations'],
            'scores': value['scores']
        }
        for key, value in relation_extraction_output.items()
    ]
    return final_output
</code></pre>
<pre><code>def extract_relation(file):
    filename = os.path.splitext(os.path.basename(file))[0]
    print(&quot;Starting to process RelationExtract part for file:&quot;, filename)
    chunk_size=10000
    # Define a function to process each row in a chunk
    def process_row(row):
        try:
            text = entity_extract(row[&quot;sent&quot;])
            return relation_extraction(text)
        except Exception as e:
            print(f&quot;Error processing row: {e}&quot;)
            return None
</code></pre>
<p>Expected output: entity pair: [entity1, entity2], relation: [relation_label]
Actual output: [entity1, ], relation: [relation_label]</p>
","nlp, spacy, named-entity-recognition, spacy-transformers, relation-extraction",
nltk add or remove some abbreviations for the specific project not working,"<p>When tokenizing paragraphs in the Czech language, I am observing that some abbreviations are not treated as abbreviations. The paragraph is stored in the file as one long line. The nltk is of the version 3.9.1, the <code>nltk_data</code> are shared -- stored in <code>c:\nltk_data\</code>; freshly downloaded (30. 1. 2025). Python 3.12 on Windows 10 was used.</p>
<p>Firstly, the example with using the <code>.sent_tokenize</code> method, and the (should be equivalent) code with <code>PunktTokenizer</code> used explicitly -- that should add the the two other Czech abbreviations. The script is stored as <code>test2.py</code> file, using the UTF-8 encoding without BOM:</p>
<pre><code>import nltk

text = '''Věta číslo 1. Věta č. 2. Toto je začátek další věty [pozn. překl. nějaká poznámka překladatele], která definuje pojem „kružnice“.'''

lst = nltk.tokenize.sent_tokenize(text, language='czech')

for n, s in enumerate(lst, 1):
    print(f'{n}: {s}')
print('---------------------------')

# The same tokenizer with added abbreviations.
tokenizer = nltk.tokenize.PunktTokenizer('czech')
tokenizer._params.abbrev_types.update('pozn', 'překl')  # adding the two abbreviatios
lst = tokenizer.tokenize(text)

for n, s in enumerate(lst, 1):
    print(f'{n}: {s}')
</code></pre>
<p><a href=""https://i.sstatic.net/tqWk7cyf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tqWk7cyf.png"" alt=""enter image description here"" /></a></p>
<p>The third sentence contains the (human) translator's note in square brackets -- the abbreviation at the beginning is <code>pozn. překl.</code>.</p>
<p>The added abbreviations are not recognized. However, when I manually add the two abbreviations into the <code>c:\nltk_data\tokenizers\punkt_tab\czech\abbrev_types.txt</code>, the tokenizer works as expected:</p>
<p><a href=""https://i.sstatic.net/045vItCY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/045vItCY.png"" alt=""enter image description here"" /></a></p>
<p>Where is the bug? How should I add the extra abbreviations in the situation when I am not allowed (or do not want) to modify the shared nltk data?</p>
<p>P.S. I am very new to nltk.</p>
","nlp, nltk, tokenize, abbreviation",
"Pyspark error - Invalid argument, not a string or column while implementing inside pandas_udf","<p>This code is working fine outside the pandas_udf but getting this error while trying to implement the same inside udf. To avoid conflicts between pyspark and python function names, I have explicitly imported specific functions from pyspark. Using fuzzywuzzy for string matching and nltk to divide the string into substrings based on ngram technique. This code was taking too long to run without udf so decided to go with pandas_udf but don't have any idea why getting this error.</p>
<pre><code>import json
from pyspark.sql.types import StringType, StructField, StructType
from pyspark.sql.functions import pandas_udf, PandasUDFType

from nltk.util import ngrams, everygrams
from fuzzywuzzy import process, fuzz

model_results_schema = StructType(
  [StructField(&quot;Output&quot;, StringType(), True)]
)

@pandas_udf( model_results_schema, PandasUDFType.GROUPED_MAP )
def get_ccep_results( model_input ):
  
  def ngram_filter(list1, list2):
    sorted_list1 = sorted(list1, key = lambda a: (a[1], len(a[0])), reverse =True)
    sorted_list2 = sorted(list2, key = lambda a: (a[1], len(a[0])), reverse =True)
    rslt1 =list(filter(lambda t: t[1]&gt;58, sorted_list1))#Change the threshold based on requirement
    rslt2 =list(filter(lambda t: t[1]&gt;58, sorted_list2))#Change the threshold based on requirement
    if len(rslt1)!=0:
        a = rslt1[0][0]
    else:
        a =''
    if len(rslt2)!=0:
        b = rslt2[0][0]
    else:
        b =''
    return a, b
  
  def ngram_fuzzy_match(n_gram, attribute):
    list_res=[]
    for i in n_gram:
        r = process.extract(i, attribute)
        list_res.append(r)
    flat_list = [x for xs in list_res for x in xs]   
    sorted_list = sorted(flat_list,key = lambda x: x[1], reverse=True )
    list_br =[]
    for j in attribute:
        p = process.extract(j, n_gram)
        list_br.append(p)
    flat_list1 = [x for xs in list_br for x in xs]     
    sorted_list1 = sorted(flat_list1,key =lambda x: x[1] , reverse=True )
    attribute_value, n_gram_value = ngram_filter(flat_list, flat_list1)
    return attribute_value, n_gram_value
  
  def ngrams_prod_desc(prod_desc, internal_att_list):
    prod_desc_temp = prod_desc
    temp_dict ={x:[] for x in range(0,len(internal_att_list))}
    for ind, x in enumerate(internal_att_list):
        
        list1 = list(everygrams(prod_desc_temp.split())) #Create n-grams for each description
        res = [' '.join(tups) for tups in list1]
        if len(res)!=0:
            r,ngram_candidate = ngram_fuzzy_match(res, x) #This functions does the fuzzy match between n-grams &amp; attributes
            
            temp_dict[ind].append(r) # getting error here
            prod_desc_temp=prod_desc_temp.replace(ngram_candidate, '').strip()
        else:
            temp_dict[ind].append(None)
        
    
    return json.dumps(temp_dict)
  
  
  dictionary = model_input['cleaned_external'].apply(lambda x: ngrams_prod_desc(x, attribute_list))
  result = pd.DataFrame([dictionary])
  
  return result

model_output = (frame_combined.groupby(['prod_id']).apply(get_ccep_results)) # getting error here
</code></pre>
<p>frame_combined -
<a href=""https://i.sstatic.net/JfsQCAf2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JfsQCAf2.png"" alt=""enter image description here"" /></a></p>
<p>Update -
Tried this one and got the exception -
<a href=""https://i.sstatic.net/oq9h8FA4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oq9h8FA4.png"" alt=""enter image description here"" /></a></p>
<pre><code>try:
  model_output = (frame_combined.groupby(['prod_id']).apply(get_ccep_results))
except Exception as e:
  print(e)
</code></pre>
","python, machine-learning, pyspark, nlp, pandas-udf",
How to check if given word is in plural or singular form?,"<p>Question like in topic - I'm trying to do that in python for app in Google App Engine. I know PyEnchant library is used for natural language recognition but I don't see if I can use it for my problem and how.</p>
","python, nlp","<p>Checkout the <a href=""https://pypi.python.org/pypi/inflect/0.2.4"" rel=""nofollow"">inflect 0.2.4</a> library.</p>

<blockquote>
  <p><strong>inflect 0.2.4</strong></p>
  
  <p>Correctly generate plurals, singular nouns, ordinals, indefinite
  articles; convert numbers to words</p>
</blockquote>
"
How To Detect Is Text Human Readable?,"<p>I am wondering if there's a way to tell a given text is human readable. By human readable, I mean: it has some meanings, format like an article written by somebody, or at least generated by a software translator that is intended to be read by a human.</p>

<p>Here's the background story: recently I am making an app that allows user to upload a short text to a database. At the early stage of deployment I noticed some user always uploaded corrupted text due to a problem with encoding. This problem is fixed later, but leaves me wonder if there's a way to pick up non human readable text before serving the text back to users.</p>

<p>Any advice will be appreciated. The scope might be too large to include other languages, so at the moment let's limit the discussion to English only.</p>
","android, ios, nlp",
What is the loss function used in Trainer from the Transformers library of Hugging Face?,"<p>What is the loss function used in Trainer from the Transformers library of Hugging Face?</p>
<p>I am trying to fine tune a BERT model using the <strong>Trainer class</strong> from the Transformers library of Hugging Face.</p>
<p>In their <a href=""https://huggingface.co/docs/transformers/main_classes/trainer"" rel=""nofollow noreferrer"">documentation</a>, they mention that one can specify a customized loss function by overriding the <code>compute_loss</code> method in the class. However, if I do not do the method override and use the Trainer to fine tine a BERT model directly for sentiment classification, what is the default loss function being use? Is it the categorical crossentropy? Thanks!</p>
","python, machine-learning, nlp, artificial-intelligence, huggingface-transformers","<p>It depends!
Especially given your relatively vague setup description, it is not clear what loss will be used. But to start from the beginning, let's first check how the default <code>compute_loss()</code> function in the <code>Trainer</code> class looks like.</p>
<p>You can find the corresponding function <a href=""https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/trainer.py#L2006"" rel=""noreferrer"">here</a>, if you want to have a look for yourself (current version at time of writing is 4.17).
The <a href=""https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/trainer.py#L2026"" rel=""noreferrer"">actual loss that will be returned with default parameters</a> is taken from the model's output values:</p>
<blockquote>
<p><code>         loss = outputs[&quot;loss&quot;] if isinstance(outputs, dict) else outputs[0]</code></p>
</blockquote>
<p>which means that the model itself is (by default) responsible for computing some sort of loss and returning it in <code>outputs</code>.</p>
<p>Following this, we can then look into the actual model definitions for BERT (source: <a href=""https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bert/modeling_bert.py"" rel=""noreferrer"">here</a>, and in particular check out the model that will be used in your Sentiment Analysis task (I assume a <a href=""https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bert/modeling_bert.py#L1501"" rel=""noreferrer""><code>BertForSequenceClassification</code> model</a>.</p>
<p>The <a href=""https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bert/modeling_bert.py#L1563-L1583"" rel=""noreferrer"">code relevant for defining a loss function</a> looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>if labels is not None:
    if self.config.problem_type is None:
        if self.num_labels == 1:
            self.config.problem_type = &quot;regression&quot;
        elif self.num_labels &gt; 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
            self.config.problem_type = &quot;single_label_classification&quot;
        else:
            self.config.problem_type = &quot;multi_label_classification&quot;

    if self.config.problem_type == &quot;regression&quot;:
        loss_fct = MSELoss()
        if self.num_labels == 1:
            loss = loss_fct(logits.squeeze(), labels.squeeze())
        else:
            loss = loss_fct(logits, labels)
    elif self.config.problem_type == &quot;single_label_classification&quot;:
        loss_fct = CrossEntropyLoss()
        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
    elif self.config.problem_type == &quot;multi_label_classification&quot;:
        loss_fct = BCEWithLogitsLoss()
        loss = loss_fct(logits, labels)

</code></pre>
<p>Based on this information, you should be able to either set the correct loss function yourself (by changing <code>model.config.problem_type</code> accordingly), or otherwise at least be able to determine whichever loss will be chosen, based on the hyperparameters of your task (number of labels, label scores, etc.)</p>
"
Why is my BERT model producing NaN loss during training for multi-label classification on imbalanced data?,"<p>I’m running into a frustrating issue while training a BERT-based multi-label text classification model on an imbalanced dataset. After a few epochs, the training loss suddenly becomes NaN, and I can’t seem to figure out why. I’ve tried a bunch of different things, but nothing has worked so far. Hoping someone here has dealt with this before.
<strong>Dataset Setup</strong>
Number of samples is around 100K
Number of labels is around 50
Imbalance: Some labels are super common (appear in 80% of samples), while others are barely there (less than 0.5%)</p>
<p><strong>My Setup</strong>
I’m using Hugging Face’s bert-base-uncased with a custom classification head.</p>
<pre><code>from transformers import BertModel
import torch.nn as nn

class MultiLabelClassifier(nn.Module):
    def __init__(self, num_labels):
        super(MultiLabelClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        logits = self.classifier(outputs.pooler_output)  # Using the [CLS] token
        return logits

</code></pre>
<p>I’m using ***BCEWithLogitsLoss ***with a weighted loss function to deal with the imbalance:</p>
<pre><code>class_weights = torch.tensor([1.0 / (freq + 1e-5) for freq in label_frequencies]).to(device)
criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)

</code></pre>
<p>after 2 or 3 epochs;</p>
<ul>
<li>The loss starts off fine but becomes NaN</li>
<li>Some logits are ridiculously large or small (1e20, -1e20) before the NaN happens.</li>
<li>Gradients also seem to explode right before the NaN loss kicks in.</li>
</ul>
<p>to solve this issue,</p>
<ol>
<li>Added gradient clipping, which helps a bit but doesn’t fully fix the issue
<code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)</code></li>
<li>Tried reducing the learning rate to 5e-6, which delays the NaN issue but doesn’t stop it completely</li>
<li>Thought the issue might be with the classifier weights, so I reinitialized them like this,</li>
</ol>
<pre><code>for layer in model.classifier.parameters():
    if isinstance(layer, nn.Linear):
        nn.init.xavier_normal_(layer.weight)

</code></pre>
<ol start=""4"">
<li>Rewrote the loss calculation using torch.logsigmoid for numerical stability...</li>
</ol>
<pre><code>loss = -labels * torch.logsigmoid(logits) - (1 - labels) * torch.logsigmoid(-logits)
loss = loss.mean()
</code></pre>
<p>Nothing seems to solve the problem completely.</p>
<p><strong>------my questions---</strong></p>
<ol>
<li>Why is this happening? Is it because of the extreme imbalance in my dataset or something else?</li>
<li>How can I fix it? Should I try something like label smoothing, or is there a better way to stabilize the training?</li>
</ol>
<p>here is a snippet of my training loop for context.</p>
<pre><code>optimizer = AdamW(model.parameters(), lr=5e-5)
scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=num_training_steps)

for epoch in range(num_epochs):
    model.train()
    for batch in dataloader:
        input_ids, attention_mask, labels = batch[&quot;input_ids&quot;], batch[&quot;attention_mask&quot;], batch[&quot;labels&quot;]
        logits = model(input_ids, attention_mask)
        loss = criterion(logits, labels)

        optimizer.zero_grad()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()
        scheduler.step()

</code></pre>
<p>---<strong>I Need</strong>--
I’m looking for practical suggestions to <strong>prevent the NaN loss issue entirely</strong> and <strong>stabilize training when working with imbalanced multi-label datasets</strong>.</p>
<p>If anyone has faced this issue or knows a good fix, I’d really appreciate the help. Thanks!</p>
","deep-learning, nlp, bert-language-model, multilabel-classification, imbalanced-data",
ImportError: cannot import name &#39;deprecated&#39; from &#39;typing_extensions&#39;,"<p>I want to download spacy, but the version of typing-extensions is lowered in the terminal:</p>
<pre><code>ERROR: pydantic 2.3.0 has requirement typing-extensions&gt;=4.6.1, but you'll have typing-extensions 4.4.0 which is incompatible.
ERROR: pydantic-core 2.6.3 has requirement typing-extensions!=4.7.0,&gt;=4.6.0, but you'll have typing-extensions 4.4.0 which is incompatible.
Installing collected packages: typing-extensions
  Attempting uninstall: typing-extensions
    Found existing installation: typing-extensions 4.7.1
    Uninstalling typing-extensions-4.7.1:
      Successfully uninstalled typing-extensions-4.7.1
Successfully installed typing-extensions-4.4.0
</code></pre>
<p>Next I want to install the language pack <code>python -m spacy download en</code>, but another error occurs：</p>
<pre><code>(base) E:\Anaconda&gt;python -m spacy download en
Traceback (most recent call last):
  File &quot;E:\Anaconda\lib\site-packages\confection\__init__.py&quot;, line 38, in &lt;module&gt;
    from pydantic.v1 import BaseModel, Extra, ValidationError, create_model
  File &quot;E:\Anaconda\lib\site-packages\pydantic\__init__.py&quot;, line 13, in &lt;module&gt;
    from . import dataclasses
  File &quot;E:\Anaconda\lib\site-packages\pydantic\dataclasses.py&quot;, line 11, in &lt;module&gt;
    from ._internal import _config, _decorators, _typing_extra
  File &quot;E:\Anaconda\lib\site-packages\pydantic\_internal\_config.py&quot;, line 9, in &lt;module&gt;
    from ..config import ConfigDict, ExtraValues, JsonEncoder, JsonSchemaExtraCallable
  File &quot;E:\Anaconda\lib\site-packages\pydantic\config.py&quot;, line 9, in &lt;module&gt;
    from .deprecated.config import BaseConfig
  File &quot;E:\Anaconda\lib\site-packages\pydantic\deprecated\config.py&quot;, line 6, in &lt;module&gt;
    from typing_extensions import Literal, deprecated
ImportError: cannot import name 'deprecated' from 'typing_extensions' (E:\Anaconda\lib\site-packages\typing_extensions.py)
</code></pre>
<p>My current python version is 3.7, should I update it? Or is there any better solution? I'm a newbie in this area, thank you all！</p>
","python, pip, nlp, spacy, python-typing","<p>You should use <code>typing_extensions==4.7.1</code></p>
<p>try :</p>
<pre><code>pip install typing_extensions==4.7.1 --upgrade
</code></pre>
<p>I also suggest you to upgrade your python version from <code>3.7</code> to <code>3.10</code> or <code>3.11</code></p>
<p>See a relevant answer:</p>
<p><a href=""https://github.com/tiangolo/fastapi/discussions/9808"" rel=""noreferrer"">https://github.com/tiangolo/fastapi/discussions/9808</a></p>
"
What is the difference between Transformer encoder vs Transformer decoder vs Transformer encoder-decoder?,"<p>I know that GPT uses Transformer decoder, BERT uses Transformer encoder, and T5 uses Transformer encoder-decoder. But can someone help me understand why GPT only uses the decoder, BERT only uses encoder, and T5 uses both?</p>
<p>What can you do with just the encoder without decoder, decoder without encoder, and both encoder and decoder?</p>
<p>I'm new to NLP so any help would be nice :D
Thanks!</p>
","nlp, bert-language-model, generative-pretrained-transformer",
Cuda out of memory while training,"<p>I'm trying to finetune a base model with my own data using prompts, however  I keep getting this error:</p>
<pre><code>OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 177.50 MiB is free. Process 230359 has 79.07 GiB memory in use. Of the allocated memory 78.24 GiB is allocated by PyTorch, and 28.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
</code></pre>
<p>Despite working on a server with GPU 2x 80GB of RAM.
Any idea what I'm doing wrong? this is not a huge base model, and I'm only trying to train it on 900 samples.</p>
<pre><code>model = AutoModelForCausalLM.from_pretrained('dicta-il/dictalm2.0')
tokenizer = AutoTokenizer.from_pretrained('dicta-il/dictalm2.0')

df= df[['Prompt']]
df= df.sample(1000).reset_index(drop=True)


train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)

dataset = DatasetDict({
    &quot;train&quot;: Dataset.from_pandas(train_df),
    &quot;validation&quot;: Dataset.from_pandas(val_df),
})

dataset = dataset.map(remove_columns=[&quot;__index_level_0__&quot;])


def tokenize_function(examples):
    return tokenizer(examples[&quot;Prompt&quot;],max_length=512)

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=['Prompt']
)

tokenized_dataset = tokenized_dataset.map(lambda example: {&quot;labels&quot;: [1] * len(example[&quot;input_ids&quot;])})

tokenizer.pad_token = tokenizer.eos_token


data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)

args = TrainingArguments(
    'results',
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset[&quot;train&quot;],
    eval_dataset=tokenized_dataset[&quot;validation&quot;]
)

trainer.train()
</code></pre>
","nlp, tokenize, large-language-model, fine-tuning",
How to segment and transcribe an audio from a video into timestamped segments?,"<p>I want to segment a video transcript into chapters based on the content of each line of speech. The transcript would be used to generate a series of start and end timestamps for each chapter. This is similar to how YouTube now &quot;auto-chapters&quot; videos.</p>
<p>Example .srt transcript:</p>
<pre><code>...

70
00:02:53,640 --&gt; 00:02:54,760
All right, coming in at number five,

71
00:02:54,760 --&gt; 00:02:57,640
we have another habit that saves me around 15 minutes a day
...
</code></pre>
<p>I have had minimal luck doing this with ChatGPT as it finds it difficult to both segment by topic and recollect start and end timestamps accurately. I am now exploring whether there are other options for doing this.</p>
<p>I know topic modeling based on time series is possible with some python libraries. I have also read about text tiling as another option. <strong>What options are there for achieving an outcome like this?</strong></p>
<p>Note: The format above (.srt) is not necessary. It's just the idea that the input is a list of text-content with start and end timestamps.</p>
","python, machine-learning, nlp, openai-api, automatic-speech-recognition",
"I am getting error while running this line of code gnb.fit(df_train, y_train)","<p><strong>Title:</strong> <code>ValueError: could not convert string to float</code> when training GaussianNB for SMS Spam Detection</p>
<p><strong>Body:</strong><br />
I'm building an SMS spam detection tool and encountering an error while predicting with a GaussianNB model. Here's the error message:</p>
<pre><code>UserWarning: X has feature names, but GaussianNB was fitted without feature names  
  warnings.warn(  
---------------------------------------------------------------------------  
ValueError: could not convert string to float: 'Dear i am not denying your words please'  
</code></pre>
<p>Here’s the relevant part of my code:</p>
<pre><code>from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score

# Training the model
gnb = GaussianNB()
gnb.fit(df_train, y_train)

# Making predictions
y_pred1 = gnb.predict(df_test)  # Error occurs here

# Evaluating the model
print(accuracy_score(y_test, y_pred1))
print(confusion_matrix(y_test, y_pred1))
print(precision_score(y_test, y_pred1))
</code></pre>
<p>I suspect the issue is related to the input data format. <code>df_train</code> and <code>df_test</code> contain a column with SMS text, but GaussianNB requires numeric input.</p>
<p>I suspect the issue is related to the input data format. <code>df_train</code> and <code>df_test</code> contain a column with SMS text, but GaussianNB requires numeric input.</p>
<p><strong>Questions:</strong></p>
<ol>
<li><p>How can I preprocess the text data so that GaussianNB can handle it?</p>
</li>
<li><p>How do I address the warning about feature names (<code>X has feature names, but GaussianNB was fitted without feature names</code>)?</p>
</li>
</ol>
<p>Here’s what I’ve tried so far:</p>
<ul>
<li><p>Converting the text data to numeric using <code>TfidfVectorizer</code>. However, I’m not sure if I implemented it correctly.</p>
</li>
<li><p>Ensuring no missing values in the dataset using <code>.isnull().sum()</code>.</p>
</li>
</ul>
<p>Any guidance or suggestions to fix this issue would be greatly appreciated. Thanks in advance!</p>
","python, nlp, nltk, gaussian",
torch.OutOfMemoryError: CUDA out of memory. (Google Colab),"<p>I tried to adapt the mBERT model to an existing code. However, I received the following issue even though I tried different solutions.</p>
<pre><code>torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 9.06 MiB is free. Process 84806 has 14.74 GiB memory in use. Of the allocated memory 14.48 GiB is allocated by PyTorch, and 129.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
</code></pre>
<p>Here's the news model that I'm trying to adapt to DST-MetaASSIST(STAR), which you can find it here:</p>
<pre><code>### For DST-MetaASSIST
!git clone https://github.com/smartyfh/DST-MetaASSIST
</code></pre>
<p>These are the new models:</p>
<pre><code># First model mBERT 
from transformers import BertTokenizer, BertForSequenceClassification

# Load the mBERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
# Load the model with the correct number of labels
model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)

# Second model XLM-R

from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments

# Load the tokenizer and model
tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')
model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=number_of_labels)


# Third model mT5
from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load the mT5 tokenizer and model
tokenizer = T5Tokenizer.from_pretrained('google/mt5-small')
model = T5ForConditionalGeneration.from_pretrained('google/mt5-small')
</code></pre>
<p><strong>I changed the 'train-S1.py' file and then run it</strong></p>
<pre><code>import torch
import os
import torch

# Set environment variable for CUDA memory management
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
torch.cuda.empty_cache()  # Clear CUDA cache
torch.backends.cudnn.benchmark = True

# Check for GPU availability
if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;)
    print(&quot;Using GPU:&quot;, torch.cuda.get_device_name(0))
else:
    device = torch.device(&quot;cpu&quot;)
    print(&quot;Using CPU&quot;)

# Reduce batch sizes for memory optimization
train_batch_size = 2  # Reduced from 4 or 16
meta_batch_size = 1    # Reduced from 2 or 8

# Run your training script with reduced batch sizes
!python3 /content/DST-MetaASSIST/STAR/train-S1.py --data_dir data/mwz2.4 --save_dir output-meta24-S1/exp --train_batch_size 2 --meta_batch_size 1 --enc_lr 4e-5 --dec_lr 1e-4 --sw_lr 5e-5 --init_weight 0.5 --n_epochs 1 --do_train
</code></pre>
<p>The new script:</p>
<pre><code># import faulthandler
# faulthandler.enable()
# learn slot-wise weight
import os
import torch
import torch.nn as nn
import numpy as np
import argparse
import random
import json
import time
import logging
from tqdm import tqdm, trange

from torch.utils.data import DataLoader, RandomSampler
from utils.data_utils import Processor, MultiWozDataset
from utils.eval_utils import model_evaluation
from utils.loss_utils import *
from utils.label_lookup import get_label_lookup_from_first_token
from models.DST import UtteranceEncoding, BeliefTracker
# Import necessary libraries
from transformers import BertTokenizer, BertForSequenceClassification

# Load the mBERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')



#====================================
import higher
import itertools
from models.WeightNet import SlotWeight
import torch.optim as optim
from torch.optim.lr_scheduler import LambdaLR
#====================================

import transformers
from transformers import BertTokenizer
from transformers import get_linear_schedule_with_warmup as get_linear_schedule_with_warmup_T

os.environ['CUDA_VISIBLE_DEVICES']='0'
# torch.cuda.set_device(0)

logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)
logger = logging.getLogger(__name__)

def get_linear_schedule_with_warmup(optimizer, enc_num_warmup_steps, dec_num_warmup_steps, num_training_steps, last_epoch=-1):
    &quot;&quot;&quot;
    see https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/optimization.py#L75
    &quot;&quot;&quot;
    def enc_lr_lambda(current_step: int):
        if current_step &lt; enc_num_warmup_steps:
            return float(current_step) / float(max(1, enc_num_warmup_steps))
        return max(
            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - enc_num_warmup_steps))
        )
    
    def dec_lr_lambda(current_step: int):
        if current_step &lt; dec_num_warmup_steps:
            return float(current_step) / float(max(1, dec_num_warmup_steps))
        return max(
            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - dec_num_warmup_steps))
        )

    return LambdaLR(optimizer, [enc_lr_lambda, enc_lr_lambda, dec_lr_lambda], last_epoch)

def set_seed(args, device):
    np.random.seed(args.random_seed)
    random.seed(args.random_seed)
    torch.manual_seed(args.random_seed)
    if device == &quot;cuda&quot;:
        torch.cuda.manual_seed(args.random_seed)
        torch.cuda.manual_seed_all(args.random_seed)
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True
        
def get_sv_lookup(slot_meta, ontology, tokenizer, sv_encoder, device):
    slot_lookup = get_label_lookup_from_first_token(slot_meta, tokenizer, sv_encoder, device)
    value_lookup = []
    for slot in ontology.keys():
        value_lookup.append(get_label_lookup_from_first_token(ontology[slot], tokenizer, sv_encoder, device))
    return slot_lookup, value_lookup

def prepare_optimizer(model, enc_learning_rate, dec_learning_rate, num_train_steps, enc_warmup_ratio, dec_warmup_ratio):
    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
    enc_param_optimizer = list(model.encoder.named_parameters())
    dec_param_optimizer = list(model.decoder.parameters())
    optimizer_grouped_parameters = [
        {'params': [p for n, p in enc_param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
        {'params': [p for n, p in enc_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},
        {'params': dec_param_optimizer, 'lr': dec_learning_rate}
        ]

    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=enc_learning_rate)
    scheduler = get_linear_schedule_with_warmup(optimizer, int(num_train_steps * enc_warmup_ratio),
                                                int(num_train_steps * dec_warmup_ratio), num_train_steps)
    print(f'Number of parameter groups: {len(optimizer.param_groups)}')
    return optimizer, scheduler

'''def prepare_optimizer(model, enc_learning_rate, dec_learning_rate, num_train_steps, enc_warmup_ratio, dec_warmup_ratio):
    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
    
    # Access all parameters of the model
    param_optimizer = list(model.named_parameters())
    
    # Group parameters for the optimizer
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},
    ]

    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=enc_learning_rate)
    
    # Calculate warmup steps
    enc_num_warmup_steps = int(num_train_steps * enc_warmup_ratio)
    
    # Create the learning rate scheduler
    scheduler = get_linear_schedule_with_warmup(optimizer, enc_num_warmup_steps, num_train_steps)
    
    print(f'Number of parameter groups: {len(optimizer.param_groups)}')
    return optimizer, scheduler
'''

def get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids):
    _, pred_all_distance = slot_value_matching(slot_output, value_lookup)
                
    loss_slot_gt = unreduced_cross_entropy_loss(pred_all_distance, label_ids)
    loss_slot_pseudo = unreduced_cross_entropy_loss(pred_all_distance, pseudo_label_ids)
    
    return loss_slot_gt, loss_slot_pseudo

def main(args):
    if not os.path.exists(args.save_dir):
        os.makedirs(args.save_dir)
        
    # logger
    logger_file_name = args.save_dir.split('/')[1]
    fileHandler = logging.FileHandler(os.path.join(args.save_dir, &quot;%s.txt&quot;%(logger_file_name)))
    logger.addHandler(fileHandler)
    logger.info(args)
    
    # cuda setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(&quot;device: {}&quot;.format(device))
    
    # set random seed
    set_seed(args, device)

    #******************************************************
    # load data
    #******************************************************
    processor = Processor(args)
    slot_meta = processor.slot_meta
    ontology = processor.ontology
    logger.info(slot_meta)
   
    # Load the mBERT tokenizer
    #tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
    tokenizer = BertTokenizer.from_pretrained(args.pretrained_model)

    
    
    if args.do_train:
        train_data_raw = processor.get_instances(args.data_dir, args.train_data, tokenizer, True)
        print(&quot;# train examples %d&quot; % len(train_data_raw))

        # Get unique labels from the training data
        unique_labels = set(label for instance in train_data_raw for label in instance.label_ids)  # Flatten the list
        num_labels = len(unique_labels)  # Count unique labels
        print(f&quot;Number of unique labels: {num_labels}&quot;)
        
        meta_data_raw = processor.get_instances(args.data_dir, args.dev_data, tokenizer)
        print(&quot;# meta examples %d&quot; % len(meta_data_raw))
        
        dev_data_raw = processor.get_instances(args.data_dir, args.dev_data, tokenizer)
        print(&quot;# dev examples %d&quot; % len(dev_data_raw))
    
    test_data_raw = processor.get_instances(args.data_dir, args.test_data, tokenizer)
    print(&quot;# test examples %d&quot; % len(test_data_raw))
    logger.info(&quot;Data loaded!&quot;)
    
    ## Initialize slot and value embeddings
    sv_encoder = UtteranceEncoding.from_pretrained(args.pretrained_model)
    for p in sv_encoder.bert.parameters():
        p.requires_grad = False  
    slot_lookup, value_lookup = get_sv_lookup(slot_meta, ontology, tokenizer, sv_encoder, device)

    #num_labels = len(value_lookup)
    # Load the mBERT model with the correct number of labels
    #model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)
    # Clear unused variables and cache
    torch.cuda.empty_cache()
    if args.do_train:
        train_data = MultiWozDataset(train_data_raw,
                                     tokenizer,
                                     word_dropout=args.word_dropout,
                                     max_seq_length=args.max_seq_length,
                                     use_pseudo_label=True)
        meta_data = MultiWozDataset(meta_data_raw,
                                    tokenizer,
                                    word_dropout=0.0, # do word dropout here???
                                    max_seq_length=args.max_seq_length)

        num_train_steps = int(len(train_data_raw) / args.train_batch_size * args.n_epochs)
        logger.info(&quot;***** Run training *****&quot;)
        logger.info(&quot; Num examples = %d&quot;, len(train_data_raw))
        logger.info(&quot; Batch size = %d&quot;, args.train_batch_size)
        logger.info(&quot; Num steps = %d&quot;, num_train_steps)

        train_sampler = RandomSampler(train_data)
        train_dataloader = DataLoader(train_data,
                                      sampler=train_sampler,
                                      batch_size=args.train_batch_size,
                                      collate_fn=train_data.collate_fn)
        
        meta_sampler = RandomSampler(meta_data)
        meta_dataloader = DataLoader(meta_data,
                                     sampler=meta_sampler,
                                     batch_size=args.meta_batch_size,
                                     collate_fn=meta_data.collate_fn)
        meta_dataloader = itertools.cycle(meta_dataloader)
        
        #******************************************************
        # build model
        #******************************************************
        ## model initialization
        base_model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,
                                  num_self_attention_layer=args.num_self_attention_layer)
        # Load the model without unsupported arguments
        # Load the mBERT model with the correct number of labels
        #base_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, num_labels=num_labels)
        base_model.to(device)




        '''
        meta_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, num_labels=num_labels)
        meta_model.to(device)
        meta_model = BertForSequenceClassification.from_pretrained(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,
                                    num_self_attention_layer=args.num_self_attention_layer)
        #
        '''
        meta_model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,
                                  num_self_attention_layer=args.num_self_attention_layer)
        meta_model.to(device)
        
        # Number of slots
        SW = SlotWeight(len(slot_meta), init_val=np.log(args.init_weight/(1.0 - args.init_weight)))
        SW.to(device)

        ## prepare optimizer
        # Prepare optimizer
        # Prepare optimizer
        #base_optimizer, base_scheduler = prepare_optimizer(base_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)
    

        base_optimizer, base_scheduler = \
        prepare_optimizer(base_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)
        
        logger.info(base_optimizer)
        # meta model is a copy of the base model, thus shares the optimizer and scheduler
        meta_optimizer, meta_scheduler = \
        prepare_optimizer(meta_model, args.enc_lr, args.dec_lr, num_train_steps, args.enc_warmup, args.dec_warmup)

        sw_param_optimizer = list(SW.parameters())
        sw_optimizer = optim.AdamW(sw_param_optimizer, lr=args.sw_lr)
        sw_scheduler = get_linear_schedule_with_warmup_T(sw_optimizer,
                                                         int(num_train_steps * args.sw_warmup),
                                                         num_train_steps)
        
        #******************************************************
        # training
        #******************************************************
        logger.info(&quot;Training...&quot;)

        best_loss = None
        best_acc = None
        last_update = None

        for epoch in trange(int(args.n_epochs), desc=&quot;Epoch&quot;):       
            batch_loss, meta_batch_loss = [], []
            for step, batch in enumerate(tqdm(train_dataloader)):
                base_model.train()

                batch = [b.to(device) for b in batch]
                input_ids, segment_ids, input_mask, label_ids, pseudo_label_ids = batch
                
                # forward (meta model)
                meta_model.load_state_dict(base_model.state_dict())
                meta_optimizer.load_state_dict(base_optimizer.state_dict())
                meta_optimizer.zero_grad()
                with higher.innerloop_ctx(meta_model, meta_optimizer) as (meta_m, meta_opt):
                    meta_m.train()
                    slot_output = meta_m(input_ids=input_ids,
                                         attention_mask=input_mask,
                                         token_type_ids=segment_ids,
                                         slot_emb=slot_lookup) # [batch_size, num_slots, dim]
                    
                    loss_slot_gt, loss_slot_pseudo = \
                    get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids)
                    
                    s_weight = SW()
                
                    meta_loss = torch.sum((1.0-s_weight)*loss_slot_gt + s_weight*loss_slot_pseudo) / loss_slot_gt.size(0)
                    # first backward
                    meta_opt.step(meta_loss)
                    
                    # compute on the meta validation set
                    batch_v = next(meta_dataloader)
                    batch_v = [b.to(device) for b in batch_v]
                    input_ids_v, segment_ids_v, input_mask_v, label_ids_v = batch_v
                    # second forward
                    meta_m.eval() # disable dropout
                    slot_output_v = meta_m(input_ids=input_ids_v,
                                           attention_mask=input_mask_v,
                                           token_type_ids=segment_ids_v,
                                           slot_emb=slot_lookup) # [batch_size, num_slots, dim]
                    _, pred_all_distance = slot_value_matching(slot_output_v, value_lookup)
                    loss_v, _, _ = hard_cross_entropy_loss(pred_all_distance, label_ids_v)
                    # backward over backward
                    sw_optimizer.zero_grad()
                    loss_v.backward()
                    sw_optimizer.step()
                    sw_scheduler.step()
                    meta_batch_loss.append(loss_v.item())
                
                # Now we have the updated weight net  
                # forward (base model)
                slot_output = base_model(input_ids=input_ids,
                                         attention_mask=input_mask,
                                         token_type_ids=segment_ids,
                                         slot_emb=slot_lookup) # [batch_size, num_slots, dim]

                loss_slot_gt, loss_slot_pseudo = \
                get_unreduced_loss(slot_output, value_lookup, label_ids, pseudo_label_ids)
                with torch.no_grad():    
                    s_weight = SW()

                loss = torch.sum((1.0-s_weight)*loss_slot_gt + s_weight*loss_slot_pseudo) / loss_slot_gt.size(0)
                # backward (base model)
                base_optimizer.zero_grad()
                loss.backward()
                base_optimizer.step()
                base_scheduler.step()

                batch_loss.append(loss.item())
                if step % 300 == 0:
                    logger.info(&quot;[%d/%d] [%d/%d] mean_loss: %.6f mean_meta_loss: %.6f&quot; % \
                               (epoch+1, args.n_epochs, step, len(train_dataloader),
                                np.mean(batch_loss), np.mean(meta_batch_loss)))
                    batch_loss, meta_batch_loss = [], []
                    logger.info(f'Slot weights: {s_weight.cpu().numpy()}')

            if (epoch+1) % args.eval_epoch == 0:
                eval_res = model_evaluation(base_model, dev_data_raw, tokenizer,
                                            slot_lookup, value_lookup, ontology, epoch+1)
                if last_update is None or best_loss &gt; eval_res['loss']:
                    best_loss = eval_res['loss']
#                     save_path = os.path.join(args.save_dir, 'model_best_loss.bin')
#                     torch.save(base_model.state_dict(), save_path)
                    print(&quot;Best Loss : &quot;, best_loss)
                    print(&quot;\n&quot;)
                if last_update is None or best_acc &lt; eval_res['joint_acc']:
                    best_acc = eval_res['joint_acc']
                    save_path = os.path.join(args.save_dir, 'model_best_acc.bin')
                    save_path_w = os.path.join(args.save_dir, 'sw.bin')
                    torch.save(base_model.state_dict(), save_path)
                    torch.save(SW.state_dict(), save_path_w)
                    last_update = epoch
                    print(&quot;Best Acc : &quot;, best_acc)
                    print(&quot;\n&quot;)

                logger.info(&quot;*** Epoch=%d, Last Update=%d, Dev Loss=%.6f, Dev Acc=%.6f, Dev Turn Acc=%.6f, Best Loss=%.6f, Best Acc=%.6f ***&quot; % (epoch, last_update, eval_res['loss'], eval_res['joint_acc'], eval_res['joint_turn_acc'], best_loss, best_acc))

            if (epoch+1) % args.eval_epoch == 0:
                eval_res = model_evaluation(base_model, test_data_raw, tokenizer,
                                            slot_lookup, value_lookup, ontology, epoch+1)

                logger.info(&quot;*** Epoch=%d, Last Update=%d, Tes Loss=%.6f, Tes Acc=%.6f, Tes Turn Acc=%.6f, Best Loss=%.6f, Best Acc=%.6f ***&quot; % (epoch, last_update, eval_res['loss'], eval_res['joint_acc'], eval_res['joint_turn_acc'], best_loss, best_acc))

            if last_update + args.patience &lt;= epoch:
                    break
            torch.cuda.empty_cache()

#         print(&quot;Test using best loss model...&quot;)
#         best_epoch = 0
#         ckpt_path = os.path.join(args.save_dir, 'model_best_loss.bin')
#         model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,
#                               num_self_attention_layer=args.num_self_attention_layer)
#         ckpt = torch.load(ckpt_path, map_location='cpu')
#         model.load_state_dict(ckpt)
#         model.to(device)

#         test_res = model_evaluation(model, test_data_raw, tokenizer, slot_lookup, value_lookup,
#                                     ontology, best_epoch, is_gt_p_state=False)
#         logger.info(&quot;Results based on best loss: &quot;)
#         logger.info(test_res)
    #----------------------------------------------------------------------
    print(&quot;Test using best acc model...&quot;)
    best_epoch = 1
    ckpt_path = os.path.join(args.save_dir, 'model_best_acc.bin')
    model = BeliefTracker(args.pretrained_model, args.attn_head, dropout_prob=args.dropout_prob,
                          num_self_attention_layer=args.num_self_attention_layer)
    ckpt = torch.load(ckpt_path, map_location='cpu')
    model.load_state_dict(ckpt)
    model.to(device)

    test_res = model_evaluation(model, test_data_raw, tokenizer, slot_lookup, value_lookup,
                                ontology, best_epoch, is_gt_p_state=False)
    logger.info(&quot;Results based on best acc: &quot;)
    logger.info(test_res)
    

if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser()

    # Required parameters
    parser.add_argument(&quot;--data_dir&quot;, default='data/mwz2.4', type=str)
    parser.add_argument(&quot;--train_data&quot;, default='train_dials_v2.json', type=str)
    parser.add_argument(&quot;--dev_data&quot;, default='dev_dials_v2.json', type=str)
    parser.add_argument(&quot;--test_data&quot;, default='test_dials_v2.json', type=str)
    #parser.add_argument(&quot;--pretrained_model&quot;, default='bert-base-uncased', type=str)
    parser.add_argument(&quot;--pretrained_model&quot;, default='bert-base-multilingual-cased', type=str)

    parser.add_argument(&quot;--save_dir&quot;, default='output-meta24-S1/exp', type=str)

    parser.add_argument(&quot;--random_seed&quot;, default=42, type=int)

    #parser.add_argument(&quot;--train_batch_size&quot;, default=16, type=int)
    #parser.add_argument(&quot;--meta_batch_size&quot;, default=8, type=int)
    parser.add_argument(&quot;--train_batch_size&quot;, default=2, type=int)  # Reduce from 16 to 8 to 2
    parser.add_argument(&quot;--meta_batch_size&quot;, default=1, type=int)   # Reduce from 8 to 4 to 1
    
    parser.add_argument(&quot;--enc_warmup&quot;, default=0.1, type=float)
    parser.add_argument(&quot;--dec_warmup&quot;, default=0.1, type=float)
    parser.add_argument(&quot;--sw_warmup&quot;, default=0.1, type=float)
    parser.add_argument(&quot;--enc_lr&quot;, default=4e-5, type=float)
    parser.add_argument(&quot;--dec_lr&quot;, default=1e-4, type=float)
    parser.add_argument(&quot;--sw_lr&quot;, default=5e-5, type=float)
    parser.add_argument(&quot;--init_weight&quot;, default=0.5, type=float)
    parser.add_argument(&quot;--n_epochs&quot;, default=15, type=int)
    parser.add_argument(&quot;--eval_epoch&quot;, default=1, type=int)
    parser.add_argument(&quot;--eval_step&quot;, default=100000, type=int)

    parser.add_argument(&quot;--dropout_prob&quot;, default=0.1, type=float)
    parser.add_argument(&quot;--word_dropout&quot;, default=0.1, type=float)
    
    parser.add_argument(&quot;--max_seq_length&quot;, default=512, type=int)
    parser.add_argument(&quot;--patience&quot;, default=6, type=int)
    parser.add_argument(&quot;--attn_head&quot;, default=4, type=int)
    parser.add_argument(&quot;--num_history&quot;, default=20, type=int)
    parser.add_argument(&quot;--num_self_attention_layer&quot;, default=6, type=int)
    
    parser.add_argument(&quot;--do_train&quot;, action='store_true')
       
    args = parser.parse_args()
    
    print('pytorch version: ', torch.__version__)
    args.torch_version = torch.__version__
    args.transformers_version = transformers.__version__
    args.save_dir = args.save_dir + \
    f'-sd{args.random_seed}-bz{args.train_batch_size}-{args.meta_batch_size}-lr{args.enc_lr}-{args.dec_lr}-{args.sw_lr}-ep{args.n_epochs}'

    main(args)

</code></pre>
<p>Any suggestions?! Thanks in advance.</p>
","python, nlp, google-colaboratory, bert-language-model",
Transformer - Attention is all you need - encoder decoder cross attention,"<p>It is my understanding that each encoder block takes the output from the previous encoder, and that the output is the attended representation (Z) of the sequence (aka sentence).  My question is, how does the last encoder block produce K, V from Z (to be used in encoder-decode attention aublayer of the decoder)</p>

<p>are we simply taking Wk and Wv from last encoder layer? </p>

<p><a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">http://jalammar.github.io/illustrated-transformer/</a></p>
","deep-learning, nlp, attention-model",
Is it Possible to feed Embeddings generate by BERT to a LSTM based autoencoder to get the latent space?,"<p>I've just learn about how BERT produce embeddings. I might not understand it fully.</p>
<p>I was thinking of doing a project of leveraging those embeddings and feed it to an autoencoder to generate latent space for my text data.</p>
<p>I am a bit skeptical that Bert embeddings has produce like sort of relationship between words in a single vector, would it not like loss those relationship information if I feed it in an autoencoder?</p>
<p>It would be nice if someone could give an opinion regarding this.</p>
<p>I am trying to group like text with similar emotions clustered in the latent space in the same time applying contrastive learning concept in  a supervised manner.</p>
","nlp, bert-language-model, autoencoder",
TypeError: isinstance() arg 2 must be a type or tuple of types with collections search in Weaviate,"<p>I have the following code:</p>
<pre><code>from weaviate.classes.query import MetadataQuery
import weaviate
from langchain_huggingface import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')
client = weaviate.connect_to_local()
auto_finance= client.collections.get(&quot;AutomotiveFinance&quot;)

query_vector = embedding_model.embed_documents(&quot;what is Honda Cash and cash equivalents?&quot;)
print(len(query_vector[0]))
#query_vector=[float(i) for i in query_vector[0]]
query_vector = [0.023]*768
response = auto_finance.query.hybrid(
    query = &quot;what is Honda Cash and cash equivalents?&quot;,
    vector = query_vector, alpha = 0.25, limit = 4  
)
</code></pre>
<p>It queries from a collections called AutomotiveFinance, which is locally hosted.
I have replaced the query_vec with a dummy vec just to debug, since that example is used in the official document.</p>
<p>The Automotive Finance collections seems to be ok: below is the inspection code and output:</p>
<pre><code>import weaviate
import weaviate.classes.config as wc


client = weaviate.connect_to_local()

result = client.collections.get('AutomotiveFinance')
for item in result.iterator():
    print(item.uuid, item.properties)

data_object = result.query.fetch_object_by_id(
    &quot;ffdc789b-b188-4fcf-94d5-2e4ad6be37ec&quot;,
    include_vector=True
)

print(len(data_object.vector[&quot;default&quot;]))

client.close()
</code></pre>
<p>output from the AutomotiveFinance inspection code has all the metadata and the id as expected:</p>
<pre><code>faab711b-f714-457c-a547-ff755b2de4d3 {'page1': 2, 'company': 'honda', 'doc_type': '10q', 'page2': 3, 'raw_text': 'THIS IS PAGE 3\nBased on the provided text from the SEC Form 10-Q, here is a structured summary of the key sections and items:\n\n### Company Information\n- **Company Name:** American Honda Finance Corporation\n- **Report 
Type:** Quarterly Report on Form 10-Q\n- **Reporting Period:** For the quarter ended June 30, 2024'}
</code></pre>
<p>And the length of the vector is 768 -&gt; this matches the vector embedding model's expected behavior.</p>
<p>However, when I run the query, I am keep getting the following error:</p>
<pre><code>PS C:\Users\ikim1&gt; &amp; C:/Users/ikim1/RAG-blog/Scripts/python.exe &quot;c:/Users/ikim1/OneDrive/Desktop/RAG file/SimSearch.py&quot;
768
Traceback (most recent call last):
  File &quot;c:/Users/ikim1/OneDrive/Desktop/RAG file/SimSearch.py&quot;, line 13, in &lt;module&gt;
    response = auto_finance.query.hybrid(
  File &quot;C:\Users\ikim1\RAG-blog\lib\site-packages\weaviate\syncify.py&quot;, line 23, in sync_method
    return _EventLoopSingleton.get_instance().run_until_complete(
  File &quot;C:\Users\ikim1\RAG-blog\lib\site-packages\weaviate\event_loop.py&quot;, line 40, in run_until_complete       
    return fut.result()
  File &quot;C:\Users\ikim1\AppData\Local\Programs\Python38\lib\concurrent\futures\_base.py&quot;, line 439, in result    
    return self.__get_result()
  File &quot;C:\Users\ikim1\AppData\Local\Programs\Python38\lib\concurrent\futures\_base.py&quot;, line 388, in __get_result
    raise self._exception
  File &quot;C:\Users\ikim1\RAG-blog\lib\site-packages\weaviate\collections\queries\hybrid\query.py&quot;, line 107, in hybrid
    res = await self._query.hybrid(
  File &quot;C:\Users\ikim1\RAG-blog\lib\site-packages\weaviate\collections\grpc\query.py&quot;, line 189, in hybrid      
    _validate_input(
  File &quot;C:\Users\ikim1\RAG-blog\lib\site-packages\weaviate\validator.py&quot;, line 31, in _validate_input
    if not any(_is_valid(exp, validate.value) for exp in validate.expected):
  File &quot;C:\Users\ikim1\RAG-blog\lib\site-packages\weaviate\validator.py&quot;, line 31, in &lt;genexpr&gt;
    if not any(_is_valid(exp, validate.value) for exp in validate.expected):
  File &quot;C:\Users\ikim1\RAG-blog\lib\site-packages\weaviate\validator.py&quot;, line 61, in _is_valid
    return all(isinstance(val, args[0]) for val in value)
  File &quot;C:\Users\ikim1\RAG-blog\lib\site-packages\weaviate\validator.py&quot;, line 61, in &lt;genexpr&gt;
    return all(isinstance(val, args[0]) for val in value)
TypeError: isinstance() arg 2 must be a type or tuple of types
</code></pre>
<p>I am not completely sure as to what is going on. I think I inputted all the parameters as described in this: <a href=""https://weaviate.io/developers/weaviate/search/hybrid#specify-a-search-vector"" rel=""nofollow noreferrer"">https://weaviate.io/developers/weaviate/search/hybrid#specify-a-search-vector</a>.</p>
<p>EDIT: I figured I should also provide on how I am doing the embedding and all:</p>
<pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
import weaviate
import weaviate.classes.config as wc
import json
import os

client = weaviate.connect_to_local()
# check if the client is alive
assert client.is_live()

# delete the existing schema + create schema 
client.collections.delete(&quot;AutomotiveFinance&quot;)
client.collections.create(
    name = 'AutomotiveFinance',
    properties=[
        wc.Property(name = 'page1', data_type = wc.DataType.INT),
        wc.Property(name = 'page2', data_type = wc.DataType.INT),
        wc.Property(name = 'company', data_type = wc.DataType.TEXT),
        wc.Property(name = 'doc_type', data_type = wc.DataType.TEXT),
        wc.Property(name = 'raw_text', data_type = wc.DataType.TEXT),
        wc.Property(name = 'embedding', data_type = wc.DataType.BLOB) # here BLOB stands for binary large object.
    ]
)
auto_finance = client.collections.get(&quot;AutomotiveFinance&quot;)

# get the path of each json file
json_top_file_path = r'C:\Users\ikim1\OneDrive\Desktop\RAG file'
json_file_path = []
for file in os.listdir(json_top_file_path):
    if file.endswith('.json'):
        file_path = os.path.join(json_top_file_path, file)
        json_file_path.append(file_path)

# Initialize the text splitter +  embedding model
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # Maximum size of each chunk
    chunk_overlap=100  # Overlap between consecutive chunks
)
embedding_model = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')
# for each file path of json get the chunks. 

chunks_with_metadata_list = []
for one_json_path in json_file_path: 
    # each json had the following structure
    # json['pages'], json['file_path'], json['company'] json['doc_type']
    # in json['pages'], there is list of each page as element 
    
    # open the json file 
    with open(one_json_path, 'r') as file: 
        json_data = json.load(file)
        pages = json_data['pages']
        company = json_data['company']
        doc_type = json_data['doc_type']
        
        # make the entire string from the pages
        # make sure to insert the page numbers as well. 
        old_page_num = 0; old_md = ''; old_raw_txt = ''
        json_string = '' 
        for i, page in enumerate(pages): 
            md = page['md']
            raw_txt = page['text']
            page_num = page['page']
            print(i)
            # if this is the second one, then start the chunking process
            if i &gt; 0: 
                old_combined_str = &quot;THIS IS PAGE &quot; + str(old_page_num) + '\n' + old_md + '\n' + old_raw_txt
                new_combined_str = &quot;THIS IS PAGE &quot; + str(page_num) + '\n' + md + '\n' + raw_txt
                combined_str = new_combined_str + '\n' + old_combined_str
                # chunk the combined_str using recursive splittin,g but inject the metadata. 
                chunks = text_splitter.split_text(combined_str)
                # inject the metadata into the chunks
                for chunk in chunks: 
                    # embed the chunk : output is already a list. so no need for conversion for Weaviate
                    embedded_chunk = embedding_model.embed_documents(chunk)[0]
                    chunk_metadata = {
                        &quot;page1&quot; : old_page_num, &quot;page2&quot; : page_num, 
                        &quot;company&quot; : company, &quot;doc_type&quot; : doc_type, 
                        'raw_text' : chunk
                        }
                    # put the chunk data into the vector database
                    auto_finance.data.insert(
                        properties = chunk_metadata, 
                        vector = [float(i) for i in embedded_chunk]
                    )
            # cache the previous one
            old_md = md
            old_raw_txt = raw_txt
            old_page_num = page_num
            

client.close()

</code></pre>
<p>Thanks!</p>
","python-3.x, nlp, vector-database, rag, weaviate",
Is it possible to evaluate Machine Translations using Sentence BERT?,"<p>I'm not referring to <a href=""https://arxiv.org/abs/1904.09675"" rel=""nofollow noreferrer"">BERTScore</a>. BERTScore uses token-level word embeddings, you compute pairwise cosine similarity of word embeddings and obtain scores using greedy matching.</p>
<p>I'm referring to <a href=""https://arxiv.org/abs/1908.10084"" rel=""nofollow noreferrer"">Sentence BERT</a>. I.e., pure cosine similarity to compare semantic similarity of sentences, not precision, recall or f1-measure. <strong>The question is, if we do this on document level, i.e., several sentences, do we then just compute the mean cosine similarity or is this metric not suitable as a machine translation evaluation metric (alternative to BLEU)?</strong></p>
<p>Because for individual sentences, it does make sense as it capture semantic similarity. Sentences that mean the same thing but are phrased differently get penalized by BLEU but get rather high values with Sentence BERT, which is exactly what I want. However, I could not find the use of Sentence BERT in recent WMT Shared Metric Task papers, so I assume there is a catch I am missing which explains why people do not use this approach.</p>
","nlp, bert-language-model, machine-translation, sentence-similarity",
how to create a Natural Language Inference pipeline in haystack,"<p>Could anyone help me with some advice on how to create a Natural Language Inference pipeline in haystack</p>
<p>I want to use the Haystack framework to create a pipeline for Natural Language Inference on the response from a Retrieval-Augmented Generation (RAG) application</p>
<p>Because I'm using haystack-ai , I cannot use farm-haystack. If I could use farm-haystack (v1.0) I believe I could do something like below:</p>
<pre><code>from haystack import Pipeline
from haystack_ai.nodes import HuggingFaceTextClassifier

classifier = HuggingFaceTextClassifier(
    model_name_or_path=entailment_model,
    task=&quot;text-classification&quot;,  # Task type: text classification
    labels=[
        &quot;entailment&quot;,
        &quot;contradiction&quot;,
        &quot;neutral&quot;,
    ],  # Define the labels your model is trained on
)

classifier_pipeline = Pipeline()
classifier_pipeline.add_cmponent(&quot;classifier_llm&quot;, classifier)
premise = &quot;The sun rises in the east and sets in the west.&quot;
hypothesis = &quot;The sun rises in the east.&quot;

classifier_pipeline.run({&quot;classifier_llm&quot;: {&quot;text&quot;: premise, &quot;text_pair&quot;: hypothesis}})
</code></pre>
<p>However I cannot see how to achieve the same in haystack v2.0 (haystack-ai) .</p>
<p>Any comments or pointers welcome.</p>
","nlp, huggingface, haystack",
Loading pickle NotFittedError: TfidfVectorizer - Vocabulary wasn&#39;t fitted,"<p><strong>multilabel classification</strong></p>
<p>I am trying to predict a multilabel classification using scikit-learn/pandas/OneVsRestClassifier/logistic regression. Building and evaluating the model works but attempting to classify new sample text does not.</p>
<p><strong>scenario 1:</strong></p>
<p>Once I build a model saved the model with the name(sample.pkl) and restarting my kernel, but when I load the saved model(sample.pkl) during prediction on sample text getting its giving error:</p>
<pre><code> NotFittedError: TfidfVectorizer - Vocabulary wasn't fitted.
</code></pre>
<p>I build the model and evaluate the model and  i save it the model wtith the name sample.pkl. i restrat my kernal  then i load the model making prediction on sample text NotFittedError: TfidfVectorizer - Vocabulary wasn't fitted</p>
<p><strong>inference</strong></p>
<pre><code>import pickle,os
import collections
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import Counter
from nltk.corpus import stopwords
import json, nltk, re, csv, pickle
from sklearn.metrics import f1_score # performance matrix
from sklearn.multiclass import OneVsRestClassifier # binary relavance
from sklearn.linear_model import LogisticRegression  
from sklearn.model_selection import train_test_split  
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
stop_words = set(stopwords.words('english'))

def cleanHtml(sentence):
'''' remove the tags '''
    cleanr = re.compile('&lt;.*?&gt;')
    cleantext = re.sub(cleanr, ' ', str(sentence))
    return cleantext


def cleanPunc(sentence): 
''' function to clean the word of any
    punctuation or special characters '''
    cleaned = re.sub(r'[?|!|\'|&quot;|#]',r'',sentence)
    cleaned = re.sub(r'[.|,|)|(|\|/]',r' ',cleaned)
    cleaned = cleaned.strip()
    cleaned = cleaned.replace(&quot;\n&quot;,&quot; &quot;)
    return cleaned

def keepAlpha(sentence):
&quot;&quot;&quot; keep the alpha sentenes &quot;&quot;&quot;
    alpha_sent = &quot;&quot;
    for word in sentence.split():
        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)
        alpha_sent += alpha_word
        alpha_sent += &quot; &quot;
    alpha_sent = alpha_sent.strip()
return alpha_sent

def remove_stopwords(text):
&quot;&quot;&quot; remove stop words &quot;&quot;&quot;
    no_stopword_text = [w for w in text.split() if not w in stop_words]
    return ' '.join(no_stopword_text)

test1 = pd.read_csv(&quot;C:\\Users\\abc\\Downloads\\test1.csv&quot;)
test1.columns

test1.head()
siNo  plot                              movie_name       genre_new
1     The story begins with Hannah...   sing             [drama,teen]
2     Debbie's favorite band is Dream.. the bigeest fan  [drama]
3     This story of a Zulu family is .. come back,africa [drama,Documentary]
</code></pre>
<p><strong>getting Error</strong>
I am getting the error here when iam inference on sample text</p>
<pre><code>def infer_tags(q):
    q = cleanHtml(q)
    q = cleanPunc(q)
    q = keepAlpha(q)
    q = remove_stopwords(q)
    multilabel_binarizer = MultiLabelBinarizer()
    tfidf_vectorizer = TfidfVectorizer()
    q_vec = tfidf_vectorizer.transform([q])
    q_pred = clf.predict(q_vec)
    return multilabel_binarizer.inverse_transform(q_pred)


for i in range(5):
    print(i)
    k = test1.sample(1).index[0] 
    print(&quot;Movie: &quot;, test1['movie_name'][k], &quot;\nPredicted genre: &quot;, infer_tags(test1['plot'][k])), print(&quot;Actual genre: &quot;,test1['genre_new'][k], &quot;\n&quot;)
</code></pre>
<p><a href=""https://i.sstatic.net/M9kFF.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M9kFF.jpg"" alt=""enter image description here"" /></a></p>
<p><strong>solved</strong></p>
<p>I solved the i save tfidf and multibiniraze into pickle model</p>
<pre><code>from sklearn.externals import joblib
pickle.dump(tfidf_vectorizer, open(&quot;tfidf_vectorizer.pickle&quot;, &quot;wb&quot;))
pickle.dump(multilabel_binarizer, open(&quot;multibinirizer_vectorizer.pickle&quot;, &quot;wb&quot;))
vectorizer = joblib.load('/abc/downloads/tfidf_vectorizer.pickle')
multilabel_binarizer = joblib.load('/abc/downloads/multibinirizer_vectorizer.pickle')


def infer_tags(q):
    q = cleanHtml(q)
    q = cleanPunc(q)
    q = keepAlpha(q)      
    q = remove_stopwords(q)
    q_vec = vectorizer .transform([q])
    q_pred = rf_model.predict(q_vec)
    return multilabel_binarizer.inverse_transform(q_pred)
</code></pre>
<p>i go though the below link i got the solution
,https://stackoverflow.com/questions/32764991/how-do-i-store-a-tfidfvectorizer-for-future-use-in-scikit-learn&gt;</p>
","python-3.x, machine-learning, nlp, pickle, tfidfvectorizer",
Properly configuring use_IO_bindings in ONNX ORTModelForSequenceClassification to improve inference speed on GPU,"<p><strong>I'm currently running into an issue where running the changes (in green) in the following diff leads to worse performance on GPU (it adds an additional 100ms/request on average, with significantly higher p(90) and above) and I'm trying to understand why this might be the case.</strong></p>
<p>Below is a diff comparing the approach I thought would be better (in green/the additions) from the current situation (in red/the substitutions).</p>
<p>The diff contains the inference pipelines for an ONNX model (a fine-tuned DeBerta model) that I'm trying to run in production on a GPU.</p>
<pre class=""lang-none prettyprint-override""><code>@@ -53,14 +53,13 @@ class Scanner():
         &quot;&quot;&quot;
         super().__init__(config)
         self.tokenizer = AutoTokenizer.from_pretrained(self.ONNX_PATH)
+        self.device = get_device()
         self.model = ORTModelForSequenceClassification.from_pretrained(
             self.ONNX_PATH,
             local_files_only=True,
+            provider=&quot;CUDAExecutionProvider&quot; if self.device.type == &quot;cuda&quot; else &quot;CPUExecutionProvider&quot;,
+            use_io_binding=self.device.type == &quot;cuda&quot;,  # This is already occurring by default, but better to be explicit
         )
-        self.device = get_device()
-        self.model = self.model.to(self.device)
-        # Disable IO binding since we're using numpy inputs
-        self.model.use_io_binding = False
 
     async def analyze(self, content, **kwargs) -&gt; AnalysisResult:
         &quot;&quot;&quot;
@@ -95,24 +94,37 @@ class Scanner():
        loop = asyncio.get_event_loop()

        def run_inference():
             By running inference in this manner, we optimize throughput while effectively
             managing resource usage through a global scanning queue. This approach is particularly
             beneficial for applications requiring high responsiveness and scalability.
+
+            INFERENCE NOTE:
+            This inference follows the same inference instructions as set out by the prompt guard tutorial:
+            https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/responsible_ai/prompt_guard/prompt_guard_tutorial.ipynb
+
+            One thing to note is that the model here is an ONNX model, rather than the PyTorch model used in the tutorial.
+            As a result, there are some small alterations from the promp_guard_tutorial.
+            For example, we have made explicit the use of IO binding when running on GPU (see init) in order to improve
+            performance.
             &quot;&quot;&quot;
+            # Temperature scaling is about calibrating the confidence of an already trained model.
+            # It adjusts how &quot;sharp&quot; or &quot;soft&quot; the probability distributions are in the model's outputs.
+            # A temperature &gt;1 makes the distribution more uniform (less confident), while &lt;1 makes it more peaked (more confident).
+            # Adding temperature scaling to reflect prompt guard tutorial, however keeping it set to 1.0 (no impact) for now.
+            temperature = 1.0
             # Tokenize input and prepare for model
-            inputs = self.tokenizer(content, return_tensors=&quot;pt&quot;, truncation=True, max_length=512)
-            # Move inputs to same device as model for consistent processing
-            inputs = {k: v.to(self.device) for k, v in inputs.items()}
+            # We're not padding here since we're using a batch size of 1; one scan per prompt (for now).
+            # We should revisit this if we start running batch scans.
+            inputs = self.tokenizer(content, return_tensors=&quot;pt&quot;, padding=False, truncation=True, max_length=512)
 
-            # Convert inputs to numpy arrays for ONNX Runtime
-            # Ensure tensors are moved to CPU (v.cpu()) before converting to Numpy arrays
-            # ONNX Runtime requires inputs in Numpy format
-            inputs = {k: v.cpu().numpy() for k, v in inputs.items()}
-            # Run inference
-            outputs = self.model(**inputs)
+            # Get logits from the model, make sure to disable gradients since we don't need to backpropagate
+            with torch.no_grad():
+                logits = self.model(input_ids=inputs[&quot;input_ids&quot;], attention_mask=inputs[&quot;attention_mask&quot;]).logits
 
+            # Apply temperature scaling (no impact for now)
+            scaled_logits = logits / temperature
             # Process the outputs
-            logits = outputs.logits
-            prediction = torch.nn.functional.softmax(torch.from_numpy(logits), dim=1)
+            prediction = torch.nn.functional.softmax(scaled_logits, dim=1)
+            probabilities = prediction[0]
             predicted_class = prediction.argmax().item()
-            confidence = prediction[0][predicted_class].item()
+            confidence = probabilities[predicted_class].item()
 
             return predicted_class, confidence
        
        # Run compute-intensive operations in thread pool
        predicted_class, confidence = await loop.run_in_executor(None, run_inference)

        findings = []
        if LABEL_MAPPING[str(predicted_class)] == &quot;TESTER&quot; and confidence &gt; 0.8:
            findings.append(self._create_finding(content))
</code></pre>
<p>In the diff, I include the inference method call. This model represent but one of a number of scanners that we're running in python co-routines (hence trying to run_in_executor) to run in the thread pool.</p>
<p>My first inclination was that perhaps this is an issue with setting padding to be false. I double checked -- the ONNX model isn't expecting fixed input sizes, so I don't think this is the case.</p>
<p><strong>My current hypothesis (and the major change in the diff) is that there is an issue with the use IO bindings -- I thought that it would have improved performance, but perhaps I've configured it poorly?</strong></p>
<p>Any guidance to reason better about this would be much appreciated!
Thanks!!</p>
","pytorch, nlp, onnx, onnxruntime, mlops",
Is it possible to Fine-Tune TinyBERT on Mac (M1 chip)?,"<p>Does fine-tuning require huge resources, or is it possible to do this task on the local machine as well? I have an Apple M1 (8GB RAM). I know bigger models GPU access but what about TinyBERT? My training dataset has 100,000 samples for your reference.</p>
","deep-learning, nlp, huggingface-transformers, text-classification, fine-tuning",
Why is the vocab size of Byte level BPE smaller than Unicode&#39;s vocab size?,"<p>I recently read GPT2 and the paper says:</p>
<blockquote>
<p>This would result in a base vocabulary of over 130,000 before any multi-symbol tokens are added.  This is prohibitively large compared to the 32,000 to 64,000 token vocabularies often used with BPE. In contrast, a byte-level version of BPE only requires a base vocabulary of size 256.</p>
</blockquote>
<p>I really don't understand the words. The number of characters that Unicode represents is 130K but how can this be reduced to 256? Where's the rest of approximately 129K characters? What am I missing? Does byte-level BPE allow duplicating of representation between different characters?</p>
<p>I don't understand the logic. Below are my questions:</p>
<ul>
<li>Why the size of vocab is reduced? (from 130K to 256)</li>
<li>What's the logic of the BBPE (Byte-level BPE)?</li>
</ul>
<hr />
<h1>Detail question</h1>
<p>Thank you for your answer but I really don't get it. Let's say we have 130K unique characters. What we want (and BBPE do) is to reduce this basic (unique) vocabulary. Each Unicode character can be converted 1 to 4 bytes by utilizing UTF-8 encoding. The original paper of BBPE says (Neural Machine Translation with Byte-Level Subwords):</p>
<blockquote>
<p>Representing text at the level of bytes and <strong>using the 256 bytes</strong> set as vocabulary is a potential solution to this issue.</p>
</blockquote>
<p>Each byte can represent 256 characters (bits, 2^8), we only need 2^17 (131072) bits for representing the unique Unicode characters. In this case, where did the <strong>256 bytes in the original paper</strong> come from? I don't know both the logic and how to derive this result.</p>
<p>I arrange my questions again, more detail:</p>
<ul>
<li>How does BBPE work?</li>
<li>Why the size of vocab is reduced? (from 130K to 256 bytes)
<ul>
<li>Anyway, we always need 130K space for a vocab. What's the difference between representing unique characters as Unicode and Bytes?</li>
</ul>
</li>
</ul>
<p>Since I have little knowledge of computer architecture and programming, please let me know if there's something I missed.</p>
<p>Sincerely, thank you.</p>
","unicode, utf-8, nlp","<p>Unicode code points are integers in the range 0..1,114,112, of which roughly 130k are in use at the moment. Every Unicode code point corresponds to a character, like &quot;a&quot; or &quot;λ&quot; or &quot;龙&quot;, which is handy to work with in many cases (but there are a lot of complicated details, eg. combining marks).</p>
<p>When you save text data to a file, you use one of the UTFs (UTF-8, UTF-16, UTF-32) to convert code points (integers) to bytes. For UTF-8 (the most popular file encoding), each character is represented by 1, 2, 3, or 4 bytes (there's some inner logic to discriminate single- and multi-byte characters).</p>
<p>So when the base vocabulary are bytes, this means that rare characters will be encoded with multiple BPE segments.</p>
<h3>Example</h3>
<p>Let's consider a short example sentence like “That’s great 👍”.</p>
<p>With a base vocabulary of all Unicode characters, the BPE model starts off with something like this:</p>
<pre><code>T      54
h      68
a      61
t      74
’    2019
s      73
       20
g      67
r      72
e      65
a      61
t      74
       20
👍   1F44D
</code></pre>
<p>(The first column is the character, the second its codepoint in hexadecimal notation.)</p>
<p>If you first encode this sentence with UTF-8, then this sequence of bytes is fed to BPE instead:</p>
<pre><code>T      54
h      68
a      61
t      74
�      e2
�      80
�      99
s      73
       20
g      67
r      72
e      65
a      61
t      74
       20
�      f0
�      9f
�      91
�      8d
</code></pre>
<p>The typographic apostrophe &quot;’&quot; and the thumbs-up emoji are represented by multiple bytes.</p>
<p>With either input, the BPE segmentation (after training) may end with something like this:</p>
<pre><code>Th|at|’s|great|👍
</code></pre>
<p>(This is a hypothetical segmentation, but it's possible that capitalised “That“ is too rare to be represented as a single segment.)</p>
<p>The number of BPE operations is different though: to arrive at the segment <code>’s</code>, only one merge step is required for code-point input, but three steps for byte input.</p>
<p>With byte input, the BPE segmentation is likely to end up with sub-character segments for rare characters.
The down-stream language model will have to learn to deal with that kind of input.</p>
"
Removing various symbols from a text,"<p>I am trying to <strong>clean</strong> some texts that are very different from one another. I would like to remove the headlines, quotation marks, abbreviations, special symbols and points that don't actually end sentences.</p>
<p>Example input:</p>
<pre><code>This is a headline

And inside the text there are 'abbreviations', e.g. &quot;bzw.&quot; in German or some German dates, like 2. Dezember 2017. Sometimes there are even enumerations, that I might just eliminate completely.
• they have
◦ different bullet points
- or even equations and 
Sometimes there are special symbols. ✓
</code></pre>
<p>Example output:</p>
<pre><code>And inside the text there are abbreviations, for example beziehungsweise in German or some German dates, like 2 Dezember 2017. Sometimes there are even enumerations, that I might just eliminate completely. Sometimes there are special symbols.
</code></pre>
<p><strong>What I did:</strong></p>
<pre><code>with open(r'C:\\Users\me\\Desktop\\ex.txt', 'r', encoding=&quot;utf8&quot;) as infile: 
    data = infile.read()
    data = data.replace(&quot;'&quot;, '')
    data = data.replace(&quot;e.g.&quot;, 'for example') 
    #and so on
with open(r'C:\\Users\me\\Desktop\\ex.txt', 'w', encoding=&quot;utf8&quot;) as outfile:
    outfile.write(data)
</code></pre>
<p><strong>My problems (although number 2 is the most important):</strong></p>
<ol>
<li><p>I just want a string with this input, but it obviously breaks because of the quotation marks, is there any way to do this other than working with files like I did? In reality, I'm copy-pasting a text and want an app to clean it.</p>
</li>
<li><p>The code seems very inefficient because I just manually write the things that I remember to delete/clean, but I don't know all the abbreviations by heart. How do I clean it in one go, so to say?</p>
</li>
<li><p>Is there any way to eliminate the headline and enumeration, and the point <code>.</code> that appears in that German date? My code doesn't do that.</p>
</li>
</ol>
<p>Edit: I just remembered stuff like <code>text = re.sub(r&quot;(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?&quot;, &quot;&quot;, text)</code>, but regex is inefficient for huge texts, isn't it?</p>
","python, string, text, replace, nlp",
SHAP for transformer-based Regression,"<p>I'm developing a transformer-based regression model using a classification model with num labels set to 1 so that it can be used for regression.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding
from torch.utils.data import DataLoader

BASE_MODEL = &quot;distilbert/distilbert-base-uncased&quot;
LEARNING_RATE = 2e-5
MAX_LENGTH = 512
BATCH_SIZE = 16
EPOCHS = 20

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)
</code></pre>
<p>This model will be trained on my dataset which consists of text and respective scores as shown below
e.g.;</p>
<pre><code>[
    {
        &quot;text&quot;: &quot;some text&quot;,
        &quot;score&quot;: &quot;7&quot;
    },
    {
        &quot;text&quot;: &quot;some text&quot;,
        &quot;score&quot;: &quot;8&quot;
    }
]
</code></pre>
<p>I want to use SHAP in order to explain on how the input text will be scored.
However, I couldn't find any documentation on this, any help would be greatly appreciated.</p>
","nlp, regression, huggingface-transformers, text-classification, shap",
(with cpu)Pytorch: IndexError: index out of range in self. (with cuda)Assertion `srcIndex &lt; srcSelectDimSize` failed. How to solve?,"<p>Today I get the following error when I use BERT with Pytorch and cuda: /pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [234,0,0], thread: [0,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize</code> failed.</p>
<pre><code>Epoch [1/100]
Iter:      0,  Train Loss:   1.1,  Train Acc: 39.06%,  Val Loss:   1.0,  Val Acc: 51.90%,  Time: 0:00:04 *
Iter:     10,  Train Loss:  0.99,  Train Acc: 57.81%,  Val Loss:   1.0,  Val Acc: 52.01%,  Time: 0:00:11 *
Iter:     20,  Train Loss:   1.0,  Train Acc: 42.19%,  Val Loss:  0.99,  Val Acc: 52.01%,  Time: 0:00:17 *
Iter:     30,  Train Loss:   1.0,  Train Acc: 40.62%,  Val Loss:  0.99,  Val Acc: 52.12%,  Time: 0:00:23 *
Iter:     40,  Train Loss:   1.0,  Train Acc: 50.00%,  Val Loss:  0.98,  Val Acc: 52.12%,  Time: 0:00:29 *
Iter:     50,  Train Loss:   1.1,  Train Acc: 43.75%,  Val Loss:  0.98,  Val Acc: 52.12%,  Time: 0:00:35 *
Traceback (most recent call last):
  File &quot;/content/drive/MyDrive/Prediction/run.py&quot;, line 38, in &lt;module&gt;
    train(config, model, train_iter, dev_iter, test_iter)
  File &quot;/content/drive/MyDrive/Prediction/train_eval.py&quot;, line 50, in train
    outputs = model(trains)
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/content/drive/MyDrive/Prediction/models/BERT+Covid.py&quot;, line 68, in forward
    output  = self.bert(context, attention_mask=mask)
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py&quot;, line 1005, in forward
    return_dict=return_dict,
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py&quot;, line 589, in forward
    output_attentions,
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py&quot;, line 475, in forward
    past_key_value=self_attn_past_key_value,
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py&quot;, line 408, in forward
    output_attentions,
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py&quot;, line 323, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [234,0,0], thread: [0,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.

 #......I SKIPPED SEVERAL LINES DUE TO THE CHARACTER LIMITATION

/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [235,0,0], thread: [127,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed
</code></pre>
<p>to find where exactly went wrong, I run my code again with CPU, and I got this error: IndexError: index out of range in self.</p>
<pre><code>traceback (most recent call last):
  File &quot;/content/drive/MyDrive/Prediction/run.py&quot;, line 37, in &lt;module&gt;
    train(config, model, train_iter, dev_iter, test_iter)
  File &quot;/content/drive/MyDrive/Prediction/train_eval.py&quot;, line 49, in train
    outputs = model(trains)
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/content/drive/MyDrive/Prediction/models/BERT+Covid.py&quot;, line 66, in forward
    output  = self.bert(context, attention_mask=mask, )
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py&quot;, line 993, in forward
    past_key_values_length=past_key_values_length,
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py&quot;, line 215, in forward
    inputs_embeds = self.word_embeddings(input_ids)
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py&quot;, line 160, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py&quot;, line 2043, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
</code></pre>
<p>According to the guidance I found online, I have ensured the following issue:</p>
<ol>
<li><p>the input length didn't exceed the maximum length in the model (the pad size I set is 98, and I had tried to print out the shape of the input before the line went wrong. it did be (batch_size, pad_size)).</p>
</li>
<li><p>len(tokenizer)==model.config.vocab_size, so this is not the problem</p>
</li>
</ol>
<p>I have no idea now what could be the problem, could anybody help me?</p>
<p>My model structure is:</p>
<pre><code>class Model(nn.Module):

    def __init__(self, config):
        super(Model, self).__init__()
        self.modelConfig = BertConfig.from_pretrained('./bert_pretrain/config.json')
        self.bert = BertModel.from_pretrained(config.bert_path,config=self.modelConfig)
        for param in self.bert.parameters():
            param.requires_grad = False
        self.cls_fc_layer = FCLayer(config.hidden_size, config.word_size, config.dropout_rate)
        self.label_classifier = FCLayer(
            config.word_size+config.numerical_size,
            config.num_classes,
            config.dropout_rate,
            use_activation=False,
        )

    def forward(self, x):
        context = x[0]  # input token ids
        mask = x[2]  # mask
        numerical=x[3] #size(batch_size,18)
        
        output  = self.bert(context, attention_mask=mask)
        pooled_output=output[1]
        ##size(batch_size,768)
        pooled_output = self.cls_fc_layer(pooled_output)
        ##size(batch_size,18)
        concat_h = torch.cat([pooled_output, numerical], dim=-1)
        ##size(batch_size,36)
        logits = self.label_classifier(concat_h)
        return logits
</code></pre>
","python, machine-learning, nlp, pytorch, bert-language-model",
Why is there a gap between the rotary embedding in LLaMA and my custom implementation?,"<p>I’m encountering an issue when applying rotary position encoding outside the LLaMA model in the Transformers library (version 4.46.0).
The problem arises in the following code block:</p>
<pre><code>from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, apply_rotary_pos_emb

# q_unrope/k_unrope/q/k are saved inside the model

rotary_emb = LlamaRotaryEmbedding(config=config, device=device)
rotary_emb.training = False

bsz, head_num, seq_len, head_dim = k.shape
position_ids = torch.arange(seq_len).unsqueeze(0).expand(bsz, seq_len).to(device)

cos, sin = rotary_emb(k, position_ids)

q_rope, k_rope = apply_rotary_pos_emb(q_unrope, k_unrope, cos, sin)

print(f&quot;{torch.max(torch.abs(k_rope - k))=}&quot;)
print(f&quot;{torch.max(torch.abs(q_rope - q))=}&quot;)
</code></pre>
<p>I noticed a gap between the q_rope computed externally and the roped q inside the LLaMA model:</p>
<pre><code>torch.max(torch.abs(k_rope - k)) = tensor(0.0039, device='cuda:0', dtype=torch.float16)
torch.max(torch.abs(q_rope - q)) = tensor(0.0078, device='cuda:0', dtype=torch.float16)
</code></pre>
<p>However, when I save and reload the rotary_emb from the model and use it instead of initializing with</p>
<pre><code>rotary_emb = LlamaRotaryEmbedding(config=config, device=device)
</code></pre>
<p>the gap reduces to zero.</p>
<p>What might be causing this discrepancy? Any help or insights would be greatly appreciated. Thank you!</p>
","nlp, huggingface-transformers, large-language-model",
Problem in tqdm function in a Doc2Vec model,"<p>I am using this article <a href=""https://actsusanli.medium.com/"" rel=""nofollow noreferrer"">https://actsusanli.medium.com/</a> to implement the Doc2Vec model and I have a problem in the training step.</p>
<pre><code>model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs = 40)
</code></pre>
<p>As you can see, I am using the tqdm function. When I ran the code the tqdm is 100%, after some minutes, but the algorithm still runs in the same shell for a long time.</p>
<p><strong>Do you have any idea if this is a problem of tqdm function or something else?</strong></p>
","python, nlp, doc2vec, tqdm","<p>By using the &quot;list comprehension&quot; (<code>[</code>..<code>]</code>)...</p>
<pre><code>[x for x in tqdm(train_tagged.values)]
</code></pre>
<p>...you are having <code>tqdm</code> iterate once over your <code>train_tagged.values</code> sequence, into an actual in-memory Python list. This will show the <code>tqdm</code> progress rather quickly – then completely finish any involvement with <code>tqdm</code>.</p>
<p>Then, you're passing that plain result list (without any <code>tqdm</code> features) into <code>Doc2Vec.train()</code>, where <code>Doc2Vec</code> does its <code>epochs=40</code> training passes. <code>tqdm</code> is no longer involved, so there'll be no incremental progress-bar output.</p>
<p>You might be tempted to try (or have already tried) something that skips the extra <code>list</code> creation, passing the <code>tqdm</code>-wrapped sequence directly in like:</p>
<pre><code>corpus = utils.shuffle(train_tagged.values)
model_dbow.train(tqdm(corpus), total_examples=len(corpus), epochs = 40)
</code></pre>
<p>But this has a different problem: the <code>tqdm</code>-wrapper is only designed to allow (&amp; report the progress of) <em>one</em> iteration over the wrapped sequence. So this will show that one iteration's incremental progress.</p>
<p>But when <code>.train()</code> tries its next necessary 39 re-iterations, to complete its <code>epochs=40</code> training-runs, the single-pass <code>tqdm</code> object will be exhausted, preventing full &amp; proper training.</p>
<p>Note that there is an option for progress-logging within Gensim, by setting the Python logging level (globally, or just for the class <code>Doc2Vec</code>) to <code>INFO</code>. <code>Doc2Vec</code> will then emit a log-line showing progress, within each epoch and between epochs, about every 1 second. But: you can also make such logging less-frequent by supplying a different seconds value to the optional <code>report_delay</code> argument of <code>.train()</code>, for example <code>report_delay=60</code> (for a log line every minute instead of every second).</p>
<p>If you really want a progress-bar, it should possible to use <code>tqdm</code> - but you will have to work around its assumption that the iterable you're wrapping with <code>tqdm()</code> will only be iterated over once.</p>
<p>I believe there'd be two possible approaches, each with different tradeoffs:</p>
<p>(1) Instead of letting <code>.train()</code> repeat the corpus N times, do it yourself - adjusting the other .<code>train()</code> parameters accordingly. Roughly, that'd mean changing a line like...</p>
<pre><code>model.train(corpus, total_examples=len(corpus), epochs=40)
</code></pre>
<p>...into something that turns your desired 40 epochs into something that looks like just <em>one</em> iteration to both <code>tqdm</code> &amp; Gensim's <code>.train()</code>, like...</p>
<pre><code>repeated_corpus = itertools.chain(*[corpus]*40)
repeated_len = 40 * len(corpus)
model.train(tqdm(repeated_corpus, total=repeated_len), total_examples=repeated_len, epochs=1)
</code></pre>
<p>(Note that you now <em>have</em> to give <code>tqdm</code> a hint as to the sequence's length, because the one-time chained-iterator from <code>itertools.chain()</code> doesn't report its own length.)</p>
<p>Then you'll get one progress-bar across the whole, training corpus - which the model is now seeing as one pass over a larger corpus, but ultimately involves the same 40 passes.</p>
<p>You'll want to reinterpret any remaining log lines with this change in mind, and you'll lose a chance to install your own per-epoch callbacks via the model's end-of-epoch callback mechanism. (But, that's a seldom-used feature, anyway.)</p>
<p>(2) Instead of wrapping the corpus with a single <code>tqdm()</code> (which can only show a progress-bar for one-iteration), wrap the corpus as a new <em>fully-re-iterable object</em> that itself will start a <em>new</em> <code>tqdm()</code> each time. For example, something like:</p>
<pre class=""lang-py prettyprint-override""><code>from collections.abc import Iterable`
class TqdmEveryIteration(Iterable):
    def __init__(self, inner_iterable):
        self.inner_iterable = inner_iterable
    def __iter__(self):
        return iter(tqdm(self.inner_iterable))
</code></pre>
<p>Then, using this new extra <code>tqdm</code>-adding wrapper, you should be able to do:</p>
<pre class=""lang-py prettyprint-override""><code>corpus = utils.shuffle(train_tagged.values)
model_dbow.train(TqdmEveryIteration(corpus), total_examples=len(corpus), epochs = 40)
</code></pre>
<p>In this case, you should get one progress bar <em>per epoch</em>, because a new <code>tqdm()</code> wrapper will be started each training pass.</p>
<p>(If you try either of these approaches &amp; they work well, please let me know! They should be roughly correct, but I haven't tested them yet.)</p>
<p>Separately: if the article from the author at <code>actsusanli.medium.com</code> that you're modeling your work on is...</p>
<p><a href=""https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"" rel=""nofollow noreferrer"">https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4</a></p>
<p>...note that it's using an overly-complex &amp; fragile anti-pattern, calling <code>.train()</code> multiple times in a loop with manual <code>alpha</code> management. That has problems as <a href=""https://stackoverflow.com/questions/62801052/my-doc2vec-code-after-many-loops-of-training-isnt-giving-good-results-what-m/62801053#62801053"">described in this other answer</a>. But that approach <em>would</em> also have the side-effect of re-wrapping the corpus each time in a new <code>tqdm</code> (like the <code>TqdmEveryIteration</code> class above), so despite its other issues, would achieve one actual progress-bar each call to <code>.train()</code>.</p>
<p>(I sent the author a private note via Medium about a month ago about this problem.)</p>
"
"Is there a database, API, or parsable text for getting verb conjugations?","<p>This isn't directly a programming question, so I apologize in advance. I've been working on a grammar-free random sentence generator for a typing game I'd like to make, and I've been having a difficult time finding any parsable (or callable) data for getting verb conjugations. Ultimately, if I can't find anything like this, I'm going to have to go through the dictionary I've created and add first-person singular and plural, second-person singular and plural, third-person singular and plural, simple past, past participle, and present participle forms for every irregular verb.</p>

<p>This wouldn't be a problem in many languages, but there are so many irregular English verbs that this could take a long, long time to do manually. I'm not against the worse option, but I want to make sure I'm not going to be wasting obscene hours doing it myself when there is some database I can use instead.</p>

<p>I've seen <a href=""http://www.scientificpsychic.com/verbs1.html"" rel=""nofollow noreferrer"">http://www.scientificpsychic.com/verbs1.html</a> and spoken with the creator, but he doesn't release his exact dictionary (just the classes for it). I've also seen sites like <a href=""http://www.verbix.com/webverbix/English/find.html"" rel=""nofollow noreferrer"">http://www.verbix.com/webverbix/English/find.html</a>, which would be great for scraping, but that's a bit of a pain as well.</p>

<p>This question has been asked here before ( <a href=""https://stackoverflow.com/questions/8424806/verb-conjugations-database"">Verb Conjugations Database</a> ), but the question was left unanswered, and the asker alluded to solving the problem but never said what the solution was.</p>
",nlp,"<p>MorphAdorner (Java) has a simple <a href=""http://morphadorner.northwestern.edu/morphadorner/conjugator/"" rel=""nofollow"">Verb conjugator</a> (with online demo).</p>

<p>But if you are interested with an exhaustive listing you can check <a href=""https://lsg3.nlm.nih.gov/LexSysGroup/Projects/lvg/current/docs/designDoc/UDF/flow/fi.html"" rel=""nofollow"">Lexical Tools' Inflection Variants</a>. After downloading <a href=""http://specialist.nlm.nih.gov/lvg/"" rel=""nofollow"">Lexical Tools</a>, you will be importing the data to your database server. Then you can just query the database using their library (Java).</p>

<p><a href=""http://code.google.com/p/simplenlg/"" rel=""nofollow"">SimpleNLG</a> also has this feature, and is very much related to Lexical Tool.</p>
"
Stanford Core NLP how to get the probability &amp; margin of error,"<p>When using the parser or for the matter any of the Annotation in <strong>Core NLP</strong>, is there a way to access the probability or the margin of error?</p>
<p>I am particularly interested in detecting ambiguity programmatically. For example, in the sentence below, <strong>desire</strong> is tagged as a noun, but it could also be a verb.</p>
<p>I want to know if there is a way to retrieve a confidence score from the CoreNLP API that indicates ambiguity.</p>
<pre><code>(NP (NP (NNP Whereas)) (, ,) (NP (NNP users) (NN desire) (S (VP (TO to) (VP (VB sell))))))
</code></pre>
<p>In this case, <strong>desire</strong> is labeled as NN (noun) instead of a verb. I need a way to check how confident <strong>CoreNLP</strong> is about this classification.</p>
","nlp, stanford-nlp, pos-tagger",
WebSocket Error 403 During Handshake with ElevenLabs API,"<p>I’m working on a conversational AI website, and I’m using the ElevenLabs API for voice generation and conversation handling. However, I’ve run into a problem when trying to establish a WebSocket connection to their API.</p>
<p>Here are the details of the issue:</p>
<p>Terminal Output:</p>
<p><code>✓ Compiled in 624ms (608 modules)   GET / 200 in 185ms   Requesting signed URL for agent: JKGi0Tt8RmaPYInI4sdL   Raw ElevenLabs response: {&quot;signed_url&quot;:&quot;wss://api.elevenlabs.io/v1/convai/conversation?agent_id=JKGi0Tt8RmaPYInI4sdL&amp;conversation_signature=Rzqesjb7S4vo7vSHV8DW&quot;} </code></p>
<p>Browser Console Error:</p>
<p><code>WebSocket connection to 'wss://api.elevenlabs.io/v1/convai/conversation?agent_id=JKGi0Tt8RmaPYInI4sdL' failed: Error during WebSocket handshake: Unexpected response code: 403   Failed to start conversation:   Event {isTrusted: true, type: 'error', target: WebSocket, currentTarget: WebSocket, eventPhase: 2, …} </code></p>
<p>Steps Taken:</p>
<p>I followed the ElevenLabs documentation for requesting a signed URL and used the provided signed_url to initiate the WebSocket connection.
The URL appears valid, but the server returns a 403 error during the WebSocket handshake.</p>
<p>Environment:</p>
<ul>
<li>React-based frontend.</li>
<li>The WebSocket client is implemented in JavaScript/TypeScript.</li>
</ul>
<p>What could be causing the 403 error during the WebSocket handshake?
Are there specific headers or configurations required that I might be missing?</p>
<p>I’ve verified that the agent ID and conversation signature are correctly passed when requesting the signed URL.</p>
","next.js, nlp, artificial-intelligence, conversational-ai, elevenlabs",
How do I include a custom component in a spaCy training pipeline using the CLI?,"<p>I'm trying to implement a simple custom component in my spaCy training pipeline. I'm using the spaCy CLI for training, which means I'm directing the pipeline configuration through the config.cfg file, though I do have scripts for generating and annotating training and evaluation data. The custom component I've created is stateless, which by all accounts means it should not need a factory and can just use the <code>@Language.component()</code> decorator. The component simply takes the doc object and adds a classification to it based on the number of occurrences of any of a given set of named entities. If there are more than one of any given entity type, the classification score is 1.0; if not, it is 0.0.</p>
<p>Here is the code for the function:</p>
<pre class=""lang-py prettyprint-override""><code># custom_classifier.py

from spacy.language import Language

@Language.component(&quot;custom_classifier&quot;)
def custom_classifier(doc):
    entity_types = [&quot;ENTITY1&quot;, &quot;ENTITY2&quot;, &quot;ENTITY3&quot;]
    entity_counts = [sum(1 for ent in doc.ents if ent.label_ == entity_type) for entity_type in entity_types]

    if any(count &gt; 1 for count in entity_counts):
        doc.cats[&quot;MULTIPLE&quot;] = 1.0
    else:
        doc.cats[&quot;MULTIPLE&quot;] = 0.0
    
    return doc
</code></pre>
<p>I'm then attempting to use this custom component in my training pipeline by adding it to my config.cfg definition, like so:</p>
<pre class=""lang-none prettyprint-override""><code>[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;,&quot;custom_classifier&quot;]
batch_size = 1000
...

[components]
...

[components.taper_classifier]
source = &quot;custom_classifier.custom_classifier&quot;
after = &quot;ner&quot;
</code></pre>
<p>No matter what I do, when I attempt to run <code>spacy train config.cfg</code>, I end up with some version of this error message:</p>
<pre class=""lang-bash prettyprint-override""><code>OSError: [E050] Can't find model 'custom_classifier.custom_classifier'. It doesn't seem to be a Python package or a valid path to a data directory.
</code></pre>
<p>So far I've tried various solutions, none of which have worked. Those include:</p>
<ul>
<li>Trying every conceivable permutation of the module path I could think of.</li>
<li>Converting the component function to a factory and modifying both the decorator and the config accordingly.</li>
<li>Registering the function using the <code>@spacy.registry</code> decorator.</li>
</ul>
<p>I have been combing through the documentation and I simply can't figure out what I'm doing wrong, although I'll concede that the documentation appears to assume that training using custom components is being done in code rather than with the CLI and does not get into detail about how to declare custom components using the config.cfg file. I feel like there's either something simple I'm just overlooking, or else that I'm fundamentally misunderstanding how these custom components are meant to work.</p>
","python, machine-learning, nlp, spacy-3",
How to increase row height in Pandas?,"<p>There are good answers on how to change column width in pandas dataframes, for example <a href=""https://stackoverflow.com/questions/39680147/can-i-set-variable-column-widths-in-pandas"">here</a> but I couldn't find anything describing how we can change row height?</p>
<p>I'm trying to view two long texts side by side to compare them and I'd like to increase the row height to make it easier. Maybe some to do with <code>expand_frame_repr</code>?</p>
","python, pandas, nlp",
Extracting English imperative mood from verb tags with spaCy,"<p>I would like to detect the imperative mood of verbs in English sentences. From <a href=""https://stackoverflow.com/questions/53755559/how-to-extract-tag-attributes-using-spacy"">this question</a> I'm aware that spaCy has access to extended morphological features, but I don't see them when I use it, using spaCy version 2.1.8, Python 3.7.1:</p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; nlp = spacy.load('en_core_web_sm')
&gt;&gt;&gt; doc = nlp(""Show me the money!"")
&gt;&gt;&gt; token = doc[0]
&gt;&gt;&gt; print(token, token.tag_, nlp.vocab.morphology.tag_map[token.tag_], token.pos_, token.dep_)
Show VB {74: 100, 'VerbForm': 'inf'} VERB ROOT
</code></pre>

<p>Looks like spaCy correctly inferred that ""show"" is a verb, but it appears that it thinks it's an infinitive, and I see no mood information from the <code>tag_map</code>. I realize that this extended information is available in my linked question because Italian was being parsed there and determining the mood may be simpler for Romance-inflected verbs. </p>

<p>In any case, does any extended information about mood exist for verbs in English sentences?</p>
","python-3.x, nlp, spacy",
"Unable to install sentence-transformers, getting error","<p>Running below command after installing python 3.10.</p>
<p>pip3 install -U sentence-transformers</p>
<ol>
<li>List item</li>
</ol>
<p>ERROR: Cannot install sentence-transformers==0.1.0, sentence-transformers==0.2.0, sentence-transformers==0.2.1, sentence-transformers==0.2.2, sentence-transformers==0.2.3, sentence-transformers==0.2.4, sentence-transformers==0.2.4.1, sentence-transformers==0.2.5, sentence-transformers==0.2.5.1, sentence-transformers==0.2.6.1, sentence-transformers==0.2.6.2, sentence-transformers==0.3.0, sentence-transformers==0.3.1, sentence-transformers==0.3.2, sentence-transformers==0.3.3, sentence-transformers==0.3.4, sentence-transformers==0.3.5, sentence-transformers==0.3.5.1, sentence-transformers==0.3.6, sentence-transformers==0.3.7, sentence-transformers==0.3.7.1, sentence-transformers==0.3.7.2, sentence-transformers==0.3.8, sentence-transformers==0.3.9, sentence-transformers==0.4.0, sentence-transformers==0.4.1, sentence-transformers==0.4.1.1, sentence-transformers==0.4.1.2, sentence-transformers==1.0.0, sentence-transformers==1.0.1, sentence-transformers==1.0.2, sentence-transformers==1.0.3, sentence-transformers==1.0.4, sentence-transformers==1.1.0, sentence-transformers==1.1.1, sentence-transformers==1.2.0, sentence-transformers==1.2.1, sentence-transformers==2.0.0 and sentence-transformers==2.1.0 because these package versions have conflicting dependencies.</p>
<p>The conflict is caused by:
sentence-transformers 2.1.0 depends on torch&gt;=1.6.0<br />
...............</p>
<p>To fix this you could try to:</p>
<ol>
<li>loosen the range of package versions you've specified</li>
<li>remove package versions to allow pip attempt to solve the dependency conflict</li>
</ol>
<p>ERROR: ResolutionImpossible: for help visit <a href=""https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies"" rel=""nofollow noreferrer"">https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies</a>
WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.
You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.10/bin/python3.10 -m pip install --upgrade pip' command.</p>
","python, nlp",
How to log only the current script file to W&amp;B code panel immediately?,"<p>How can I ensure that only the current script file (e.g., <code>train.py</code>) is logged to the W&amp;B Code panel when running a script, without logging the entire directory?</p>
<p>Currently, I'm using:</p>
<pre class=""lang-py prettyprint-override""><code>wandb.run.log_code(f&quot;./{os.path.basename(__file__)}&quot;)
</code></pre>
<p>I want to confirm if this approach works reliably across different environments and if there are better practices for this use case.</p>
<p>Main part of the code:</p>
<pre class=""lang-py prettyprint-override""><code>def _main(**kwargs):
    from datetime import datetime
    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'
    run_name = f'{today}' 
    kwargs = kwargs | {'today': today}
    # run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)
    run = wandb.init(mode=kwargs.get('mode', 'online'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)
    wandb.run.log_code(f&quot;./{os.path.basename(__file__)}&quot;) # maybe logscode immediately
    # wandb.config.update()
    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)
    output_dir = main(**kwargs)
    run_eval_logic_contamination(output_dir)
    # from train.utils import copy_to_dfs
    # copy_to_dfs(output_dir)
    run.alert(title=&quot;Run Completed&quot;, text=f&quot;Run finished, run url: {run.get_url()}&quot;)
    print(f'{run.get_url()=}')
    wandb.finish()
</code></pre>
<p>All the code:</p>
<pre class=""lang-py prettyprint-override""><code>from datetime import datetime
from typing import Optional
import random
import torch
from transformers import PushToHubCallback
from transformers import get_cosine_schedule_with_warmup
from trl import SFTConfig, SFTTrainer
import os
import fire
import wandb
import sys

from train.callbacks import GenCallbackWithHFGenerate
from train.data import load_math_style_dataset, print_first_example_after_decode
import train.models

from train.utils import seed_everything

def main(**config):
    # -- Seed everything
    seed_everything(seed=config.get('seed', 0))
    
    # -- HF login
    from huggingface_hub import login
    token = open(os.path.expanduser(&quot;~/keys/master_hf_token.txt&quot;)).read().strip()
    login(token=token)

    # -- Get model
    model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'google/gemma-2-2b')) 
    # model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'meta-llama/Llama-3.1-8B')) 

    # -- Load datasets
    ds_name_or_path = config.get('ds_name_or_path', 'Putnam-AXIOM/putnam-axiom-dataset')
    train_split, val_split = config.get('train_split', 'func_original_53_10_30_2024'), config.get('val_split', 'func_variations_265_11_23_2024')
    print(f'\n---&gt; {ds_name_or_path=} {train_split=} {val_split=}\n')
    train_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=1, split=train_split)
    print_first_example_after_decode(train_dataset, tok)
    # eval_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=15, split=val_split)
    eval_dataset = train_dataset
    print(f'{len(train_dataset)=}\n{len(eval_dataset)=}')
    wandb.config.update({'dataset': f'{ds_name_or_path} ({train_split=} {val_split=})'})

    # -- Prepare output directory
    today: str = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss')
    output_dir: str = os.path.expanduser(f&quot;~/data/runs_logic_cont/run_{config.get('today', today)}&quot;)
    print(f'{output_dir=}')
    
    # Save the initial model and tokenizer as checkpoint-0
    initial_checkpoint_dir = os.path.join(output_dir, &quot;checkpoint-0&quot;)
    os.makedirs(initial_checkpoint_dir, exist_ok=True)
    print(f&quot;Saving initial checkpoint and tokenizer at {initial_checkpoint_dir}&quot;)
    model.save_pretrained(initial_checkpoint_dir)
    tok.save_pretrained(initial_checkpoint_dir)

    # -- Train model
    # max_steps = 50  # Limit fine-tuning to a few steps
    # os.environ['CUDA_VISIBLE_DEVICES'] = str(random.randint(0, 7))
    # config = {'max_steps': 2, 'eval_steps': 1, 'logging_steps': 1, 
    #           'save_strategy': 'steps', 'save_steps': 1, 'eval_strategy': 'steps'}
    # config = config | {'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES', 'maybe 0')}
    training_args = SFTConfig(
        max_steps=config.get('max_steps', 30),
        # --
        output_dir=output_dir,
        bf16=torch.cuda.is_bf16_supported(),
        fp16=not torch.cuda.is_bf16_supported(),
        # -- logging opts
        save_steps=config.get('save_steps', 5), 
        save_strategy=config.get('save_strategy', 'steps'),
        eval_on_start=config.get('eval_on_start', True),
        evaluation_strategy=config.get('eval_strategy', 'steps'), 
        eval_steps=config.get('eval_steps', 1), 
        logging_first_step=config.get('logging_first_step', True), # Default to False, unsure 100% what this does but looks like a good idea
        logging_strategy=config.get('logging_strategy', 'steps'),
        logging_steps=config.get('logging_steps', 1),
        # --
        num_train_epochs=config.get('num_train_epochs', 10),
        max_seq_length=config.get('max_seq_length', 512),
        per_device_train_batch_size=config.get('batch_size', 2),
        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 2),
    )
    # Calculate Total Steps
    steps_per_epoch = (len(train_dataset) // training_args.per_device_train_batch_size) // training_args.gradient_accumulation_steps
    total_steps = steps_per_epoch * training_args.num_train_epochs
    print(f'{steps_per_epoch=}')

    # Optimizer and Scheduler
    # optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 1e-4}]
    optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 0}]
    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=config.get('learning_rate', 1e-5))

    # Add Cosine Learning Rate Scheduler
    # warmup_steps = int(0.01 * total_steps)  # Warm-up for 1% of total steps
    warmup_steps = 0
    scheduler = get_cosine_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps,
    )
    scheduler = None
    print(f'{total_steps=} {warmup_steps=}')
    trainer = SFTTrainer(
        model=model,
        tokenizer=tok,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        args=training_args,
        optimizers=(optimizer, scheduler),
        callbacks=[GenCallbackWithHFGenerate(model, tok)]
    )
    print(f&quot;\nStarting fine-tuning...&quot;)
    trainer.train()
    # - end run
    return os.path.expanduser(output_dir)

def run_eval_logic_contamination(output_dir: str):
    &quot;&quot;&quot;
    Runs the eval_logic_contamination.py script with the specified output directory.

    Args:
        output_dir (str): The directory where the model is saved, expanded using `os.path.expanduser`.
    &quot;&quot;&quot;
    import gc
    torch.cuda.empty_cache()
    gc.collect()
    output_dir = os.path.expanduser(output_dir)  # Ensure `output_dir` is expanded 
    from eval_logic_contamination import main
    task='putnam_axiom_53'
    res: dict = main(model_name_or_path=output_dir, task=task)
    print(f'Results for {task=}: {res}')
    print(res)
    # task='putnam_axiom_53' # for debugging
    task='putnam_axiom_variations'
    res: dict = main(model_name_or_path=output_dir, task=task)
    print(f'Results for {task=}: {res}')
    print(res)
    # wandb.run.define_metric(&quot;eval/accuracy&quot;, step_metric=&quot;eval/checkpoint_idx&quot;)
    # wandb.run.define_metric(&quot;eval/checkpoint_idx&quot;) 
    # for idx, acc in [(10,5), (20,10), (30,15)]:
    #     wandb.log({'eval/accuracy': acc, 'eval/checkpoint_idx': idx})

def _main(**kwargs):
    from datetime import datetime
    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'
    run_name = f'{today}' 
    kwargs = kwargs | {'today': today}
    # run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)
    run = wandb.init(mode=kwargs.get('mode', 'online'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)
    wandb.run.log_code(f&quot;./{os.path.basename(__file__)}&quot;) # maybe logscode immediately
    # wandb.config.update()
    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)
    output_dir = main(**kwargs)
    run_eval_logic_contamination(output_dir)
    # from train.utils import copy_to_dfs
    # copy_to_dfs(output_dir)
    run.alert(title=&quot;Run Completed&quot;, text=f&quot;Run finished, run url: {run.get_url()}&quot;)
    print(f'{run.get_url()=}')
    wandb.finish()

if __name__ == &quot;__main__&quot;:
    import time
    start_time = time.time()
    fire.Fire(_main)
    print(f&quot;Time taken: {time.time() - start_time:.2f} seconds, or {(time.time() - start_time) / 60:.2f} minutes, or {(time.time() - start_time) / 3600:.2f} hours.\a&quot;)
</code></pre>
<p>Cross: <a href=""https://community.wandb.ai/t/how-to-log-only-the-current-script-file-to-w-b-code-panel-immediately/8537"" rel=""nofollow noreferrer"">https://community.wandb.ai/t/how-to-log-only-the-current-script-file-to-w-b-code-panel-immediately/8537</a></p>
","deep-learning, nlp, wandb",
How can one get the LLM model name and version using Google AI Edge SDK?,"<p>I use Google AI Edge SDK to call an on-device LLM (<a href=""https://github.com/android/ai-samples/tree/main"" rel=""nofollow noreferrer"">example app</a>). How can I get the LLM model name and version that my code uses?</p>
<p><a href=""https://developer.android.com/ai/gemini-nano"" rel=""nofollow noreferrer"">https://developer.android.com/ai/gemini-nano</a> says it uses Gemini Nano:</p>
<p><a href=""https://i.sstatic.net/fzpdHEX6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fzpdHEX6.png"" alt=""enter image description here"" /></a></p>
<p>but there are at least 2 versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters), and I guess there'll be more soon, if not already (different training set, compression methods, parameter count, etc.).</p>
","android, nlp, large-language-model, google-ai-edge-sdk",
Issues with nltk&#39;s ne_chunk,"<p>I have been trying to use nltk's entity chunker, and tried different approaches but I keep getting the error:</p>
<pre><code>LookupError                               Traceback (most recent call last)
       ...
     8 pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] 
     10 # Create the named entity chunks: chunked_sentences
---&gt; 11 chunked_sentences = nltk.ne_chunk(pos_sentences, binary=True)
     13 # Test for stems of the tree with 'NE' tags
     14 for sent in chunked_sentences:

    178 &quot;&quot;&quot;
    179 Use NLTK's currently recommended named entity chunker to
    180 chunk the given list of tagged tokens.
   (...)
    187 
    188 &quot;&quot;&quot;
    189 if binary:
--&gt; 190     chunker = ne_chunker(fmt=&quot;binary&quot;)
    191 else:
    192     chunker = ne_chunker()

    170 def ne_chunker(fmt=&quot;multiclass&quot;):
    171     &quot;&quot;&quot;
    172     Load NLTK's currently recommended named entity chunker.
    173     &quot;&quot;&quot;
--&gt; 174     return Maxent_NE_Chunker(fmt)
...
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'

</code></pre>
<p>I've attempted to use both my own code, and the below example, which i found on a blog post:</p>
<pre><code>from nltk.tokenize import word_tokenize

import nltk
from nltk.chunk import ne_chunk
nltk.download('punkt')
import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

article=&quot;The taxi-hailing company Uber brings into very sharp focus the question of whether corporations can be said to have a moral character. If any human being were...&quot;
print(article)

# Tokenize the article into sentences: sentences
sentences = nltk.sent_tokenize(article)

# Tokenize each sentence into words: token_sentences
token_sentences = [nltk.word_tokenize(sent) for sent in sentences]

# Tag each tokenized sentence into parts of speech: pos_sentences
pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] 

# Create the named entity chunks: chunked_sentences
chunked_sentences = nltk.ne_chunk(pos_sentences, binary=True)

# Test for stems of the tree with 'NE' tags
for sent in chunked_sentences:
    for chunk in sent:
        if hasattr(chunk, &quot;label&quot;) and chunk.label() == &quot;NE&quot;:
            print(chunk)
</code></pre>
<p>I've tried &quot;from nltk.chunk import ne_chunk&quot; and &quot;from nltk import ne_chunk&quot;, and I have also tried to use ne_chunk_sents() instead of ne_chunk(). I've tried reproducing multiple other code examples, but it seems like I still get the same error when using nltk's ne_chunk.</p>
<p>My question is, what could be causing this?</p>
","python, nlp, nltk, pos-tagger, ent",
word reduction of a list of words,"<p>Besides using a chatbot like o1 mini, is there more local way to reduce a list of words from 10 words down to 3 words? I feel that due to context, word association, semantic meanings of the words, a there has to be some form of extrapolation to reduce the list but so far my attempts have not been nearly as good.</p>
<p>The ultimate goal is to reduce a list to something that can be searchable.</p>
<p>example using chatgpt 1o mini</p>
<p>reduce the list of words to 3 terms able to used in search. Combine words if they make sense in context to all other words in the list. Each term should combine words meaningfully, and no word should be repeated across the terms. return in list form</p>
<pre><code>[&quot;rpg&quot;, &quot;role playing&quot;, &quot;fantasy&quot;, &quot;monster&quot;, &quot;Dungeons&quot;, &quot;dragons&quot;, &quot;master&quot;, &quot;monster&quot;, &quot;job&quot;, &quot;class&quot;]
</code></pre>
<blockquote>
<p>result: Dungeons Dragons, Role Playing, Monster Class</p>
</blockquote>
<p>here are the attempts I tried using python</p>
<p>attempt 1: UMAP, Cosine similarity</p>
<pre><code>from sklearn.metrics.pairwise import cosine_similarity

def concatenate_overlap(list_of_list):
    pooled = [set(subList) for subList in list_of_list]
    merging = True
    while merging:
        merging = False
        for i, group in enumerate(pooled):
            merged = next((g for g in pooled[i + 1:] if g.intersection(group)), None)
            if not merged: 
                continue
            group.update(merged)
            pooled.remove(merged)
            merging = True
    return [list(x) for x in pooled]


text_list = [&quot;rpg&quot;, &quot;role playing&quot;, &quot;fantasy&quot;, &quot;monster&quot;, &quot;Dungeons&quot;, &quot;dragons&quot;, &quot;master&quot;, &quot;monster&quot;, &quot;job&quot;, &quot;class&quot;]

MODEL_NAME = 'Alibaba-NLP/gte-multilingual-base'
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)
batch_dict = tokenizer(text_list, max_length=8192, padding=True, truncation=True, return_tensors='pt')
outputs = model(**batch_dict)
embeddings = outputs.last_hidden_state[:, 0][:768]
similarity_matrix = cosine_similarity(embeddings.detach())
np.fill_diagonal(similarity_matrix, 0)
to_merge_list: List[List[int]] = []
for idx, topic_similarity_scores in enumerate(similarity_matrix):
    similar_words = list(np.where(0.8&gt; min_similarity)[0])
    similar_words.append(idx)
    to_merge_list.append(similar_words)

to_concat = concatenate_overlap(to_merge_list)
words = [text_list[a[0]] for a in to_concat]

</code></pre>
<blockquote>
<p>result: ['rpg', 'monster', 'Dungeons', 'dragons', 'master', 'monster', 'job', 'class']</p>
</blockquote>
<p>attempt 2: summurization model</p>
<pre><code>summarizer = pipeline(&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot;)

word_list = [&quot;rpg&quot;, &quot;role playing&quot;, &quot;fantasy&quot;, &quot;monster&quot;, &quot;Dungeons&quot;, &quot;dragons&quot;, &quot;master&quot;, &quot;monster&quot;, &quot;job&quot;, &quot;class&quot;]
text = &quot;, &quot;.join(word_list)
summarizer(text, max_length=50, do_sample=False)
</code></pre>
<blockquote>
<p>result: rpgs, role playing, fantasy, monster, Dungeons, dragons, master, monster. job, class. rpg, roleplaying, fantasy,. monster, dungeons, dragons,. master, monsters, master. job,. class</p>
</blockquote>
<p>edit: the criteria is simple, shrink the word list to a 3 or 4 word phrase that can be searchable. In the case of chatgpt 01, it gave me 3 multi word expressions for searching. For the two examples. Ideally it would give me something also as small.</p>
<p>I plan to take word lists and boil them down to searchable prompts</p>
<p>edit 2:
to provide more context to what the expected outcome should be here is a unit test and explanation.</p>
<pre><code>input: [&quot;bad&quot;, &quot;horrible&quot;, &quot;unfun&quot;, &quot;waste&quot;, &quot;big&quot;, &quot;mutha&quot;, &quot;truckers&quot;, &quot;racing&quot;, &quot;incomplete&quot;, &quot;big mutha&quot;]

output: [&quot;big mutha truckers&quot;, &quot;unfun&quot;, &quot;incomplete&quot;]
</code></pre>
<p>contextually, the list of 10 words is focusing on a game called &quot;big mutha truckers&quot; and as such this would get pulled from the word list. The second would be all the synonyms which refer to the game being not good, so &quot;unfun&quot; or &quot;bad&quot; or even &quot;horrible racing&quot;. Lastly &quot;incomplete&quot; would get pulled as it isn't a close synonym of bad and is is descriptive</p>
<p>with these 3 words, I can throw it into google and ideally search results of negative reviews for big mutha truckers should appear.</p>
","python, nlp",
Fuzzy String Comparison,"<p>What I am striving to complete is a program which reads in a file and will compare each sentence according to the original sentence. The sentence which is a perfect match to the original will receive a score of 1 and a sentence which is the total opposite will receive a 0. All other fuzzy sentences will receive a grade in between 1 and 0. </p>

<p>I am unsure which operation to use to allow me to complete this in Python 3. </p>

<p>I have included the sample text in which the Text 1 is the original and the other preceding strings are the comparisons.  </p>

<h2>Text: Sample</h2>

<p>Text 1: It was a dark and stormy night. I was all alone sitting on a red chair. I was not completely alone as I had three cats.</p>

<p>Text 20: It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines
// Should score high point but not 1</p>

<p>Text 21: It was a murky and tempestuous night. I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines
// Should score lower than text 20</p>

<p>Text 22: I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines. It was a murky and tempestuous night.
// Should score lower than text 21 but NOT 0</p>

<p>Text 24: It was a dark and stormy night. I was not alone. I was not sitting on a red chair. I had three cats.
// Should score a 0!</p>
","python, nlp, fuzzy-comparison",
How can I use BERT for long text classification?,"<p>We know that BERT has a maximum length limit of tokens = 512. So if an article has a length of much bigger than 512, such as 10000 tokens in text, how can BERT be used?</p>
","nlp, text-classification, bert-language-model",
Is there any flag to stop langgraph for further executing?,"<p>I want to create a interviewing bot that may ask 5 questions. The questions are specified in the system prompt. I just want the langgraph to stop execution when 5 certain questions are asked.
I tried quick start of the langgraph official docs. I read the documentation and I found human-in-the-loop concept but I don't want that. I want the langgraph to completely quits once user answers the questions correctly. If user does not answer correctly, langgraph should ask the question again. I have done that. There might be more attempts of user input than the total questions if user answers incorrectly. I expect that once user answers the questions correclty, langgraph must exist so that user can not exploit the resources.</p>
","python, nlp, chatbot, large-language-model, langgraph",
Can&#39;t compile Marian NMT,"<p>I'm using endeavouros. I'm trying to compile Marian with these instructions: <a href=""https://marian-nmt.github.io/docs/#installation"" rel=""nofollow noreferrer"">https://marian-nmt.github.io/docs/#installation</a>. But it fails.</p>
<p>The error message seemingly indicates a conflict between the code and c++20. But in all the <code>CMakeLists.txt</code> files of the repo, there is the line <code>set (CMAKE_CXX_STANDARD 11)</code>.</p>
<p>These are the steps that I followed:</p>
<pre class=""lang-bash prettyprint-override""><code>git clone https://github.com/marian-nmt/marian
mkdir marian/build
cd marian/build
cmake ..
make -j4
</code></pre>
<p>This is the result I had:</p>
<pre><code>➜ make -j4
[  1%] Built target 3rd_party_installs
[  1%] Built target marian_version
[  6%] Built target sentencepiece_train-static
[ 19%] Built target libyaml-cpp
[ 25%] Built target SQLiteCpp
[ 25%] Built target pathie-cpp
[ 32%] Built target zlib
[ 35%] Built target intgemm
[ 35%] Built target faiss
[ 53%] Built target sentencepiece-static
[ 55%] Built target spm_decode
[ 55%] Built target spm_normalize
[ 55%] Built target spm_encode
[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/aliases.cpp.o
[ 55%] Building CXX object src/CMakeFiles/marian.dir/common/fastopt.cpp.o
[ 56%] Built target spm_train
[ 57%] Built target spm_export_vocab
[ 57%] Building CXX object src/CMakeFiles/marian.dir/common/utils.cpp.o
[ 58%] Building CXX object src/CMakeFiles/marian.dir/common/logging.cpp.o
In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,
                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,
                 from /data/tools/marian/src/common/logging.h:5,
                 from /data/tools/marian/src/common/definitions.h:3,
                 from /data/tools/marian/src/common/fastopt.h:3,
                 from /data/tools/marian/src/common/fastopt.cpp:1:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]
  138 |     registry_t&lt;Mutex&gt;() {}
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]
  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’
In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,
                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,
                 from /data/tools/marian/src/common/logging.h:5,
                 from /data/tools/marian/src/common/utils.cpp:2:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]
  138 |     registry_t&lt;Mutex&gt;() {}
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]
  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’
In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,
                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,
                 from /data/tools/marian/src/common/logging.h:5,
                 from /data/tools/marian/src/common/logging.cpp:1:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]
  138 |     registry_t&lt;Mutex&gt;() {}
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]
  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’
In file included from /data/tools/marian/src/3rd_party/spdlog/details/spdlog_impl.h:12,
                 from /data/tools/marian/src/3rd_party/spdlog/spdlog.h:139,
                 from /data/tools/marian/src/common/logging.h:5,
                 from /data/tools/marian/src/common/definitions.h:3,
                 from /data/tools/marian/src/common/cli_wrapper.h:6,
                 from /data/tools/marian/src/common/config_parser.h:4,
                 from /data/tools/marian/src/common/aliases.cpp:1:
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]
  138 |     registry_t&lt;Mutex&gt;() {}
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:138:22: note: remove the ‘&lt; &gt;’
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]
  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’
cc1plus: all warnings being treated as errors
make[2]: *** [src/CMakeFiles/marian.dir/build.make:93: src/CMakeFiles/marian.dir/common/fastopt.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
cc1plus: all warnings being treated as errors
make[2]: *** [src/CMakeFiles/marian.dir/build.make:121: src/CMakeFiles/marian.dir/common/utils.cpp.o] Error 1
cc1plus: all warnings being treated as errors
make[2]: *** [src/CMakeFiles/marian.dir/build.make:79: src/CMakeFiles/marian.dir/common/aliases.cpp.o] Error 1
cc1plus: all warnings being treated as errors
make[2]: *** [src/CMakeFiles/marian.dir/build.make:135: src/CMakeFiles/marian.dir/common/logging.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:374: src/CMakeFiles/marian.dir/all] Error 2
make: *** [Makefile:156: all] Error 2
</code></pre>
<p>Please help.</p>
","gcc, cmake, nlp, g++","<p>The diagnostic that your build is tripping, <code>Wtemplate-id-cdtor</code>, was introduced
with GCC 14.1. It is a warning, not an error, but your build promotes all warnings to
errors, so it breaks your build.</p>
<p>Although your build specifies <code>-std=c++11</code> in <code>src/3rd_party/spdlog/CMakeLists.txt</code>, which
generates the failure, g++-14 emits <code>Wtemplate-id-cdtor</code> to warn you that the code <em>would be</em>
illegal under the more recent standard c++20 (and later). Then the warning is made an error.</p>
<p>The warning is made an error by the compile option <code>-Werror</code>. This option is included in the list
of compile options <code>ALL_WARNINGS</code>, which is created in the top-level <code>marian/CMakeLists.txt</code>
at line 227 <em>et seq</em>:</p>
<pre><code># These are used in src/CMakeLists.txt on a per-target basis
list(APPEND ALL_WARNINGS -Wall; -Werror; -Wextra; -Wno-unused-result; -Wno-deprecated;
-Wno-pragmas; -Wno-unused-parameter; -Wno-unused-function;
-Wno-unused-value; -Wno-unknown-pragmas; -Wno-sign-compare;
-Wno-missing-field-initializers;)
</code></pre>
<p>and then applied as compile options for the <code>marian</code> library target in <code>src/CMakeLists.txt</code>
at line 133:</p>
<pre><code>target_compile_options(marian PRIVATE ${ALL_WARNINGS})
</code></pre>
<p>whence the options are operative for the failing compilation of <code>src/CMakeFiles/marian.dir/common/logging.cpp</code>.</p>
<p>This failure is a bug in the <code>marian</code> repo which you should <a href=""https://github.com/marian-nmt/marian/issues"" rel=""nofollow noreferrer"">report to the maintainers</a>, as
it does not seem to have been reported already. The head revision v1.12.0 is more than a year older than GCC 14.</p>
<p>Pending a fix, you seem to have three interim options to get your build done. Either:</p>
<ul>
<li><p>Make the code legal for both c++11 and c++20 by doing what the diagnostic advice says at each occurrence:</p>
<pre><code>/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: error: template-id not allowed for constructor in C++20 [-Werror=template-id-cdtor]
  139 |     registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;
      |                      ^
/data/tools/marian/src/3rd_party/spdlog/details/registry.h:139:22: note: remove the ‘&lt; &gt;’
</code></pre>
</li>
</ul>
<p>e.g. make it <code>registry_t(const registry_t&lt;Mutex&gt;&amp;) = delete;</code> in this occurrence.</p>
<p>Or:</p>
<ul>
<li><p>Locally disable <code>-Wtemplate-id-cdtor</code> at each occurrence, e.g:</p>
<pre><code>#pragma GCC diagnostic push
#pragma GCC diagnostic ignored &quot;-Wtemplate-id-cdtor&quot;
registry_t&lt;Mutex&gt;(const registry_t&lt;Mutex&gt;&amp;) = delete;
#pragma GCC diagnostic pop
</code></pre>
</li>
</ul>
<p>Or:</p>
<ul>
<li>Remove <code>-Werror</code> from the <code>ALL_WARNINGS</code> list in <code>marian/CMakeLists.txt</code> so that <code>Wtemplate-id-cdtor</code> remains just a warning. This may result in other diagnostics being demoted from errors to warnings (their default status).</li>
</ul>
<p>I haven't tested any of these options as I'd need to go to the trouble of installing CUDA.</p>
"
Arabic lemmatization and Stanford NLP,"<p>I try to make lemmatization, ie identifying the lemma and possibly the Arabic root of a verb, for example:
يتصل ==> lemma (infinitive of the verb) ==> اتصل ==> root (triliteral root / Jidr thoulathi)
==> و ص ل</p>

<p>Do you think Stanford NLP can do that?</p>

<p>Best Regards,</p>
","nlp, stanford-nlp, lexical-analysis, stemming, lemmatization","<p>The Stanford Arabic segmenter can't do true lemmatization. However, it is possible to train a new model to do something like stemming:</p>

<ul>
<li>تكتبون ← ت+ كتب +ون</li>
<li>يتصل ← ي+ تصل</li>
</ul>

<p>If it is very important that the output is real Arabic lemmas (""تصل"" is not a true lemma), you might be better off with a tool like MADAMIRA (<a href=""http://nlp.ldeo.columbia.edu/madamira/"">http://nlp.ldeo.columbia.edu/madamira/</a>).</p>

<p><em>Elaboration:</em> The Stanford Arabic segmenter produces its output character-by-character using only these operations (implemented in <code>edu.stanford.nlp.international.arabic.process.IOBUtils</code>):</p>

<ul>
<li>Split a word between two characters</li>
<li>Transform lil- (للـ) into li+ al- (ل+ الـ)</li>
<li>Transform ta (ت) or ha (ه) into ta marbuta (ة)</li>
<li>Transform ya (ي) or alif (ا) into alif maqsura (ى)</li>
<li>Transform alif maqsura (ى) into ya (ي)</li>
</ul>

<p>So lemmatizing يتصل to ي+ اتصل would require implementing an extra rule, i.e., to insert an alif after ya or ta. Lemmatization of certain irregular forms would be completely impossible (for example, نساء ← امرأة).</p>

<p>The version of the Stanford segmenter available for download also only breaks off pronouns and particles:</p>

<p>وسيكتشفونه ← و+ س+ يكتشفون +ه</p>

<p>However, if you have access to the LDC Arabic Treebank or a similarly rich source of Arabic text with morphological segmentation annotated, it is possible to train your own model to remove all morphological affixes, which is closer to lemmatization:</p>

<p>وسيكتشفونه ← و+ س+ ي+ كتشف +ون +ه</p>

<p>Note that ""كتشف"" is not a real Arabic word, but the segmenter should at least consistently produce ""كتشف"" for تكتشفين ,أكتشف ,يكتشف, etc. If this is acceptable, you would need to change the ATB preprocessing script to instead use the morphological segmentation annotations. You could do this by replacing the script called <code>parse_integrated</code> with a modified version like this: <a href=""https://gist.github.com/futurulus/38307d98992e7fdeec0d"">https://gist.github.com/futurulus/38307d98992e7fdeec0d</a></p>

<p>Then follow the instructions for ""TRAINING THE SEGMENTER"" in the README.</p>
"
Validation and Training Loss when using HuggingFace,"<p>I do not seem to find an explanation on how the validation and training losses are calculated when we finetune a model using the huggingFace trainer. Does anyone know here to find this information?</p>
","nlp, huggingface-transformers, huggingface, huggingface-trainer",
how to get custom column in the model&#39;s forward() function when training with Huggingface Trainer?,"<p>I am using Huggingface Trainer to train a cumstom model subclassing a Llama llm. After tokenized by the tokenizer, my dataset has these fields '<code>input_ids</code>', '<code>labels</code>' and so on, and I additionally add 2 custom colunms '<code>interact_ids</code> ' and '<code>candidate_ids</code> '. But i can't get these custom fields in the forward() function of my Model '<code>class LLMWithCustomLayer(LlamaForCausalLM)</code>'.</p>
<pre class=""lang-py prettyprint-override""><code>    def forward(
            self,
            input_ids: torch.LongTensor = None,
            attention_mask: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.LongTensor] = None,
            past_key_values: Optional[List[torch.FloatTensor]] = None,
            inputs_embeds: Optional[torch.FloatTensor] = None,
            labels: Optional[torch.LongTensor] = None,
            use_cache: Optional[bool] = None,
            output_attentions: Optional[bool] = None,
            output_hidden_states: Optional[bool] = None,
            return_dict: Optional[bool] = None,
            interact_ids = None,
            candidate_ids = None,
        ):
            print('interact_ids, candidate_ids', interact_ids, candidate_ids) # they are none
    
            interact_embs = []
            candidate_embs = []
            for i in range(interact_ids.shape(0)):
                # O_i = F_i (e_i)
                interact_embs.append(self.item_emb_proj(self.get_item_emb(interact_ids)))
                # O_i = F_i (e_i)
                candidate_embs.append(self.item_emb_proj(self.get_item_emb(candidate_ids)))
                # replace [CandidateEmb] and [HistoryEmb]
                inputs_embeds = self.replace_hist_candi_token(input_ids, inputs_embeds ,interact_embs, candidate_embs)
    
            return super().forward(
                input_ids=input_ids,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                inputs_embeds=inputs_embeds,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
                labels = labels
            )
</code></pre>
<p>I an new in LLM fine tuning. Can anyone help me? I would be grateful so much.</p>
","pytorch, nlp, large-language-model, huggingface-trainer","<p>You need to modify the data collator to pass <code>interact_ids</code> and <code>candidate_ids</code> to your model, as Trainer ignores extra columns by default.</p>
<p>To modify the <strong>data collator</strong></p>
<pre class=""lang-py prettyprint-override""><code>class CustomDataCollator(DataCollatorWithPadding):
    def __call__(self, features):
        batch = super().__call__(features)
        batch[&quot;interact_ids&quot;] = torch.tensor([f[&quot;interact_ids&quot;] for f in features])
        batch[&quot;candidate_ids&quot;] = torch.tensor([f[&quot;candidate_ids&quot;] for f in features])
        return batch
</code></pre>
<p>then pass it to <code>Trainer</code></p>
<pre class=""lang-py prettyprint-override""><code>trainer = Trainer(
    model=LLMWithCustomLayer.from_pretrained(&quot;your-llama-model&quot;),
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=CustomDataCollator(tokenizer)
)
</code></pre>
<p>Now, your <code>forward()</code> method will receive <code>interact_ids</code> and <code>candidate_ids</code>.</p>
<p>Hope, it will work!</p>
"
Using WN-Affect to detect emotion/mood of a string,"<p>I  downloaded <a href=""http://wndomains.fbk.eu/wnaffect.html"" rel=""noreferrer"">WN-Affect</a>. I am however not sure how to use it to detect the mood of a sentence. For example if I have a string ""I hate football."" I want to be able to detect whether the mood is bad and the emotion is fear. WN-Affect has no tutorial on how to do it, and I am kind of new to python. Any help would be great!</p>
","python, nlp, nltk, sentiment-analysis, wordnet","<p><strong>In short</strong>: Use SentiWordNet instead and look at <a href=""https://github.com/kevincobain2000/sentiment_classifier"">https://github.com/kevincobain2000/sentiment_classifier</a></p>

<hr>

<p><strong>In Long</strong>:</p>

<p><strong>Affectedness vs Sentiment</strong></p>

<p>The line between affect and sentiment is very fine. One should looking into <code>Affectedness</code> in linguistics studies, e.g. <a href=""http://compling.hss.ntu.edu.sg/events/2014-ws-affectedness/"">http://compling.hss.ntu.edu.sg/events/2014-ws-affectedness/</a> and <code>Sentiment Analysis</code> in computational researches. For now, let's call both the task of identifying affect and sentiment, sentiment analysis.</p>

<p>Also note that <code>WN-Affect</code> is a rather old resource compared to <code>SentiWordNet</code>, <a href=""http://sentiwordnet.isti.cnr.it/"">http://sentiwordnet.isti.cnr.it/</a>. </p>

<p><strong>Here's a good resource for using SentiWordNet for sentiment analysis</strong>: <a href=""https://github.com/kevincobain2000/sentiment_classifier"">https://github.com/kevincobain2000/sentiment_classifier</a>. </p>

<p>Often sentiment analysis has only two classes, <code>positive</code> or <code>negative</code> sentiment. Whereas the WN-affect uses 11 types of affectedness labels:</p>

<ul>
<li>emotion</li>
<li>mood </li>
<li>trait    </li>
<li>cognitive state  </li>
<li>physical state   </li>
<li>hedonic signal   </li>
<li>emotion-eliciting </li>
<li>emotional response   </li>
<li>behaviour    </li>
<li>attitude </li>
<li>sensation</li>
</ul>

<p>For each type, there are multiple classes, see <a href=""https://github.com/larsmans/wordnet-domains-sentiwords/blob/master/wn-domains/wn-affect-1.1/a-hierarchy.xml"">https://github.com/larsmans/wordnet-domains-sentiwords/blob/master/wn-domains/wn-affect-1.1/a-hierarchy.xml</a></p>

<hr>

<p>To answer the question of how one can use the WN-Affect, there're several things you need to do:</p>

<p>First map WN1.6 to WN3.0 (it's not an easy task, you have to do several mappings, especially the mapping between 2.0-2.1)</p>

<p>Now using the WN-Affect with WN3.0, you can apply </p>

<ul>
<li>the same classification technique as he SentiWordNet sentiment classifier or</li>
<li>try to maximize the classes within text and then use some heuristics to choose 'positive' / 'negative'</li>
</ul>
"
Which is better? OpenCyc or ConceptNet?,"<p>I'm doing a NLP project where I need to recognise concepts in sentences to find other similar concepts. I do this to infer word valences from a list I already have. I started using WordNet, but it gave many contradictory results. By contradictory results I mean word expansions that had contradictory valences.</p>
<p>So now I'm looking into ConceptNet and OpenCyc. I've already implemented ConceptNet and it was all very easy and I love it. Problem is that OpenCyc appears to have a much larger and more logically rigid database, which is important when I found so many &quot;contradictions&quot; on WordNet... But I wouldn't know because I haven't tried it.</p>
<p>Could someone tell me if it's worth going through the (considerable, for me) effort to implement OpenCyc, or is ConceptNet good enough to infer word valences? Are they that different?</p>
","nlp, wordnet, conceptnet",
How do I install language model for spacy on Kaggle?,"<p>Aloha! Everybody knows how to install model at home:</p>
<blockquote>
<p><code>python -m spacy download ru_core_news_md</code></p>
</blockquote>
<p>But since python notebook on Kaggle is isolated of the global web, it does not seem possible to do so.</p>
<p>Of course, I can download ru_core_news_sm-3.7.0-py3-none-any.whl file from spacy.io website and upload it into my dataset &quot;ru-core-news-sm&quot; on Kaggle.</p>
<p>Question is, how do I import it in my Spacy? I will be very appreciative for any kind of answer!</p>
","python, nlp, spacy, kaggle",
getting an error: object of type &#39;float&#39; has no len(),"<p>I am a naive in python and started learning python few months ago
I am working on twitter data in my local, it has 4 columns. ID, brand, sentiment, comment</p>
<pre><code>def data_clean_pipeline(text):
    #removing html tags
    text = str(BeautifulSoup(text).get_text())
    #removing any nonletter words 
    text = re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, text)
    text = text.lower()
    text = nltk.word_tokenize(text)
    SW = stopwords.words('english')
    text = [t for t in text if not t in set(SW)]
    #Then we can apply stemming and then lemmitize the data
    SS_stem = SnowballStemmer(language='english')
    text = [SS_stem.stem(t) for t in text]
    word_lemmitize = WordNetLemmatizer()
    text = [word_lemmitize.lemmatize(t) for t in text]
    return &quot; &quot;.join(text)
</code></pre>
<p>When I apply this function to one of the review in twitter data it works, but when i apply to the entire column<br />
for example</p>
<pre><code>data_clean_pipeline(twitter_data['comment'][0] #it works fine and return the output
</code></pre>
<p>but the error occurs when i apply this function to a column</p>
<pre><code>twitter_data['clean'] = twitter_data['comment'].apply(data_clean_pipeline)  
</code></pre>
<p>Any feedback would be helpful, thank you:)</p>
<p>i have attached an image of my error code in the image description for the python error window</p>
<p><a href=""https://i.sstatic.net/JpUuncF2.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I was expecting that it will apply the function to the entire comment column which is not happening.<br />
I have tried to make multiple unsuccessful attempts</p>
","python, python-3.x, twitter, nlp, project",
How do I combine lists in column of dataframe to a single list,"<p>Some context, I have some data that I'm doing some text analysis on, I have just tokenized them and I want to combine all the lists in the dataframe column for some further processing.</p>
<p>My df is as:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({'title': ['issue regarding app', 'graphics should be better'], 'text': [[&quot;'app'&quot;, &quot;'load'&quot;, &quot;'slowly'&quot;], [&quot;'interface'&quot;, &quot;'need'&quot;, &quot;'to'&quot;, &quot;'look'&quot;, &quot;'nicer'&quot;]]})`
</code></pre>
<p>I want to merge all the lists in the 'text' column into one list, and also remove the open/close inverted commas.</p>
<p>Something like this:</p>
<pre><code>lst = ['app', 'load', 'slowly', 'interface', 'need', 'to', 'look', 'nicer']`
</code></pre>
<p>Thank you for all your help!</p>
","python, pandas, list, dataframe, nlp",
Is n-gram precision the number of elements in the intersection of one hypothesis and possibly many references?,"<p>I was trying to understand how BLEU score works and noticed that if I had to compute the n-gram precisions and have multiple reference sentences, it makes more sense to turn everything into sets to remove duplicates. Order in terms of n-grams does not seem to matter, the order of n-tokens is persisted by n-grams but the order each individial n-gram is irrelevant, so we can indeed just use sets to compute n-gram precision of the hypothesis with respect to many references.</p>
<p>In other words, if I'm given many references and one hypothesis, I compute the n-gram precision by:</p>
<ol>
<li>Forming two sets, one set is the union of all n-gram sets of all references, the other set consists of all n-grams in the hypothesis. Duplicates are eliminated.</li>
<li>Look for the intersection between both sets. The number of n-grams in that intersection divided by the number of n-grams in the hypothesis set is the n-gram precision.</li>
</ol>
<p>I think this idea is also backed up by the <a href=""https://www.nltk.org/_modules/nltk/translate/bleu_score.html"" rel=""nofollow noreferrer"">nltk implementation</a> of BLEU scores.</p>
<pre class=""lang-py prettyprint-override""><code>    # Extracts all ngrams in hypothesis
    # Set an empty Counter if hypothesis is empty.
    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) &gt;= n else Counter()
    
    # Extract a union of references' counts.
    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])
    max_counts = {}
    for reference in references:
        reference_counts = (
            Counter(ngrams(reference, n)) if len(reference) &gt;= n else Counter()
        )
        for ngram in counts:
            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])

    # Assigns the intersection between hypothesis and references' counts.
    clipped_counts = {
        ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()
    }
</code></pre>
<p><strong>Am I misunderstanding something or this a correct way of computing n-gram precision?</strong></p>
","nlp, nltk, bleu",
How to make api.ai agent learn something dynamically?,"<p>I am currently using api.ai , to create agent to perform specific tasks, but one question i don't have answer to is , can i make it learn something while chatting , mean that i speak <strong>my name is 'John Cena'</strong> and she should store it and then whenever i ask her again bot should answer me that. i know there is a way to do it by logging into <strong>api.ai</strong> web and manually add entries , but it will not help, is there any work around programmatically or automatically ?  the file i've been using to practice is given in <a href=""https://github.com/sitepoint-editors/Api-AI-Personal-Assistant-Demo/blob/master/index.html"">github</a> . and here is working <a href=""http://patcat.com/demos/barry/"">DEMO</a></p>
","android, json, nlp, artificial-intelligence, dialogflow-es",
Split each line in a file based on delimitters,"<p>This is the sample data in a file. I want to split each line in the file and add it to a dataframe. In some cases they have more than 1 child. So whenever they have more than one child new set of column have to be added child2 Name and DOB</p>
<pre><code>(P322) Rashmika Chadda 15/05/1995 – Rashmi C 12/02/2024
(P324) Shiva Bhupati 01/01/1994 – Vinitha B 04/08/2024
(P356) Karthikeyan chandrashekar 22/02/1991 – Kanishka P 10/03/2014
(P366) Kalyani Manoj 23/01/1975 - Vandana M 15/05/1995 - Chandana M 18/11/1998 
</code></pre>
<p>This is the code I have tried but this splits only by taking &quot;-&quot; into consideration</p>
<pre><code>with open(&quot;text.txt&quot;) as read_file:
    file_contents = read_file.readlines()
content_list = []
temp = []
for each_line in file_contents:
    temp = each_line.replace(&quot;â€“&quot;, &quot; &quot;).split()

    content_list.append(temp)

print(content_list)
</code></pre>
<p>Current output:</p>
<pre><code>[['(P322)', 'Rashmika', 'Chadda', '15/05/1995', 'Rashmi', 'Chadda', 'Teega', '12/02/2024'], ['(P324)', 'Shiva', 'Bhupati', '01/01/1994', 'Vinitha', 'B', 'Sahu', '04/08/2024'], ['(P356)', 'Karthikeyan', 'chandrashekar', '22/02/1991', 'Kanishka', 'P', '10/03/2014'], ['(P366)', 'Kalyani', 'Manoj', '23/01/1975', '-', 'Vandana', 'M', '15/05/1995', '-', 'Chandana', 'M', '18/11/1998']]

</code></pre>
<p>Final output should be like below</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Code</th>
<th>Parent_Name</th>
<th>DOB</th>
<th>Child1_Name</th>
<th>DOB</th>
<th>Child2_Name</th>
<th>DOB</th>
</tr>
</thead>
<tbody>
<tr>
<td>P322</td>
<td>Rashmika Chadda</td>
<td>15/05/1995</td>
<td>Rashmi C</td>
<td>12/02/2024</td>
<td></td>
<td></td>
</tr>
<tr>
<td>P324</td>
<td>Shiva Bhupati</td>
<td>01/01/1994</td>
<td>Vinitha B</td>
<td>04/08/2024</td>
<td></td>
<td></td>
</tr>
<tr>
<td>P356</td>
<td>Karthikeyan chandrashekar</td>
<td>22/02/1991</td>
<td>Kanishka P</td>
<td>10/03/2014</td>
<td></td>
<td></td>
</tr>
<tr>
<td>P366</td>
<td>Kalyani Manoj</td>
<td>23/01/1975</td>
<td>Vandana M</td>
<td>15/05/1995</td>
<td>Chandana M</td>
<td>18/11/1998</td>
</tr>
</tbody>
</table></div>
","python, split, nlp, delimiter",
Fetch proximity between two languages through Python,"<p>Input 1 - &quot;English&quot;</p>
<p>Input 2 - &quot;French&quot;</p>
<p>Expected Output - 46.9 (Based on this website - <a href=""http://www.elinguistics.net/Compare_Languages.aspx"" rel=""nofollow noreferrer"">http://www.elinguistics.net/Compare_Languages.aspx</a>)</p>
<p>Is there any Python library that supports such a language similarity request? There are plenty of options for checking similarity of two sentences, but what about that of two whole languages.</p>
<p>The website in the question is the best one I could find but I'm unable to use it through HTTP request. Selenium is not an option here either.</p>
","python, nlp",
Getting all leaf words (reverse stemming) into one Python List,"<p>On the same lines as the solution provided <a href=""https://stackoverflow.com/questions/65559962/get-all-leaf-words-for-a-stemmed-keyword"">in this link</a>, I am trying to get all leaf words of one stem word. I am using the community-contributed (@Divyanshu Srivastava) package <code>get_word_forms</code></p>
<p>Imagine I have a shorter sample word list as follows:</p>
<pre><code>my_list = [' jail', ' belief',' board',' target', ' challenge', ' command']
</code></pre>
<p>If I work it manually, I do the following (which is go word-by-word, which is very time-consuming if I have a list of 200 words):</p>
<pre><code>get_word_forms(&quot;command&quot;)
</code></pre>
<p>and get the following output:</p>
<pre><code>{'n': {'command',
  'commandant',
  'commandants',
  'commander',
  'commanders',
  'commandership',
  'commanderships',
  'commandment',
  'commandments',
  'commands'},
 'a': set(),
 'v': {'command', 'commanded', 'commanding', 'commands'},
 'r': set()}
</code></pre>
<p>'n' is noun, 'a' is adjective, 'v' is verb, and 'r' is adverb.</p>
<p>If I try to reverse-stem the entire list in one go:</p>
<pre><code>[get_word_forms(word) for word in sample]
</code></pre>
<p>I fail at getting any output:</p>
<pre><code>[{'n': set(), 'a': set(), 'v': set(), 'r': set()},
 {'n': set(), 'a': set(), 'v': set(), 'r': set()},
 {'n': set(), 'a': set(), 'v': set(), 'r': set()},
 {'n': set(), 'a': set(), 'v': set(), 'r': set()},
 {'n': set(), 'a': set(), 'v': set(), 'r': set()},
 {'n': set(), 'a': set(), 'v': set(), 'r': set()},
 {'n': set(), 'a': set(), 'v': set(), 'r': set()}]
</code></pre>
<p>I think I am failing at saving the output to the dictionary. Eventually, I would like my output to be a list without breaking it down into noun, adjective, adverb, or verb:</p>
<p>something like:</p>
<pre><code>['command','commandant','commandants',  'commander', 'commanders', 'commandership',
'commanderships','commandment', 'commandments', 'commands','commanded', 'commanding', 'commands', 'jail', 'jailer', 'jailers', 'jailor', 'jailors', 'jails', 'jailed', 'jailing'.....] .. and so on. 
</code></pre>
","python, nlp, nltk","<p>One solution using nested list comprehensions after stripping forgotten spaces:</p>
<pre><code>all_words = [setx for word in my_list for setx in get_word_forms(word.strip()).values() if len(setx)]

# Flatten the list of sets
all_words = [word for setx in all_words for word in setx]

# Remove the repetitions and sort the set
all_words = sorted(set(all_words))
print(all_words)

['belief', 'beliefs', 'believabilities', 'believability', 'believable', 'believably', 'believe', 'believed', 'believer', 'believers', 'believes', 'believing', 'board', 'boarded', 'boarder', 'boarders', 'boarding', 'boards', 'challenge', 'challengeable', 'challenged', 'challenger', 'challengers', 'challenges', 'challenging', 'command', 'commandant', 'commandants', 'commanded', 'commander', 'commanders', 'commandership', 'commanderships', 'commanding', 'commandment', 'commandments', 'commands', 'jail', 'jailed', 'jailer', 'jailers', 'jailing', 'jailor', 'jailors', 'jails', 'target', 'targeted', 'targeting', 'targets']
</code></pre>
"
How to split and spelling correct arabic text without spaces into list of words,"<p>I'm looking for a way to split the Arabic text and correct the spelling. Whitespace is the first splitting criterion, then, Maybe based on a dictionary of correct words the splitting should done, considering spelling issues:</p>
<pre><code>تشرابالقطط الحليب =&gt; [تشرب، القطط، الحليب]

مخمديوسف =&gt; [&quot;محمد&quot;, &quot;يوسف&quot;]

انا ارييدان اشراب =&gt; [&quot;انا&quot;, &quot;أريد&quot;, &quot;أن&quot;, &quot;أشرب&quot;]

جملة صحيحة =&gt; [&quot;جملة&quot;, &quot;صحيحة&quot;]
</code></pre>
<p>And if there are multiple correct splitting ways, return them all:</p>
<pre><code>مخمديوسف =&gt; [ [&quot;محمد&quot;, &quot;يوسف&quot;] , [&quot;احمد&quot;, &quot;يوسف&quot;] ]
</code></pre>
<p>If any libraries can do the same. Otherwise, A custom algorithm/code that we can implement?</p>
","algorithm, deep-learning, nlp",
How to label review having both positive and negative sentiment words,"<p>I have used vader library for labeling of amazon's reviews but it doesn't handle these types of reviews &quot;No problems with it and does job well. Using it for Apple TV and works great. I would buy again no problem&quot;. This is positive sentence but the code label it as negative. How can I handle these types of reviews.</p>
<pre><code>import nltk
nltk.download('vader_lexicon')
nltk.download('punkt')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()
output['sentiment'] = output['review_body'].apply(lambda x: sid.polarity_scores(x))
def convert(x):
 if x &lt; 0:
     return &quot;negative&quot;
 elif x &gt; .2:
     return &quot;positive&quot;
 else:
     return &quot;neutral&quot;
output['result'] = output['sentiment'].apply(lambda x:convert(x['compound']))
</code></pre>
<p>Here is the sample file:</p>
<pre><code>0 No problems with it and does job well. Using it for Apple TV and works great. I would buy 
again no problem
1 I don't know what happened with other buyers, but I received it today from Yakodo in a good 
  bubble envelope and the product is not a counterfeit: it's a genuine Apple product. It comes 
  into a little box with two information booklets. The adapter is genuine, I'm completely sure 
  because looks and feels the same compared with one from an Apple reseller. The latest iOS 
  (7.1.2) has no issues with it (no warnings) using it with an iPhone 5S and an iPad mini 
  Retina... No issues because it is genuine. Don't hesitate: it's a great deal for a fraction 
  of its price.
2 Keeps me smiling when listening to the music.
3 DOES THE JOB. HAPPY.
4 Don't like it
</code></pre>
","python-3.x, nlp, sentiment-analysis, review, vader",
LLM Question answer models for yes or no answers,"<p>imagine I have the following dataset:</p>
<pre><code>import pandas as pd

# Positive and negative sentences
positive_sentences = [
    &quot;I love this product!&quot;,
    &quot;The weather is beautiful today.&quot;,
    &quot;The team did an excellent job.&quot;,
    &quot;She is a very talented musician.&quot;
]

negative_sentences = [
    &quot;I am not satisfied with the service.&quot;,
    &quot;The food was terrible at that restaurant.&quot;,
    &quot;The movie was a complete disappointment.&quot;,
    &quot;He made a lot of mistakes in the project.&quot;
]

# Combine positive and negative sentences
sentences = positive_sentences + negative_sentences

# Create a DataFrame with a &quot;snippet&quot; column
df = pd.DataFrame({'snippet': sentences})

# Display the DataFrame
print(df)
</code></pre>
<p>I want to use a LLM model that answers the following question. Is the following sentence positive, negative or neutral?</p>
<p>This is what I have tried so far:</p>
<pre><code>## installing the libraries:
import pandas as pd
from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer
from transformers import RobertaTokenizer
from transformers import AutoTokenizer, RobertaForQuestionAnswering
import torch
from transformers import BertForQuestionAnswering, BertTokenizer

# Setting up the model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

# Create a question answering pipeline
question_answerer = pipeline(&quot;question-answering&quot;, model=model, tokenizer=tokenizer)

for index, row in df.iterrows():
    article = row[&quot;snippet&quot;]  
    prompt = f&quot;Is the following sentence positive, negative or neutral? {article}&quot;

    result = question_answerer(question=prompt, context=article)

    # Check if &quot;answer&quot; key is in the result
    if &quot;answer&quot; in result:
        main_theme = result[&quot;answer&quot;]
        print(f&quot;Article {index+1} main theme: {main_theme}&quot;)
    else:
        print(f&quot;Article {index+1} main theme not found in the result.&quot;)

</code></pre>
","nlp, huggingface-transformers, large-language-model, nlp-question-answering",
torch.nn.functional.softmax giving inaccurate softmax output,"<p>I am trying to implement masked self-attention from scratch but when calculating the softmax for the similarity scores I get odd results. I looked at the documentation and other questions posted on here but I still cant figure out what I am doing wrong. Below is a test I set up with the results.</p>
<p>What I tried:</p>
<pre class=""lang-py prettyprint-override""><code>print(sims)
print(torch.nn.functional.softmax(sims, dim=1))
</code></pre>
<p>Which gives the following output:</p>
<pre class=""lang-py prettyprint-override""><code>tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],
        [ 7.,  8.,  9., 10., 11., 12.],
        [13., 14., 15., 16., 17., 18.],
        [19., 20., 21., 22., 23., 24.],
        [25., 26., 27., 28., 29., 30.],
        [31., 32., 33., 34., 35., 36.],
        [37., 38., 39., 40., 41., 42.],
        [43., 44., 45., 46., 47., 48.],
        [49., 50., 51., 52., 53., 54.],
        [55., 56., 57., 58., 59., 60.]])

tensor([[0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],
        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337]])
</code></pre>
<p>As an example, I am expecting the output of the <code>softmax</code> function on the first row &quot;sims&quot;</p>
<pre><code>[ 1.,  2.,  3.,  4.,  5.,  6.]
</code></pre>
<p>to show</p>
<pre><code>[ .047,  .095,  .142,  .19,  .238,  .285]
</code></pre>
<p>which would be the accurate softmax attention percentages needed to apply to my value tensor</p>
","python, pytorch, nlp, softmax",
Why does my contrastive learning model&#39;s loss and gradients explode during training?,"<p>I am fine-tuning an embedding model using contrastive learning. For the loss function, I’m using <code>torch.nn.CrossEntropyLoss</code>.</p>
<p>The training process initially seems to work fine — the loss decreases steadily on average. However, at some point during training (usually around step 16,000 in this case), the loss and gradients explode. After this, the model is unable to stabilize, and training becomes unusable.</p>
<p>Here is a graph showing the behavior:</p>
<p><a href=""https://i.sstatic.net/o0g7mnA4.png"" rel=""nofollow noreferrer"">Tensorboard loss by step graph</a></p>
<h4>What I have tried so far:</h4>
<ol>
<li><p><strong>Data preprocessing</strong>:</p>
<ul>
<li><p>Removed outliers (e.g., very long texts).</p>
</li>
<li><p>Cleaned and filtered the dataset for consistency.</p>
</li>
</ul>
</li>
<li><p><strong>Hyperparameter tuning</strong>:</p>
<ul>
<li><p>Adjusted the learning rate and tried different values.</p>
</li>
<li><p>Changed the optimizer (e.g., switching from Adam to SGD)</p>
</li>
</ul>
</li>
<li><p><strong>Gradient clipping</strong>:</p>
<ul>
<li>Clipped gradients to a max norm of 1 using <code>torch.nn.utils.clip_grad_norm_</code>.</li>
</ul>
</li>
</ol>
<h4>My setup:</h4>
<ul>
<li><p><strong>Dataset size</strong>: ~14,000 samples</p>
</li>
<li><p><strong>Model architecture</strong>: Transformer-based embedding model</p>
</li>
<li><p><strong>Batch size</strong>: 1 (given my gpu capacity)</p>
</li>
<li><p><strong>Learning rate</strong>: 1e-5</p>
</li>
<li><p><strong>Optimizer</strong>: Adam with weight decay</p>
</li>
</ul>
<p><strong>Training loop (relevant part):</strong></p>
<pre><code>for epoch in range(epochs):
    model.train()
    epoch_loss = 0.0
    for step, batch in enumerate(dataset_train):
        temperature = max(0.1, 0.05 * (1 - step / num_training_steps))

        # Move data to device
        anchor_input_ids = batch[&quot;anchor_input_ids&quot;].to(device)
        anchor_attention_mask = batch[&quot;anchor_attention_mask&quot;].to(device)
        positive_input_ids = batch[&quot;positive_input_ids&quot;].to(device)
        positive_attention_mask = batch[&quot;positive_attention_mask&quot;].to(device)
        negative_input_ids = batch[&quot;negative_input_ids&quot;].to(device)
        negative_attention_mask = batch[&quot;negative_attention_mask&quot;].to(device)

        anchor_input_ids = anchor_input_ids.unsqueeze(0)  # Add a dimension for the batch
        anchor_attention_mask = anchor_attention_mask.unsqueeze(0)  # Add a dimension for the batch
        positive_input_ids = positive_input_ids.unsqueeze(0)  # Add a dimension for the batch
        positive_attention_mask = positive_attention_mask.unsqueeze(0)  # Add a dimension for the batch
        negative_input_ids = negative_input_ids.unsqueeze(0)  # Add a dimension for the batch
        negative_attention_mask = negative_attention_mask.unsqueeze(0)  # Add a dimension for the batch

        optimizer.zero_grad()

        # Generate embeddings
        anchor_embeddings = model.forward(anchor_input_ids, anchor_attention_mask)
        positive_embeddings = model.forward(positive_input_ids, positive_attention_mask)
        negative_embeddings = model.forward(negative_input_ids, negative_attention_mask)

        # Calculate cosine similarities
        pos_sim = cosine_similarity(anchor_embeddings, positive_embeddings)
        neg_sim = cosine_similarity(anchor_embeddings, negative_embeddings)

        # Calculate logits
        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim.unsqueeze(1)], dim=1) / temperature
        labels = torch.zeros(logits.size(0), dtype=torch.long).to(device)  # The positive class is always the first

        # Calculate InfoNCE loss
        loss = torch.nn.CrossEntropyLoss()(logits, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
</code></pre>
<p><strong>Dataset generation:</strong></p>
<pre><code>import torch
from torch.utils.data import Dataset
import random

class FineTuneContrastiveDataset(Dataset):
    def __init__(self, pairs_data, df_1, df_2, tokenizer, max_tokens=512):
        &quot;&quot;&quot;
        pairs_data: List of tuples (id_1, id_2, label).
        df_1: DataFrame containing text data associated with id_1.
        df_2: DataFrame containing text data associated with id_2.
        tokenizer: Hugging Face tokenizer.
        max_tokens: Maximum allowed length for the tokenized text.
        &quot;&quot;&quot;
        self.pairs_data = pairs_data
        self.df_1 = df_1.set_index(&quot;id_1&quot;)
        self.df_2 = df_2.set_index(&quot;id_2&quot;)
        self.tokenizer = tokenizer
        self.max_tokens = max_tokens
        self.id_2_list = list(self.df_2.index)  # For selecting negative samples

    def __len__(self):
        return len(self.pairs_data)

    def __getitem__(self, idx):
        # Retrieve data from the pair
        id_1, id_2_positive, label = self.pairs_data[idx]
        
        # Text associated with id_1 (anchor)
        text_1 = &quot; &quot;.join(self.df_1.loc[id_1][&quot;chunks&quot;])

        # Positive text associated with id_2
        text_2_positive = &quot; &quot;.join(self.df_2.loc[id_2_positive][&quot;chunks&quot;])

        # Generate a negative sample from id_2
        id_2_negative = random.choice(
            [candidate_id for candidate_id in self.id_2_list if candidate_id != id_2_positive]
        )
        text_2_negative = &quot; &quot;.join(self.df_2.loc[id_2_negative][&quot;chunks&quot;])

        # Tokenize inputs
        inputs_anchor = self.tokenizer(
            text_1, truncation=True, max_length=self.max_tokens, 
            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;
        )
        inputs_positive = self.tokenizer(
            text_2_positive, truncation=True, max_length=self.max_tokens, 
            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;
        )
        inputs_negative = self.tokenizer(
            text_2_negative, truncation=True, max_length=self.max_tokens, 
            padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;
        )

        return {
            &quot;anchor_input_ids&quot;: inputs_anchor[&quot;input_ids&quot;].squeeze(0),
            &quot;anchor_attention_mask&quot;: inputs_anchor[&quot;attention_mask&quot;].squeeze(0),
            &quot;positive_input_ids&quot;: inputs_positive[&quot;input_ids&quot;].squeeze(0),
            &quot;positive_attention_mask&quot;: inputs_positive[&quot;attention_mask&quot;].squeeze(0),
            &quot;negative_input_ids&quot;: inputs_negative[&quot;input_ids&quot;].squeeze(0),
            &quot;negative_attention_mask&quot;: inputs_negative[&quot;attention_mask&quot;].squeeze(0),
            &quot;label&quot;: torch.tensor(label, dtype=torch.float),
            &quot;id_1&quot;: id_1,
        }
</code></pre>
","deep-learning, pytorch, nlp, word-embedding",
Finetuning LLaMa with Lora - bf16 errors on A100 GPU on Colab,"<p>I am attempting to fine-tune Llama3.2-1b model from huggingface on colab using A100 gpu, following <a href=""https://medium.com/@harsh.vardhan7695/fine-tuning-llama-2-using-lora-and-qlora-a-comprehensive-guide-fd2260f0aa5f"" rel=""nofollow noreferrer"">this guide</a>.</p>
<p>My understanding is that the A100 gpu supports bf16, but when attempting to finetune, it throws the error that</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-23-e92ea0b437bd&gt; in &lt;cell line: 9&gt;()
      8 # Generate output (inference)
      9 with torch.no_grad():  # Disable gradient calculation for inference
---&gt; 10     output = model.generate(
     11         inputs[&quot;input_ids&quot;],
     12         num_return_sequences=1,  # Only generate one output

8 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)
    123 
    124     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 125         return F.linear(input, self.weight, self.bias)
    126 
    127     def extra_repr(self) -&gt; str:

RuntimeError: expected scalar type BFloat16 but found Float
</code></pre>
<p>My code is as in the article with some tweaks:</p>
<pre class=""lang-py prettyprint-override""><code>import os
import sys


import torch
from datasets import load_dataset
from transformers import ( 
    AutoTokenizer, 
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
    
    )
from peft import LoraConfig, PeftModel
from trl import SFTTrainer
from datasets import DatasetDict

torch.cuda.empty_cache()

def create_llama_prompt(system_prompt: str, user_message: str, assistant_message:str = None) -&gt; str:
    prompt = f&quot;&quot;&quot;
    &lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;
    {system_prompt}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;

    {user_message}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
    &quot;&quot;&quot;

    if assistant_message:
        prompt += f&quot;{assistant_message}&lt;|eot_id|&gt;&quot;

    return prompt

model_name = &quot;meta-llama/Llama-3.2-1B&quot;
dataset_name = &quot;therapara/summary-of-news-articles_new&quot;
new_model = &quot;./models/llama-3.2-1B-summarisation&quot;

lora_r = 64         #lora attention dimension/ rank
lora_alpha = 16     #lora scaling parameter
lora_dropout = 0.1  #lora dropout probability

use_4bit = True
bnb_4bit_compute_dtype = &quot;float16&quot;
bnb_4bit_quant_type = &quot;nf4&quot;
use_nested_quant = False

output_dir = &quot;./results&quot;

num_train_epochs = 3

#enable fp16/bf16 training (set bf16 to True when using A100 GPU in google colab)
fp16 = False
bf16 = True

#training params here

device_map = {&quot;&quot;:0}
dataset = load_dataset(dataset_name)

#load tokenizer and model with QLoRA config
compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit = use_4bit,
    bnb_4bit_quant_type = bnb_4bit_quant_type,
    bnb_4bit_compute_dtype = compute_dtype,
    bnb_4bit_use_double_quant = use_nested_quant,)

#load base model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config = bnb_config,
    device_map = device_map,
)

model.config.use_cache = False
model.config.pretraining_tp = 1

#Load LLama tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code = True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;


sys_prompt = &quot;&quot;&quot;You are an expert in summarizing news articles. Your task is to generate clear, concise, 
and accurate summaries of provided news articles. Only summarize the content that is presented and do not 
add additional information. If the article lacks context or cannot be summarized effectively, state this 
clearly. Maintain neutrality and focus on the core facts. Your summaries should be succinct and avoid unnecessary details.
&quot;&quot;&quot;

def preprocess_function(examples):
    formatted_texts = []
    for article, summarisation in zip(examples[&quot;article&quot;], examples[&quot;highlights&quot;]):
        formatted_text = create_llama_prompt(system_prompt=sys_prompt, user_message=article, assistant_message=summarisation)
        formatted_texts.append(formatted_text)
    return {&quot;formatted_text&quot;: formatted_texts}

# Apply preprocessing to the dataset
formatted_dataset = dataset.map(preprocess_function, batched=True, num_proc=8)

def tokenize_function(examples):
    return tokenizer(examples[&quot;formatted_text&quot;], truncation=True, padding=&quot;max_length&quot;, max_length=max_seq_length)


# tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True, num_proc=8, remove_columns=[&quot;article&quot;, &quot;highlights&quot;, &quot;id&quot;])
tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True, num_proc=8)

print(tokenized_dataset)

#Load QLoRA config
peft_config = LoraConfig(
    lora_alpha = lora_alpha,
    lora_dropout = lora_dropout,
    r  = lora_r,
    bias = &quot;none&quot;,
    task_type = &quot;CAUSAL_LM&quot;,
)

#Set Training parameters
training_arguments = TrainingArguments(
    output_dir = output_dir,
    num_train_epochs = num_train_epochs,
    per_device_train_batch_size = per_device_train_batch_size,
    gradient_accumulation_steps = gradient_accumulation_steps,
    optim = optim,
    save_steps = save_steps,
    logging_steps = logging_steps,
    learning_rate = learning_rate,
    fp16 = fp16,
    bf16 = bf16,
    max_grad_norm = max_grad_norm,
    weight_decay = weight_decay,
    lr_scheduler_type = lr_scheduler_type,
    warmup_ratio = warmup_ratio,
    group_by_length = group_by_length,
    max_steps = max_steps,
    report_to = &quot;tensorboard&quot;,
)
print(&quot;Set Training Args&quot;)

#SFT Trainer
trainer = SFTTrainer(
    model = model,
    train_dataset = tokenized_dataset[&quot;train&quot;],
    eval_dataset = tokenized_dataset[&quot;validation&quot;],
    peft_config = peft_config,
    # dataset_text_field = &quot;formatted_text&quot;,
    # max_seq_length = max_seq_length,
    args = training_arguments,
    tokenizer = tokenizer,
    # packing = packing,
)
print(&quot;Create SFTTrainer&quot;)

print(&quot;Starting Training&quot;)
with torch.autocast(&quot;cuda&quot;): 
  trainer.train()
print(&quot;Training Ended&quot;)

#save trained model
trainer.model.save_pretrained(new_model)
print(&quot;Saved pretrained model&quot;)

# Ignore warnings
logging.set_verbosity(logging.CRITICAL)

# Run text generation pipeline with our next model
instance_0 = tokenized_dataset[&quot;test&quot;][0]
prompt = create_llama_prompt(sys_prompt, instance_0[&quot;article&quot;])
print(prompt)

pipe = pipeline(task=&quot;text-generation&quot;, model=model, tokenizer=tokenizer, max_new_tokens=500)
result = pipe(prompt)

print(result[0]['generated_text'])

print(&quot;Merging model with LoRA weights&quot;)
# Reload model in FP16 and merge it with LoRA weights
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map=device_map,
)
model = PeftModel.from_pretrained(base_model, new_model)
model = model.merge_and_unload()

# Reload tokenizer to save it
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;
</code></pre>
<p>I have attempted to load the models in bf16 with</p>
<pre class=""lang-py prettyprint-override""><code>model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config = bnb_config,
    torch_dtype = torch.bfloat16,
    device_map = device_map,
)

...

base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.bfloat16,
    device_map=device_map,
)
</code></pre>
","nlp, llama, fine-tuning",
How to apply semantic tokenize on sentence in java by NLP?,"<p>Can an NLP model be used to tokenize a sentence based on its semantic meaning?</p>
<p>For example,
for the sentence: <strong>If the driver's age is more than 20</strong>,
the tokens would be:</p>
<p>Token1: if</p>
<p>Token2: driver age</p>
<p>Token3: more than</p>
<p>Token4: 20</p>
","java, nlp, stanford-nlp",
How to install spacy?,"<p>I am using trying to install spacy library using 'pip install -U spacy' in the command prompt (run as admin) in Windows-11 O.S., but it shows some error I don't understand. I am using Python 3.13.0, gcc 13.2.0 and make 4.4.1. What could be the problem? Or is there any other way to install spacy?</p>
<pre class=""lang-none prettyprint-override""><code>C:\&gt;pip install -U spacy
Collecting spacy
  Using cached spacy-3.8.2.tar.gz (1.3 MB)
  Installing build dependencies ... error
  error: subprocess-exited-with-error

  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─&gt; [113 lines of output]
      Ignoring numpy: markers 'python_version &lt; &quot;3.9&quot;' don't match your environment
      Collecting setuptools
        Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)
      Collecting cython&lt;3.0,&gt;=0.25
        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)
      Collecting cymem&lt;2.1.0,&gt;=2.0.2
        Using cached cymem-2.0.10-cp313-cp313-win_amd64.whl.metadata (8.6 kB)
      Collecting preshed&lt;3.1.0,&gt;=3.0.2
        Using cached preshed-3.0.9.tar.gz (14 kB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'done'
        Getting requirements to build wheel: started
        Getting requirements to build wheel: finished with status 'done'
        Preparing metadata (pyproject.toml): started
        Preparing metadata (pyproject.toml): finished with status 'done'
      Collecting murmurhash&lt;1.1.0,&gt;=0.28.0
        Using cached murmurhash-1.0.11-cp313-cp313-win_amd64.whl.metadata (2.0 kB)
      Collecting thinc&lt;8.4.0,&gt;=8.3.0
        Using cached thinc-8.3.2.tar.gz (193 kB)
        Installing build dependencies: started
        Installing build dependencies: still running...
        Installing build dependencies: finished with status 'error'
        error: subprocess-exited-with-error

        pip subprocess to install build dependencies did not run successfully.
        exit code: 1

        [74 lines of output]
        Ignoring numpy: markers 'python_version &lt; &quot;3.9&quot;' don't match your environment
        Collecting setuptools
          Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)
        Collecting cython&lt;3.0,&gt;=0.25
          Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)
        Collecting murmurhash&lt;1.1.0,&gt;=1.0.2
          Using cached murmurhash-1.0.11-cp313-cp313-win_amd64.whl.metadata (2.0 kB)
        Collecting cymem&lt;2.1.0,&gt;=2.0.2
          Using cached cymem-2.0.10-cp313-cp313-win_amd64.whl.metadata (8.6 kB)
        Collecting preshed&lt;3.1.0,&gt;=3.0.2
          Using cached preshed-3.0.9.tar.gz (14 kB)
          Installing build dependencies: started
          Installing build dependencies: finished with status 'done'
          Getting requirements to build wheel: started
          Getting requirements to build wheel: finished with status 'done'
          Preparing metadata (pyproject.toml): started
          Preparing metadata (pyproject.toml): finished with status 'done'
        Collecting blis&lt;1.1.0,&gt;=1.0.0
          Using cached blis-1.0.1.tar.gz (3.6 MB)
          Installing build dependencies: started
          Installing build dependencies: finished with status 'done'
          Getting requirements to build wheel: started
          Getting requirements to build wheel: finished with status 'done'
          Preparing metadata (pyproject.toml): started
          Preparing metadata (pyproject.toml): finished with status 'done'
        Collecting numpy&lt;2.1.0,&gt;=2.0.0
          Using cached numpy-2.0.2.tar.gz (18.9 MB)
          Installing build dependencies: started
          Installing build dependencies: finished with status 'done'
          Getting requirements to build wheel: started
          Getting requirements to build wheel: finished with status 'done'
          Installing backend dependencies: started
          Installing backend dependencies: finished with status 'done'
          Preparing metadata (pyproject.toml): started
          Preparing metadata (pyproject.toml): finished with status 'error'
          error: subprocess-exited-with-error

          Preparing metadata (pyproject.toml) did not run successfully.
          exit code: 1

          [22 lines of output]
          + C:\Users\rohan\AppData\Local\Programs\Python\Python313\python.exe C:\Users\rohan\AppData\Local\Temp\pip-install-s6zj7q4q\numpy_fe36df85b8944a7fb67f6135b78a4bde\vendored-meson\meson\meson.py setup C:\Users\rohan\AppData\Local\Temp\pip-install-s6zj7q4q\numpy_fe36df85b8944a7fb67f6135b78a4bde C:\Users\rohan\AppData\Local\Temp\pip-install-s6zj7q4q\numpy_fe36df85b8944a7fb67f6135b78a4bde\.mesonpy-c4lb8p4h -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\Users\rohan\AppData\Local\Temp\pip-install-s6zj7q4q\numpy_fe36df85b8944a7fb67f6135b78a4bde\.mesonpy-c4lb8p4h\meson-python-native-file.ini
          The Meson build system
          Version: 1.4.99
          Source dir: C:\Users\rohan\AppData\Local\Temp\pip-install-s6zj7q4q\numpy_fe36df85b8944a7fb67f6135b78a4bde
          Build dir: C:\Users\rohan\AppData\Local\Temp\pip-install-s6zj7q4q\numpy_fe36df85b8944a7fb67f6135b78a4bde\.mesonpy-c4lb8p4h
          Build type: native build
          Project name: NumPy
          Project version: 2.0.2
          C compiler for the host machine: gcc (gcc 13.2.0 &quot;gcc (GCC) 13.2.0&quot;)
          C linker for the host machine: gcc ld.bfd 2.41
          C++ compiler for the host machine: c++ (gcc 6.3.0 &quot;c++ (MinGW.org GCC-6.3.0-1) 6.3.0&quot;)
          C++ linker for the host machine: c++ ld.bfd 2.28
          Cython compiler for the host machine: cython (cython 3.0.11)
          Host machine cpu family: x86
          Host machine cpu: x86
          Program python found: YES (C:\Users\rohan\AppData\Local\Programs\Python\Python313\python.exe)
          Need python for x86, but found x86_64
          Run-time dependency python found: NO (tried sysconfig)

          ..\meson.build:41:12: ERROR: Python dependency not found

          A full log can be found at C:\Users\rohan\AppData\Local\Temp\pip-install-s6zj7q4q\numpy_fe36df85b8944a7fb67f6135b78a4bde\.mesonpy-c4lb8p4h\meson-logs\meson-log.txt
          [end of output]

          note: This error originates from a subprocess, and is likely not a problem with pip.
        error: metadata-generation-failed

        Encountered error while generating package metadata.

        See above for output.

        note: This is an issue with the package mentioned above, not pip.
        hint: See above for details.
        [end of output]

        note: This error originates from a subprocess, and is likely not a problem with pip.
      error: subprocess-exited-with-error

      pip subprocess to install build dependencies did not run successfully.
      exit code: 1

      See above for output.

      note: This error originates from a subprocess, and is likely not a problem with pip.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─&gt; See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
</code></pre>
","python, pip, nlp, spacy",
What&#39;s the major difference between glove and word2vec?,"<p>What is the difference between word2vec and glove? 
Are both the ways to train a word embedding? if yes then how can we use both?</p>
","machine-learning, nlp, word2vec, word-embedding, glove","<p>Yes, they're both ways to train a word embedding. They both provide the same core output: one vector per word, with the vectors in a useful arrangement. That is, the vectors' relative distances/directions roughly correspond with human ideas of overall word relatedness, and even relatedness along certain salient semantic dimensions.</p>
<p>Word2Vec does incremental, 'sparse' training of a neural network, by repeatedly iterating over a training corpus.</p>
<p>GloVe works to fit vectors to model a giant word co-occurrence matrix built from the corpus.</p>
<p>Working from the same corpus, creating word-vectors of the same dimensionality, and devoting the same attention to meta-optimizations, the quality of their resulting word-vectors will be roughly similar. (When I've seen someone confidently claim one or the other is definitely better, they've often compared some tweaked/best-case use of one algorithm against some rough/arbitrary defaults of the other.)</p>
<p>I'm more familiar with Word2Vec, and my impression is that Word2Vec's training better scales to larger vocabularies, and has more tweakable settings that, if you have the time, might allow tuning your own trained word-vectors more to your specific application. (For example, using a small-versus-large <code>window</code> parameter can have a strong effect on whether a word's nearest-neighbors are 'drop-in replacement words' or more generally words-used-in-the-same-topics. Different downstream applications may prefer word-vectors that skew one way or the other.)</p>
<p>Conversely, some proponents of GLoVe tout that it does fairly well without needing metaparameter optimization.</p>
<p>You probably wouldn't use both, unless comparing them against each other, because they play the same role for any downstream applications of word-vectors.</p>
"
Inspect all probabilities of BERTopic model,"<p>Say I build a BERTopic model using</p>
<pre><code>from bertopic import BERTopic
topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20)
topics, probs = topic_model.fit_transform(docs)
</code></pre>
<p>Inspecting <code>probs</code> gives me just a single value for each item in <code>docs</code>.</p>
<pre><code>probs
array([0.51914467, 0.        , 0.        , ..., 1.        , 1.        ,
       1.        ])
</code></pre>
<p>I would like the entire probability vector across all topics (so in this case, where <code>nr_topics=20</code>, I want a vector of 20 probabilities for each item in <code>docs</code>). In other words, if I have N items in <code>docs</code> and K topics, I would like an NxK output.</p>
","python, nlp, topic-modeling","<p>For individual topic probability across each document you need to add one more argument.</p>
<pre><code>topic_model = BERTopic(n_gram_range=(1, 1), nr_topics=20, calculate_probabilities=True)
</code></pre>
<p>Note: This calculate_probabilities = True will only work if you are using <strong><code>HDBSCAN</code></strong> clustering embedding model. And Bertopic by default uses <code>all-MiniLM-L6-v2</code>.</p>
<p><strong>Official documentation:</strong> <a href=""https://maartengr.github.io/BERTopic/api/bertopic.html"" rel=""nofollow noreferrer"">https://maartengr.github.io/BERTopic/api/bertopic.html</a></p>
<p>They have mentioned the same in document as well.</p>
"
How to make an AI bot of Natural Language Processing?,"<p>I want to make AI bot which can understand only 4 words &quot;Up&quot;, &quot;Down&quot;, &quot;Left&quot;, &quot;Right&quot;.</p>
<p>as my friend make a python script which executes some task by the voice like to open youtube just say &quot;Youtube&quot; and Chrome browser will open with youtube.com URL. But the system was slow as they were using google assistant/ai to process the voice which makes me feel impatient.</p>
<p>Then I got an idea what if an AI system offline which Understand only a few words and we can get some desired result and will be super fast.</p>
<p>for example:- I have a remote control car I want to make voice-activated as when I say &quot;Up&quot; car should move forward, similarly for &quot;Down&quot; -&gt; Backward, &quot;Left&quot; -&gt; Left and &quot;Right&quot; -&gt; Right &amp; &quot;{Any other voice}&quot; -&gt; blink the led to tell that the system didn't understand</p>
<p>so:</p>
<p>how should i start?<br>
how should i train the AI Bot?<br>
what should be my requirements?<br>
and other thing that i should know.</p>
","python, machine-learning, nlp, artificial-intelligence, opennlp","<p>how should you start:  reading ;-) Or I would recommend to take the coursera course on deep neural network. Your question is extremely generic. </p>

<p>An ad-hoc approach -- which should work on your problem -- could be to extract the audio spectrum from samples which are long enough to contain your words but not much longer. With these information you can train a convolutional neural network -- I would try 1d convolution first. </p>
"
Any tutorial for developing spoken dialogue system in prolog?,"<p>I would like to implement a human-machine <em><strong>spoken dialogue system</strong></em> in a context of bar, which takes as input a text phrase from the user (a request for a given product: chips, cola, water, coffee, etc.) and return as output the corresponding response (Beyond the action, but this doesn't matter).</p>
<p>An example of conversation I would like to reach:</p>
<pre><code>U: Hi, how are you?

S: Hey there. I'm good. How may I help you?

U: I would like to have a coffee and a Croissant with jam&quot;

S: Sure. Here your order!

U: Thank you! bye.

S: Thank to you, too. Have a good day!
</code></pre>
<p>That is, rigth now I would just a simple dialogue between them about order at bars.</p>
<p>Anyone know some tutorial about how to implement a dialogue system, preferably in Prolog?</p>
<p>Anyway, any other language is good for me to at least understand how such systems are implemented.</p>
<p>I searched on Internet, but I found a lot of material that has confused me and none of them makes an example of how to implement a dialogue system.</p>
<p>Finally I just want it to be as a software on my PC (not a web application or similar).</p>
","prolog, nlp, artificial-intelligence",
How do I correct for quivering artifacts in XTTS_v2?,"<p>This is a piece of text-to-speech I have done using XTTS_v2: <a href=""https://whyp.it/tracks/237949/xtts-v2-sample?token=poO68"" rel=""nofollow noreferrer"">you can find it here</a>. The sentence is &quot;It brings Vedanta and other spiritual philosophies to common man.&quot;.</p>
<p>My configuration for the XTTS was:</p>
<pre><code>def process_sentence(self, sentence, idx):
    print(f&quot;Processing index {idx}...&quot;, flush=True)
    audio_file = f&quot;temp_{idx}.wav&quot;
    self.model.tts_to_file(text=sentence,
    file_path=audio_file,
    language=self.language,
    speed=0.9,
    speaker=&quot;Ana Florence&quot;,
    emotion=&quot;Happy&quot;)
</code></pre>
<p>Can anybody help me out with the crack in sound when the speaker says &quot;Vedanta&quot;? How can I fix it? I am a noob to TTS and I was hoping to get some help regarding this.</p>
","nlp, text-to-speech",
Benepar for syntactic segmentation,"<p>I want to use Benepar with a French model to do a syntactic segmentation.</p>
<p>I followed the tutorial but I have always have this error</p>
<blockquote>
<p>RuntimeError: Error(s) in loading state_dict for ChartParser:
Unexpected key(s) in state_dict: &quot;pretrained_model.embeddings.position_ids&quot;.</p>
</blockquote>
<p>I tried to do the following:</p>
<pre><code>import benepar
benepar.download('benepar_fr2')
import spacy

nlp = spacy.load('fr_core_news_sm')
parser = benepar.Parser(&quot;benepar_fr2&quot;)

def parse_sentence(text):
    doc = nlp(text)
    parsed_sents = [parser.parse(sent.text) for sent in doc.sents]
    return parsed_sents

sentence = &quot;Le chat mange une souris.&quot;
parsed_sentence = parse_sentence(sentence)
print(parsed_sentence)
</code></pre>
","text, nlp, text-segmentation, french, benepar",
catelog sentences into 5 words that represent them,"<p>I have dataframe with 1000 text rows. <code>df['text']</code></p>
<p>I also have 5 words that I want to know for each one of them how much they represnt the text  (between 0 to 1)</p>
<p>every score will be in <code>df[&quot;word1&quot;]</code> ,<code>df[&quot;word2&quot;]</code> and etc</p>
<p>I will glad for recomendations how to do that</p>
<p><strong>edit</strong></p>
<p>represnt = the semantic distance between the word to the text.</p>
<p>for example -
lets say in row 1 the text is &quot;i want to eat&quot;
and I have 2 words : food and house.</p>
<p>so in <code>df[&quot;food &quot;]</code> it would be higher score than in <code>df[&quot;house&quot;]</code></p>
","python, pandas, nlp, text-mining, similarity","<p>You could use a pre-trained sentence transformer model from <a href=""https://pypi.org/project/sentence-transformers/"" rel=""nofollow noreferrer""><code>sentence_transformers</code></a>:</p>
<pre><code>import pandas as pd
from sentence_transformers import SentenceTransformer, util


class SemanticSimilarityCalculator:
  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -&gt; None:
    self.model = SentenceTransformer(model_name)
    self.word_embeddings = None

  def encode_words(self, words: list[str]) -&gt; None:
    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)
    self.words = words

  def calculate_similarity(self, text: str) -&gt; list[float]:
    if self.word_embeddings is None:
      raise ValueError('Words must be encoded before calculating similarity.')
    text_embedding = self.model.encode(text, convert_to_tensor=True)
    similarities = util.cos_sim(text_embedding, self.word_embeddings)[
      0
    ].tolist()
    return similarities

  def add_similarity_scores_to_df(
    self, df: pd.DataFrame, text_column: str
  ) -&gt; pd.DataFrame:
    if self.words is None:
      raise ValueError(
        'Words must be encoded before adding scores to the DataFrame.'
      )
    similarity_columns = ['word_' + word for word in self.words]
    df[similarity_columns] = df[text_column].apply(
      lambda text: pd.Series(self.calculate_similarity(text))
    )
    return df


def main():
  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}
  df = pd.DataFrame(data)
  words = ['food', 'house', 'sleep', 'drink', 'run']
  calculator = SemanticSimilarityCalculator()
  calculator.encode_words(words)
  df_with_scores = calculator.add_similarity_scores_to_df(
    df, text_column='text'
  )
  print(df_with_scores)


if __name__ == '__main__':
  main()
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>               text  word_food  word_house  word_sleep  word_drink  word_run
0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350
1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716
2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838
</code></pre>
"
Determining most popular words in the English dictionary within a dictionary of words,"<p>Forgive me if my wording is awful, but I'm trying to figure out how to determine the most used words in the English language from a set of words in a dictionary I've made. I've done some research on NLTK but can't seem to find a function within it (or any other library for that matter) that will help me do what I need to do.</p>
<p>For example:
A sentence &quot;I enjoy a cold glass of water on a hot day&quot; would return &quot;water&quot; because it's the most used word in day to day conversation from the sentence. Essentially I need a returned value of the most frequently used word in conversations.</p>
<p>I figure I'll likely have to involve AI, but any time I've tried to use AI I wind up copy and pasting code because I just don't understand it, so I'm trying to avoid going that route</p>
<p>Any and all help is welcome and appreciated.</p>
<p>For context, I decided to start a project that would essentially guess a predetermined word based on characters the user says it has and doesn't have from the computers guess.</p>
","python, nlp, nltk, detection","<p>You need a external dataset for this task. You can try dataset such as google n gram dataset.</p>
<p>Here is the breakdown of the problem statement:</p>
<ol>
<li>Input: &quot;I enjoy a cold glass of water on a hot day&quot;. <code>Output</code>: &quot;water&quot;.</li>
<li>Split the sentences into words list.</li>
</ol>
<blockquote>
<p>Example: [&quot;I&quot;, &quot;enjoy&quot;, &quot;a&quot;, &quot;cold&quot;, &quot;glass&quot;, &quot;of&quot;, &quot;water&quot;, &quot;on&quot;,
&quot;a&quot;, &quot;hot&quot;, &quot;day&quot;]</p>
</blockquote>
<ol start=""3"">
<li>First loop in through all the word of the sentences. so let say you are at first word &quot;I&quot;.</li>
<li>Now you will look the same word &quot;I&quot; in external dataset and will look for the frequency of that word.
Let say the word &quot;I&quot; in external dataset is repeated <code>5000000</code> times</li>
<li>Repeat this task for all the word.</li>
<li>Now you will have a dictionary where each word of the sentence is key and value is frequency of that word that you will get from external data.
Frequency in the below example is random value not exact value.</li>
</ol>
<blockquote>
<pre><code>{
    &quot;I&quot;: 5000000,
    &quot;enjoy&quot;: 50000,
    &quot;a&quot;: 10000000,
    &quot;cold&quot;: 30000,
    &quot;glass&quot;: 100000,
    &quot;of&quot;: 8000000,
    &quot;water&quot;: 1200000,
    &quot;on&quot;: 6000000,
    &quot;hot&quot;: 700000,
    &quot;day&quot;: 400000
}
</code></pre>
</blockquote>
<ol start=""7"">
<li>Pick the word with highest frequency.</li>
</ol>
<p>Note: You can try any big corpus as external data. using big corpus will have most of the English word which is used in conversation. And even if the frequency is not mentioned then you can create that yourself</p>
"
How to save checkpoints for thie transformer gpt2 to continue training?,"<p>I am retraining the GPT2 language model, and am following this blog :</p>
<p><a href=""https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171"" rel=""nofollow noreferrer"">https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171</a></p>
<p>Here, they have trained a network on GPT2, and I am trying to recreate a same. However, my dataset is too large(250Mb), so I want to continue training in intervals. In other words, I want to checkpoint the model training. How could I do this?</p>
","tensorflow, nlp, gpt-2","<pre><code>training_args = TrainingArguments(
    output_dir=model_checkpoint,
    # other hyper-params
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_set,
    eval_dataset=dev_set,
    tokenizer=tokenizer
)

trainer.train()
# Save the model to model_dir
trainer.save_model()

def prepare_model(tokenizer, model_name_path):
    model = AutoModelForCausalLM.from_pretrained(model_name_path)
    model.resize_token_embeddings(len(tokenizer))
    return model

# Assume tokenizer is defined, You can simply pass the saved model directory path.
model = prepare_model(tokenizer, model_checkpoint)
</code></pre>
"
How to correctly identify entity types for tokens using spaCy using python?,"<p>I'm using spaCy to extract and identify entity types (like ORG, GPE, DATE, etc.) from a text description. However, I am noticing some incorrect results, and I'm unsure how to fix this.</p>
<p>Here is the code I am using:</p>
<pre><code>import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)

def getPayeeName(description):
    description = description.replace(&quot;-&quot;, &quot; &quot;).replace(&quot;/&quot;, &quot; &quot;).strip()
    doc = nlp(description)

    for token in doc:
        print(f&quot;Token: {token.text}, Entity: {token.ent_type_ if token.ent_type_ else 'None'}&quot;)

# Example input
description = &quot;UPI DR 400874707203 BENGALORE 08 JAN 2024 14:38:56 MEDICAL LTD HDFC 50200&quot;
getPayeeName(description)
</code></pre>
<p>Token: UPI, Entity: ORG</p>
<p>Token: DR, Entity: ORG</p>
<p>Token: 400874707203, Entity: None</p>
<p>Token: BENGALORE, Entity: None</p>
<p>Token: 08, Entity: DATE</p>
<p>Token: JAN, Entity: DATE</p>
<p>Token: 2024, Entity: DATE</p>
<p>Token: 14:38:56, Entity: None</p>
<p>Token: MEDICAL, Entity: ORG</p>
<p>Token: LTD, Entity: ORG</p>
<p>Token: HDFC, Entity: ORG</p>
<p>Token: 50200, Entity: ORG</p>
<ul>
<li><p>50200 is identified as ORG, but it is just a number.</p>
</li>
<li><p>BENGALORE is a city, but it is not recognized as a GPE or location
(returns None).</p>
</li>
<li><p>UPI and DR are acronyms/abbreviations, but they are incorrectly
identified as ORG.</p>
</li>
</ul>
<p>I want the entity recognition to be more accurate and reliable.
How can I fix these issues? Are there additional spaCy configurations, custom rules, or pre-trained models I should use to improve the entity recognition?</p>
<p>Note: I tried ChatGPT as well, but still this issue is not solved.</p>
","python, machine-learning, nlp, spacy",
Bert model splits words by its own,"<p>I am tokenizing the input words using bert model.
The code is :</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower_case = False)
model = BertModel.from_pretrained(&quot;bert-base-multilingual-cased&quot;, add_pooling_layer=False, output_hidden_states=True, output_attentions=True)

marked_text =  text + &quot; [SEP]&quot;
    tokenized_text = tokenizer.tokenize(marked_text)
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    print(tokenized_text)
    print(indexed_tokens)
</code></pre>
<p>The model I used is from HuggingFace.</p>
<p>My goal is to print the embedded vectors of all words Bert model has, so I searched and found that this model has 119296 tokens available.</p>
<p>I don't know this number of the tokens is reason, but the model splits the words by its own, which is unwanted for me.</p>
<p>for example,</p>
<pre><code>
only -&gt; [only]
ONLY -&gt; [ON,L,Y]

stradivarius -&gt; ['St', '##radi', '##vari', '##us']
</code></pre>
<p>Is this natural Bert thing or I am doing something wrong ?</p>
","python, nlp, huggingface-transformers, bert-language-model","<p>You are not doing anything wrong. Bert uses a so-called <a href=""https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece"" rel=""nofollow noreferrer"">wordpiece</a> subword tokenizer as a compromise for meaningful embeddings and acceptable memory consumption between a character-level (small vocabulary) and a word-level tokenizer (large vocabulary).</p>
<p>A common approach to retrieve word embeddings from a subword-based model is to take the mean of the respective tokens. The code below shows you have you can retrieve the word embeddings (non-contextualized and contextualized) by taking the mean. It uses a fasttokenizer to utilize the methods of the <a href=""https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding"" rel=""nofollow noreferrer"">BatchEncoding</a> object.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BertTokenizerFast, BertModel

t = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')
# whole model
m = BertModel.from_pretrained(&quot;bert-base-multilingual-cased&quot;)
# token embedding layer
embedding_layer = m.embeddings.word_embeddings

sample_sentence = 'This is an example with token-embeddings and word-embeddings'
encoded = t([sample_sentence])
# The BatchEncoding object allows us to map the token back to the string indices
print(*[(token_id, encoded.token_to_chars(idx)) for idx, token_id in enumerate(encoded.input_ids[0])], sep=&quot;\n&quot;)
# And we can also check the mapping of word to token indices
print(*[(word, encoded.word_to_tokens(idx)) for idx, word in enumerate(sample_sentence.split())], sep=&quot;\n&quot;)
</code></pre>
<p>Output:</p>
<pre><code>(101, None)
(10747, CharSpan(start=0, end=4))
(10124, CharSpan(start=5, end=7))
(10151, CharSpan(start=8, end=10))
(14351, CharSpan(start=11, end=18))
(10169, CharSpan(start=19, end=23))
(18436, CharSpan(start=24, end=27))
(10136, CharSpan(start=27, end=29))
(118, CharSpan(start=29, end=30))
(10266, CharSpan(start=30, end=32))
(33627, CharSpan(start=32, end=35))
(13971, CharSpan(start=35, end=39))
(10107, CharSpan(start=39, end=40))
(10111, CharSpan(start=41, end=44))
(12307, CharSpan(start=45, end=49))
(118, CharSpan(start=49, end=50))
(10266, CharSpan(start=50, end=52))
(33627, CharSpan(start=52, end=55))
(13971, CharSpan(start=55, end=59))
(10107, CharSpan(start=59, end=60))
(102, None)
('This', TokenSpan(start=1, end=2))
('is', TokenSpan(start=2, end=3))
('an', TokenSpan(start=3, end=4))
('example', TokenSpan(start=4, end=5))
('with', TokenSpan(start=5, end=6))
('token-embeddings', TokenSpan(start=6, end=8))
('and', TokenSpan(start=8, end=9))
('word-embeddings', TokenSpan(start=9, end=13))
</code></pre>
<p>To retrieve the word embeddings:</p>
<pre><code>with torch.inference_mode():
  token_embeddings = embedding_layer(encoded.convert_to_tensors(&quot;pt&quot;).input_ids).squeeze()
  # we need the attention mechanism of the whole model to get the contextualized token representations
  contextualized_token_embeddings = m(**encoded.convert_to_tensors(&quot;pt&quot;)).last_hidden_state.squeeze()

def fetch_word_embeddings(sample_sentence:str, encoded, embeddings:torch.Tensor) -&gt; dict[str,torch.Tensor]:
  word_embeddings = {}
  for idx, word in enumerate(sample_sentence.split()):
    start, end = encoded.word_to_tokens(idx)
    word_embeddings[word] = embeddings[start:end].mean(dim=0)
  return word_embeddings

word_embeddings = fetch_word_embeddings(sample_sentence, encoded, token_embeddings)
contextualized_word_embeddings = fetch_word_embeddings(sample_sentence, encoded, contextualized_token_embeddings)
print(word_embeddings[&quot;token-embeddings&quot;])
print(contextualized_word_embeddings[&quot;token-embeddings&quot;])
</code></pre>
<p>Output:</p>
<pre><code>tensor([ 1.2455e-02, -3.8478e-02,  8.0834e-03, ..., -1.8502e-02,  1.1511e-02, -6.5307e-02])
tensor([-5.1564e-01, -1.6266e-01, -3.9420e-01, ..., -5.9969e-02,  3.0784e-01, -3.4451e-01])
</code></pre>
"
Recommending a pre-train NER model for geospatial entities,"<p>I am trying to find the best pre-trained Hugging Face Transformer model exclusively dedicated to geospatial or location entities to extract location entities in English from a text. Does it work way better than roberta-large?</p>
","nlp, geolocation, pipeline, geospatial, huggingface-transformers",
--user-dir in Fairseq in failing,"<p>I’m trying to fine-tune the IndicTrans2 model using fairseq-train, but I keep encountering the following error:</p>
<p>fairseq-train: error: argument --user-dir: invalid Optional value: 'C:/Users/sasid/Downloads/en-indic-exp/model_configs'</p>
<p>I’ve provided the --user-dir argument as the path to the model_configs directory (e.g., --user-dir C:/Users/sasid/Downloads/IndicTrans2/model_configs), but the training script fails with the above error.</p>
<p>I figured out that the problem is it's not taking path in windows convention with backslashes instead it is taking forward slashes so it is failing with that error.</p>
<p>So how can I make it take the path in windows convention?</p>
","python, nlp, huggingface-transformers, large-language-model, fairseq",
Searching for specific words in Corpus with R (tm package),"<p>I have a <code>Corpus</code> (<code>tm</code> package), containing a collection of 1.300 different text documents [Content:  documents: 1.300].</p>
<p>My goal is now to search the frequency of a specific wordlist in each of those documents. E.g. if my <code>wordlist contains the words &quot;january, february, march,....&quot;</code>. I want to analyze how often the documents refer to these words.</p>
<pre><code>Example: 
Text 1: I like going on holiday in january and not in february.
Text 2: I went on a holiday in march.
Text 3: I like going on vacation.
</code></pre>
<p>The result should look like this:</p>
<pre><code>Text 1: 2 
Text 2: 1
Text 3: 0
</code></pre>
<p>I tried using the following codes:</p>
<pre><code>library(quanteda)
toks &lt;- tokens(x) 
toks &lt;- tokens_wordstem(toks) 

dtm &lt;- dfm(toks)

dict1 &lt;- dictionary(list(c(&quot;january&quot;, &quot;february&quot;, &quot;march&quot;)))

dict_dtm2 &lt;- dfm_lookup(dtm, dict1, nomatch=&quot;_unmatched&quot;)                                 
tail(dict_dtm2)  
</code></pre>
<p>This code was proposed in a different chat, however it does not work on mine and an error, saying it is only applicaple on text or corpus elements occurs.</p>
<p>How can I search for my wordlist using my existing <code>Corpus</code> in <code>tm</code> package in R?</p>
","r, nlp, tm, corpus","<p>To make your Quanteda code work, you first have to convert your <em>tm VCorpus</em> object <code>x</code> + fix few other minor issues:</p>
<ul>
<li><code>dictionary()</code> expects a named list</li>
<li>English stemmer will return <em>&quot;januari&quot;</em>, <em>&quot;februari&quot;</em> instead of <em>&quot;january&quot;</em>, <em>&quot;february&quot;</em>.</li>
</ul>
<pre class=""lang-r prettyprint-override""><code>library(tm)
library(quanteda)

## prepare reprex, create tm VCorpus:
docs &lt;- c(&quot;I like going on holiday in january and not in february.&quot;,
          &quot;I went on a holiday in march.&quot;,
          &quot;I like going on vacation.&quot;)
x &lt;- VCorpus(VectorSource(docs))
class(x)
#&gt; [1] &quot;VCorpus&quot; &quot;Corpus&quot;

### tm VCorpus object to Quanteda corpus:
x &lt;- corpus(x)
class(x)
#&gt; [1] &quot;corpus&quot;    &quot;character&quot;

### continue with tokenization and stemmming
toks &lt;- tokens(x) 
toks &lt;- tokens_wordstem(toks) 
dtm &lt;- dfm(toks)

# dictionary() takes a named list, i.e. list(months = c(..))
# and &quot;january&quot;, &quot;february&quot; are stemmed to &quot;januari&quot;, &quot;februari&quot;
dict1 &lt;- dictionary(list(months = c(&quot;januar*&quot;, &quot;februar*&quot;, &quot;march&quot;)))
dict_dtm2 &lt;- dfm_lookup(dtm, dict1, nomatch=&quot;_unmatched&quot;)                                 
dict_dtm2
#&gt; Document-feature matrix of: 3 documents, 2 features (16.67% sparse) and 7 docvars.
#&gt;        features
#&gt; docs    months _unmatched
#&gt;   text1      2         10
#&gt;   text2      1          7
#&gt;   text3      0          6
</code></pre>
<p><sup>Created on 2023-09-02 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.0.2</a></sup></p>
"
Avoiding overlap in frequency and document frequency count in Quanteda,"<p>Below is a dummy corpus of 4 documents.</p>
<p>The dictionary was developed to identify the frequency of words or phrases in the corpus, as well as the number of documents a word or phrases occurs in.</p>
<p>The world 'Australians' occurs in two dictionary keys (peep, indig). Key content is intended to be mutually exclusive.</p>
<p>Similarly 'Australia' (oz and Australia Post), foreign (foreign and multinat) and farm/farmers (dairy and farmers) occur in two dictionary keys each,
but are intended to be counted once, according to the dictionary.</p>
<p>The expected overall frequency count is (extracted from the 'pattern&quot; column of the kwic table) and reported as x2 below. Note the word industry appears but is not allocated to industry because it is define din the indig key.</p>
<p>Dairy is the most frequency occuring key, occuring in three documents. This can calculated from unique rows in the kwic table 'doc names' column for each key.</p>
<p>I have three questions:</p>
<ol>
<li>are there any problems/issues that could affect output accuracy using this approach?</li>
<li>is there a better/more parsimonius approach to achieve what I am trying to do?</li>
<li>what would be the best way to extract the equivalent of tetxstat frequency count data from the kwic table?</li>
</ol>
<pre><code>        library (quanteda)
        library(quanteda.textstats)

        txt &lt;- c(doc1 = &quot;A significant percent of all farms in Australia, are dairy. 
         Although there are a lot of dairy farms in this country, 
         it is not the biggest farm industry. The life of a farmer is not easy, a dairy 
        farmer has to be an early riser. &quot;,
         doc2 = &quot;Australian people like milk so a healthy dairy industry is important in 
         our country&quot;,
         doc3 = &quot;Dairy and sheep farms developed at the expense of Indigenous 
         Australians. Further many companies  are now foreign-owned&quot;,
         doc4 = &quot;Some farmers are lucky to receive a service from Australia Post. Mail is 
         sent to many foreign countries and received more quickly than 
         delivered in some locations in Australia.&quot;)



         x &lt;- x %&gt;%
         tokens_compound(phrase(&quot;dairy farmers&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;dairy farms&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;dairy farm&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;dairy farming&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;dairy industry&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;indigenous australians&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;australia post&quot;), concatenator = &quot; &quot;) %&gt;%
         tokens_compound(phrase(&quot;dairy farmer&quot;), concatenator = &quot; &quot;)
              x

         dict &lt;- dictionary(list(multinat = c(&quot;offshore petroleum companies&quot;, &quot;foreign- 
         owned&quot;, &quot;foreign owned&quot;, &quot;foreign companies&quot;, &quot;multinational&quot;, &quot;multinational 
         oil companies&quot;, &quot;multinationals&quot;, &quot;transnational&quot;),
         dairy = c(&quot;dairy farmers&quot;, &quot;dairy farms&quot;,&quot;dairy farm&quot;,&quot;dairy farming&quot;,&quot;dairy 
         industry&quot;, &quot;dairy farmer&quot;,&quot;dairy&quot;, &quot;milk&quot;),
         auspost = &quot;australia post&quot;,
         oz = c(&quot;australia&quot;, &quot;this country&quot;, &quot;our country&quot;),
         farmers = c(&quot;farmers&quot;, &quot;farmer&quot;, &quot;farm&quot;, &quot;farms&quot;),
         foreign = c(&quot;foreign&quot;, &quot;foreigner&quot;, &quot;foreigners&quot;), 
         business =c(&quot;small business&quot;, &quot;business&quot;, &quot;businesses&quot;, &quot;company&quot;, &quot;companies&quot;),
         indig = c(&quot;aboriginal&quot;, &quot;aboriginals&quot;, &quot;indigenous australians&quot;, &quot;torres 
         strait&quot;),
         peep = c(&quot;australians&quot;, &quot;people of australia&quot;, &quot;australian people&quot;, &quot;people of 
         this nation&quot;, &quot;people of this country&quot;),
         industry = c(&quot;industry&quot;, &quot;industries&quot;)))

        kwicdict &lt;- kwic(x, pattern = dict, window = 4)
        write.csv (kwicdict, &quot;D:/Output/TEST.csv&quot;)

       DF &lt;- read.csv(&quot;D://Output/TEST.csv&quot;,header=T)

       ## obtaining frequency count of KWIC table 'pattern ' values
       &gt; x2 &lt;- DF[,8]
       &gt; 
       &gt; table (x2)
       x2
       auspost business    dairy  farmers  foreign    indig industry multinat  oz  peep    
          1        1        6        5        1        1        1        1     5    2 
</code></pre>
","r, count, nlp, overlap, quanteda","<p>I don't think that <code>kwic()</code> is what you want here. <code>tokens_lookup()</code> lets you specify that the nested scope should be mutually exclusive across keys, not just within keys. Observe the difference below. (And note the use of wildcarding for dairy key.)</p>
<pre class=""lang-r prettyprint-override""><code>library(quanteda)
#&gt; Package version: 4.1.0
#&gt; Unicode version: 14.0
#&gt; ICU version: 71.1
#&gt; Parallel computing: 10 of 10 threads used.
#&gt; See https://quanteda.io for tutorials and examples.
library(quanteda.textstats)

txt &lt;- c(doc1 = &quot;A significant percent of all farms in Australia, are dairy. 
         Although there are a lot of dairy farms in this country, 
         it is not the biggest farm industry. The life of a farmer is not easy, a dairy 
        farmer has to be an early riser. &quot;,
         doc2 = &quot;Australian people like milk so a healthy dairy industry is important in 
         our country&quot;,
         doc3 = &quot;Dairy and sheep farms developed at the expense of Indigenous 
         Australians. Further many companies  are now foreign-owned&quot;,
         doc4 = &quot;Some farmers are lucky to receive a service from Australia Post. Mail is 
         sent to many foreign countries and received more quickly than 
         delivered in some locations in Australia.&quot;)

dict &lt;- dictionary(list(multinat = c(&quot;offshore petroleum companies&quot;, &quot;foreign-owned&quot;, 
                                     &quot;foreign owned&quot;, &quot;foreign companies&quot;, &quot;multinational&quot;, 
                                     &quot;multinational oil companies&quot;, &quot;multinationals&quot;, &quot;transnational&quot;),
                        dairy = c(&quot;dairy farm*&quot;, &quot;dairy industry&quot;, &quot;dairy&quot;, &quot;milk&quot;),
                        auspost = &quot;australia post&quot;,
                        oz = c(&quot;australia&quot;, &quot;this country&quot;, &quot;our country&quot;),
                        farmers = c(&quot;farmers&quot;, &quot;farmer&quot;, &quot;farm&quot;, &quot;farms&quot;),
                        foreign = c(&quot;foreign&quot;, &quot;foreigner&quot;, &quot;foreigners&quot;), 
                        business =c(&quot;small business&quot;, &quot;business&quot;, &quot;businesses&quot;, &quot;company&quot;, &quot;companies&quot;),
                        indig = c(&quot;aboriginal&quot;, &quot;aboriginals&quot;, &quot;indigenous australians&quot;, &quot;torres strait&quot;),
                        peep = c(&quot;australians&quot;, &quot;people of australia&quot;, &quot;australian people&quot;, 
                                 &quot;people of this nation&quot;, &quot;people of this country&quot;),
                        industry = c(&quot;industry&quot;, &quot;industries&quot;)))

x &lt;- tokens(txt)

# with overlap
tokens_lookup(x, dict) |&gt;
    dfm()
#&gt; Document-feature matrix of: 4 documents, 10 features (55.00% sparse) and 0 docvars.
#&gt;       features
#&gt; docs   multinat dairy auspost oz farmers foreign business indig peep industry
#&gt;   doc1        0     3       0  2       5       0        0     0    0        1
#&gt;   doc2        0     2       0  1       0       0        0     0    1        1
#&gt;   doc3        1     1       0  0       1       0        1     1    1        0
#&gt;   doc4        0     0       1  2       1       1        0     0    0        0

# without overlap
tokens_lookup(x, dict, nested_scope = &quot;dictionary&quot;) |&gt;
    dfm()
#&gt; Document-feature matrix of: 4 documents, 10 features (60.00% sparse) and 0 docvars.
#&gt;       features
#&gt; docs   multinat dairy auspost oz farmers foreign business indig peep industry
#&gt;   doc1        0     3       0  2       3       0        0     0    0        1
#&gt;   doc2        0     2       0  1       0       0        0     0    1        0
#&gt;   doc3        1     1       0  0       1       0        1     1    0        0
#&gt;   doc4        0     0       1  1       1       1        0     0    0        0
</code></pre>
<p><sup>Created on 2024-10-06 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.1.1</a></sup></p>
"
RuntimeError: &quot;element 0 of tensors does not require grad and does not have a grad_fn&quot;,"<p>I am facing an issue while training a comment classification model using PyTorch Lightning with a pre-trained BERT model.</p>
<p>I encountered the following error during the training process:</p>
<pre><code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>To provide some context, I have already enabled gradients for all parameters of the model using the function enable_gradients(model). However, the error still persists.</p>
<p>The model I am using is based on the aubmindlab/bert-base-arabertv02-twitter pre-trained model, and I noticed that some weights of the BERT model were not initialized properly upon loading. I have ensured that I am using the latest versions of PyTorch, Transformers, and PyTorch Lightning.</p>
<p>I attempted to pretrain the BERT model on a downstream task before training my specific model, but the error remains unresolved.</p>
<p>How to resolve this issue?</p>
<p>this is my code :</p>
<pre class=""lang-py prettyprint-override""><code>from pytorch_lightning import Trainer

def enable_gradients(model):
    for param in model.parameters():
        param.requires_grad = True

# datamodule
ucc_data_module = UCC_Data_Module(train_path, val_path, test_path, attributes=attributes, batch_size=config['batch_size'])
ucc_data_module.setup()

# model
model = UCC_Comment_Classifier()

enable_gradients(model)

# trainer and fit
# Instantiation of the Lightning Trainer
trainer = Trainer(max_epochs=config['n_epochs'], accelerator='gpu', num_sanity_val_steps=1)

try:
    trainer.fit(model, ucc_data_module)
    torch.save(model.state_dict(), PATH)
except RuntimeError as e:
    print(e)
</code></pre>
<p>This is the error :</p>
<pre><code>ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py&quot;, line 69, in _wrap
    fn(i, *args)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py&quot;, line 
147, in _wrapping_function
    results = function(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py&quot;, line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py&quot;, line 973, in _run
    results = self._run_stage()
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py&quot;, line 1016, in _run_stage
    self.fit_loop.run()
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py&quot;, line 201, in run
    self.advance()
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py&quot;, line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py&quot;, line 133, in run
    self.advance(data_fetcher)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py&quot;, line 218, in 
advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py&quot;, line 185, in 
run
    self._optimizer_step(kwargs.get(&quot;batch_idx&quot;, 0), closure)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py&quot;, line 260, in 
_optimizer_step
    call._call_lightning_module_hook(
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py&quot;, line 144, in 
_call_lightning_module_hook
    output = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/module.py&quot;, line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py&quot;, line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py&quot;, line 256, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py&quot;, line 225, in 
optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py&quot;, line 114,
in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py&quot;, line 69, in wrapper
    return wrapped(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py&quot;, line 280, in wrapper
    out = func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context
    return func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/transformers/optimization.py&quot;, line 439, in step
    loss = closure()
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py&quot;, line 101,
in _wrap_closure
    closure_result = closure()
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py&quot;, line 140, in 
__call__
    self._result = self.closure(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py&quot;, line 135, in 
closure
    self._backward_fn(step_output.closure_loss)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py&quot;, line 232, in 
backward_fn
    call._call_strategy_hook(self.trainer, &quot;backward&quot;, loss, optimizer)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py&quot;, line 291, in 
_call_strategy_hook
    output = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py&quot;, line 200, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py&quot;, line 67, 
in backward
    model.backward(tensor, *args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/module.py&quot;, line 1046, in backward
    loss.backward(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/_tensor.py&quot;, line 487, in backward
    torch.autograd.backward(
  File &quot;/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py&quot;, line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
","machine-learning, nlp, model, training-data, kaggle",
similarity from word to sentence after doing words Embedding,"<p>I have dataframe with 1000 text rows.</p>
<p>I did word2vec .</p>
<p>Now I want to create a new field which give me the distance from each sentence to the word that i want, lets say the word &quot;king&quot;.</p>
<p>I thought about taking in each sentence the 4 closet words to the word king  and make average of them.
maybe by using <code>model.wv.similarity</code>.
the avg of each sentnce will be in the field df['king']</p>
<p>I will glad to know how to do that or to hear about another method.</p>
<p>example data:</p>
<pre><code>    data = {
    'text': [
        &quot;The king sat on the throne with wisdom.&quot;,
        &quot;A queen ruled the kingdom alongside the king.&quot;,
        &quot;Knights were loyal to their king.&quot;,
        &quot;The empire prospered under the rule of a wise monarch.&quot;
    ]
}
df = pd.DataFrame(data)
df['text']=df['text'].str.split()    

model = Word2Vec(df['text'], vector_size=100, window=2, min_count=1 )

model.wv.similarity('Knights','king')
</code></pre>
<p><strong>edit</strong>:</p>
<p>My mission is:</p>
<p>I have 1000 text rows (people that complain about something)
I want to catalog them into 4 words.
Lets say that word 1 is king. Word 2 is castle…
I want to know about each sentence which word from the  4 words most represent the sentence.
In order to do that I thought about taking each word from the 4 words and calculate <code>model.wv.similarity</code> to all of the words in  df['text'].
After that, for each sentence, take the 3  words that have  the highest score to word king  (and to the word  castle and ets..)  .
calculate mean of the 3 highest score and that would be the value of df['king'] for the sentence</p>
","python, nlp, text-mining, word2vec, similarity",
Langchain Chatbot with Memory + Vector Database,"<p>In Langchain, what is the suggested way to build a chatbot with memory and retrieval from a vector embedding database at the same time?</p>
<p>The examples in the <a href=""https://python.langchain.com/en/latest/modules/memory/how_to_guides.html"" rel=""nofollow noreferrer"">docs</a> add memory modules to chains that do not have a vector database. Related <a href=""https://github.com/hwchase17/langchain/issues/2185"" rel=""nofollow noreferrer"">issue</a>.</p>
","nlp, chatbot, language-model, langchain",
Questions about training LLMs on large text datasets for text generation from scratch,"<p>I made a fully custom made GPT in Jax (with Keras 3), using Tensorflow for the data pipeline.</p>
<p>I've trained the model on the Shakespeare dataset and got good results (so no problem with the model).
Now I want to train it on the Tiny-Stories dataset which is pretty big with GPT of 15M parameters.</p>
<p>Here is the code for loading the data:</p>
<pre class=""lang-py prettyprint-override""><code>def get_dataset_lists(ds_path:str):
    dataset = open(ds_path, &quot;r&quot;, encoding=&quot;utf-8&quot;).read() # [...]
    dataset = dataset.split(&quot;&lt;|endoftext|&gt;&quot;)
    r.shuffle(dataset)
    dataset:list = spm.Encode( # llama's sentence piece encoder
            tf.strings.strip(dataset).numpy().tolist(), 
            add_bos=True,
            add_eos=False
        ) # [[SOS story], ..., [SOS story]]
    print(&quot;\tNumber of stories:&quot;, len(dataset))
    return dataset

def tf_dataload(
    dataset:list,
    batch_size:int,
    maxlen:int,
    shift:int,
):
    import functools; import operator
    dataset = functools.reduce(operator.iconcat, dataset, [])
    num_tokens = len(dataset); print(&quot;\tNumber of tokens in the dataset is&quot;, num_tokens)
    unique_tok = set(dataset); print(&quot;\tNumber of unique tokens in the dataset is&quot;, len(unique_tok))
    # [SOS story ... SOS story]
    dataset = tf.data.Dataset.from_tensor_slices(dataset)
    dataset = dataset.window(maxlen+1, shift=shift, drop_remainder=True)
    # [[...], [...], [...], ...] shape(m, maxlen+1)
    dataset = dataset.flat_map(lambda window: window.batch(maxlen+1))
    dataset = dataset.shuffle(10_000*batch_size, reshuffle_each_iteration=reshuffle_each_iteration)
    # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1)
    dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.shuffle(batch_size*100)
    dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)
    return dataset # (shape(m//B, B, maxlen) shape(m//B, B, maxlen))

def load_data(
    train_ds_path:str,
    val_ds_path:str,
    batch_size:int,
    maxlen:int,
    shift:int,
):  
    print(&quot;Training Dataset:&quot;)
    train_ds = tf_dataload(get_dataset_lists(train_ds_path), batch_size, maxlen, shift, reshuffle_each_iteration=True)
    print(&quot;Validation Dataset:&quot;)
    val_ds = tf_dataload(get_dataset_lists(val_ds_path), batch_size, maxlen, shift, reshuffle_each_iteration=True)
    print(f&quot;\n{train_ds}\n{val_ds}&quot;)
    datasets = {&quot;train&quot;: train_ds.repeat(), &quot;val&quot;:val_ds}
    return datasets
</code></pre>
<ul>
<li><strong>I've certain questions regarding
the value of the <code>shift</code>?</strong><br />
First I set it equal to 1, but the training was very slow, even after 100000 steps it didn't converge even though it was decreasing slowly (I think there's no problem with the learning rate as I plotted Loss Vs Lr and selected the max learning rate possible and used cosine decay with warmup)</li>
</ul>
<p><a href=""https://i.sstatic.net/MoKO4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MoKO4.png"" alt=""enter image description here"" /></a></p>
<p>So I looked into <a href=""https://github.com/karpathy/llama2.c/blob/master/tinystories.py#L217"" rel=""nofollow noreferrer"">Karpathy's llama-2 repo</a> and the shift was equal to maxlen.
So I set it equal to <code>maxlen</code> and trained it for 100000 steps but the model is learning very slowly, and didn't get a loss even close to what <a href=""https://github.com/karpathy/llama2.c#:%7E:text=15M-,1.072,-stories15M.bin"" rel=""nofollow noreferrer"">Karpathy got</a>
(I don't know what's the problem, as I've closely followed Karpathy's llama2 repo)
<strong>What is shift generally equal to when pre-training an LLM on Language Modelling?
Shouldn't it be 1, because the transformer model is not positionally invariant, and it would affect model performance if <code>shift</code> is not equal to 1? But then the number of samples will be very large...?</strong></p>
<ul>
<li>And for what number of steps to train a LLM given the number of tokens</li>
</ul>
<p>You may find the below helpful...</p>
<pre class=""lang-py prettyprint-override""><code>@dataclass
class GPTArgs:
    &quot;&quot;&quot;GPT Configuration&quot;&quot;&quot;
    d_model:int = 288
    num_layers:int = 6
    num_heads:int = 6
    max_context_length:int = 256
    vocab_size:int = VOCAB_SIZE # 32K
    output_units:int = None # equal to vocab_size if None in model init  
    assert d_model % 2 == 0
    assert d_model % num_heads == 0
    dropout_rate:float = 0.1

@dataclass
class TArgs:
    # lr scheduler
    init_lr:float = 1e-7
    max_lr:float = 6.5e-4
    min_lr:float = 0.1*max_lr # The factor is usually 0.1 or 0.0
    num_steps:int = 100_000
    warmup_steps:int = 1000 # 1000, to make training more stable instead of 2000
    decay_steps:int = num_steps

    # optimizer
    beta1:float = 0.9
    beta2:float = 0.95
    weight_decay:float = 1e-1
    clipvalue:float = 1e0
    num_grad_accumalation_steps:int = 4
    # num_tok_per_update = batch_size * maxlen * gradient_accumalation = 128 * 256 * 4 = 131_072

    # training
    checkpoint:str = 'weights/GPTstories/Epoch{epoch}.weights.h5'
    train_ds_path:str = &quot;TinyStoriesDataset/TinyStories-train.txt&quot;
    val_ds_path:str = &quot;TinyStoriesDataset/TinyStories-valid.txt&quot;
    steps_per_epoch = eval_freq = 2000
    eval_steps:int = 200
    batch_size:int = 128 
    patience:int = 10 # early stopping with restore best weights
</code></pre>
<h2>Update 1:</h2>
<p>I thought that the model wasn't getting the training samples uniformly so I modified the data pipeline and also increased the number of steps to <a href=""https://github.com/karpathy/llama2.c#:%7E:text=brief%20training%20guide"" rel=""nofollow noreferrer"">200,000</a>.
But there were no significant improvements. The training was still very slow by the end and loss was decreasing by 0.01 every epoch (of 2000 steps)... Got a loss of 1.67 on validation set</p>
<pre class=""lang-py prettyprint-override""><code>def pretokenize_and_save_dataset(dataset_path:str, num_shards:int, shard_dir:str):
    dataset = open(dataset_path, &quot;r&quot;, encoding=&quot;utf-8&quot;).read() # [...]
    dataset = dataset.split(&quot;&lt;|endoftext|&gt;&quot;)
    r.shuffle(dataset)
    dataset:list = spm.Encode(
            tf.strings.strip(dataset).numpy().tolist(), 
            add_bos=True,
            add_eos=False
        ) # [[SOS story], ..., [SOS story]]
    print(&quot;Dataset:&quot;)
    print(&quot;\tNumber of stories:&quot;, len(dataset))

    # flatten
    dataset = functools.reduce(operator.iconcat, dataset, [])
    num_tokens = len(dataset); print(&quot;\tNumber of tokens in the dataset:&quot;, num_tokens)
    print(&quot;\tNumber of unique tokens in the dataset:&quot;, len(set(dataset)))
    
    dataset = np.asarray(dataset, dtype=np.uint16) # [SOS story ... SOS story]
    print(&quot;\tAvg length of story:&quot;, num_tokens/((dataset==1).sum()))

    # shard and save dataset
    sharded_datasets_list = np.array_split(dataset, num_shards) # [[SOS story...], [...], [...], ...]
    filenames = [os.path.join(shard_dir, f&quot;shard{i+1}.npy&quot;) for i in range(num_shards)]
    
    for filename, sharded_ds in zip(filenames, sharded_datasets_list):
        with open(filename, &quot;wb&quot;) as f:
            np.save(f, sharded_ds)
    return filenames

def load_data_as_tfds(
    dataset:np.ndarray,
    maxlen:int,
    shift:int,
):
    # [SOS story ... SOS story]
    dataset = tf.data.Dataset.from_tensor_slices(dataset.tolist())
    dataset = dataset.window(maxlen+1, shift=shift, drop_remainder=True)
    # [[...], [...], [...], ...] shape(m, maxlen+1)
    dataset = dataset.flat_map(lambda window: window.batch(maxlen+1))
    dataset = dataset.shuffle(10_000*128)
    return dataset

def batch_tfds(
        dataset:tf.data.Dataset,
        batch_size:int,
):
    dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.shuffle(batch_size*1000)
    dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]), num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.repeat().prefetch(tf.data.AUTOTUNE)
    return dataset

def load_data(
    dataset_path:str,
    batch_size:int,
    maxlen:int,
    shift:int,
    num_shards:int,
    shard_dir:str
):  
    if os.path.exists(shard_dir) and os.listdir(shard_dir):
        filenames = glob.glob(os.path.join(shard_dir, &quot;*.npy&quot;))
    else:
        os.makedirs(shard_dir)
        filenames = pretokenize_and_save_dataset(dataset_path, num_shards=num_shards, shard_dir=shard_dir)
    r.shuffle(filenames)
    to_tfds = lambda dataset: load_data_as_tfds(dataset, maxlen=maxlen, shift=shift)
    num_train_shards = round(0.9651*num_shards)
    num_val_shards = num_shards-num_train_shards

    print(&quot;Training Dataset:&quot;)
    print(f&quot;\tNumber of files taken for training: {num_train_shards}/{num_shards}&quot;)
    train_datasets_lists = [to_tfds(np.load(filename)) for filename in filenames[:num_train_shards]]
    train_ds = tf.data.Dataset.sample_from_datasets(train_datasets_lists, weights=[1/num_train_shards]*num_train_shards)
    # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1)
    train_ds = batch_tfds(train_ds, batch_size=batch_size)

    print(&quot;Validation Dataset:&quot;)
    print(f&quot;\tNumber of files taken for validation: {num_val_shards}/{num_shards}&quot;)
    val_datasets_lists = [to_tfds(np.load(filename)) for filename in filenames[num_train_shards:]]
    val_ds = tf.data.Dataset.sample_from_datasets(val_datasets_lists, weights=[1/num_val_shards]*num_val_shards)
    # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1)
    val_ds = batch_tfds(val_ds, batch_size=batch_size)

    print(f&quot;\n{train_ds}\n{val_ds}&quot;)
    datasets = {&quot;train&quot;: train_ds, &quot;val&quot;:val_ds}
    return datasets
</code></pre>
","python, tensorflow, deep-learning, nlp, tf.data.dataset","<p>Replaced keras's <a href=""https://keras.io/api/optimizers/adamw/#:%7E:text=loss%20scale%20factor.-,gradient_accumulation_steps,-%3A%20Int%20or%20None"" rel=""nofollow noreferrer"">gradient accumulation argument in AdamW</a> with a custom implementation like in <a href=""https://github.com/karpathy/llama2.c/blob/b3c4b6c3c4bbff42e5211293280307019368ccb5/train.py#L304"" rel=""nofollow noreferrer"">karpathy's</a> and now the loss is decreasing faster.</p>
<p>If you are using keras's <code>num_grad_accum</code>, increase <code>num_steps</code> to
<code>num_steps *= num_grad_accum</code></p>
"
"Huggingface: Fine-tuning (not enough values to unpack (expected 2, got 1))","<p>I'm trying to fine-tune <a href=""https://huggingface.co/erfan226/persian-t5-paraphraser"" rel=""nofollow noreferrer"">erfan226/persian-t5-paraphraser</a> paraphrase generator model for Persian sentences. I used the Persian dataset of <a href=""https://huggingface.co/datasets/tapaco"" rel=""nofollow noreferrer"">tapaco</a> and reformatted it to match the <a href=""https://huggingface.co/datasets/glue/viewer/mrpc"" rel=""nofollow noreferrer"">glue (mrpc) dataset</a> which is used in the <a href=""https://huggingface.co/docs/transformers/training"" rel=""nofollow noreferrer"">fine-tuning documentation</a>. I have uploaded my dataset in <a href=""https://huggingface.co/datasets/alighasemi/farsi_paraphrase_detection"" rel=""nofollow noreferrer"">alighasemi/farsi_paraphrase_detection</a>.</p>
<p>I followed every step of the Trainer video (Tokenization, ComputerMetrics, TrainingArgs, ...). However, when I run trainer.train() I get the following error:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-28-3435b262f1ae&gt; in &lt;module&gt;
----&gt; 1 trainer.train()

7 frames
/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    941             inputs_embeds = self.embed_tokens(input_ids)
    942 
--&gt; 943         batch_size, seq_length = input_shape
    944 
    945         # required mask seq length can be calculated via length of past

ValueError: not enough values to unpack (expected 2, got 1)
</code></pre>
<p>Here's my code:</p>
<pre><code>dataset = load_dataset(&quot;alighasemi/farsi_paraphrase_detection&quot;)

tokenizer = AutoTokenizer.from_pretrained(&quot;erfan226/persian-t5-paraphraser&quot;)

def tokenize_function(examples):
    return tokenizer(examples[&quot;sentence1&quot;], examples[&quot;sentence2&quot;], truncation=True)

tokenized_dataset = dataset.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer)

model = AutoModelForSeq2SeqLM.from_pretrained(&quot;erfan226/persian-t5-paraphraser&quot;, num_labels=2)
</code></pre>
<p>I could not use the AutoModelForSequenceClassification to load the model since I would get this error:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-29-5650b38a14f3&gt; in &lt;module&gt;
----&gt; 1 model = AutoModelForSequenceClassification.from_pretrained(&quot;erfan226/persian-t5-paraphraser&quot;, num_labels=2)

/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    464                 pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    465             )
--&gt; 466         raise ValueError(
    467             f&quot;Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n&quot;
    468             f&quot;Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.&quot;

ValueError: Unrecognized configuration class &lt;class 'transformers.models.t5.configuration_t5.T5Config'&gt; for this kind of AutoModel: AutoModelForSequenceClassification.
Model type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BloomConfig, CamembertConfig, CanineConfig, ConvBertConfig, CTRLConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, EsmConfig, FlaubertConfig, FNetConfig, FunnelConfig, GPT2Config, GPTNeoConfig, GPTJConfig, IBertConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MBartConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, OpenAIGPTConfig, OPTConfig, PerceiverConfig, PLBartConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig, TransfoXLConfig, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, YosoConfig.
</code></pre>
<pre><code>training_args = TrainingArguments(
    &quot;farsi_paraphraser&quot;,
    num_train_epochs=5,
    evaluation_strategy=&quot;epoch&quot;
)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_dataset[&quot;train&quot;],
    eval_dataset=tokenized_dataset[&quot;validation&quot;],
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()
</code></pre>
<p>I would be happy if you could help me</p>
","nlp, huggingface, fine-tuning",
HuggingFace Transformers For Text Generation with CTRL with Google Colab&#39;s free GPU,"<p>I wanted to test TextGeneration with CTRL using PyTorch-Transformers, before using it for fine-tuning. But it doesn't prompt anything like it does with GPT-2 and other similar language generation models. I'm very new for this and am stuck and can't figure out what's going on.</p>
<p>This is the procedure I followed in my Colab notebook,</p>
<pre><code>!pip install transformers
</code></pre>
<pre><code>!git clone https://github.com/huggingface/pytorch-transformers.git
</code></pre>
<pre><code>!python pytorch-transformers/examples/run_generation.py \
    --model_type=ctrl \
    --length=100 \
    --model_name_or_path=ctrl \
    --temperature=0.2 \
    --repetition_penalty=1.2 \
</code></pre>
<p>And this is what I get after running the script</p>
<pre><code>02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-vocab.json from cache at /root/.cache/torch/transformers/a858ad854d3847b02da3aac63555142de6a05f2a26d928bb49e881970514e186.285c96a541cf6719677cfb634929022b56b76a0c9a540186ba3d8bbdf02bca42
02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-merges.txt from cache at /root/.cache/torch/transformers/aa2c569e6648690484ade28535a8157aa415f15202e84a62e82cc36ea0c20fa9.26153bf569b71aaf15ae54be4c1b9254dbeff58ca6fc3e29468c4eed078ac142
02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   loading configuration file https://storage.googleapis.com/sf-ctrl/pytorch/ctrl-config.json from cache at /root/.cache/torch/transformers/d6492ca334c2a4e079f43df30956acf935134081b2b3844dc97457be69b623d0.1ebc47eb44e70492e0c20494a084f108332d20fea7fe5ad408ef5e7a8f2baef4
02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   Model config CTRLConfig {
  &quot;architectures&quot;: null,
  &quot;attn_pdrop&quot;: 0.1,
  &quot;bos_token_id&quot;: 0,
  &quot;dff&quot;: 8192,
  &quot;do_sample&quot;: false,
  &quot;embd_pdrop&quot;: 0.1,
  &quot;eos_token_ids&quot;: 0,
  &quot;finetuning_task&quot;: null,
  &quot;from_tf&quot;: false,
  &quot;id2label&quot;: {
    &quot;0&quot;: &quot;LABEL_0&quot;
  },
  &quot;initializer_range&quot;: 0.02,
  &quot;is_decoder&quot;: false,
  &quot;label2id&quot;: {
    &quot;LABEL_0&quot;: 0
  },
  &quot;layer_norm_epsilon&quot;: 1e-06,
  &quot;length_penalty&quot;: 1.0,
  &quot;max_length&quot;: 20,
  &quot;model_type&quot;: &quot;ctrl&quot;,
  &quot;n_ctx&quot;: 512,
  &quot;n_embd&quot;: 1280,
  &quot;n_head&quot;: 16,
  &quot;n_layer&quot;: 48,
  &quot;n_positions&quot;: 50000,
  &quot;num_beams&quot;: 1,
  &quot;num_labels&quot;: 1,
  &quot;num_return_sequences&quot;: 1,
  &quot;output_attentions&quot;: false,
  &quot;output_hidden_states&quot;: false,
  &quot;output_past&quot;: true,
  &quot;pad_token_id&quot;: 0,
  &quot;pruned_heads&quot;: {},
  &quot;repetition_penalty&quot;: 1.0,
  &quot;resid_pdrop&quot;: 0.1,
  &quot;summary_activation&quot;: null,
  &quot;summary_first_dropout&quot;: 0.1,
  &quot;summary_proj_to_labels&quot;: true,
  &quot;summary_type&quot;: &quot;cls_index&quot;,
  &quot;summary_use_proj&quot;: true,
  &quot;temperature&quot;: 1.0,
  &quot;top_k&quot;: 50,
  &quot;top_p&quot;: 1.0,
  &quot;torchscript&quot;: false,
  &quot;use_bfloat16&quot;: false,
  &quot;vocab_size&quot;: 246534
}

02/10/2020 01:02:31 - INFO - transformers.modeling_utils -   loading weights file https://storage.googleapis.com/sf-ctrl/pytorch/seqlen256_v1.bin from cache at /root/.cache/torch/transformers/c146cc96724f27295a0c3ada1fbb3632074adf87e9aef8269e44c9208787f8c8.b986347cbab65fa276683efbb9c2f7ee22552277bcf6e1f1166557ed0852fdf0
tcmalloc: large alloc 1262256128 bytes == 0x38b92000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x5096b7 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9
tcmalloc: large alloc 1262256128 bytes == 0x19fdda000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50d390 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5a067e 0x50d966 0x508245
^C
</code></pre>
<p>and then terminates. Could this be because of a GPU problem?</p>
","python, deep-learning, nlp, pytorch, huggingface-transformers","<p>The solution was to increase the RAM. Since I was using the Google Colab's free GPU, I was going through this: <a href=""https://github.com/googlecolab/colabtools/issues/253"" rel=""nofollow noreferrer"">GitHub issue</a>
and found this useful: <a href=""https://github.com/googlecolab/colabtools/issues/253#issuecomment-551056637"" rel=""nofollow noreferrer"">Solution</a></p>

<p>The following piece of code will crash the session in Colab and select 'Get more RAM', which will increase the RAM up to 25.51GB</p>

<pre><code>d=[]
while(1):
  d.append('1')
</code></pre>
"
How to import and use stopwords list from NLTK?,"<p>I already imported <code>stopwords</code> from <code>nltk.corpus</code>, but I get <code>STOPWORDS is not defined</code> error. Below is my code:</p>
<pre class=""lang-py prettyprint-override""><code>import nltk
from nltk.corpus import stopwords
#Create stopword list:
stopwords = set(STOPWORDS)
</code></pre>
<p>The above gives the following error:</p>
<pre><code>NameError: name 'STOPWORDS' is not defined
</code></pre>
","python, machine-learning, nlp, nltk, stop-words",
Fine-tune BERT for a specific domain on a different language?,"<p>I want to fine-tune on a pre-trained BERT model.
However, my task uses data within a specific domain (say biomedical data).
Additionally, my data is also in a language different from English (say Dutch).</p>
<p>Now I could fine-tune the Dutch bert-base-dutch-cased pre-trained model.
However, how would I go about fine-tuning a Biomedical BERT model, like BioBERT,
which is in the correct domain, but wrong language?</p>
<p>I have thought about using NMT, but don't think it's viable and worth the effort.
If I fine-tune without any alterations to the model, I fear that the model will not learn the task well
since it was pre-trained on a completely different language.</p>
","python-3.x, deep-learning, nlp, bert-language-model",
Counting the Frequency of Some Words within some other Key Words in Text,"<p>I have two sets of word lists - first one I called <code>search words</code> and the second one I called <code>key words</code>. My goal is to calculate the frequency of <code>search words</code> within 10 words of <code>key words</code>. For example, assume that the word - <strong>acquire</strong> - is in <code>key words</code> list, then I will look for the words in <code>search words</code> list within 10 words of <strong>acquire</strong>. Within 10 words mean, 10 words forward from key words and 10 words backward from key words, meaning that both forward and backward movement.</p>
<p>Below is my <code>search word</code> and <code>key word</code> lists -</p>
<pre><code>search_words = ['access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',
 'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',
 'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',
 'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',
 'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',
 'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security', 
 'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',
 'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout', 
 'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security', 
 'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',
 'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',
 'Securion', 'security event management', 'security information and event management', 
 'security information management', 'SentinelOne', 'Seqrite', 'Sophos',
 'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense', 
 'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm']

key_words = ['acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',
 'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',
 'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install', 
 'integrate', 'invest', 'lease',
 'modernize', 'modify', 'move', 'obtain', 'plan', 'project', 'purchase', 'replace', 'spend',
  'upgrade', 'use']
</code></pre>
<p>A small Example -</p>
<pre><code>text_dict = {
    'ITEM7':[&quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process \
    of adopting Symantec. We believe these technologies will improve our access control. \
        Moreover, we also integrated data security diagnostic program.&quot;,
        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;]
}

df = pd.DataFrame(text_dict)
</code></pre>
<p>My expected outcome is -</p>
<pre><code>                 ITEM7                          Frequency
Last year, from AVG we have acquired Alibaba S...   6
We are planning to install end-point security,...   2
</code></pre>
<p>For the first row in <code>df</code>, we see the word <code>AVG</code> and <code>Alibaba Security</code> are from <code>search_words</code> list and around the word <strong>acquired</strong>, the base form of which - <strong>acquire</strong> - is in the <code>key_words</code> list. Similarly, <code>Symantec</code>, <code>Access Control</code>, <code>data security</code>, <code>diagnostic program</code> are from <code>search_words</code> list and these words are within 10 words of <code>adopting</code>, <code>improve</code>, <code>integrated</code> from <code>key_words</code> list. So, total search words are 6 (AVG+Alibaba Security+Symantec+Access Control+Data Security+Diagnostic Program). Therefore, in the <code>Frequency</code> column of <code>df</code>, the value is 6.</p>
<p>Please note that the words in <code>key_words</code> are in basically base form, so their variation (like adopted, adopting) should be counted as key words also.</p>
","python, pandas, nlp","<p>You need to process each row of text by identifying occurrences of <code>key_words</code> and capturing a 10-word window around them. Within this window, you need to check for multi-word search_words, ensuring they are matched as phrases. Each unique <code>search_word</code> found within these windows needs to be counted, avoiding double-counting across the row. Stored the results as a frequency count for each row, accurately reflecting the number of unique <code>search_words</code> near <code>key_words</code>.</p>
<pre><code>import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import string
import re

text_dict = {
    'ITEM7': [
        &quot;Last year, from AVG we have acquired Alibaba Security. This year we are in the process &quot;
        &quot;of adopting Symantec. We believe these technologies will improve our access control. &quot;
        &quot;Moreover, we also integrated data security diagnostic program.&quot;,
        &quot;We are planning to install end-point security, which will upgrade intrusion detection system.&quot;
    ]
}
df = pd.DataFrame(text_dict)

search_words = [
    'access control', 'Acronis', 'Adaware', 'AhnLab', 'AI Max Dev Labs', 'Alibaba Security',
    'anti-adware', 'anti-keylogger', 'anti-malware', 'anti-ransomware', 'anti-rootkit', 'anti-spyware',
    'anti-subversion', 'anti-tamper', 'anti-virus', 'Antiy', 'Avast', 'AVG', 'Avira', 'Baidu', 'Barracuda',
    'Bitdefender', 'BullGuard', 'Carbon Black', 'Check Point', 'Cheetah Mobile', 'Cisco', 'Clario',
    'Comodo', 'computer security', 'CrowdStrike', 'cryptography', 'Cybereason', 'cybersecurity',
    'Cylance', 'data security', 'diagnostic program', 'Elastic', 'Emsisoft', 'encryption', 'Endgame', 'end point security',
    'Ensilo', 'eScan', 'ESET', 'FireEye', 'firewall', 'Fortinet', 'F-Secure', 'G Data',
    'Immunet', 'information security', 'Intego', 'intrusion detection system', 'K7', 'Kaspersky', 'log management software', 'Lookout',
    'MacKeeper', 'Malwarebytes', 'McAfee', 'Microsoft', 'network security',
    'NOD32', 'Norton', 'Palo Alto Networks', 'Panda Security', 'PC Matic', 'PocketBits',
    'Qihoo', 'Quick Heal', 'records management', 'SafeDNS', 'Saint Security', 'sandbox', 'Sangfor',
    'Securion', 'security event management', 'security information and event management',
    'security information management', 'SentinelOne', 'Seqrite', 'Sophos',
    'SparkCognition', 'steganography', 'Symantec', 'Tencent', 'Total AV', 'Total Defense',
    'Trend Micro', 'Trustport', 'Vipre', 'Webroot', 'ZoneAlarm'
]

key_words = [
    'acquire', 'adopt', 'advance', 'agree', 'boost', 'capital resource',
    'capitalize', 'change', 'commitment', 'complete', 'configure', 'design', 'develop', 'enhance', 'expand',
    'expenditure', 'expense', 'implement', 'improve', 'increase', 'initiate', 'install',
    'integrate', 'invest', 'lease', 'modernize', 'modify', 'move', 'obtain', 'plan', 'project',
    'purchase', 'replace', 'spend', 'upgrade', 'use'
]

def preprocess_text_no_lemmatization(text):
    tokens = re.findall(r'\b\w+\b', text.lower())  
    return tokens

def calculate_final_frequency(row, search_phrases, key_phrases):
    text = row.lower()
    tokens = preprocess_text_no_lemmatization(text) 
    search_phrases = [phrase.lower() for phrase in search_phrases]  
    key_phrases = [phrase.lower() for phrase in key_phrases] 

    all_matches = set()
    token_len = len(tokens)
    
    for idx, token in enumerate(tokens):
        if any(token.startswith(key) for key in key_phrases):  
            window_start = max(0, idx - 10)
            window_end = min(token_len, idx + 10 + 1)
            window_tokens = tokens[window_start:window_end]
            window_text = &quot; &quot;.join(window_tokens)  

            for phrase in search_phrases:
                if phrase in window_text:
                    all_matches.add(phrase)  
    return len(all_matches)

df['Frequency'] = df['ITEM7'].apply(lambda x: calculate_final_frequency(x, search_words, key_words))

print(df)
</code></pre>
<p>Which returns</p>
<pre><code>                                               ITEM7  Frequency
0  Last year, from AVG we have acquired Alibaba S...          6
1  We are planning to install end-point security,...          2
</code></pre>
"
Keyword/phrase extraction from free text using NLTK and Python for structured queries,"<p>I want to interpret specific keywords from a free text such as &quot;I want to order boiled eggs and spinach soup from nearest restaurants' and use them to search content from my database.
For e.g i want to extract following specific keyword(s) and want an output similar to following like a tuple of (Keyword type , Keyword value).
e.g ('Food Item','Boiled Eggs') , ('Food Item','Spinach Soup') , ('Location','Nearest Restaurants') etc.</p>
<p>Need to use these values and types to further refine and to query my tables in database.</p>
<p>I was trying hard to find answer using various techniques using python and NLTK but need help to point me to right direction. Am i using the right techniques / frameworks?</p>
","python, nlp, nltk, information-retrieval",
NLP - Specific Text Extraction,"<p>I have to identify the country name from a random text. I have the country list.</p>
<p>I am struggling to find a solution that can train the model on the country list and when I provide a random text to that model as an input, it identifies the country name as an output.</p>
<p>eg:-</p>
<ul>
<li>&quot;I live in India&quot; will give &quot;India&quot;</li>
<li>&quot;London is the capital of United Kingdom&quot; will give &quot;United Kingdom&quot;</li>
</ul>
","nlp, text-extraction",
What does surface form mean in relation extraction?,"<p>The term &quot;entity surface form&quot; is repeatedly mentioned in most of relation extraction papers. What does it mean?</p>
<p>For example, in the REBEL paper, the author mentions that &quot;a relation is considered correct only if the head and tail entity surface forms are correctly extracted.&quot;</p>
<p>What is meant by &quot;head&quot; and &quot;tail&quot;?</p>
","machine-learning, nlp, relation-extraction",
How to decide correct NLP approach for a project,"<p>I'm working on an NLP project. My task is to determine the category and Sentiment Score of Turkish Call Center conversations from the conversations themselves. We are using Python as our programming language. However, I'm stuck in the initial stages of the project among hundreds of alternative solutions. I have 300,000 rows of Customer Representative and Customer Text data, and I have cleaned them nicely through the preprocessing stage. All are currently in a sentence tokenized form and have been through other standard preprocessing stages. Customer representative and customer conversations are ready in different columns in a ~600 MB csv. Before deciding on modeling algorithms, my manager expects me to prepare the training dataset. The dataset should contain the following information, and we will decide later which ones are necessary and which ones are unnecessary, and then we will finalize the dataset:</p>
<ol>
<li>Word vectors should be extracted</li>
<li>Sentence vectors should be extracted</li>
<li>Summaries of the conversations should be extracted, and sentence and word vectors of these summaries should be extracted</li>
<li>NER counts should be extracted</li>
<li>POS counts should be extracted</li>
<li>Morphological analysis should be extracted, and affixes, conjunctions, etc. should be shown numerically</li>
<li>Sentiment score should be extracted in terms of both subjectivity and polarity</li>
<li>Keywords and their vectors should be extracted</li>
<li>Topic modeling should be done, and which topic each conversation is closest to should be added to the dataset</li>
<li>Similarity scores between summary and main conversations should be extracted</li>
<li>The rarity ratio of the words mentioned in a conversation should be extracted and the average should be taken sentence by sentence (We think it will give an idea about how rich the sentence is in terms of meaning)</li>
</ol>
<p>The problems I'm facing are as follows:</p>
<ol>
<li><p>What is the best word vector extraction library for the Turkish language? There are techniques like Word2vec, NGram, GloVe, etc. There are also relatively newer techniques like Fasttext. BERT is also an option. Which one should I choose? How will I compare their performance? Should I train my own model, or should I prefer pre-trained models? (E.g., Fasttext has models trained on Turkish language) Which one is superior or more current to which? Which article or research should I base on, if they all used a different technique?</p>
</li>
<li><p>Gensim seems like a library with tools that provide solutions to a lot of NLP problems. It's such a big library that I couldn't even grasp its full capabilities. Should I just proceed using Gensim, or should I use different tools together? Will it meet all my needs? How will I know?</p>
</li>
<li><p>There are a lot of tools doing lemmatizing, these tools are also vectorizing, since I will also do lemmatizing, should I use their vectorization features, or should I benefit from the most mentioned vectorization tools above? Which one gives the best result? I've read a lot of comparison articles, they all talk about different results.</p>
</li>
<li><p>There are SBERT models trained on Turkish for extracting sentence vectors. Will the vectors I will get when I use SBERT become meaningless when I extract word vectors with another tool? After all, I will extract these with different methods and they will be in the same dataset.</p>
</li>
</ol>
<p>Due to the ambiguity of the superiority of such alternative solutions to each other, I'm confused.</p>
<p>How should an NLP project be conducted correctly?</p>
","machine-learning, text, nlp",
NLP software for classification of large datasets,"<p>For years I've been using my own Bayesian-like methods to categorize new items from external sources based on a large and continually updated training dataset.</p>
<p>There are three types of categorization done for each item:</p>
<ol>
<li>30 categories, where each item must belong to one category, and at most two categories.</li>
<li>10 other categories, where each item is only associated with a category if there is a strong match, and each item can belong to as many categories as match.</li>
<li>4 other categories, where each item must belong to only one category, and if there isn't a strong match the item is assigned to a default category.</li>
</ol>
<p>Each item consists of English text of around 2,000 characters.  In my training dataset there are about 265,000 items, which contain a rough estimate of 10,000,000 features (unique three word phrases).</p>
<p>My homebrew methods have been fairly successful, but definitely have room for improvement.  I've read the NLTK book's chapter &quot;Learning to Classify Text&quot;, which was great and gave me a good overview of NLP classification techniques.  I'd like to be able to experiment with different methods and parameters until I get the best classification results possible for my data.</p>
<h2>The Question</h2>
<p>What off-the-shelf NLP tools are available that can efficiently classify such a large dataset?</p>
<p>Those I've tried so far:</p>
<ul>
<li>NLTK</li>
<li>TIMBL</li>
</ul>
<p>I tried to train them with a dataset that consisted of less than 1% of the available training data: 1,700 items, 375,000 features.  For NLTK I used a sparse binary format, and a similarly compact format for TIMBL.</p>
<p>Both seemed to rely on doing everything in memory, and quickly consumed all system memory.  I can get them to work with tiny datasets, but nothing large.  I suspect that if I tried incrementally adding the training data the same problem would occur either then or when doing the actual classification.</p>
<p>I've looked at Google's Prediction API, which seem to do much of what I'm looking for but not everything.  I'd also like to avoid relying on an external service if possible.</p>
<p>About the choice of features: in testing with my homebrew methods over the years, three word phrases produced by far the best results.  Although I could reduce the number of features by using words or two word phrases, that would most likely produce inferior results and would still be a large number of features.</p>
","machine-learning, nlp",
How to extract specific entities from unstructured text,"<p>Given a generic text sentence (in a specific context) how can I extract word/entities of interest belonging to a specific &quot;category&quot; using python and any NLP library?</p>
<p>For example given a step for a culinary recipe <code>Add an onion to a bowl of carrots</code> as input text, I'd like to retrive <code>onion</code> and <code>carrots</code> while given <code>Sprinkle with paprika.</code> should return <code>paprika</code>.
But this should also work with sentences like <code>stir well, and cook an additional minute.</code> that do not contain any food entity in them.</p>
<p>So far, what I was able to achieve is using the <code>spacy</code> library for training a NER module to parse sentences. The problem with the NER pipeline is that it is a rule-based parsing, it is trained providing a set of sentences and entities/matches/labels to learn, which works fine as expeted on sentences similar to the one used during train, but performs bad on new and different sentences:</p>
<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load('trained_model')

document = nlp('Add flour, mustard, and salt')
[(ent.text, ent.label_) for ent in document.ents]
# &gt;&gt; [('Add flour', 'FOOD'), ('mustard', 'FOOD'), ('salt', 'FOOD')]
# (quite) correct output

document = nlp('I took a building, car and squirrel on the weekend')
[(ent.text, ent.label_) for ent in document.ents]
# &gt;&gt; [('building', 'FOOD'), ('car', 'FOOD'), ('squirrel', 'FOOD')]
# wrong output

document = nlp('stir well, and cook an additional minute.')
[(ent.text, ent.label_) for ent in document.ents]
# &gt;&gt; [('stir well', 'FOOD'), ('cook', 'FOOD'), ('additional minute.', 'FOOD')]
# wrong output
</code></pre>
<p>I am aware that there are several similar questions and posts, but I have found only solutions working for &quot;semi-structured&quot; text, i.e. list of ingredients as <code>1 tsp. of sugar, 1 cup of milk, ...</code> which can be easily solved using the previous rule-based approach. Also <code>nltk</code> and part-of-speech (POS) are an option, but I'd prefer an alternative solution rather than having to compare each noun with an exhaustive list of foods.</p>
<p>What instead I am looking for is a way of to extract specific entities or at least to classify words in generic text with additional categories beyond those of the basic parsing.
Which methods should I use/look at to achieve this?</p>
","python, machine-learning, nlp, nltk, spacy",
"Why does moving ML model initialization into a function prevent GPU OOM errors when del, gc.collect(), and torch.cuda.empty_cache() fail?","<pre class=""lang-py prettyprint-override""><code>for model_name in model_list:
    model = LLM(model_name, trust_remote_code=True)
    results = evaluate_model(model, task)
    del model
    gc.collect()
    torch.cuda.empty_cache()
</code></pre>
<p>Despite explicitly deleting the model object, calling gc.collect(), and clearing the CUDA cache with torch.cuda.empty_cache(), I still encountered GPU out-of-memory (OOM) errors.</p>
<p>After experimenting, I moved the model instantiation into a separate function, like so:</p>
<pre class=""lang-py prettyprint-override""><code>def do_lm_eval(model_name: str, task: str) -&gt; dict:
    gc.collect()
    torch.cuda.empty_cache()
    model = LLM(model_name, trust_remote_code=True)
    results = evaluate_model(model, task)
    del model
    gc.collect()
    torch.cuda.empty_cache()
    return results

for model_name in model_list:
    results = do_lm_eval(model_name, task)
</code></pre>
<p>Surprisingly, this resolved the OOM errors completely. My questions are:</p>
<ul>
<li>Why does moving the model instantiation into its own function prevent GPU OOM errors, even though I was already using del, gc.collect(), and torch.cuda.empty_cache() in the original code?</li>
<li>Is there something about Python’s memory management or PyTorch's interaction with CUDA that makes scoping within a function more effective?</li>
<li>Are there additional best practices for managing memory when iterating over large models on GPU?</li>
</ul>
<p>Additional context:</p>
<ul>
<li>The issue occurs when evaluating multiple checkpoints of a large model.</li>
<li>I suspect memory fragmentation or lingering references may play a role, but gc.collect() doesn't seem to fix it.</li>
<li>Using a dedicated function appears to clean up resources more effectively, but I'm unsure why.</li>
</ul>
<hr />
<p>Note:</p>
<p>I didn't try this:</p>
<pre class=""lang-py prettyprint-override""><code>INFO 12-05 13:11:12 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing gpu_memory_utilization or switching to eager mode. You can also reduce the max_num_seqs as needed to decrease memory usage.
</code></pre>
<p>error</p>
<pre><code>ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
</code></pre>
<hr />
<p>Full code:</p>
<pre class=""lang-py prettyprint-override""><code># using lm-harness/eval: https://chatgpt.com/c/67477267-a740-8001-894a-c5f22b24cc5f
from socket import gethostname
import os
import gc
import torch
from vllm import LLM, SamplingParams
from lm_eval import evaluator, tasks
from lm_eval.api.model import LM
import lm_eval.evaluator 
from os.path import expanduser
import fire
import wandb
from datetime import datetime
from typing import List, Tuple

_STOP_TOKENS: list[str] = [&quot;Solution:&quot;, &quot;Problem:&quot;, &quot;Question:&quot;, &quot;USER:&quot;, &quot;USER:&quot;, &quot;USER&quot;, &quot;ASSISTANT:&quot;, &quot;ASSISTANT&quot;, &quot;Instruction:&quot;, &quot;Instruction&quot;, &quot;Response:&quot;, &quot;Response&quot;]
print(f'Original stop toks I had once: {len(_STOP_TOKENS)=} {_STOP_TOKENS=}')

#     raw_str_2_train_str = lambda examples : {'text': [f'problem: {prob}\n\nsolution: {sol}' for prob, sol in zip(examples['problem'], examples['solution'])]}
#     new, v2, no trian yet raw_str_2_train_str = lambda examples : {'text': [f'Problem:\n{prob}\n\nSolution:\n{sol}' for prob, sol in zip(examples['problem'], examples['solution'])]}
STOP_TOKENS: list[str] = [&quot;problem:&quot;, &quot;problem: &quot;, &quot;problem:\n&quot;, &quot;Problem:&quot;, &quot;Problem: &quot;, &quot;Problem:\n&quot;, &quot;Question:&quot;, &quot;USER:&quot;, &quot;USER&quot;]
print(f'New stop tokens: {len(STOP_TOKENS)=} {STOP_TOKENS=}')

class MyLM(LM):
    def __init__(self, model, batch_size: int = 16):
        self.model = model
        super().__init__()
        self.batch_size = batch_size

    def loglikelihood(self, requests: List) -&gt; List[Tuple[float, bool]]:
        results = []
        for request in requests:
            context, continuation = request.args()
            logprob = self.model.compute_loglikelihood(context, continuation)
            isgreedy = self.model.is_greedy(context, continuation)
            results.append((logprob, isgreedy))
        return results

    def loglikelihood_rolling(self, requests: List) -&gt; List[float]:
        results = []
        for request in requests:
            context, = request.args()
            logprob = self.model.compute_rolling_loglikelihood(context)
            results.append(logprob)
        return results

    def generate_until(self, requests: List) -&gt; List[str]:
        print(f'--- {self.generate_until=}')
        # params = SamplingParams(temperature=0.9, top_p=0.95, max_tokens=2048, stop='Problem:\n')
        # params = SamplingParams(temperature=0.9, top_p=0.95, max_tokens=2048, stop=STOP_TOKENS)
        # params = SamplingParams(temperature=0.9, top_p=0.95, max_tokens=512, stop=STOP_TOKENS)
        params = SamplingParams(temperature=0, top_p=1, max_tokens=512, stop=STOP_TOKENS)
        prompts: List[str] = [request.args[0] for request in requests]
        outputs: list = self.model.generate(prompts, params)
        results: list[str] = [output.outputs[0].text for output in outputs]
        # print prompt result, what is going on during eval?
        assert len(prompts) == len(results), f'Fatal error: {wandb.run.alert(title=&quot;error during eval&quot;, text=&quot;len(prompts) != len(results)&quot;)}'
        for req_idx, (prompt, result) in enumerate(zip(prompts, results)):
            print('--' * 40 + 'start of problem --&gt; model response')
            print(f'Request index: {req_idx}')
            print(f'{&quot;--&quot;*20}\nPrompt:\n{prompt}\n')
            print(f'{&quot;--&quot;*20}\nResult:\n{result}\n')
        print(f'--- {self.generate_until=}')
        return results

def do_lm_eval(model_name: str, task: str, kwargs: dict) -&gt; dict: 
    gc.collect()
    torch.cuda.empty_cache()
    
    model = LLM(model_name, trust_remote_code=True)
    lm_obj = MyLM(model=model)
    task_manager = tasks.TaskManager()
    results = lm_eval.evaluator.simple_evaluate(
        model=lm_obj,
        tasks=[task],
        task_manager=task_manager,
        write_out=False,
        limit=kwargs.get('limit', 1), # limit the number of examples, if &lt;1 then interpreted as %, default None --&gt; all task benchmark
        random_seed=None,
        numpy_random_seed=None,
        torch_random_seed=None,
        fewshot_random_seed=None
    )

    del model
    del lm_obj
    gc.collect()
    torch.cuda.empty_cache()
    return results

def main(**kwargs):
    gc.collect()
    torch.cuda.empty_cache()

    print(f'{&quot;__&quot;*32} Start of eval main: {main=}')
    print(f'{STOP_TOKENS=}')
    # task = kwargs.get('task', 'putnam_axiom_original')
    # task = kwargs.get('task', 'putnam_axiom_variations')
    task = kwargs.get('task', 'putnam_axiom_53')
    print(f'{task=}')
    directory_path = expanduser(&quot;~/data/runs_logic_cont/run_2024_m11_d20_t21h_22m_22s&quot;) # putnam-axiom 53 train 20 epochs
    directory_path = expanduser(&quot;~/data/runs_logic_cont/run_2024_m11_d23_t21h_55m_07s&quot;) # putnam-axiom 53 train 100 epochs
    directory_path = expanduser(&quot;~/data/runs_logic_cont/run_2024_m12_d05_t12h_07m_56s&quot;)
    # directory_path = expanduser(kwargs.get('model_name_or_path', 'google/gemma-2-2b'))
    print(f'{directory_path=}')
    # Extract and sort the model_list by checkpoint index
    model_list = sorted(
        [
            os.path.join(directory_path, entry)
            for entry in os.listdir(directory_path)
            if os.path.isdir(os.path.join(directory_path, entry)) and entry.startswith(&quot;checkpoint-&quot;)
        ],
        key=lambda x: int(x.split('/')[-1].split('-')[-1])  # Extract the checkpoint index for sorting
    )
    print(f'{model_list}')
    print(&quot;\n&quot;.join(model_list))
    print(f'{len(model_list)=} (should be same as the expect steps/epochs +1 roughly)')
    # since we might be running multiple evals at once, the next time we run an eval we add it to the config, since wandb doesn't let you update the config if a key already has a value
    if task in wandb.config:
        next_task: list = wandb.config['task'].append(task)
        wandb.config.update({&quot;model_list&quot;: model_list, 'task': next_task}, allow_val_change=True)
    else:
        wandb.config.update({&quot;model_list&quot;: model_list, 'task': [task]}, allow_val_change=True)
    print(f'{wandb.config=}')
    # - Start eval run
    wandb.run.define_metric(f&quot;{task}/eval_bench/accuracy&quot;, 
                            step_metric=f&quot;{task}/eval_bench/checkpoint_idx&quot;)
    wandb.run.define_metric(f&quot;{task}/eval_bench/checkpoint_idx&quot;) 
    model_2_accuracy = {model_name: None for model_name in model_list}
    print(f'{model_2_accuracy}')
    accs: list[float] = []
    for model_name in model_list:
        torch.cuda.empty_cache()
        gc.collect()

        print(f&quot;{'=' * 100}&quot;)
        print(f'running model with name: {model_name}')
        print(f&quot;{'=' * 100}&quot;)

        # model = LLM(model_name, trust_remote_code=True)
        # lm_obj = MyLM(model=model)
        # task_manager = tasks.TaskManager()
        # results = lm_eval.evaluator.simple_evaluate(
        #     model=lm_obj,
        #     tasks=[task],
        #     task_manager=task_manager,
        #     write_out=False,
        #     limit=kwargs.get('limit', 1), # limit the number of examples, if &lt;1 then interpreted as %, default None --&gt; all task benchmark
        #     random_seed=None,
        #     numpy_random_seed=None,
        #     torch_random_seed=None,
        #     fewshot_random_seed=None
        # )
        results = do_lm_eval(model_name, task, kwargs)
        print(f&quot;arguments: {results['samples'][task][0]['arguments'][0][0]=}&quot;)
        print(f&quot;resps: {results['samples'][task][0]['resps'][0][0]=}&quot;)
        print(f'{results.keys()=}')
        print(f'{results[&quot;results&quot;][task].keys()=}')

        print(&quot;results:&quot;, results[&quot;results&quot;][task].get(&quot;exact_match,none&quot;, None))
        accuracy = results[&quot;results&quot;][task].get(&quot;exact_match,none&quot;, None) 
        model_2_accuracy[model_name] = accuracy
        accs.append(float(accuracy))
        checkpoint_idx = int(model_name.split('/')[-1].split('-')[-1])  # e.g., 'checkpoint-70'
        
        # Log accuracy for each checkpoint
        print(f'Checkpoint idx: {checkpoint_idx=}')
        print(f&quot;Accuracy for that checkpoint: {accuracy=} ({checkpoint_idx=})&quot;)
        print(f&quot;Checkpoint full name: {model_name=}&quot;)
        wandb.log({f&quot;{task}/eval_bench/checkpoint_idx&quot;: checkpoint_idx, 
                   f&quot;{task}/eval_bench/accuracy&quot;: accuracy})
        # del model
        # del lm_obj
        gc.collect()
        torch.cuda.empty_cache()
        # gc.collect()
        # torch.cuda.empty_cache()
    print(f'{model_2_accuracy=}\n{accs=}')
    print(f&quot;{'=' * 100}&quot;)
    return {'model_2_accuracy': model_2_accuracy, 'accs': accs}

def _main(**kwargs):
    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss')
    run_name = f'{today}'
    run = wandb.init(
        # mode=kwargs.get('mode', 'dryrun'), 
        mode=kwargs.get('mode', 'online'), 
        project=&quot;putnam-axiom&quot;, 
        name=run_name, 
        save_code=True, 
        config=kwargs | {'hostname': gethostname()}
    )
    kwargs = kwargs | {'today': today}
    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)
    print(&quot;--&gt; WARNING/REMINDER: cude device harcoded in script!\n&quot;*10)
    main(**kwargs)
    print(f'{run.get_url()=}')
    wandb.finish()

if __name__ == &quot;__main__&quot;:
    import time
    start_time = time.time()
    fire.Fire(_main)
    elapsed_time = time.time() - start_time
    print(f&quot;Time taken: {elapsed_time:.2f} seconds, or {elapsed_time / 60:.2f} minutes, or {elapsed_time / 3600:.2f} hours.\a&quot;)
</code></pre>
<hr />
<p>Note I've had to do this trick too for training scripts too, full script:</p>
<pre class=""lang-py prettyprint-override""><code>from datetime import datetime
from typing import Optional
import random
import torch
from transformers import PushToHubCallback
from transformers import get_cosine_schedule_with_warmup
from trl import SFTConfig, SFTTrainer
import os
import fire
import wandb
import sys

from train.callbacks import GenCallbackWithHFGenerate
from train.data import load_math_style_dataset, print_first_example_after_decode
import train.models

from train.utils import seed_everything

def main(**config):
    # -- Seed everything
    seed_everything(seed=config.get('seed', 0))
    
    # -- HF login
    from huggingface_hub import login
    token = open(os.path.expanduser(&quot;~/keys/master_hf_token.txt&quot;)).read().strip()
    login(token=token)

    # -- Get model
    model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'google/gemma-2-2b')) 
    # model, tok = train.models.load_mdl_and_tok(config.get('pretrained_model_name_or_path', 'meta-llama/Llama-3.1-8B')) 

    # -- Load datasets
    ds_name_or_path = config.get('ds_name_or_path', 'Putnam-AXIOM/putnam-axiom-dataset')
    train_split, val_split = config.get('train_split', 'func_original_53_10_30_2024'), config.get('val_split', 'func_variations_265_11_23_2024')
    print(f'\n---&gt; {ds_name_or_path=} {train_split=} {val_split=}\n')
    train_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=1, split=train_split)
    print_first_example_after_decode(train_dataset, tok)
    # eval_dataset = load_math_style_dataset(ds_name_or_path, tok, config.get('max_seq_length', 512), end=15, split=val_split)
    eval_dataset = train_dataset
    print(f'{len(train_dataset)=}\n{len(eval_dataset)=}')
    wandb.config.update({'dataset': f'{ds_name_or_path} ({train_split=} {val_split=})'})

    # -- Prepare output directory
    today: str = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss')
    output_dir: str = os.path.expanduser(f&quot;~/data/runs_logic_cont/run_{config.get('today', today)}&quot;)
    print(f'{output_dir=}')
    
    # Save the initial model and tokenizer as checkpoint-0
    initial_checkpoint_dir = os.path.join(output_dir, &quot;checkpoint-0&quot;)
    os.makedirs(initial_checkpoint_dir, exist_ok=True)
    print(f&quot;Saving initial checkpoint and tokenizer at {initial_checkpoint_dir}&quot;)
    model.save_pretrained(initial_checkpoint_dir)
    tok.save_pretrained(initial_checkpoint_dir)

    # -- Train model
    # max_steps = 50  # Limit fine-tuning to a few steps
    # os.environ['CUDA_VISIBLE_DEVICES'] = str(random.randint(0, 7))
    # config = {'max_steps': 2, 'eval_steps': 1, 'logging_steps': 1, 
    #           'save_strategy': 'steps', 'save_steps': 1, 'eval_strategy': 'steps'}
    # config = config | {'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES', 'maybe 0')}
    training_args = SFTConfig(
        max_steps=config.get('max_steps', 30),
        # --
        output_dir=output_dir,
        bf16=torch.cuda.is_bf16_supported(),
        fp16=not torch.cuda.is_bf16_supported(),
        # -- logging opts
        save_steps=config.get('save_steps', 5), 
        save_strategy=config.get('save_strategy', 'steps'),
        eval_on_start=config.get('eval_on_start', True),
        evaluation_strategy=config.get('eval_strategy', 'steps'), 
        eval_steps=config.get('eval_steps', 1), 
        logging_first_step=config.get('logging_first_step', True), # Default to False, unsure 100% what this does but looks like a good idea
        logging_strategy=config.get('logging_strategy', 'steps'),
        logging_steps=config.get('logging_steps', 1),
        # --
        num_train_epochs=config.get('num_train_epochs', 10),
        max_seq_length=config.get('max_seq_length', 512),
        per_device_train_batch_size=config.get('batch_size', 2),
        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 2),
    )
    # Calculate Total Steps
    steps_per_epoch = (len(train_dataset) // training_args.per_device_train_batch_size) // training_args.gradient_accumulation_steps
    total_steps = steps_per_epoch * training_args.num_train_epochs
    print(f'{steps_per_epoch=}')

    # Optimizer and Scheduler
    # optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 1e-4}]
    optimizer_grouped_parameters = [{'params': [p for p in model.parameters()], 'weight_decay': 0}]
    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=config.get('learning_rate', 1e-5))

    # Add Cosine Learning Rate Scheduler
    # warmup_steps = int(0.01 * total_steps)  # Warm-up for 1% of total steps
    warmup_steps = 0
    scheduler = get_cosine_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps,
    )
    scheduler = None
    print(f'{total_steps=} {warmup_steps=}')
    trainer = SFTTrainer(
        model=model,
        tokenizer=tok,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        args=training_args,
        optimizers=(optimizer, scheduler),
        callbacks=[GenCallbackWithHFGenerate(model, tok)]
    )
    print(f&quot;\nStarting fine-tuning...&quot;)
    trainer.train()
    # - end run
    return os.path.expanduser(output_dir)

def run_eval_logic_contamination(output_dir: str):
    &quot;&quot;&quot;
    Runs the eval_logic_contamination.py script with the specified output directory.

    Args:
        output_dir (str): The directory where the model is saved, expanded using `os.path.expanduser`.
    &quot;&quot;&quot;
    import gc
    torch.cuda.empty_cache()
    gc.collect()
    output_dir = os.path.expanduser(output_dir)  # Ensure `output_dir` is expanded 
    from eval_logic_contamination import main
    task='putnam_axiom_53'
    res: dict = main(model_name_or_path=output_dir, task=task)
    print(f'Results for {task=}: {res}')
    print(res)
    # task='putnam_axiom_53' # for debugging
    task='putnam_axiom_variations'
    res: dict = main(model_name_or_path=output_dir, task=task)
    print(f'Results for {task=}: {res}')
    print(res)
    # wandb.run.define_metric(&quot;eval/accuracy&quot;, step_metric=&quot;eval/checkpoint_idx&quot;)
    # wandb.run.define_metric(&quot;eval/checkpoint_idx&quot;) 
    # for idx, acc in [(10,5), (20,10), (30,15)]:
    #     wandb.log({'eval/accuracy': acc, 'eval/checkpoint_idx': idx})

def _main(**kwargs):
    from datetime import datetime
    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'
    run_name = f'{today}' 
    kwargs = kwargs | {'today': today}
    # run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)
    run = wandb.init(mode=kwargs.get('mode', 'online'), project=&quot;putnam-axiom&quot;, name=run_name, save_code=True, config=kwargs)
    # wandb.run.log_code(f&quot;./{os.path.basename(__file__)}&quot;) # maybe logscode immediately # ref: https://stackoverflow.com/questions/79256112/how-to-log-only-the-current-script-file-to-wb-code-panel-immediately
    # wandb.config.update()
    os.environ['CUDA_VISIBLE_DEVICES'] = str(6)
    print(&quot;--&gt; WARNING/REMINDER: cude device harcoded in script!\n&quot;*10)
    output_dir = main(**kwargs)
    run_eval_logic_contamination(output_dir)
    # from train.utils import copy_to_dfs
    # copy_to_dfs(output_dir)
    run.alert(title=&quot;Run Completed&quot;, text=f&quot;Run finished, run url: {run.get_url()}&quot;)
    print(f'{run.get_url()=}')
    wandb.finish()

if __name__ == &quot;__main__&quot;:
    import time
    start_time = time.time()
    fire.Fire(_main)
    print(f&quot;Time taken: {time.time() - start_time:.2f} seconds, or {(time.time() - start_time) / 60:.2f} minutes, or {(time.time() - start_time) / 3600:.2f} hours.\a&quot;)
</code></pre>
<p>Cross: <a href=""https://discuss.huggingface.co/t/why-does-moving-ml-model-initialization-into-a-function-prevent-gpu-oom-errors-when-del-gc-collect-and-torch-cuda-empty-cache-fail/129487"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/why-does-moving-ml-model-initialization-into-a-function-prevent-gpu-oom-errors-when-del-gc-collect-and-torch-cuda-empty-cache-fail/129487</a></p>
","python-3.x, nlp, garbage-collection, huggingface-transformers, vllm",
Output probabilities of tokens generated by Llama 2 using Transformers,"<p>Given input tokens, LLMs output the tokens in their vocabulary that have the highest probability of coming after the input tokens.</p>
<p>I would like to print the probability of each token generated by the model in response to a prompt to see how confident the model is in its generated tokens. I would like to do this on Llama-2-7b-chat-hf on a simple prompt like : &quot;Could you give me 3 cities located in Europe ?&quot;.</p>
<p>In order to do this I have the following code :</p>
<pre><code>from transformers import LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer
import torch
import numpy as np

device = &quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;

model=LlamaForCausalLM.from_pretrained(&quot;Llama-2-7b-chat-hf&quot;).to(device)
tokenizer= LlamaTokenizer.from_pretrained(&quot;Llama-2-7b-chat-hf&quot;)

prompt = &quot;Could you give me 3 cities located in Europe ?&quot;

inputs = tokenizer([prompt], return_tensors=&quot;pt&quot;).to(device)

outputs=model.generate(**inputs,return_dict_in_generate=True, output_scores=True,max_new_tokens=75)

transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)

input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]
generated_tokens = outputs.sequences[:,input_length:]

for tok, score in zip(generated_tokens[0], transition_scores[0]):
        # | token | token string | logits | probability
            print(f&quot;| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}&quot;)
</code></pre>
<p>As a result I receive the following :</p>
<pre><code>| token | token string | logits | probability
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
| 10605 | Here     | -3.0267 | 4.85%
|   526 | are      | 0.0000 | 100.00%
| 29871 |          | -0.3506 | 70.43%
| 29941 | 3        | 0.0000 | 100.00%
| 14368 | cities   | 0.0000 | 100.00%
|  5982 | located  | 0.0000 | 100.00%
|   297 | in       | 0.0000 | 100.00%
|  4092 | Europe   | 0.0000 | 100.00%
| 29901 | :        | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
| 29896 | 1        | 0.0000 | 100.00%
| 29889 | .        | 0.0000 | 100.00%
|  3681 | Paris    | 0.0000 | 100.00%
| 29892 | ,        | 0.0000 | 100.00%
|  3444 | France   | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
| 29906 | 2        | 0.0000 | 100.00%
| 29889 | .        | 0.0000 | 100.00%
|  9184 | Rome     | -1.0413 | 35.30%
| 29892 | ,        | 0.0000 | 100.00%
| 12730 | Italy    | 0.0000 | 100.00%
|    13 | &lt;0x0A&gt;   | 0.0000 | 100.00%
| 29941 | 3        | 0.0000 | 100.00%
| 29889 | .        | 0.0000 | 100.00%
|  4517 | London   | 0.0000 | 100.00%
| 29892 | ,        | 0.0000 | 100.00%
|  3303 | United   | 0.0000 | 100.00%
| 12626 | Kingdom  | 0.0000 | 100.00%
|     2 | &lt;/s&gt;     | 0.0000 | 100.00%
</code></pre>
<p>As you can see, most of the words have a probability of 100% of being chosen which seems very odd to me. Llama 2 has a vocabulary of 32000 tokens surely there are other tokens that could be used at the place of those tokens, I would agree with something like 70% but 100% should be impossible. Which makes me believe something is wrong in my code.</p>
<p>Would you agree with me ? And if yes, would you know what is wrong in my code ?</p>
","nlp, huggingface-transformers, large-language-model, llama",
Installing SpaCy in Fedora 41,"<p>I am getting into troubles as I try to install spaCy in a Fedora 41, AMD Ryzen machine. I got this:</p>
<p>Preparing metadata (pyproject.toml): finished with status 'error'
error: subprocess-exited-with-error</p>
<pre><code>          × Preparing metadata (pyproject.toml) did not run successfully.
          │ exit code: 1
          ╰─&gt; [22 lines of output]
              + /home/herlimenezes/AmbientesVirtuais/PNL-env/bin/python /tmp/pip-install-oviwb629/numpy_b484673443964b1592ca24f081a16466/vendored-meson/meson/meson.py setup /tmp/pip-install-oviwb629/numpy_b484673443964b1592ca24f081a16466 /tmp/pip-install-oviwb629/numpy_b484673443964b1592ca24f081a16466/.mesonpy-2yo3onu6 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=/tmp/pip-install-oviwb629/numpy_b484673443964b1592ca24f081a16466/.mesonpy-2yo3onu6/meson-python-native-file.ini
              The Meson build system
              Version: 1.4.99
              Source dir: /tmp/pip-install-oviwb629/numpy_b484673443964b1592ca24f081a16466
              Build dir: /tmp/pip-install-oviwb629/numpy_b484673443964b1592ca24f081a16466/.mesonpy-2yo3onu6
</code></pre>
<p>How to get rid of this?</p>
","python, nlp, spacy, fedora",
How to get Bigram/Trigram of word from prelisted unigram from a document corpus / dataframe column,"<p>I have a dataframe with text in one of its columns.</p>
<p>I have listed some predefined keywords which I need for analysis and words associated with it (and later make a wordcloud and counter of occurrences) to understand topics /context associated with such keywords.</p>
<p>Use case:</p>
<pre><code>df.text_column()

keywordlist = [coca , food, soft, aerated, soda]
</code></pre>
<p>lets say one of the rows of the text column has text : <code>' coca cola is expanding its business in soft drinks and aerated water'</code>.</p>
<p>another entry like : <code>'lime soda is the best selling item in fast food stores'</code></p>
<p>my objective is to get Bigram/trigram like:</p>
<pre><code>'coca_cola','coca_cola_expanding', 'soft_drinks', 'aerated_water', 'business_soft_drinks', 'lime_soda', 'food_stores'
</code></pre>
<p>Kindly help me to do that [Python only]</p>
","python, nlp, nltk","<p>First, you can optioanlly load the nltk's <strong>stop word list</strong> and remove any stop words from the text (such as &quot;is&quot;, &quot;its&quot;, &quot;in&quot;, and &quot;and&quot;). Alternatively, you can define your own stop words list, as well as even extend the nltk's list with additional words. Following, you can use <code>nltk.bigrams()</code> and <code>nltk.trigrams()</code> methods to get bigrams and trigrams joined with an underscore <code>_</code>, as you asked. Also, have a look at <a href=""https://www.nltk.org/howto/collocations.html"" rel=""nofollow noreferrer"">Collocations</a>.</p>
<p><strong>Edit:</strong>
If you haven't already, you need to include the following once in your code, in order to download the stop words list.</p>
<pre><code>nltk.download('stopwords')
</code></pre>
<p>Code:</p>
<pre><code>import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

word_data = &quot;coca cola is expanding its business in soft drinks and aerated water&quot;
#word_data = &quot;lime soda is the best selling item in fast food stores&quot;

# load nltk's stop word list
stop_words = list(stopwords.words('english'))
# extend the stop words list
#stop_words.extend([&quot;best&quot;, &quot;selling&quot;, &quot;item&quot;, &quot;fast&quot;])

# tokenize the string and remove stop words
word_tokens = word_tokenize(word_data)
clean_word_data = [w for w in word_tokens if not w.lower() in stop_words]
    
# get bigrams
bigrams_list = [&quot;_&quot;.join(item) for item in nltk.bigrams(clean_word_data)]
print(bigrams_list)

# get trigrams 
trigrams_list = [&quot;_&quot;.join(item) for item in nltk.trigrams(clean_word_data)]
print(trigrams_list)
</code></pre>
<h2>Update</h2>
<p>Once you get the bigram and trigram lists, you can check for matches against your keyword list to keep only the relevant ones.</p>
<pre><code>keywordlist = ['coca' , 'food', 'soft', 'aerated', 'soda']

def find_matches(n_grams_list):
    matches = []
    for k in keywordlist:
        matching_list = [s for s in n_grams_list if k in s]
        [matches.append(m) for m in matching_list if m not in matches]
    return matches

all_matching_bigrams = find_matches(bigrams_list) # find all mathcing bigrams  
all_matching_trigrams = find_matches(trigrams_list) # find all mathcing trigrams

# join the two lists
all_matches = all_matching_bigrams + all_matching_trigrams
print(all_matches)
</code></pre>
<p>Output:</p>
<pre><code>['coca_cola', 'business_soft', 'soft_drinks', 'drinks_aerated', 'aerated_water', 'coca_cola_expanding', 'expanding_business_soft', 'business_soft_drinks', 'soft_drinks_aerated', 'drinks_aerated_water']
</code></pre>
"
TfidfVectorizer seems to be giving incorrect results,"<ul>
<li>I have a list of length 7 (7 subjectes)</li>
<li>Each element in the list contains a long string of words.</li>
<li>Each element of the list can be viewed as a topic with a long sentence that sets it apart</li>
<li>I want to check which words make each topic unique (each element in the list)</li>
</ul>
<p>Heres my code:</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
train = read_train_file() # A list with huge sentences that I can't paste here

tfidfvectorizer = TfidfVectorizer(analyzer= 'word', stop_words= 'english')
tfidf_wm        = tfidfvectorizer.fit_transform(train)
tfidf_tokens    = tfidfvectorizer.get_feature_names()

df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(), index=train_df.discourse_type.unique(), columns = tfidf_tokens)


for col in df_tfidfvect.T.columns:    
    print(f&quot;\nsubjetct: {col}&quot;)
    print(df_tfidfvect.T[col].nlargest(2))
</code></pre>
<p>Part of the train data:</p>
<pre><code>for i, v in enumerate(train):
    print(f&quot;subject: {i}: {train[i][:50]}&quot;)

subject: 0: like policy people average cant play sports b poin
subject: 1: also stupid idea sports suppose fun privilege play
subject: 2: failing fail class see act higher c person could g
subject: 3: unfair rule thought think new thing shaped land fo
subject: 4: land form found human thought many either fight de
subject: 5: want say know trying keep class also quite expensi
subject: 6: even less sense saying first find something really
</code></pre>
<p>Output:</p>
<pre><code>subjetct: Position
people    0.316126
school    0.211516
Name: Position, dtype: float64

subjetct: Claim
people    0.354722
school    0.296632
Name: Claim, dtype: float64

subjetct: Evidence
people    0.366234
school    0.282213
Name: Evidence, dtype: float64

subjetct: Concluding Statement
people    0.385200
help      0.267567
Name: Concluding Statement, dtype: float64

subjetct: Lead
people    0.399011
school    0.336605
Name: Lead, dtype: float64

subjetct: Counterclaim
people       0.361070
electoral    0.321909
Name: Counterclaim, dtype: float64

subjetct: Rebuttal
people    0.31029
school    0.26789
Name: Rebuttal, dtype: float64
</code></pre>
<p>As you can see, <code>people</code> and <code>school</code> have high tf-idf values.</p>
<p>Maybe I'm wrong, but I was expecting words that specialize in a topic won't be the same words in all topics (according to TF-IDF formula ).</p>
<p>So what is wrong with <code>TfidfVectorizer</code> ?</p>
","python, scikit-learn, nlp, tf-idf, tfidfvectorizer",
Error in getting Captum text explanations for text classification,"<p>I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset</p>
<pre><code>import pandas as pd
import torch
from torch.utils.data import DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.metrics import accuracy_score
from captum.attr import IntegratedGradients

# Loading data
train_df = pd.read_csv('train_dataset.csv')
test_df = pd.read_csv('test_dataset.csv')

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def preprocess_data(df, tokenizer, max_len=128):
    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=&quot;pt&quot;)
    labels = torch.tensor(df['label'].values)
    return inputs, labels

train_inputs, train_labels = preprocess_data(train_df, tokenizer)
test_inputs, test_labels = preprocess_data(test_df, tokenizer)

# DataLoader
train_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

test_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)
test_loader = DataLoader(test_dataset, batch_size=16)

# Model setup
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)

# Optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Training Loop
model.train()
for epoch in range(3):  # Train for 3 epochs
    for batch in train_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
    print(f&quot;Epoch {epoch+1} loss: {loss.item()}&quot;)

# Evaluation
model.eval()
correct_predictions = []
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)
        correct_predictions.extend(
            (preds == labels).cpu().numpy().tolist()
        )
accuracy = accuracy_score(test_labels.numpy(), correct_predictions)
print(f&quot;Test Accuracy: {accuracy:.2f}&quot;)

# Integrated Gradients
ig = IntegratedGradients(model)

def get_influential_words(input_text, model, tokenizer, ig, device):
    model.eval()
    # Tokenizing the input text
    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)
    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor
    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor

    print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)
    print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)
    # forward function for IG
    def forward_func(input_ids):
        outputs = model(input_ids, attention_mask=attention_mask)
        return outputs.logits

    # Applying Integrated Gradients
    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()

    return list(zip(tokens, token_importances))

# Analysing influential words for correctly predicted texts
for idx, correct in enumerate(correct_predictions):
    if correct:
        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)
        print(influential_words)
</code></pre>
<p>But I am getting the following error in running the above.</p>
<pre><code>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Epoch 1 loss: 0.4719192385673523
Epoch 2 loss: 0.39585667848587036
Epoch 3 loss: 0.14659778773784637
Test Accuracy: 0.70
Input IDs shape: torch.Size([1, 8]) dtype: torch.int64
Attention mask shape: torch.Size([1, 8]) dtype: torch.int64
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-9-f047b509c98d&gt; in &lt;cell line: 90&gt;()
     90 for idx, correct in enumerate(correct_predictions):
     91     if correct:
---&gt; 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
     93         print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)
     94         print(influential_words)

18 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2549         # remove once script supports set_grad_enabled
   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2552 
   2553 

RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)
</code></pre>
","machine-learning, pytorch, nlp, huggingface-transformers, text-classification","<p>You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.</p>
<p>I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial <a href=""https://captum.ai/tutorials/Bert_SQUAD_Interpret"" rel=""nofollow noreferrer"">https://captum.ai/tutorials/Bert_SQUAD_Interpret</a></p>
<p>Below please find snippet that works:</p>
<pre><code>from captum.attr import LayerIntegratedGradients


def custom_forward(inputs):
    preds = predict(inputs)
    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)
lig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)
def get_influential_words(input_text, model, tokenizer, ig, device):
    model.eval()
    # Tokenizing the input text
    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=128)
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    # print(&quot;Input IDs shape:&quot;, input_ids.shape, &quot;dtype:&quot;, input_ids.dtype)
    # print(&quot;Attention mask shape:&quot;, attention_mask.shape, &quot;dtype:&quot;, attention_mask.dtype)

    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)
    
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()

    return list(zip(tokens, token_importances))

results = []

for idx, correct in enumerate(correct_predictions):
    if correct:
        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
        print(f&quot;Influential words for text: {test_df['text'].iloc[idx]}&quot;)
        print(influential_words)
</code></pre>
"
euclidian distance from word to sentence after doing Vectorizer,"<p>I have dataframe with 1000 text rows.</p>
<p>I did TfidfVectorizer.</p>
<p>Now  I want to create a new field which give me the distance from  each sentence to the word that i want, lets say the word &quot;king&quot;. df['king']</p>
<p>I thought about taking in each sentence the 5 closet words to the word king and make average of them.</p>
<p>I will glad to know how to do that or to hear about another method.</p>
","pandas, dataframe, nlp, text-classification, tf-idf","<p>I am not convinced that the Euclidean distance would be the optimal measure. I would actually look at similarity scores:</p>
<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

data = {
    'text': [
        &quot;The king sat on the throne with wisdom.&quot;,
        &quot;A queen ruled the kingdom alongside the king.&quot;,
        &quot;Knights were loyal to their king.&quot;,
        &quot;The empire prospered under the rule of a wise monarch.&quot;
    ]
}
df = pd.DataFrame(data)

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(df['text'])

try:
    king_vector = tfidf.transform([&quot;king&quot;]).toarray()
except KeyError:
    print(&quot;The word 'king' is not in the vocabulary.&quot;)
    king_vector = np.zeros((1, tfidf_matrix.shape[1]))

similarities = cosine_similarity(tfidf_matrix, king_vector).flatten()

feature_names = np.array(tfidf.get_feature_names_out())

def get_top_n_words(row_vector, top_n=5):
    indices = row_vector.argsort()[::-1][:top_n]
    return feature_names[indices]

averages = []
for i in range(tfidf_matrix.shape[0]):
    sentence_vector = tfidf_matrix[i].toarray().flatten()
    top_words = get_top_n_words(sentence_vector)
    top_similarities = [cosine_similarity(tfidf.transform([word]), king_vector).flatten()[0] for word in top_words]
    averages.append(np.mean(top_similarities))

df['king_similarity'] = similarities
df['avg_closest_similarity'] = averages

print(df)
</code></pre>
<p>which would give you</p>
<pre><code>                                                text  king_similarity  \
0            The king sat on the throne with wisdom.         0.240614   
1      A queen ruled the kingdom alongside the king.         0.259779   
2                  Knights were loyal to their king.         0.274487   
3  The empire prospered under the rule of a wise ...         0.000000   

   avg_closest_similarity  
0                     0.0  
1                     0.0  
2                     0.0  
3                     0.0  
</code></pre>
<p>That being said, if you absolutely want to focus on Euclidean distance, here is a method:</p>
<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from scipy.spatial.distance import euclidean

data = {
    'text': [
        &quot;The king sat on the throne with wisdom.&quot;,
        &quot;A queen ruled the kingdom alongside the king.&quot;,
        &quot;Knights were loyal to their king.&quot;,
        &quot;The empire prospered under the rule of a wise monarch.&quot;
    ]
}
df = pd.DataFrame(data)

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(df['text']).toarray()

feature_names = tfidf.get_feature_names_out()
if &quot;king&quot; in feature_names:
    king_index = np.where(feature_names == &quot;king&quot;)[0][0]
    king_vector = np.zeros_like(tfidf_matrix[0])
    king_vector[king_index] = 1
else:
    print(&quot;The word 'king' is not in the vocabulary.&quot;)
    king_vector = np.zeros_like(tfidf_matrix[0])

df['king_distance'] = [euclidean(sentence_vector, king_vector) for sentence_vector in tfidf_matrix]

print(df)

</code></pre>
<p>which gives</p>
<pre><code>                                                text  king_distance
0            The king sat on the throne with wisdom.       1.232385
1      A queen ruled the kingdom alongside the king.       1.216734
2                  Knights were loyal to their king.       1.204586
3  The empire prospered under the rule of a wise ...       1.414214
</code></pre>
"
Llama-3.2-1B-Instruct generate inconsistent output,"<p>I want to use <code>Llama-3.2-1B-Instruct</code> model, and although I have set <code>&quot;temperature&quot;: 0.0, &quot;top_p&quot;:0.0 and &quot;top_k&quot;:0</code>, it still generates inconsistent output. This is how my pipeline looks like:</p>
<pre><code>pipe = pipeline(
    &quot;text-generation&quot;,
    model=model_id,
    torch_dtype=torch.bfloat16,
    device_map=&quot;mps&quot;,
        model_kwargs={&quot;temperature&quot;: 0.0,
                  &quot;do_sample&quot;:True,
                              &quot;top_p&quot;:0.0,
                              &quot;top_k&quot;:0,},
)
</code></pre>
<p>Any idea how to solve this issue?</p>
","python, nlp, huggingface-transformers, large-language-model","<p>The model inconsistent output can be due to two main factors:</p>
<p><strong>1. Temperature:</strong></p>
<p>setting temperature to zero give more inconsistent result. You can refer <a href=""https://community.openai.com/t/why-the-api-output-is-inconsistent-even-after-the-temperature-is-set-to-0/329541/2"" rel=""nofollow noreferrer"">Opeani discussion page</a> for detail.</p>
<p>So the best option is to set temperature to very low values such as 0.00001 instead of zero.</p>
<p><strong>2. do_sample</strong></p>
<p>You already set it false, and it should remain that way only.</p>
"
Split Sentences at Bullets and Numbering?,"<p>I am trying to input text into my word processor to be split into sentences first and then into words.</p>

<p>An example paragraph:</p>

<pre><code>When the blow was repeated,together with an admonition in
childish sentences, he turned over upon his back, and held his paws in a peculiar manner.

1) This a numbered sentence
2) This is the second numbered sentence

At the same time with his ears and his eyes he offered a small prayer to the child.

Below are the examples
- This an example of bullet point sentence
- This is also an example of bullet point sentence

</code></pre>

<p>I tried the following codes</p>

<pre><code>from nltk.tokenize import TweetTokenizer, sent_tokenize

tokenizer_words = TweetTokenizer()
tokens_sentences = [tokenizer_words.tokenize(t) for t in 
nltk.sent_tokenize(input_text)]
print(tokens_sentences)
</code></pre>

<pre><code>import nltk

sent_text = nltk.sent_tokenize(text) # this gives us a list of sentences
# now loop over each sentence and tokenize it separately
for sentence in sent_text:
    tokenized_text = nltk.word_tokenize(sentence)
    print(tokenized_text)
</code></pre>

<p><strong>The Output I got</strong></p>

<pre><code>[
['When', 'the', 'blow', 'was', 'repeated', ',', 'together', 'with', 'an', 'admonition', 'in', 'childish', 'sentences', ',', 'he', 'turned', 'over', 'upon', 'his', 'back', ',', 'and', 'held', 'his', 'paws', 'in', 'a', 'peculiar', 'manner', '.'], 
['1', ')', 'This', 'a', 'numbered', 'sentence', '2', ')', 'This', 'is', 'the', 'second', 'numbered', 'sentence','At', 'the', 'same', 'time', 'with', 'his', 'ears', 'and', 'his', 'eyes', 'he', 'offered', 'a', 'small', 'prayer', 'to', 'the', 'child', '.']
['Below', 'are', 'the', 'examples', '-', 'This', 'an', 'example', 'of', 'bullet', 'point', 'sentence',
'-', 'This', 'also','an', 'example', 'of', 'bullet', 'point', 'sentence']
]
</code></pre>

<p><strong>Required Output</strong></p>

<pre><code>[
['When', 'the', 'blow', 'was', 'repeated', ',', 'together', 'with', 'an', 'admonition', 'in', 'childish', 'sentences', ',', 'he', 'turned', 'over', 'upon', 'his', 'back', ',', 'and', 'held', 'his', 'paws', 'in', 'a', 'peculiar', 'manner', '.'], 
['1', ')', 'This', 'a', 'numbered', 'sentence']
['2', ')', 'This', 'is', 'the', 'second', 'numbered', 'sentence']
['At', 'the', 'same', 'time', 'with', 'his', 'ears', 'and', 'his', 'eyes', 'he', 'offered', 'a', 'small', 'prayer', 'to', 'the', 'child', '.']
['Below', 'are', 'the', 'examples']
['-', 'This', 'an', 'example', 'of', 'bullet', 'point', 'sentence']
['-', 'This', 'also','an', 'example', 'of', 'bullet', 'point', 'sentence']
]
</code></pre>

<p><strong>How to split the sentence at Bullets and Numbering?</strong></p>

<p>Solutions on spaCy  also would be very helpful</p>
","python, nlp, nltk, sentence",
Split text into sentences by and/or,"<p>Text String:</p>
<pre><code>text = ‘Turn left and take the door between stairs and elevator. Turn right to the corridor.’
</code></pre>
<p>Desire Output:</p>
<pre><code>splitted_sentences= [‘turn left’, ‘take the door between stairs and elevator’, ‘turn right to the corridor’]
</code></pre>
<p>How can we split this text into sentences as shown in the splitted_sentences list by Python?</p>
","python, text, split, nlp, sentence",
Normalization of token embeddings in BERT encoder blocks,"<p>Following the multi-headed attention layer in a BERT encoder block, is layer normalization done separately on the embedding of each token (i.e., one mean and variance per token embedding), or on the concatenated vector of all token embeddings (the same mean and variance for all embeddings)?</p>
","nlp, normalization, bert-language-model, attention-model","<p>I tracked down full details of layer normalization (LN) in BERT <a href=""https://stackoverflow.com/questions/79231978/why-do-layernorm-layers-in-bert-base-have-768-and-not-512-weight-and-bias-para"">here</a>.</p>
<p>Mean and variance are computed per token. But the weight and bias parameters learned in LN are not per token - it's per embedding dimension.</p>
"
How to parse a resume with few shot method using the specified models from HuggingFace and Langchain?,"<h1>Model selection confusion and some errors while trying to parse a resume with the following codes</h1>
<ul>
<li>Trying to do a few shot prompting with a google flan t5 base model</li>
<li>While Doing so I am getting an error
<code>ERROR:Service.service:Error parsing resume: &quot;'ContactInformation'&quot;</code></li>
<li>exception as <code>&quot;detail&quot;: &quot;Failed to parse the file: An error occurred in parsed_resume: \&quot;'ContactInformation'\&quot;&quot;</code></li>
</ul>
<h2>The code is given below</h2>
<pre class=""lang-none prettyprint-override""><code>from typing import List, Optional, Union, Any, re
import json



class ContactInformation(BaseModel):
    Name: Optional[str] = None
    Email: Optional[str] = None
    Contact: Optional[str] = None
    Links: Optional[List[str]] = None



class Experience(BaseModel):
    title: Optional[str] = None
    company: Optional[str] = None
    duration: Optional[str] = None


class Education(BaseModel):
    course: Optional[str] = None
    branch: Optional[str] = None
    institute: Optional[str] = None


class Projects(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    link: Optional[str] = None

class OutputFormat(BaseModel):
    ContactInformation: Optional[Any] = None
    AboutMe: Optional[Any] = None
    Experiences: Optional[List[Any]] = None
    Educations: Optional[List[Any]] = None
    Skills: Optional[List[Any]] = None
    Certificates: Optional[List[Any]] = None
    Projects: Optional[List[Any]] = None
    Achievements: Optional[List[Any]] = None
    Volunteer: Optional[List[Any]] = None

</code></pre>
<pre class=""lang-none prettyprint-override""><code>    def __init__(self, model_name=model_1, fine_tune_model_path: str = None):
        # Initialize LLM service with specified model


        if fine_tune_model_path:
            # Load fine-tuned model from local directory
            self.tokenizer = AutoTokenizer.from_pretrained(fine_tune_model_path)
            self.model = AutoModel.from_pretrained(fine_tune_model_path)
        else:
            # Load base model
            self.llm_service = HuggingFaceHub(
                repo_id=&quot;google/flan-t5-base&quot;,
                huggingfacehub_api_token=huggingface_api_key,
                model_kwargs={
                    &quot;temperature&quot;: 0.5,
                    &quot;max_new_tokens&quot;: 200
                }  # Model parameters for consistent output
            )
    def parsed_resume(self, resume_txt: str):
        df = pd.read_csv(r&quot;C:\Users\Sarthak\PycharmProjects\JobAxle\Service\data\for_model_resume_dataset.csv&quot;)
        print(df['prompt'][0])
        examples = [
            {'prompt':df['prompt'][0], &quot;completion&quot;:df['completion'][0]},
            {'prompt': df['prompt'][1], &quot;completion&quot;: df['completion'][1]}
        ]
        print('Examples:',examples[0])
        example_formatter_template = &quot;&quot;&quot;
        {prompt}
        {completion}\n
        &quot;&quot;&quot;
        example_prompt = PromptTemplate(
            input_variables=[&quot;prompt&quot;, &quot;completion&quot;],
            template=example_formatter_template,
        )
        parser = PydanticOutputParser(pydantic_object=OutputFormat)
        few_shot_prompt_template = FewShotPromptTemplate(
            examples=examples,
            example_prompt=example_prompt,
            suffix=&quot;&quot;&quot;
                Parse the given resume text, ensuring the output in JSON format:
        
                Resume:
                {resume}
        
                {format_instructions}
        
                Output as JSON below:
                completion:&quot;&quot;&quot;,
            input_variables=[&quot;resume&quot;],
            example_separator=&quot;\n&quot;,
            partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}
        )
        print(&quot;Few-Shot Prompt Template with Examples and JSON Instructions:\n&quot;, few_shot_prompt_template)

        prompt_template = PromptTemplate(
            input_variables=['resume'],
            template=Prompt_2
        )
        # print(few_shot_prompt_template)
        # Initialize the LLM chain
        chain = LLMChain(
            llm=self.llm_service,
            prompt=few_shot_prompt_template,
            verbose=True
        )
        print(&quot;Chain:&quot;, chain)
        print(resume_txt)
        try:
            response = chain.invoke({'resume': resume_txt}, verbose=True)
            print(response)
            logger.info('Model Response: %s', response)
            print(&quot;Type of Response:&quot;,type(response))
            # Invoke the chain and get a response
            response_json = self.process_response(response)
            parsed_json = self.structure_response(response_json)
            return OutputFormat(**parsed_json)  # Return as OutputFormat object

        except Exception as e:
            logger.error(&quot;Error parsing resume: %s&quot;, e)
            raise Exception(f&quot;An error occurred in parsed_resume: {e}&quot;)

    def process_response(self, response_text: str) -&gt; Dict:
        &quot;&quot;&quot;Process LLM response into JSON format.&quot;&quot;&quot;
        response_json = json.dumps(response_text).strip()

        # Remove extraneous characters
        if response_json.startswith(&quot;```json&quot;):
            response_json = response_json[len(&quot;```json&quot;):].strip()
        if response_json.endswith(&quot;```&quot;):
            response_json = response_json[:-len(&quot;```&quot;)].strip()

        response_json = remove_trailing_commas(response_json)

        try:

            return json.loads(response_json)
        except json.JSONDecodeError as e:
            logger.error(&quot;JSON decoding error: %s. Response text: %s&quot;, e, response_json)
            raise ValueError(&quot;Failed to parse response as valid JSON&quot;)

    def structure_response(self, parsed_json: Dict) -&gt; Dict:
        &quot;&quot;&quot;
        Structure response JSON to match the OutputFormat schema, ensuring lists for 'Experiences' and 'Educations'.
        &quot;&quot;&quot;
        # Ensure ContactInformation, Experiences, and Educations are present in the expected structure
        parsed_json[&quot;ContactInformation&quot;] = parsed_json.get(&quot;ContactInformation&quot;, {})
        if isinstance(parsed_json.get(&quot;Experiences&quot;), dict):
            parsed_json[&quot;Experiences&quot;] = [parsed_json[&quot;Experiences&quot;]]
        else:
            parsed_json[&quot;Experiences&quot;] = parsed_json.get(&quot;Experiences&quot;, [])

        if isinstance(parsed_json.get(&quot;Educations&quot;), dict):
            parsed_json[&quot;Educations&quot;] = [parsed_json[&quot;Educations&quot;]]
        else:
            parsed_json[&quot;Educations&quot;] = parsed_json.get(&quot;Educations&quot;, [])

        parsed_json[&quot;Projects&quot;] = parsed_json.get(&quot;Projects&quot;, [])

        return parsed_json
</code></pre>
<p>The df contains prompt, completion feature with resume_text as a prompt and json output as completion.</p>
<p>Or Is there any Alternative to doing the task on a low-end specs laptop? I am in need of help because I can't access big parameter models. anybody??</p>
","nlp, prompt, langchain, large-language-model, huggingface",
how can i fuse embeddings in a manner such that it increase efficiency and score?,"<p>I've been working on a problem where the goal is to supplement traditional embeddings with LLM-generated embeddings (I'm using the last_hidden_state for this purpose). So far, I've tried simply concatenating them and using a cross-attention mechanism. While concatenating the embeddings yields similar results to using traditional embeddings alone (and the score is definitely not better), the cross-attention mechanism unexpectedly degraded the performance. Are there other methods that could potentially improve the score? Code is provided below:</p>
<p>Code for Simple Concatenation:</p>
<pre class=""lang-py prettyprint-override""><code>    def forward(self, depot_xy, node_xy_demand_tw, llm_embeddings):
        moe_loss = 0
        
        # Get traditional embeddings
        if isinstance(self.embedding_depot, MoE) or isinstance(self.embedding_node, MoE):
            embedded_depot, loss_depot = self.embedding_depot(depot_xy)
            embedded_node, loss_node = self.embedding_node(node_xy_demand_tw)
            moe_loss = moe_loss + loss_depot + loss_node
        else:
            embedded_depot = self.embedding_depot(depot_xy)
            embedded_node = self.embedding_node(node_xy_demand_tw)

        # Project LLM embeddings and normalize
        # print(320, self.llm_projection[0].weight.dtype, llm_embeddings.dtype)
        projected_llm = self.llm_projection(llm_embeddings)
        projected_llm = self.layer_norm(projected_llm)
        
        # Combine traditional embeddings with LLM embeddings
        depot_combined = embedded_depot + projected_llm[:, :1, :]  # For depot
        node_combined = embedded_node + projected_llm[:, 1:, :]   # For nodes
        
        out = torch.cat((depot_combined, node_combined), dim=1)

        for layer in self.layers:
            out, loss = layer(out)
            moe_loss = moe_loss + loss

        return out, moe_loss
</code></pre>
<p>Code for Cross attention based fusion with Cross attention class:</p>
<pre class=""lang-py prettyprint-override""><code>########################################
# CROSS ATTENTION
########################################

class CrossAttentionFusion(nn.Module):
    def __init__(self, embedding_dim, head_num, qkv_dim):
        super().__init__()
        self.head_num = head_num
        
        # Cross attention layers for traditional -&gt; LLM
        self.Wq_trad = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False).to(dtype=torch.bfloat16)
        self.Wk_llm = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False).to(dtype=torch.bfloat16)
        self.Wv_llm = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False).to(dtype=torch.bfloat16)
        
        # Cross attention layers for LLM -&gt; traditional
        self.Wq_llm = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False).to(dtype=torch.bfloat16)
        self.Wk_trad = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False).to(dtype=torch.bfloat16)
        self.Wv_trad = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False).to(dtype=torch.bfloat16)
        
        # Output projections
        self.W_out_trad = nn.Linear(head_num * qkv_dim, embedding_dim).to(dtype=torch.bfloat16)
        self.W_out_llm = nn.Linear(head_num * qkv_dim, embedding_dim).to(dtype=torch.bfloat16)
        
        # Layer norms
        self.norm_trad = nn.LayerNorm(embedding_dim).to(dtype=torch.bfloat16)
        self.norm_llm = nn.LayerNorm(embedding_dim).to(dtype=torch.bfloat16)

    def forward(self, trad_emb, llm_emb):
        # Cross attention: traditional -&gt; LLM
        # print(f&quot;trad_emb dtype: {trad_emb.dtype}, shape: {trad_emb.shape}&quot;)
        # print(f&quot;llm_emb dtype: {llm_emb.dtype}, shape: {llm_emb.shape}&quot;)
        # print(f&quot;Wq_trad type: {self.Wq_trad.weight.dtype}, shape: {self.Wq_trad.weight.shape}&quot;)
        q_trad = reshape_by_heads(self.Wq_trad(trad_emb), self.head_num)
        k_llm = reshape_by_heads(self.Wk_llm(llm_emb), self.head_num)
        v_llm = reshape_by_heads(self.Wv_llm(llm_emb), self.head_num)
        
        trad_attends_llm = multi_head_attention(q_trad, k_llm, v_llm)
        trad_fused = self.W_out_trad(trad_attends_llm)
        trad_out = self.norm_trad(trad_emb + trad_fused)
        
        # Cross attention: LLM -&gt; traditional
        q_llm = reshape_by_heads(self.Wq_llm(llm_emb), self.head_num)
        k_trad = reshape_by_heads(self.Wk_trad(trad_emb), self.head_num)
        v_trad = reshape_by_heads(self.Wv_trad(trad_emb), self.head_num)
        
        llm_attends_trad = multi_head_attention(q_llm, k_trad, v_trad)
        llm_fused = self.W_out_llm(llm_attends_trad)
        llm_out = self.norm_llm(llm_emb + llm_fused)
        
        # Combine the cross-attended features
        fused_embeddings = trad_out + llm_out
        
        return fused_embeddings


########################################
# ENCODER
########################################

class MTL_Encoder(nn.Module):
    def __init__(self, **model_params):
        super().__init__()
        self.model_params = model_params
        embedding_dim = self.model_params['embedding_dim']
        hidden_dim = self.model_params['ff_hidden_dim']
        encoder_layer_num = self.model_params['encoder_layer_num']
        head_num = self.model_params['head_num']
        qkv_dim = self.model_params['qkv_dim']
        llama_hidden_size = 4096  # Llama-2 7B hidden size
        
        # Project Llama embeddings to the model's embedding dimension with dtype torch.bfloat16
        self.llm_projection = nn.Sequential(
            nn.Linear(llama_hidden_size, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embedding_dim)
        ).to(dtype=torch.bfloat16)

        
        # Add layer normalization for better embedding fusion
        self.layer_norm = nn.LayerNorm(embedding_dim).to(dtype=torch.bfloat16)
        self.layer_norm_trad = nn.LayerNorm(embedding_dim).to(dtype=torch.bfloat16)

        if self.model_params['num_experts'] &gt; 1 and &quot;Raw&quot; in self.model_params['expert_loc']:
            self.embedding_depot = MoE(input_size=2, output_size=embedding_dim, 
                                     num_experts=self.model_params['num_experts'],
                                     k=self.model_params['topk'], T=1.0, 
                                     noisy_gating=True, 
                                     routing_level=self.model_params['routing_level'],
                                     routing_method=self.model_params['routing_method'], 
                                     moe_model=&quot;Linear&quot;)
            self.embedding_node = MoE(input_size=5, output_size=embedding_dim, 
                                    num_experts=self.model_params['num_experts'],
                                    k=self.model_params['topk'], T=1.0, 
                                    noisy_gating=True, 
                                    routing_level=self.model_params['routing_level'],
                                    routing_method=self.model_params['routing_method'], 
                                    moe_model=&quot;Linear&quot;)
        else:
            self.embedding_depot = nn.Linear(2, embedding_dim)
            self.embedding_node = nn.Linear(5, embedding_dim)

        # Cross-attention fusion module
        self.cross_attention_fusion = CrossAttentionFusion(
            embedding_dim=embedding_dim,
            head_num=head_num,
            qkv_dim=qkv_dim
        )
        
        self.layers = nn.ModuleList([EncoderLayer(i, **model_params) 
                                   for i in range(encoder_layer_num)])

    def forward(self, depot_xy, node_xy_demand_tw, llm_embeddings):
        moe_loss = 0
        
        # Get traditional embeddings
        if isinstance(self.embedding_depot, MoE) or isinstance(self.embedding_node, MoE):
            embedded_depot, loss_depot = self.embedding_depot(depot_xy)
            embedded_node, loss_node = self.embedding_node(node_xy_demand_tw)
            moe_loss = moe_loss + loss_depot + loss_node
        else:
            embedded_depot = self.embedding_depot(depot_xy)
            embedded_node = self.embedding_node(node_xy_demand_tw)
        
        # Combine depot and node embeddings
        traditional_embeddings = torch.cat((embedded_depot, embedded_node), dim=1).to(dtype=torch.bfloat16)
        
        # Project and normalize LLM embeddings
        projected_llm = self.llm_projection(llm_embeddings)
        projected_llm = self.layer_norm(projected_llm)
        
        # Normalize traditional embeddings
        traditional_embeddings = self.layer_norm_trad(traditional_embeddings)
        
        # Apply cross-attention fusion
        fused_embeddings = self.cross_attention_fusion(
            traditional_embeddings,
            projected_llm
        )
        
        # Pass through encoder layers
        out = fused_embeddings
        for layer in self.layers:
            out, loss = layer(out)
            moe_loss = moe_loss + loss

        return out, moe_loss
</code></pre>
<p>and following is how I'm getting the LLM embeddings,</p>
<pre class=""lang-py prettyprint-override""><code>                with torch.no_grad():
                    outputs = self.llama(**inputs)
                    # Use the last hidden state's [CLS] token
                    new_embeddings = outputs.hidden_states[-1][:, 0, :]
</code></pre>
<p>Am I doing something wrong? Or Do the traditional embeddings even need ones generated by LLM?</p>
","python, machine-learning, nlp, large-language-model, vehicle-routing",
How to recognize if string is human name?,"<p>So I have some text data that's been messily parsed, and due to that I get names mixed in with the actual data. Is there any kind of package/library that helps identify whether a word is a name or not? (In this case, I would be assuming US/western/euro-centric names)</p>
<p>Otherwise, what would be a good way to flag this? Maybe train a model on a corpus of names and assign each word in the dataset a classification? Just not sure the best way to approach this problem/what kind of model would be suited, or if a solution already exists</p>
","python, nlp","<pre><code>import nltk
from nltk.tag.stanford import NERTagger
st = NERTagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')
text = &quot;&quot;&quot;YOUR TEXT GOES HERE&quot;&quot;&quot;

for sent in nltk.sent_tokenize(text):
    tokens = nltk.tokenize.word_tokenize(sent)
    tags = st.tag(tokens)
    for tag in tags:
        if tag[1]=='PERSON': print(tag)
</code></pre>
<p>via <a href=""https://stackoverflow.com/questions/20290870/improving-the-extraction-of-human-names-with-nltk"">Improving the extraction of human names with nltk</a></p>
"
"NER versus LLM to extract name, gender, role and company from text","<p>I need to extract the name, gender, job title and employer/company name from newspaper articles, running the process on local hardware (no Cloud allowed) due to copyright reasons.</p>
<p>I've been playing around with Llama 3.1 but I'm finding I don't get useable results with the models smaller than 70B parameters, and at that size the models run much too slowly on the best hardware I have to throw at them.</p>
<p>Is there another, smaller LLM that might be good at this while using fewer processing resources?</p>
<p>Is there is NER I can use to extract all that data? The NERs I've looked into extract name but not gender. (I don't know if they extract the other data because gender is a showstopper for me.)</p>
<p>Alternatively, is there an approach I can take where I do a first pass with a NER, and then pass the names through an LLM together with the original newspaper article to extract the other data, and get better results, faster than a single LLM pass?</p>
<p>Or if the answer is I should be training some model, what is a good model for me to use as my starting point? I'm very much at the beginning of my machine learning journey and would love to be pointed in the right direction.</p>
<p>Thanks in advance!</p>
","nlp, large-language-model, named-entity-recognition","<p>Apart from your limitations, I wouldn't recommend using LLMs like Llamma 3.1 for such a task. <code>NER</code> is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal. You can use <code>NLTK</code> or <code>SpaCy</code> for this matter. My personal choice is <code>SpaCy</code>, however a <code>gender</code> as you defined is not a known named entity. you can see a list of named entities in <a href=""https://github.com/explosion/spaCy/discussions/9147"" rel=""nofollow noreferrer"">this doc</a>.</p>
<p>I guess what you mean by <code>gender</code> is the possible <code>gender</code> associated with the names of a <code>PERSON</code> mentioned in your articles. There are a few python packages that you can use to lookup genders, however, you should note that this can be very ambiguous and there should be a substantial tolerance for error. You can use <a href=""https://pypi.org/project/gender-guesser/"" rel=""nofollow noreferrer""><code>gender-guesser</code> package</a>.</p>
<p>A possible solution would be like this:</p>
<pre><code>import spacy
import gender_guesser.detector as gender


nlp = spacy.load(&quot;en_core_web_sm&quot;)

def extract_info(text):
    doc = nlp(text)
    gender_detector = gender.Detector()

    for ent in doc.ents:
        if ent.label_ == &quot;PERSON&quot;:
            name = ent.text
            name_gender = gender_detector.get_gender(name)
    
    return doc.ents, name_gender
</code></pre>
<p>Note that <code>en_core_web_sm</code> is the small model available via spaCy, you can use the large model by specifying <code>en_core_web_lg</code>, just make sure that the model is downloaded before running your code. here's how you can download the model:</p>
<pre><code>python -m spacy download en_core_web_sm
</code></pre>
"
Get attention masks from HF pipelines,"<p>How should returned attention masks be accessed from the FeatureExtractionPipeline in Huggingface?</p>
<p>The code below takes an embedding model, distributes it and a huggingface dataset across 8 GPUs on a single node, and performs inference on the inputs. The code requires the attention masks for mean pooling.</p>
<p>Code example:</p>
<pre class=""lang-py prettyprint-override""><code>from accelerate import Accelerator
from accelerate.utils import tqdm
from transformers import AutoTokenizer, AutoModel
from optimum.bettertransformer import BetterTransformer

import torch

from datasets import load_dataset

from transformers import pipeline

accelerator = Accelerator()

model_name = &quot;BAAI/bge-large-en-v1.5&quot;

tokenizer = AutoTokenizer.from_pretrained(model_name,)

model = AutoModel.from_pretrained(model_name,)

pipe = pipeline(
    &quot;feature-extraction&quot;,
    model=model,
    tokenizer=tokenizer,
    max_length=512,
    truncation=True,
    padding=True,
    pad_to_max_length=True,
    batch_size=256,
    framework=&quot;pt&quot;,
    return_tensors=True,
    return_attention_mask=True,
    device=(accelerator.device)
)

dataset = load_dataset(
    &quot;wikitext&quot;,
    &quot;wikitext-2-v1&quot;,
    split=&quot;train&quot;,
)

#Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


# Assume 8 processes

with accelerator.split_between_processes(dataset[&quot;text&quot;]) as data:

    for out in pipe(data):

        sentence_embeddings = mean_pooling(out, out[&quot;attention_mask&quot;])
</code></pre>
<p>I need the attention maks from pipe to use for mean pooling.</p>
<p>Best,</p>
<p>Enrico</p>
","nlp, huggingface-transformers, huggingface, huggingface-tokenizers, accelerate",
CBOW and Skip-Gram learning perspective,"<p>I want to know if I am having a correct understanding of those methods</p>
<p>In CBOW we use nearby words to predict the target word whereas in skip gram we use target word to predict the nearby words,so does that mean that in CBOW we are creating embedding based on what words come often together like &quot;I&quot; will have &quot;am&quot; closely mapped in the dimensional space ,whereas embedding in Skip gram it is more about the meaning of that word like what &quot;am&quot; is used for . Am I correct?</p>
","nlp, word-embedding",
Montreal Forced Aligner(MFA) taking too much time(almost 18 days still going on) to train a 33 GB corpus,"<p>WE are  using Montreal Forced Aligner (MFA) 3.x to train an acoustic model on a large dataset (~33GB of audio and transcripts in an Indian language). The training process takes an extremely long time(almost 18 days still going on) to complete.</p>
<p>Here are the details of my setup:</p>
<ul>
<li><p><strong>Hardware:</strong> [16 vCPU, 16 GB RAM]</p>
</li>
<li><p><strong>Audio Format:</strong> WAV files</p>
</li>
<li><p><strong>Corpus Details:</strong> 33GB.</p>
</li>
<li><p><strong>Command Used:</strong> <code>mfa train</code> with default parameters.</p>
</li>
<li><p><strong>Version:</strong> MFA 3.x</p>
</li>
</ul>
<p>Can we reduce the training time? Can we predict a ballpark time for the total time taken for training?</p>
","python, nlp, speech-recognition, kaldi, phoneme",
Memory increasing after hugging face generate method,"<p>I wanted to make an inference with codegemma model from huggingface, but when I use model.generate(**inputs) method GPU memory cost increases from 39 GB to 49 GB in peak usage with torch profiler no matter max_token_len number. I understand that we need to save activations of model on inference and context of like 4096 input tokens but I can’t believe that it can increase inference memory usage on 10 GB. Can someone explain me how could it be?
<a href=""https://i.sstatic.net/51GUIlVH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/51GUIlVH.png"" alt=""enter image description here"" /></a></p>
","machine-learning, pytorch, nlp, artificial-intelligence, huggingface-transformers",
Encoder Decoder Transformer model generate a repetitive token as output in text summarization,"<p>I implemented a transformer Encoder Decoder (Bert2Bert) for text summarization task. In train phase train loss decreases but in prediction phase it generate a repetitive token as output for example [2,2,2,2,.....]. what should I do?</p>
<pre><code>class BERT2BERT(nn.Module):
    def __init__(self, encoder_model_name='HooshvareLab/bert-base-parsbert-uncased', decoder_model_name='HooshvareLab/bert-base-parsbert-uncased',tokenizer=None,vocab_size=0):
        super(BERT2BERT, self).__init__()
        self.vocab_size=vocab_size
        # Encoder: ParsBERT transformer layers
        self.encoder = BertModel.from_pretrained(encoder_model_name)

        # Decoder: ParsBERT transformer layers with modifications
        decoder_config = BertConfig.from_pretrained(decoder_model_name)
        decoder_config.is_decoder = True  # Enable cross-attention
        decoder_config.add_cross_attention = True
        decoder_config.decoder_start_token_id=tokenizer.cls_token_id
        self.decoder = BertModel(config=decoder_config)
        self.lm_head = nn.Linear(decoder_config.hidden_size, self.vocab_size)
        # Adjust weights
        #self.initialize_decoder_weights()

    # def initialize_decoder_weights(self):
    #     for name, param in self.decoder.named_parameters():
    #         if &quot;cross_attention&quot; in name:
    #             # Random initialization for cross-attention layers
    #             if param.requires_grad:
    #                 nn.init.xavier_uniform_(param.data)
    #         else:
    #             # Use pre-trained weights from ParsBERT
    #             param.data.copy_(self.encoder.state_dict().get(name, param.data))

    def forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask):
        # Encoder: Encode the input sequence
        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        encoded_sequence = encoder_outputs.last_hidden_state

        # Decoder: Decode the sequence conditioned on the encoder outputs
        decoder_outputs = self.decoder(
            input_ids=decoder_input_ids,
            attention_mask=decoder_attention_mask,
            encoder_hidden_states=encoded_sequence,
            encoder_attention_mask=attention_mask,
        )

        logits = self.lm_head(decoder_outputs.last_hidden_state)
        return logits
</code></pre>
<p>training phase:</p>
<pre><code>def train_model(model,tokenizer, input_texts, target_summaries):
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
    padding_idx = tokenizer.pad_token_id
    loss_fn = nn.CrossEntropyLoss(ignore_index=padding_idx)
    # Create PyTorch DataLoader
    train_data = list(zip(input_texts, target_summaries))
    train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
    for epoch in range(EPOCH):
        model.train()
        total=len(train_loader)
        index=0
        for batch in train_loader:
            optimizer.zero_grad()
            # Tokenize and convert to tensors
            input_texts, target_texts = batch
            inputs = encode_batch(input_texts, tokenizer)
            targets = encode_batch(target_texts, tokenizer)

            input_ids = inputs['input_ids']
            attention_mask = inputs['attention_mask']
            decoder_input_ids = targets['input_ids']
            decoder_attention_mask = targets['attention_mask']

            # Forward pass
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                decoder_input_ids=decoder_input_ids,
                decoder_attention_mask=decoder_attention_mask
            )

            logits = outputs
            labels = decoder_input_ids
            logits=logits.view(-1, logits.size(-1))
            loss = loss_fn(logits, labels.view(-1))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            index+=1
            print(f&quot;Epoch {epoch + 1}, Total Batches:{total} ,Batch:{index}, Loss: {loss.item()}&quot;)
            
        torch.save(model.state_dict(), &quot;model.model&quot;)
</code></pre>
<p>prediction phase:</p>
<pre><code>def evaluate_model(model, tokenizer, test_texts, reference_summaries):
    #model.load_state_dict(torch.load(MODEL_PATH))
    model.eval()
    rou = Rouge()

    predictions = []

    for text in test_texts:
        # Tokenize the input text
        inputs = tokenizer(
            text,
            return_tensors=&quot;pt&quot;,
            max_length=512,
            truncation=True,
            padding=&quot;max_length&quot;,
        )

        # Generate decoder inputs (start with &lt;sos&gt; token)
        decoder_input_ids = torch.tensor([[tokenizer.cls_token_id]])
        decoder_attention_mask = torch.ones_like(decoder_input_ids)

        output_tokens = []

        with torch.no_grad():
            for _ in range(50):  # Generate up to 50 tokens
                logits = model(
                    input_ids=inputs[&quot;input_ids&quot;],
                    attention_mask=inputs[&quot;attention_mask&quot;],
                    decoder_input_ids=decoder_input_ids,
                    decoder_attention_mask=decoder_attention_mask,
                )
                # Get the token with the highest probability
                next_token = torch.argmax(logits[:, -1, :], dim=-1)
                if (
                    next_token.item() == tokenizer.sep_token_id
                ):  # Stop if end-of-sequence token is generated
                    break
                output_tokens.append(next_token.item())

                # Update decoder inputs
                decoder_input_ids = torch.cat(
                    [decoder_input_ids, next_token.unsqueeze(0)], dim=1
                )
                decoder_attention_mask = torch.ones_like(decoder_input_ids)

        # Decode the generated tokens into text
        prediction = tokenizer.decode(output_tokens, skip_special_tokens=True)
        predictions.append(prediction)

    # Compute ROUGE scores
    results = rou.get_scores(predictions, reference_summaries, avg=True)

    return results, predictions

</code></pre>
<p>It generate a repetitive list of tokens as output like [2,2,2,2,.....]</p>
","deep-learning, nlp, huggingface-transformers, bert-language-model",
How to run inference large size model in multi-GPU effeciently?,"<p>I'm trying to run only <strong>inference</strong> with <strong>large 70B sized model with multi-GPU</strong> env, but facing some issues.</p>
<ol>
<li>The loading time takes so long, about 15mins.</li>
<li>I'm not sure this works properly to shard model over multi-GPU automatically and do their jobs in batch. Currently, it keeps error with such messages. I guess if it worked well, using only 4 GPU with this sized model should properly work.</li>
</ol>
<pre><code>[rank4]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 47.51 GiB of which 319.56 MiB is free. Including non-PyTorch memory, this process has 24.97 GiB memory in use. Process 1068220 has 22.22 GiB memory in use. Of the allocated memory 24.55 GiB is allocated by PyTorch, and 1.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
</code></pre>
<p>Is there any suggestions to fix my code?</p>
<p>p.s. Also wonder what would be the good prompts for the Instruction models? I know there's tons of prompt engineering methods, but just curious <strong>how to control the output quality</strong> well for the <code>Llama-Instruction model</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import os
import torch
import pandas as pd
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM


os.environ[&quot;HF_HOME&quot;] = &quot;/mystorage&quot;  
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;3,4,5,6,7&quot; 


MODEL_ID = &quot;meta-llama/Meta-Llama-3-70B-Instruct&quot;
CSV_FILE = &quot;my_csv.csv&quot;
COLUMN_NAME = &quot;column1&quot;
ADDITIONAL_COLUMNS = [&quot;column2&quot;]
OUTPUT_FILE = &quot;output.csv&quot;
BATCH_SIZE = 32  


print(&quot;Loading model...&quot;)
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=&quot;left&quot;)
tokenizer.pad_token_id = tokenizer.eos_token_id

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map=&quot;auto&quot;,  
    torch_dtype=torch.float16,  
)


print(&quot;Loading data...&quot;)
data = pd.read_csv(CSV_FILE)
cells = data[COLUMN_NAME].dropna().tolist()
additional_data = data[ADDITIONAL_COLUMNS].loc[data[COLUMN_NAME].notna()]

PROMPTS = [
    f&quot;myprompt {cell}\n Given this cell, summarize in one sentence concisely.&quot;
    for cell in cells
]


results = []


print(&quot;Starting inference...&quot;)
progress_bar = tqdm(total=len(PROMPTS))

for i in range(0, len(PROMPTS), BATCH_SIZE):
    batch_prompts = PROMPTS[i:i + BATCH_SIZE]
    
    
    inputs = tokenizer(batch_prompts, return_tensors=&quot;pt&quot;, padding=True, truncation=True).to(&quot;cuda&quot;)
    
    with torch.no_grad():
        output_ids = model.generate(
            input_ids=inputs[&quot;input_ids&quot;],
            attention_mask=inputs[&quot;attention_mask&quot;],
            max_new_tokens=40,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    
    decoded_outputs = [
        tokenizer.decode(output, skip_special_tokens=True).split(prompt)[-1].strip()
        for output, prompt in zip(output_ids, batch_prompts)
    ]
    results.extend(zip(batch_prompts, decoded_outputs))
    progress_bar.update(len(batch_prompts))


print(&quot;Saving results...&quot;)
result_df = pd.DataFrame(results, columns=[&quot;Prompt&quot;, &quot;Result&quot;])
result_df = pd.concat([result_df, additional_data.reset_index(drop=True)], axis=1)
result_df.to_csv(OUTPUT_FILE, index=False)

print(f&quot;Results saved to '{OUTPUT_FILE}'&quot;)
</code></pre>
","python, nlp, multiprocessing, huggingface-transformers, large-language-model",
How to get domain of words using WordNet in R?,"<p>I am trying to use WordNet in R to determine the domain of a list of words (e.g., sports, biology) and represent the results in a dataframe. The goal is to create a dataframe where each domain is a column, and each cell contains 0 or 1 depending on whether the word is associated with that domain.</p>
<p>Example:</p>
<p>Input</p>
<pre><code>| word   |
|--------|
| soccer |
| pie    |
| gene   |
</code></pre>
<p>Output</p>
<pre><code>| word   | sports | biology | ... |
|--------|--------|---------|-----|
| soccer | 1      | 0       | ... |
| pie    | 0      | 0       | ... |
| gene   | 0      | 1       | ... |
</code></pre>
<p>I've installed the <code>wordnet</code> R package, but it seems that it does not have access to WordNet Domains. Is there a good way to achieve this goal?</p>
<p>Thank you!</p>
","r, nlp, wordnet",
Extracting data from natural language text,"<p>I have a set of text newspaper ads, I'd like to extract information like the item being sold and its price. These ads don't follow any structured format. I have access to many thousands of these ads.</p>
<p>Where should I start on this project? Is there some library that would help out?</p>
","machine-learning, nlp",
Is there a way to extract information from contracts using ML with including contract files and targeted strings as inputs and outputs?,"<p>I'm studying whether we could extract certain fields of information (e.g. contract parties, start and end dates) from contracts automatically.</p>
<p>I am wondering if those pieces of information could be extracted with ML by having the whole contract as input and the information as output without tagging or annotating the whole text.</p>
<p>I understand that the extraction should be ran separately for each targeted field.</p>
","machine-learning, nlp, information-extraction","<p>First question - how are the contracts stored?  Are they PDFs or text-based?</p>

<p>If they're PDFs, there are a handful of packages that can extract text from a PDF (e.g. pdftotext).  </p>

<p>Second question - is the data you're looking for in the same place in every document?  </p>

<p>If so, you can extract the information you're looking for (like start and end dates) from a known location in the contract.  If not, you'll have to do something more sophisticated.  For example you may need to do a text search for ""start date"", if the same terminology is used in every contract.  If different terminology is used from contract to contract, you may need to work to extract <em>meaning</em> from the text, which can be done using some sophisticated natural language processing (NLP).  </p>

<p>Without more knowledge of your problem or a concrete example, it's hard to say what your best option may be.</p>
"
A Multiple Intent Query to Multiple Single Intent Query,"<h3>Problem Description</h3>
<p>I am designing an algorithm that inputs an string, looks in the Q&amp;A dataset, and brings back the closest Q&amp;A to our user query.</p>
<p>However, when the user sends a <strong>multiple-intent query</strong> like:</p>
<blockquote>
<p>&quot;Tell me about price, warranty, and delivery time.&quot;</p>
</blockquote>
<p>The algorithm only returns a single Q&amp;A, even though the query contains multiple intents.</p>
<hr />
<h3>Objective</h3>
<p>To handle such cases, I need a way to <strong>split a multiple-intent query into multiple single-intent queries</strong>. For example, the query above should be split into:</p>
<ul>
<li>&quot;Tell me about price&quot;</li>
<li>&quot;Tell me about warranty&quot;</li>
<li>&quot;Tell me about delivery time&quot;</li>
</ul>
<p>This will allow the algorithm to process each intent individually and return the closest Q&amp;A for each.</p>
","nlp, chatbot, large-language-model, nlu",
Strategy for parsing natural language descriptions into structured data,"<p>I have a set of requirements and I'm looking for the best <strong>Java-based</strong> strategy / algorthm / software to use.  Basically, I want to take a set of recipe ingredients entered by real people in natural english and parse out the meta-data into a structured format (see requirements below to see what I'm trying to do).</p>
<p>I've looked around here and other places, but have found nothing that gives a high-level advice on what direction follow.  So, I'll put it to the smart people :-):</p>
<p>What's the best / simplest way to solve this problem?  Should I use a natural language parser, dsl, lucene/solr, or some other tool/technology?  NLP seems like it may work, but it looks really complex.  I'd rather not spend a whole lot of time doing a deep dive just to find out it can't do what I'm looking for or that there is a simpler solution.</p>
<h1>Requirements</h1>
<p>Given these recipe ingredient descriptions....</p>
<ol>
<li>&quot;8 cups of mixed greens (about 5 ounces)&quot;</li>
<li>&quot;Eight skinless chicken thighs (about 1¼ lbs)&quot;</li>
<li>&quot;6.5 tablespoons extra-virgin olive oil&quot;</li>
<li>&quot;approximately 6 oz. thinly sliced smoked salmon, cut into strips&quot;</li>
<li>&quot;2 whole chickens (3 .5 pounds each)&quot;</li>
<li>&quot;20 oz each frozen chopped spinach, thawed&quot;</li>
<li>&quot;.5 cup parmesan cheese, grated&quot;</li>
<li>&quot;about .5 cup pecans, toasted and finely ground&quot;</li>
<li>&quot;.5 cup Dixie Diner Bread Crumb Mix, plain&quot;</li>
<li>&quot;8 garlic cloves, minced (4 tsp)&quot;</li>
<li>&quot;8 green onions, cut into 2 pieces&quot;</li>
</ol>
<p>I want to turn it into this....</p>
<pre>
|-----|---------|-------------|-------------------------|--------|-----------|--------------------------------|-------------|
|     | Measure |             |                         | weight | weight    |                                |             |
| #   | value   | Measure     | ingredient              | value  | measure   | preparation                    | Brand Name  |
|-----|---------|-------------|-------------------------|--------|-----------|--------------------------------|-------------|
| 1.  | 8       | cups        | mixed greens            | 5      | ounces    | -                              | -           |
| 2.  | 8       | -           | skinless chicken thigh  | 1.5    | pounds    | -                              | -           |
| 3.  | 6.5     | tablespoons | extra-virgin olive oil  | -      | -         | -                              | -           |
| 4.  | 6       | ounces      | smoked salmon           | -      | -         | thinly sliced, cut into strips | -           |
| 5.  | 2       | -           | whole chicken           | 3.5    | pounds    | -                              | -           |
| 6.  | 20      | ounces      | forzen chopped spinach  | -      |           | thawed                         | -           |
| 7.  | .5      | cup         | parmesean cheese        | -      | -         | grated                         | -           |
| 8.  | .5      | cup         | pecans                  | -      | -         | toasted, finely ground         | -           |
| 9.  | .5      | cup         | Bread Crumb Mix, plain  | -      | -         | -                              | Dixie Diner |
| 10. | 8       | -           | garlic clove            | 4      | teaspoons | minced                         | -           |
| 11. | 8       | -           | green onions            | -      | -         | cut into 2 pieces              | -           |
|-----|---------|-------------|-------------------------|--------|-----------|--------------------------------|-------------|
</pre>
<p>Note the diversity of the descriptions.  Some things are abbreviated, some are not.  Some numbers are numbers, some are spelled out.</p>
<p>I would love something that does a perfect parse/translation.  But, would settle for something that does reasonably well to start.</p>
<p>Bonus question:  after suggesting a strategy / tool, how would you go about it?</p>
<p>Thanks!</p>
<p>Joe</p>
","java, nlp, dsl, text-parsing",
Understanding byte-pair encoding tokenization for Greek characters,"<p>I am trying to train a new tokenizer with Greek text to later add the new tokens into the Llama 3.1 tokenizer using</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer.add_tokens(list(new_tokens)).
</code></pre>
<p>However, upon training the byte-pair encoding tokenizer on Greek and Spanish text, the result looks something like this:</p>
<pre class=""lang-py prettyprint-override""><code>\['Translate', 'Ġfrom', 'ĠGreek', 'Ġto', 'ĠSpanish', ':', 'ĠÎĿÎ±', 'ĠÎŃÏĥÎ¹', 'ĠÎ¿Î³Î¯', 'ĠÎ³ÎŃÏģÎ¿Ïħ'\]
</code></pre>
<p>When extending the token vocabulary in the tokenizer, it seems that those encoded tokens are being passed literally, not as encodings of Greek characters, and they are not recognized by the tokenizer to encode a sentence. However, when using the same method and new tokens are hardcoded, such as in</p>
<pre class=""lang-py prettyprint-override""><code>extender_tokenizer.add_tokens(['Αυτό', 'είναι'])
</code></pre>
<p>it does work.</p>
<p>I assume this is an encoding issue or it is related to BPE inner workings.
Why are Greek characters shown that way? Is it related to encoding, BPE or both? How to obtain a list of Greek character tokens that can be added to the tokenizer?</p>
<p>Reference code:</p>
<pre class=""lang-py prettyprint-override""><code>from tokenizers import Tokenizer, models, trainers, pre_tokenizers

tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()
trainer = trainers.BpeTrainer(vocab_size = 2000, min_frequency = 3, show_progress = True)
tokenizer.train_from_iterator(training_corpus, trainer = trainer)
</code></pre>
","encoding, nlp, tokenize, byte-pair-encoding",
Haystack: save InMemoryDocumentStore and load it in retriever later to save embedding generation time,"<p>I am using InMemory Document Store and an Embedding retriever for the Q/A pipeline.</p>
<pre><code>from haystack.document_stores import InMemoryDocumentStore
document_store = InMemoryDocumentStore(embedding_dim =768,use_bm25=True) 
document_store.write_documents(docs_processed)
     
from haystack.nodes import EmbeddingRetriever
retriever_model_path ='downloaded_models\local\my_local_multi-qa-mpnet-base-dot-v1'
retriever = EmbeddingRetriever(document_store=document_store,
                              embedding_model=retriever_model_path,
                              use_gpu=True)

document_store.update_embeddings(retriever=retriever)
</code></pre>
<p>As the embedding takes a while, I want to load the embeddings and later use them again in the retriever. (in rest API side). I don't want to use ElasticSearch or Faiss. How can I achieve this using In Memory Store? I tried to use Pickle, but there is no way to store the embeddings. Again, in the embedding retriever, there is no load function.</p>
<p>I tried to do the following:</p>
<pre><code>with open(&quot;document_store_res.pkl&quot;, &quot;wb&quot;) as f:
    pickle.dump(document_store.get_all_documents(), f)
</code></pre>
<p>And in the rest API, I am trying to load the document store :</p>
<pre><code>def reader_retriever():
# Load the pickled model        
        with open(os.path.join(settings.BASE_DIR,'\downloaded_models\document_store_res.pkl'), 'rb') as f:
            document_store_new = pickle.load(f)

            retriever_model_path = os.path.join(settings.BASE_DIR, '\downloaded_models\my_local_multi-qa-mpnet-base-dot-v1')

            retriever = EmbeddingRetriever(document_store=document_store_new,
                               embedding_model=retriever_model_path,
                               use_gpu=True)

            document_store_new.update_embeddings(retriever=retriever,
                                batch_size=100)
            farm_reader_path = os.path.join(settings.BASE_DIR, '\downloaded_models\my_local_bert-large-uncased-whole-word-masking-squad2')

            reader = FARMReader(model_name_or_path=farm_reader_path,
                                    use_gpu=True)
            

            return reader, retriever
</code></pre>
","python, nlp, haystack","<h2><code>InMemoryDocumentStore</code>: features and limitations</h2>
<p>From <a href=""https://docs.haystack.deepset.ai/docs/document_store"" rel=""nofollow noreferrer"">Haystack docs</a>:</p>
<blockquote>
<p>Use the InMemoryDocumentStore, if you are just giving Haystack a
quick try on a small sample and are working in a restricted
environment that complicates running Elasticsearch or other databases.</p>
</blockquote>
<ul>
<li>Slow retrieval on larger datasets.</li>
<li>No Approximate Nearest Neighbours (ANN).</li>
<li>Not recommended for production.</li>
</ul>
<h2>Possible lightweight alternatives</h2>
<p>To overcome the limitations of <code>InMemoryDocumentStore</code>, if you don't want to use FAISS or ElasticSearch, you could also consider adopting <a href=""https://haystack.deepset.ai/blog/qdrant-integration"" rel=""nofollow noreferrer"">Qdrant</a> which can run smoothly and lightly on Haystack.</p>
<h2>Pickling <code>InMemoryDocumentStore</code></h2>
<p>As you can see, I do not recommend this solution.
In any case, I would pickle the document store (which also contains the embeddings):</p>
<pre><code>with open(&quot;document_store_res.pkl&quot;, &quot;wb&quot;) as f:
    pickle.dump(document_store, f)
</code></pre>
<p>In the REST API, you can change your method as follows:</p>
<pre><code>def reader_retriever():
# Load the pickled model        
    with open(os.path.join(settings.BASE_DIR,'\downloaded_models\document_store_res.pkl'), 'rb') as f:
        document_store_new = pickle.load(f)

    retriever_model_path = os.path.join(settings.BASE_DIR, '\downloaded_models\my_local_multi-qa-mpnet-base-dot-v1')
    retriever = EmbeddingRetriever(document_store=document_store_new,
                       embedding_model=retriever_model_path,
                       use_gpu=True)

    ### DO NOT UPDATE THE EMBEDDINGS, AS THEY HAVE ALREADY BEEN CALCULATED
    
    farm_reader_path = os.path.join(settings.BASE_DIR, '\downloaded_models\my_local_bert-large-uncased-whole-word-masking-squad2')
    reader = FARMReader(model_name_or_path=farm_reader_path,
                            use_gpu=True)
    
    return reader, retriever
</code></pre>
"
Calculating words similarity score in python,"<p>I'm trying to calculate books similarity by comparing the topics lists.</p>
<p>Need to get similarity score from the 2 lists between 0-1.</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>book1_topics = [&quot;god&quot;, &quot;bible&quot;, &quot;book&quot;, &quot;holy&quot;, &quot;religion&quot;, &quot;Christian&quot;]

book2_topics = [&quot;god&quot;, &quot;Christ&quot;, &quot;idol&quot;, &quot;Jesus&quot;]
</code></pre>
<p>Tried using wordnet but not sure how to calculate the score.</p>
<p>Any suggestions?</p>
","python, nlp, wordnet, cosine-similarity, sentence-similarity",
BERTopic partial_fit placeholder cluster representation,"<p>I have approximately 2M text documents, each small, and want to cluster them.  (This will grow to about 500M eventually.)  While I am open to suggestions, I am currently using on-line techniques with the Python3 package BERTopic.  After using <code>partial_fit</code> on 140 chunks of 15k documents each I am left with a call to <code>topic_model.get_topic_info()</code> that returns unfinished cluster representations.  That is, I see cluster name <code>0____</code> with representation <code>[,,,,,,,,]</code>.  I see that for most of the clusters.  The advice I get from Google Gemini is to call <code>topic_model.fit(all_documents)</code> on all of my documents, which is currently a minuscule 86 GB compressed on disk, way too much for RAM.  How can I fill in representations for these clusters?</p>
","machine-learning, nlp, cluster-analysis, bert-language-model",
Pyspark sentiment analysis invalid output,"<p>I am trying to perform sentiment analysis for a use case. Most of the time, it is giving correct results, but in some cases, even positive comments are being marked as negative. How can I fix my code to achieve better accuracy?</p>
<p>My code</p>
<pre><code>from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from transformers import pipeline
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Define the filter_stopwords function
def filter_stopwords(sentence):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(sentence)
    filtered_sentence = [w for w in word_tokens if not w in stop_words]
    return &quot; &quot;.join(filtered_sentence)

# Initialize the sentiment analysis pipeline with a different model
sentiment_pipeline = pipeline(&quot;sentiment-analysis&quot;, model=&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;)

# Define a function to get sentiment using the pipeline
def get_sentiment(text):
    filtered_text = filter_stopwords(text)
    result = sentiment_pipeline(filtered_text)[0]
    return result['label'].lower()  # returns 'positive', 'negative', etc.

# Register the function as a UDF
sentiment_udf = udf(get_sentiment, StringType())

# df = df.withColumn(&quot;sentiment&quot;, sentiment_udf(col(&quot;text_column&quot;)))
</code></pre>
<p>Input data</p>
<ol>
<li><p>Didn't get it right the first 2 times but when it was fixed it was fixed well.</p>
</li>
<li><p>The Response time was grate -- <strong>Note</strong> Looks like spelling mistake but his review is positive</p>
</li>
<li><p>The initial agent contact could not resolve my issue but escalated it quickly to someone who could.</p>
</li>
</ol>
<p>for this inputs i am expecting all should be <strong>positive</strong> instead i am getting <strong>negative</strong></p>
","pyspark, nlp, nltk, huggingface-transformers",
Best way of using hugging face&#39;s Mask Filling for more than 1 masked token at a time,"<p>I am able to use hugging face's mask filling pipeline to predict 1 masked token in a sentence using the below:</p>

<pre><code>!pip install -q transformers
from __future__ import print_function
import ipywidgets as widgets
from transformers import pipeline

nlp_fill = pipeline('fill-mask')
nlp_fill(""I am going to guess &lt;mask&gt; in this sentence"")
</code></pre>

<p>But does anyone have an opinion on what is the best way to do this if I want to predict 2 masked tokens? e.g. if the sentence is instead <code>""I am going to &lt;mask&gt; &lt;mask&gt; in this sentence""</code>?  </p>

<p>If i try and put this exact sentence into nlp_fill I get the error <code>""ValueError: only one element tensors can be converted to Python scalars""</code> so it doesn't work automatically.</p>

<p>Any help would be much appreciated!</p>
","python, neural-network, nlp, huggingface-transformers",
When to set `add_special_tokens=False` in huggingface transformers tokenizer?,"<p>this is the default way of setting <code>tokenizer</code> in the Hugging Face &quot;transformers&quot; library:</p>
<pre><code>from transformers import BertForSequenceClassification,BertTokenizer
tokenizer=BertTokenizer.from_pretrained('ProsusAI/finbert')
tokens=tokenizer.encode_plus(text,add_special_tokens=True, max_length=512, truncation=True, padding=&quot;max_length&quot;)
</code></pre>
<p>As far as I understand, setting <code>add_special_tokens=True</code> adds the special tokens like [CLS], [SEP], and padding tokens to the input sequences. This is useful for the model's correct interpretation of the input. However, I've come across code samples where people set it to <code>False</code>.</p>
<p>I'm wondering in which specific situations should I set add_special_tokens=True and when should I set it to False when using tokenizer.encode_plus()? Are there any scenarios where managing special tokens manually after chunking or other preprocessing steps would be beneficial?</p>
","python, nlp, tokenize, huggingface-transformers",
Emotion Analysis with bhadresh-savani/bert-base-uncased-emotion,"<p>Hope I can get some help here please!
I am trying to run an emotion analysis model from Hugging Face rep. (bhadresh-savani/bert-base-uncased-emotion) and I am struggling with the model run as it's extremely slow and also the output I am trying to get is not correct (I need to append the top emotion score next to my &quot;Comment_new&quot; variable). This is in Colab.</p>
<p>By running this model I am trying to get the top emotion for each review.I have attached a print of 3 reviews from the dataset which in total contains around 4K review lines.</p>
<p>Any idea please how I can run this fast and with the right output?</p>
<p><a href=""https://i.sstatic.net/CUhkPmyr.png"" rel=""nofollow noreferrer"">data sample, top 3 rows</a></p>
<p><a href=""https://i.sstatic.net/EDqDu5KZ.png"" rel=""nofollow noreferrer"">Python Code</a></p>
","python, nlp, huggingface-transformers, bert-language-model, emotion",
A language model for machine translation between a low-resource language and Portuguese using Tensorflow,"<p>I'm trying to train a language model for machine translation between a low-resource language and Portuguese using Tensorflow. unfortunately, I'm getting the following error:</p>
<pre><code>PS C:\Users\myuser\PycharmProjects\teste&gt; python .\tensorflow_model.py                   
2024-08-23 21:29:50.839647: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File &quot;.\tensorflow_model.py&quot;, line 52, in &lt;module&gt;
    dataset = tf.data.Dataset.from_tensor_slices((src_tensor, tgt_tensor)).shuffle(BUFFER_SIZE)
  File &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py&quot;, line 831, in from_tensor_slices
    return from_tensor_slices_op._from_tensor_slices(tensors, name)
  File &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\data\ops\from_tensor_slices_op.py&quot;, line 25, in _from_tensor_slices
    return _TensorSliceDataset(tensors, name=name)
  File &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\data\ops\from_tensor_slices_op.py&quot;, line 45, in __init__
    batch_dim.assert_is_compatible_with(
  File &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\framework\tensor_shape.py&quot;, line 300, in assert_is_compatible_with
    raise ValueError(&quot;Dimensions %s and %s are not compatible&quot; %
ValueError: Dimensions 21 and 22 are not compatible
</code></pre>
<p>How can I overcome this error?</p>
<pre><code>import tensorflow as tf
import numpy as np
import re
import os

# Clean data
def preprocess_sentence(sentence):
    sentence = sentence.lower().strip()
    sentence = re.sub(r&quot;([?.!,¿])&quot;, r&quot; \1 &quot;, sentence)
    sentence = re.sub(r'[&quot; &quot;]+', &quot; &quot;, sentence)
    sentence = re.sub(r&quot;[^a-zA-Z?.!,¿]+&quot;, &quot; &quot;, sentence)
    sentence = sentence.strip()
    sentence = '&lt;start&gt; ' + sentence + ' &lt;end&gt;'
    return sentence

#Function to load data
def load_data(file_path_src, file_path_tgt):
    src_sentences = open(file_path_src, 'r', encoding='utf-8').read().strip().split('\n')
    tgt_sentences = open(file_path_tgt, 'r', encoding='utf-8').read().strip().split('\n')

    src_sentences = [preprocess_sentence(sentence) for sentence in src_sentences]
    tgt_sentences = [preprocess_sentence(sentence) for sentence in tgt_sentences]

    return src_sentences, tgt_sentences

#load data
src_sentences, tgt_sentences = load_data('src_language.txt', 'portuguese.txt')

#Tokenization
src_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')
tgt_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')

src_tokenizer.fit_on_texts(src_sentences)
tgt_tokenizer.fit_on_texts(tgt_sentences)

src_tensor = src_tokenizer.texts_to_sequences(src_sentences)
tgt_tensor = tgt_tokenizer.texts_to_sequences(tgt_sentences)

src_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_tensor, padding='post')
tgt_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_tensor, padding='post')

BUFFER_SIZE = len(src_tensor)

#Creating the Dataset
dataset = tf.data.Dataset.from_tensor_slices((src_tensor, tgt_tensor)).shuffle(BUFFER_SIZE) 
</code></pre>
","python, tensorflow, machine-learning, nlp",
DASK to_csv() problems due to memory,"<p>I'm cleaning my text data and afterwards want to save it to csv. Defined cleaning functions work fine, but when to_csv() part comes, here come the problems as well.
Maybe someone have faced similar problem and has a trick to share with me how it would be possible to solve it? Maybe saving data to csv in chunks or something?</p>
<p>`if <strong>name</strong> == '<strong>main</strong>':
# Initialize the Dask client
client = Client(n_workers=3, threads_per_worker=1,
memory_limit='1.5GB')<br />
print('Dask client created')</p>
<pre><code>PATH = &quot;C:\\Users\\el ruchenzo\\jobsproject\\jobsproject\\lt_data.csv&quot;
reqd = ['description', 'title', 'code']
blocksize = 25e6  # REDUCED FROM 100 GB TO 25 GB

# Load the CSV with Dask
df = dd.read_csv(PATH,
                 usecols=reqd,
                 blocksize=blocksize,
                 dtype={'Code': 'float'},
                 engine='python',
                 encoding='utf-8',
                 on_bad_lines='skip')

# Apply the cleaning function to the 'title' column in the DataFrame
start_time = time.time()
df['cleaned_title'] = df['title'].map_partitions(lambda partition: partition.apply(wrapper_func), meta=('title', 'object'))
gc.collect()
end_time = time.time()
print(f&quot;1. Processing time: {end_time - start_time:.2f} seconds&quot;)

# Apply the cleaning function to the 'description' column in the DataFrame
start_time = time.time()
df['cleaned_description'] = df['description'].map_partitions(lambda partition: partition.apply(wrapper_func), meta=('description', 'object'))
gc.collect()
end_time = time.time()
print(f&quot;2. Processing time: {end_time - start_time:.2f} seconds&quot;)

df.to_csv('cleaned_lemma_*.csv', index=False, encoding='utf-8', single_file = False)
print('Saved to csv successfully')

print('Work ended successfully')`
</code></pre>
<p>Tried changing workers number, blocksize, memory limit and etc., but nothing seems to work. Also change map() to apply(), tried using df = df.persist(), writing data to csv with pandas in chunks instead with dask, and so on.</p>
","csv, text, nlp, export-to-csv, dask",
Using sub-classes as anchors and classes as positives and negatives in a Siamese network with triplet loss?,"<p>I’m experimenting with a Siamese network using triplet loss to categorize sub-classes into broader classes. My setup differs from traditional triplet loss models: It involves using the sub-class as the anchor and the broader class as the positive (where the sub-class fits) and a different class as the negative (where it doesn’t fit). The goal is to position each sub-class embedding closer to its relevant class and farther from unrelated classes. Would this architecture make sense for capturing context-dependent relationships between sub-classes and classes? Are there any limitations I should be aware of?</p>
<p>I havent written any code. I'm curious about the theorhetical possibility.</p>
","nlp, distance, bert-language-model, semantics, triplet",
Need help Implementing Context aware decoding on Llama,"<p>I have written the code below, but I run into the following error:</p>
<pre><code>AttributeError: 'CAD' object has no attribute 'generate'
</code></pre>
<p>I want to override the forward function without rewriting the generate function how am I supposed to do this?</p>
<pre><code>class CAD(AutoModelForCausalLM):
    def __init__(self):
        super().__init__()

        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(&quot;huggyllama/llama-13b&quot;)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.alpha = 0.5
  
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
        model = AutoModelForCausalLM.from_pretrained(&quot;huggyllama/llama-13b&quot;, **kwargs)
        model.__class__ = cls  # Change the class to CAD
        model.alpha = 0.5  # Add the custom alpha attribute
        model.tokenizer = AutoTokenizer.from_pretrained(&quot;huggyllama/llama-13b&quot;)
        model.tokenizer.pad_token = model.tokenizer.eos_token
        return model


    def forward(self, input_ids,attention_mask,*args, **kwargs):
        #to be overwritten
        return final_logits


model_name = &quot;huggyllama/llama-13b&quot;
cad_model = 
CAD.from_pretrained(model_name,torch_dtype=torch.float16,low_cpu_mem_usage=True,use_cache=True)

inputs = ['Better late than']
contexts = ['Write a quote that ends in the word &quot;early&quot;:']
contexts = None

text_token = cad_model.tokenizer(inputs ,return_tensors='pt')
input_ids = text_token['input_ids']
inputs_and_contexts = input_ids

if contexts:
    inputs_with_contexts = [context + cad_model.tokenizer.eos_token + input_text for context, input_text in zip(contexts, inputs)]
    text_token_with_contexts = cad_model.tokenizer(inputs_with_contexts, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=256)
    inputs_and_contexts = text_token_with_contexts['input_ids']
    attention_mask = text_token_with_contexts['attention_mask']

output = cad_model.generate([22,33],
                            attention_mask=attention_mask,
                            # eos_token_id=cad_model.tokenizer.eos_token_id,
                            max_new_tokens=1,
                            # pad_token_id=cad_model.tokenizer.pad_token_id
                        )
generated_text = cad_model.tokenizer.decode(output[0], skip_special_tokens=True)
generated_text 

</code></pre>
<p>I have also tried</p>
<pre><code>model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16,low_cpu_mem_usage=True,use_cache=True)
model.__class__ = CAD  # Change the model class to use the custom forward

</code></pre>
<p>but I encounter the same error.</p>
","nlp, large-language-model, llama",
Hugging face tokenizer cannot load files properly,"<p>I am trying to train a translation model from sratch using HuggingFace's BartModel architecture. I am using a ByteLevelBPETokenizer to tokenize things.</p>
<p>The issue that I am facing is that when I save the tokenizer after training it is not loaded properly even though it apparently creates the correct vocab.json and merges.txt file.</p>
<pre><code>from tokenizers import ByteLevelBPETokenizer

# Initialize a tokenizer
tokenizer = ByteLevelBPETokenizer()

# Customize training
tokenizer.train(files=['/work/vaibhav/iwslt14/de-en/train.en', '/work/vaibhav/iwslt14/de-en/train.de'], vocab_size=8000, min_frequency=2, special_tokens=[
    &quot;&lt;s&gt;&quot;,
    &quot;&lt;pad&gt;&quot;,
    &quot;&lt;/s&gt;&quot;,
    &quot;&lt;unk&gt;&quot;,
    &quot;&lt;mask&gt;&quot;,
])

# Save files to disk
print(tokenizer)
tokenizer.save_model(&quot;tokens&quot;)
</code></pre>
<p>This is how I am training and saving my tokenizer. The print statement prints this:</p>
<pre><code>Tokenizer(vocabulary_size=8000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)
</code></pre>
<p>However when I try to load the tokenizer while training my model by the following lines of code:</p>
<pre><code>#get the tokenizer
tokenizer = ByteLevelBPETokenizer()
tokenizer.from_file('tokens/vocab.json', 'tokens/merges.txt')
print(tokenizer)
return tokenizer
</code></pre>
<p>Then the print statement prints the following:</p>
<pre><code>Tokenizer(vocabulary_size=0, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)
</code></pre>
<p>Now this is weird to me because the vocab_size is supposed to be 8000 not zero and because of this it stops working basically. If I retrain and use it directly without saving and loading then it works but that's not efficient.</p>
<p>This is a view into the vocab.json (truncated).</p>
<pre><code>{&quot;&lt;s&gt;&quot;:0,&quot;&lt;pad&gt;&quot;:1,&quot;&lt;/s&gt;&quot;:2,&quot;&lt;unk&gt;&quot;:3,&quot;&lt;mask&gt;&quot;:4,&quot;!&quot;:5,&quot;\&quot;&quot;:6,&quot;#&quot;:7,&quot;$&quot;:8,&quot;%&quot;:9,&quot;&amp;&quot;:10,&quot;'&quot;:11,&quot;(&quot;:12,&quot;)&quot;:13,&quot;*&quot;:14,&quot;+&quot;:15,&quot;,&quot;:16,&quot;-&quot;:17,&quot;.&quot;:18,&quot;/&quot;:19,&quot;0&quot;:20,&quot;1&quot;:21,&quot;2&quot;:22,&quot;3&quot;:23,&quot;4&quot;:24,&quot;5&quot;:25,&quot;6&quot;:26,&quot;7&quot;:27,&quot;8&quot;:28,&quot;9&quot;:29,&quot;:&quot;:30,&quot;;&quot;:31,&quot;&lt;&quot;:32,&quot;=&quot;:33,&quot;&gt;&quot;:34,&quot;?&quot;:35,&quot;@&quot;:36,&quot;A&quot;:37,&quot;B&quot;:38,&quot;C&quot;:39,&quot;D&quot;:40,&quot;E&quot;:41,&quot;F&quot;:42,&quot;G&quot;:43,&quot;H&quot;:44,&quot;I&quot;:45,&quot;J&quot;:46,&quot;K&quot;:47,&quot;L&quot;:48,&quot;M&quot;:49,&quot;N&quot;:50,&quot;O&quot;:51,&quot;P&quot;:52,&quot;Q&quot;:53,&quot;R&quot;:54,&quot;S&quot;:55,&quot;T&quot;:56,&quot;U&quot;:57,&quot;V&quot;:58,&quot;W&quot;:59,&quot;X&quot;:60,&quot;Y&quot;:61,&quot;Z&quot;:62,&quot;[&quot;:63,&quot;\\&quot;:64,&quot;]&quot;:65,&quot;^&quot;:66,&quot;_&quot;:67,&quot;`&quot;:68,&quot;a&quot;:69,&quot;b&quot;:70,&quot;c&quot;:71,&quot;d&quot;:72,&quot;e&quot;:73,&quot;f&quot;:74,&quot;g&quot;:75,&quot;h&quot;:76,&quot;i&quot;:77,&quot;j&quot;:78,&quot;k&quot;:79,&quot;l&quot;:80,&quot;m&quot;:81,&quot;n&quot;:82,&quot;o&quot;:83,&quot;p&quot;:84,&quot;q&quot;:85,&quot;r&quot;:86,&quot;s&quot;:87,&quot;t&quot;:88,&quot;u&quot;:89,&quot;v&quot;:90,&quot;w&quot;:91,&quot;x&quot;:92,&quot;y&quot;:93,&quot;z&quot;:94,&quot;{&quot;:95,&quot;|&quot;:96,&quot;}&quot;:97,&quot;~&quot;:98,&quot;¡&quot;:99,&quot;¢&quot;:100,&quot;£&quot;:101,&quot;¤&quot;:102,&quot;¥&quot;:103,&quot;¦&quot;:104,&quot;§&quot;:105,&quot;¨&quot;:106,&quot;©&quot;:107,&quot;ª&quot;:108,&quot;«&quot;:109,&quot;¬&quot;:110,&quot;®&quot;:111,&quot;¯&quot;:112,&quot;°&quot;:113,&quot;±&quot;:114,&quot;²&quot;:115,&quot;³&quot;:116,&quot;´&quot;:117,&quot;µ&quot;:118,&quot;¶&quot;:119,&quot;·&quot;:120,&quot;¸&quot;:121,&quot;¹&quot;:122,&quot;º&quot;:123,&quot;»&quot;:124,&quot;¼&quot;:125,&quot;½&quot;:126,&quot;¾&quot;:127,&quot;¿&quot;:128,&quot;À&quot;:129,&quot;Á&quot;:130,&quot;Â&quot;:131,&quot;Ã&quot;:132,&quot;Ä&quot;:133,&quot;Å&quot;:134,&quot;Æ&quot;:135,&quot;Ç&quot;:136,&quot;È&quot;:137,&quot;É&quot;:138,&quot;Ê&quot;:139,&quot;Ë&quot;:140,&quot;Ì&quot;:141,&quot;Í&quot;:142,&quot;Î&quot;:143,&quot;Ï&quot;:144,&quot;Ð&quot;:145,&quot;Ñ&quot;:146,&quot;Ò&quot;:147,&quot;Ó&quot;:148,&quot;Ô&quot;:149,&quot;Õ&quot;:150,&quot;Ö&quot;:151,&quot;×&quot;:152,&quot;Ø&quot;:153,&quot;Ù&quot;:154,&quot;Ú&quot;:155,&quot;Û&quot;:156,&quot;Ü&quot;:157,&quot;Ý&quot;:158,&quot;Þ&quot;:159,&quot;ß&quot;:160,&quot;à&quot;:161,&quot;á&quot;:162,&quot;â&quot;:163,&quot;ã&quot;:164,&quot;ä&quot;:165,&quot;å&quot;:166,&quot;æ&quot;:167,&quot;ç&quot;:168,&quot;è&quot;:169,&quot;é&quot;:170,&quot;ê&quot;:171,&quot;ë&quot;:172,&quot;ì&quot;:173,&quot;í&quot;:174,&quot;î&quot;:175,&quot;ï&quot;:176,&quot;ð&quot;:177,&quot;ñ&quot;:178,&quot;ò&quot;:179,&quot;ó&quot;:180,&quot;ô&quot;:181,&quot;õ&quot;:182,&quot;ö&quot;:183,&quot;÷&quot;:184,&quot;ø&quot;:185,&quot;ù&quot;:186,&quot;ú&quot;:187,&quot;û&quot;:188,&quot;ü&quot;:189,&quot;ý&quot;:190,&quot;þ&quot;:191,&quot;ÿ&quot;:192,&quot;Ā&quot;:193,&quot;ā&quot;:194,&quot;Ă&quot;:195,&quot;ă&quot;:196,&quot;Ą&quot;:197,&quot;ą&quot;:198,&quot;Ć&quot;:199,&quot;ć&quot;:200,&quot;Ĉ&quot;:201,&quot;ĉ&quot;:202,&quot;Ċ&quot;:203,&quot;ċ&quot;:204,&quot;Č&quot;:205,&quot;č&quot;:206,&quot;Ď&quot;:207,&quot;ď&quot;:208,&quot;Đ&quot;:209,&quot;đ&quot;:210,&quot;Ē&quot;:211,&quot;ē&quot;:212,&quot;Ĕ&quot;:213,&quot;ĕ&quot;:214,&quot;Ė&quot;:215,&quot;ė&quot;:216,&quot;Ę&quot;:217,&quot;ę&quot;:218,&quot;Ě&quot;:219,&quot;ě&quot;:220,&quot;Ĝ&quot;:221,&quot;ĝ&quot;:222,&quot;Ğ&quot;:223,&quot;ğ&quot;:224,&quot;Ġ&quot;:225,&quot;ġ&quot;:226,&quot;Ģ&quot;:227,&quot;ģ&quot;:228,&quot;Ĥ&quot;:229,&quot;ĥ&quot;:230,&quot;Ħ&quot;:231,&quot;ħ&quot;:232,&quot;Ĩ&quot;:233,&quot;ĩ&quot;:234,&quot;Ī&quot;:235,&quot;ī&quot;:236,&quot;Ĭ&quot;:237,&quot;ĭ&quot;:238,&quot;Į&quot;:239,&quot;į&quot;:240,&quot;İ&quot;:241,&quot;ı&quot;:242,&quot;Ĳ&quot;:243,&quot;ĳ&quot;:244,&quot;Ĵ&quot;:245,&quot;ĵ&quot;:246,&quot;Ķ&quot;:247,&quot;ķ&quot;:248,&quot;ĸ&quot;:249,&quot;Ĺ&quot;:250,&quot;
</code></pre>
<p>This is a view into the merges.txt (truncated).</p>
<pre><code>#version: 0.2 - Trained by `huggingface/tokenizers`
e n
e r
i n
Ġ t
c h
Ġ a
Ġ d
Ġ w
Ġ s
Ġt h
n d
i e
e s
i s
a t
o n
i t
Ġ m
a s
a n
r e
Ġth e
Ġ h
</code></pre>
<p>As you can see the files are normal. Any help with this issue will be appreciated.</p>
","python, nlp, huggingface-tokenizers","<p>So I figured this out myself, finally! There is some error in huggingface code so i loaded the tokenizer like this and it worked.</p>
<pre><code>tokenizer = ByteLevelBPETokenizer('tokens/vocab.json', 'tokens/merges.txt')
</code></pre>
"
How to convert character indices to BERT token indices,"<p>I am working with a question-answer dataset <code>UCLNLP/adversarial_qa</code>.</p>
<pre><code>from datasets import load_dataset
ds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)
</code></pre>
<p>How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset:</p>
<pre><code>d0 = ds['train'][0]
d0

{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',
 'title': 'Brain',
 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',
 'question': 'What sare the benifts of the blood brain barrir?',
 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},
 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}
</code></pre>
<p>After tokenization, the answer indices are 56  and 16:</p>
<pre><code>from transformers import BertTokenizerFast
bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)

bert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])
'isolated from the bloodstream'
</code></pre>
<p>I want to create a new dataset with the answer's token indices, e.g., 56 ad 60.</p>
<p>This is from a <a href=""https://www.linkedin.com/learning/introduction-to-transformer-models-for-nlp/bert-for-question-answering?autoSkip=true&amp;resume=false"" rel=""nofollow noreferrer"">linkedin learning class</a>. The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:<a href=""https://i.sstatic.net/GsZ6mfcQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GsZ6mfcQ.png"" alt=""QA dataset with token answer indices"" /></a></p>
","python, nlp, dataset, large-language-model, bert-language-model","<p>You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices.</p>
<p>The following function does the above for you:</p>
<pre><code>def get_token_indices(example):
    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token
    encoded = tokenizer(
        example['question'], 
        example['context'], 
        return_offsets_mapping=True
    )

    # Find character start and end from the original answer
    char_start = example['answers']['answer_start'][0]
    char_end = char_start + len(example['answers']['text'][0])

    # Identify token indices for the answer
    start_token_idx = None
    end_token_idx = None
    
    for i, (start, end) in enumerate(encoded['offset_mapping']):
        if start &lt;= char_start &lt; end: 
            start_token_idx = i
        if start &lt; char_end &lt;= end:
            end_token_idx = i
            break

    example['answer_start_token_idx'] = start_token_idx
    example['answer_end_token_idx'] = end_token_idx
    return example
</code></pre>
<p>Here's how you can use and test this function:</p>
<pre><code>ds = load_dataset(&quot;UCLNLP/adversarial_qa&quot;, &quot;adversarialQA&quot;)
tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)

tokenized_ds = ds['train'].map(get_token_indices)


# Example
d0_tokenized = tokenized_ds[0]
print(&quot;Tokenized start index:&quot;, d0_tokenized['answer_start_token_idx'])
print(&quot;Tokenized end index:&quot;, d0_tokenized['answer_end_token_idx'])

answer_tokens = tokenizer.decode(
    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]
)
print(&quot;Tokenized answer:&quot;, answer_tokens)
</code></pre>
<p>Output:</p>
<pre><code>Tokenized start index: 56
Tokenized end index: 60
Tokenized answer: isolated from the bloodstream
</code></pre>
"
Bert models show tokenizing statistics,"<p>Is there any built-in way to request some tokenizing statistics when using BertTokenizer.from_pretrained('bert-base-uncased') and BertModel.from_pretrained('bert-base-uncased') to understand how efficiently my texts are being processed?</p>
<p>I am using texts that are not very large, but their length differentiates from 4 to 250 characters, depending on the training image. The texts sometimes may contain some weird and unpopular words. I am afraid that due to such conditions, the tokenizing process may not be very effective.</p>
<p>I am looking for some way of checking tokenizing statistics when processing all my text images.</p>
<p>I tried using this code, which was founded on Github, but there are a lot of errors inside it and it's not very verbal:</p>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

texts = [&quot;This is a sample text.&quot;, &quot;Another text with some uncommon words.&quot;]

encoded_texts = tokenizer(texts, return_tensors='pt')

# Vocabulary Coverage
vocab_size = len(tokenizer.vocab)
total_tokens = sum([len(text) for text in encoded_texts['input_ids']])
oov_tokens = sum([1 for token in encoded_texts['input_ids'].flatten() if token not in tokenizer.vocab])
vocab_coverage = 1 - (oov_tokens / total_tokens)

# Average Token Length
token_lengths = [len(text) for text in encoded_texts['input_ids']]
average_token_length = sum(token_lengths) / len(token_lengths)

print(f&quot;Vocabulary Coverage: {vocab_coverage:.2f}&quot;)
print(f&quot;Average Token Length: {average_token_length:.2f}&quot;)
</code></pre>
","python, nlp, bert-language-model",
why does num_train_epoch always save the latest value instead of corresponding value of maximum acuracy. how to capture the correct epoch value,"<p>here are my arguments</p>
<pre><code>args = TrainingArguments(
    output_dir=&quot;bert-finetuned-ner&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    save_total_limit=3,
    metric_for_best_model=&quot;eval_accuracy&quot;, # load model with highest accuracy score
    load_best_model_at_end=True, # load model with highest accuracy score
    learning_rate=best_params['learning_rate'],
    num_train_epochs=best_params['num-train_epoch'],  
    weight_decay=0.01,
    logging_steps=10,
    logging_strategy=&quot;epoch&quot;,
    fp16=True
)
trainer = Trainer(
    model_init=model_init,
    args=args,
    train_dataset=tokenized_datasets[&quot;train&quot;],
    eval_dataset=tokenized_datasets[&quot;test&quot;],
    compute_metrics=compute_metrics_all,
    tokenizer=tokenizer
    )
</code></pre>
<p>this is after the objective function where in training to capture</p>
<pre><code>with mlflow.start_run(run_id=parent_run.info.run_id, nested=True) as run:
    # log run name
    mlflow.set_tag('mlflow.runName', run_name)
    
    # Log tokenizer parameters
    tokenizer_params = {
        &quot;padding_side&quot;: tokenizer.padding_side,
        &quot;truncation_side&quot;: tokenizer.truncation_side
    }
    mlflow.log_params(tokenizer_params)

    # train
    trainer.train()

    # log best params
    mlflow.log_params(best_params)

    # Evaluate and log all metrics
    results = trainer.evaluate()
    for key, value in results.items():
        mlflow.log_metric(key, value)
</code></pre>
<p>how to capture the correct epoch value in mlflow instead of the latest epoch value of the best trial with epoch value corresponding to highest accuracy. If it needs to corrected in the objective fuction itself then please illustrate how to do that correction step.when logging also this causes the issue.df1 = study.trials_dataframe()</p>
","nlp, huggingface-transformers, large-language-model, transformer-model, huggingface-trainer",
Handling Multiple Entity Candidates in Short Texts for Entity Linking with SciSpacy,"<p>I am working on linking short texts to entities in a biomedical knowledge graph (UMLS CUIs) using SciSpacy for a research project. The goal is to analyze the relationship between the linked entity and a separate predefined entity.</p>
<p>My challenge is managing multiple possible entities identified in the texts, which introduces noise into the results. Although I use heuristics such as regex, a manual stop list, and filtering by semantic categories (TUIs) to clean the data, the issue persists due to the text complexity. I typically select the top ~3 entities per text based on the NER score, with a relatively high threshold.</p>
<p>For instance, the text &quot;Standard PRS for Alzheimer's&quot; incorrectly links entities for &quot;Standard&quot; and &quot;PRS,&quot; in addition to &quot;Alzheimer's.&quot; Another example, &quot;Other diseases of respiratory system, NEC,&quot; captures &quot;respiratory&quot; and &quot;diseases&quot; but misses &quot;NEC&quot; (Necrotizing enterocolitis), which should be prioritized.</p>
<p>I've tried filtering results by semantic similarity using a biomedical model, but this approach is still imprecise and heavily dependent on the number of results. The linker often seems to prioritize entities appearing earlier in the text. I also use an abbreviation expander to handle non-standard acronym forms.</p>
<p>I think a smarter linker (not supported by scispacy) might help, or better matching at the sentence/whole text level, but I don't know much about that. (I do some filtering of results using sentence transformers, but that's just cossine sim - I couldn't find a clear cutoff that generalized well).</p>
<p>I do not have the resources/time to learn to fine-tune a new linker model+data (this is just a sub-component in my overall phd).</p>
<p>I'm looking for advice on more effective strategies for entity linking at the sentence or whole-text level without the resources to fine-tune a new model. Compatability with SciSpacy is important, since linkage to the UMLS ontology (for the KG CUI entites) is a must.</p>
","nlp, data-science, spacy, named-entity-recognition, entity-linking",
How to Separate Text and Code in Python Strings?,"<p>I've encountered an issue in python. I have a string that contains both a message and code, and I need to separate them and pass each to different functions. An example:</p>
<pre class=""lang-py prettyprint-override""><code>text = &quot;&quot;&quot;
Can you change sum operation to multiplication?

def process(a: int, b: int):
    print(a + b)
&quot;&quot;&quot;
</code></pre>
<p>The text and code sections can appear in any order. I've tried using regex to separate them, but it fails with other languages. Do you have any suggestions on how to handle this without involving LLMS?<br />
Thanks!</p>
<p>I've searched about how LLMS differentiate between code and text and found out they use Markdown formatting to separate them, like:</p>
<pre><code>&quot;See if anything is wrong with this code:&quot;

```python
def process(a: int, b: int):
    print(a - b)
- ```
</code></pre>
<p>and I'm afraid I cannot follow this structure because the given input or string is not categorized like this</p>
","python, regex, string, nlp, text-processing",
How can I share a complex spaCy NLP model across multiple Python processes to minimize memory usage?,"<p>I'm working on a multiprocessing python application where multiple processes need access to a large, pre-loaded spaCy NLP model (e.g., en_core_web_lg). Since the model is memory-intensive, I want to avoid loading it separately in each process, since I quickly run out of main memory and the object is read-only. Instead, I’d like to load it once in a shared location so that all processes can read from it without duplicating memory usage.</p>
<p>I have looked into multiprocessing.Manager and multiprocessing.shared_memory, but these approaches seem better suited to NumPy arrays, raw data buffers or simple objects, not complex objects with internal references like an NLP model. I have also looked into MPI's MPI.Win.Allocate_shared() but I ran into the same issues. Using a redis server and make rank 0 do all the processing works with MPI, but since all the processing is done by a single rank, it defeats the propose I had for using multiprocessing.</p>
<ul>
<li><p>Is there an efficient way to share a spaCy model instance across multiple processes in Python to avoid reloading it for each process?</p>
</li>
<li><p>Are there libraries or techniques specifically suited for sharing complex, read-only objects like NLP models in memory across processes?</p>
</li>
<li><p>If multiprocessing.Manager or shared_memory is viable here, are there ways to improve performance or reduce memory overhead when working with complex objects?</p>
</li>
</ul>
<p>Any suggestions or examples would be greatly appreciated! Thank you!</p>
","nlp, multiprocessing, python-multiprocessing, spacy","<p>I would strongly advise you not to treat NLP models like any other Python object. I would always prefer to load an NLP model using a microservice approach, which is more aligned with ML/software engineering best practices by separating the model logic from the main application.</p>
<p>Instead of loading the model in each process (which can be memory-intensive), the model is loaded just once in a dedicated service. This setup allows the model to be used by multiple parts of the application without duplicating memory usage, making it efficient, modular, and scalable. Not only is your concern about memory efficiency addressed, but scalability and modularity are also improved.</p>
<p>An example of implementing such a microservice using FastAPI + Docker could look like this:</p>
<pre><code># main.py: FastAPI service with spaCy model
from fastapi import FastAPI
import spacy

app = FastAPI()
nlp = spacy.load(&quot;en_core_web_lg&quot;)  # Load model once

@app.post(&quot;/process/&quot;)
async def process_text(text: str):
    doc = nlp(text)
    return {&quot;tokens&quot;: [(token.text, token.pos_) for token in doc]}
</code></pre>
<p>To containerize above FastAPI service:</p>
<pre><code># Dockerfile for the NLP model microservice
FROM python:3.9-slim
COPY requirements.txt .
RUN pip install -r requirements.txt &amp;&amp; python -m spacy download en_core_web_lg
COPY . /app
WORKDIR /app
CMD [&quot;gunicorn&quot;, &quot;-w&quot;, &quot;4&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;main:app&quot;]
</code></pre>
"
Error 404 while calling customised assistant with ChatGPT API in R,"<p>To analyse sentences, I've trained a ChatGPT assistant that I'm calling from an R function to classify sentences:</p>
<pre><code>library(openai)

Sys.setenv(
  OPENAI_API_KEY = 'XXXXXXXXXXXXXXXXXX'
)

score_sentence_CSM &lt;- function(mySentence) {
  
  myScore &lt;- create_chat_completion(
    
    model = &quot;asst_3JVWE1StXCNELM4PBbJrDe8O&quot;,
    temperature = 0.5,
    messages = list(
      list(
        &quot;role&quot; = &quot;system&quot;,
        &quot;content&quot; = &quot;You are a helpful assistant&quot;
      ),
      list(
        &quot;role&quot; = &quot;user&quot;,
        &quot;content&quot; = paste(&quot;How to classify the following sentence according to your instructions (your answer is only a number): &quot;, mySentence)
      )
    )
  )
  
  return(myScore[[&quot;choices&quot;]][[&quot;message.content&quot;]])
  
}

score_sentence_CSM(&quot;The future is bright&quot;)
</code></pre>
<p>The code returns the following error:</p>
<blockquote>
<p>Error: OpenAI API request failed [404]:
The model <code>asst_3JVWE1StXCNELM4PBbJrDe8O</code> does not exist or you do not have access to it.</p>
</blockquote>
<p>The function return an answer when the model = &quot;gpt-3.5-turbo-16k&quot;
The custom assistant works from openai playground</p>
<p>Should I change something to the code or is there something specific to be done in the configuration of the assistant?</p>
","r, nlp, openai-api, chatgpt-api, assistant",
Text Classification + NLP + Data-mining + Data Science: Should I do stop word removal and stemming before applying tf-idf?,"<p>I am working on a text classification problem. The problem is explained below:</p>

<p>I have a dataset of events which contains three columns - name of the event, description of the event, category of the event. There are about 32 categories in the dataset, such as, travel, sport, education, business etc. I have to classify each event to a category depending on its name and description.</p>

<p>What I understood is this particular task of classification is highly dependent on keywords, rather than, semantics. I am giving you two examples:</p>

<p>If the word 'football' is found either in the name or description or in both, it is highly likely that the event is about sport.</p>

<p>If the word 'trekking' is found either in the name or description or in both, it is highly likely that the event is about travel.</p>

<p>We are not considering multiple categories for an event(however, that's a plan for future !! )</p>

<p>I hope applying tf-idf before Multinomial Naive Bayes would lead to decent result for this problem. My question is:</p>

<p>Should I do stop word removal and stemming before applying tf-idf or should I apply tf-idf just on raw text? Here text means entries in name of event and description columns.</p>
","nlp, data-mining, data-science, text-classification, tf-idf",
Finding Root Form of Verbs using Curiosity-AI/Catalyst,"<p>I'm trying to find the root form of a verb. I run text through the pipeline and can identify all tokens which match <code>PartOfSpeech.VERB</code> but I don't know how to continue from there.</p>
<p>This is what I have so far:</p>
<pre><code>const string text = &quot;The disastrous cat runs after the fat field mouse.&quot;;
Catalyst.Models.English.Register();

Storage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);
var nlp = await Pipeline.ForAsync(Language.English);
var doc = new Document(text, Language.English);
nlp.ProcessSingle(doc);


foreach (var sentence in doc.TokensData)
{
    foreach (var token in sentence)
    {
        if(token.Tag == PartOfSpeech.VERB)
        {
            //  so here I'd like to the root form of the verb
        }
    }
}
</code></pre>
<p>Any help is greatly appreciated.</p>
","c#, nlp","<p>The following code (targeting .NET 8.0) illustrates one method to obtain the root form of a verb from an inflected form.</p>
<p>(I have annonoted, as code comments, the three NuGet packages (with versions) required. Most of the code is identical to your original sample above.)</p>
<pre><code>//// Installed Curiosity.Library v24.10.52882
//// Installed Catalyst v1.0.51118
//// Installed Catalyst.Models.English v1.0.30952

using Catalyst;

using Mosaik.Core;

const string text = &quot;The disastrous cat quickly runs after the fat field mouse.&quot;;
Catalyst.Models.English.Register();

Storage.Current = new DiskStorage(AppDomain.CurrentDomain.BaseDirectory);
var nlp = await Pipeline.ForAsync(Language.English);
var doc = new Document(text, Language.English);
nlp.ProcessSingle(doc);

foreach (var span in doc.Spans)
{
    foreach (var token in span.Tokens)
    {
        if (token.POS == PartOfSpeech.VERB)
        {
            Console.WriteLine($&quot;Root of the verb '{token.Value}' is '{token.Lemma}'.&quot;);
        }
    }
}

Console.WriteLine();
Console.WriteLine(&quot;Complete; press any key.&quot;);
Console.ReadKey();
</code></pre>
<p><strong>Note:</strong> For this specific sentence, I have added an adverb (&quot;quickly&quot;) before the verb (&quot;runs&quot;). Without this, the library incorrectly interprets &quot;runs&quot; as a noun. Depending on your source text, this might be an issue for you, but I believe it is separate from the question being asked.</p>
"
DSPy can&#39;t retrieve passage with text embeddings in ChromaDB,"<p>I am working on a RAG application using DSPy and ChromaDB for pdf files.</p>
<p>At first I fetched the text from the pdf and add it to the Chromadb as chunks. Also added the embeddings of the chunks. And tried to Retrieve the chunk that is related to the query using DSPy. But its getting an error</p>
<p>storing data and embeddings</p>
<pre><code>def store_document_in_chromadb(text):
    chunks = chunk_document(text)
    ids = [f'chunk_{i}' for i in range(len(chunks))]
    embeddings = [get_embedding(chunk).tolist() for chunk in chunks]

    collection.add(ids=ids, documents=chunks, embeddings=embeddings)
</code></pre>
<p>And I try to retrieve the relevant chunks like this,</p>
<pre><code>retriever_model = ChromadbRM(&quot;contracts_collection&quot;, 'db/', k=2)
dspy.settings.configure(lm=llama2_model, rm=retriever_model)

class GenerateAnswer(dspy.Signature): 
    &quot;&quot;&quot;Answer the question based on the context given.&quot;&quot;&quot;
    context = dspy.InputField(desc=&quot;may contain relevant context&quot;)
    question = dspy.InputField()
    answer = dspy.OutputField(desc=&quot;often between 5 to 10 words&quot;)

class RAG(dspy.Module): 
    def __init__(self, num_passages=2):
        super().__init__()
        self.retrieve = dspy.Retrieve(k=num_passages)
        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)
    
    def forward(self, question):
        context = self.retrieve(question).passages
        prediction = self.generate_answer(context=context, question=question)
        return dspy.Prediction(context=context, answer=prediction.answer)

with dspy.context(lm=llama2_model, rm=retriever_model):
  module = RAG()
  response = module(&quot;What is the Total Spend&quot;)
  print(response)
</code></pre>
<p>When I a running this, getting this error</p>
<p><code>InvalidDimensionException: Embedding dimension 384 does not match collection dimensionality 768</code></p>
<p>but when I remove the embedding from the ChromaDB, its retrieving the relevant chunks correctly.</p>
<p>Why is this not getting while using embeddings?</p>
","machine-learning, nlp, large-language-model, chromadb, dspy",
Good-Turing smoothing implementation problems,"<p>I want to implement the Good-Turing smoothing method which will improve my language model.</p>

<p>Let's start with the theory (for simplicity, consider the unigram model).</p>

<p>There is a corpus (e.g. a literary work or any huge text). Through it we are building a language model. After we built it, we can find the probability of occurrence of the word. I have created this model using MLE (Maximum Likelihood Estimation) with a high value of perplexity (the value of the quality of the constructed model, a high value is bad).</p>

<p>I found a way to improve it with using Good-Turing smoothing:</p>

<p><a href=""https://i.sstatic.net/8X20F.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8X20F.gif"" alt=""Good-Turing smoothing""></a></p>

<p><a href=""https://i.sstatic.net/YNI0t.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YNI0t.gif"" alt=""Good-Turing smoothing 2""></a></p>

<p>Here: </p>

<pre><code>P - the probability of use of the word
c - the number of use of the word
N_c - the count words with a frequency - c
N - the count words in the corpus
</code></pre>

<p>My code on Python 3:</p>

<pre><code>def good_turing(tokens):
    N = len(tokens) + 1    
    C = Counter(tokens)    
    N_c = Counter(list(C.values()))
    assert(N == sum([k * v for k, v in N_c.items()]))
    default_value = N_c[1] / N
    model = defaultdict(lambda: default_value)
    types = C.keys()
    B = len(types)  
    for _type in types:
        c = C[_type]
        model[_type] = (c + 1) * N_c[c + 1] / (N_c[c] * N) # Exception - ""math domain error""
    return model
</code></pre>

<p>There are problems here:</p>

<ul>
<li><p><strong>word with a frequency c + 1 can not be in the corpus</strong>, so we try to
take log(0) and get Math Domain Error</p></li>
<li><p>how to calculate <strong>the probability of the most used word</strong> after smoothing?</p></li>
</ul>

<p>After reading a few articles my study of this subject came to the construction of the following formulas:</p>

<p><a href=""https://i.sstatic.net/LuTFg.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LuTFg.gif"" alt=""estimate""></a></p>

<p>So the main question is: <strong>how to get this <em>E</em> function</strong>?
I found this:</p>

<p><a href=""https://i.sstatic.net/Fzyjf.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fzyjf.gif"" alt=""log""></a></p>

<p>but I don't know how to search <em>a</em> and <em>b</em> coefficients in scipy or etc.</p>
","python, scipy, nlp, nltk, smoothing",
Is it possible to get embeddings from NV-Embed using Candle?,"<p>What I want to do is a CLI program that outputs embeddings of an arbitrary input.
To do that, I want to do an inference with an embeddings model, and I chose <code>NV-Embed-v2</code>. My framework of choice is <a href=""https://github.com/huggingface/candle"" rel=""nofollow noreferrer"">Candle</a>, but I also looked at <a href=""https://github.com/EricLBuehler/mistral.rs"" rel=""nofollow noreferrer"">Mistral-RS</a>.</p>
<p>Basically, what I'm trying to do is this code fragment:
<a href=""https://huggingface.co/nvidia/NV-Embed-v2"" rel=""nofollow noreferrer"">https://huggingface.co/nvidia/NV-Embed-v2</a>
but with Rust and Candle.</p>
<p>What I tried is to start off with <a href=""https://github.com/huggingface/candle/blob/main/candle-examples/examples/mistral/main.rs"" rel=""nofollow noreferrer"">Mistral Candle's example</a> because the NV-Embed's HF page says: <code>Model Details / Base Decoder-only LLM: Mistral-7B-v0.1</code>.</p>
<p>I replaced the model id in the original code with <code>nvidia/NV-Embed-v2</code>, and was able to download the weights from Hugging Face, but upon loading the config, I got this:</p>
<pre><code>Error: missing field `vocab_size` at line 101 column 1
</code></pre>
<p>Then I hardcoded the values from the JSON config loaded from HF to a newly created <code>candle_transformers::models::mistral::Config</code> instance. And after that, <code>Mistral::new(&amp;config, vb)</code> fails with:</p>
<pre><code>Error: cannot find tensor model.embed_tokens.weight
</code></pre>
<p>Is there a way around that — maybe there are some other Candle-based open source works that I could use as an inspiration? Or, maybe that's a common mistake that could easily be diagnosed?</p>
","machine-learning, rust, nlp","<p>candle looking for <code>model.embed_tokens.weight</code> whereas the original tensor name is <code>embedding_model.embed_tokens.weight</code>. You just have to change this line of <code>mistral.rs</code> in candle_transformers.</p>
<pre class=""lang-rust prettyprint-override""><code>// from
let vb_m = vb.pp(&quot;model&quot;);
//to
let vb_m = vb.pp(&quot;embedding_model&quot;);
</code></pre>
"
"Dutch sentiment analysis RobBERTje outputs just positive/negative labels, netural label is missing","<p>When I run Dutch sentiment analysis RobBERTje, it outputs just positive/negative labels, netural label is missing in the data.</p>
<p><a href=""https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment"" rel=""nofollow noreferrer"">https://huggingface.co/DTAI-KULeuven/robbert-v2-dutch-sentiment</a></p>
<p>There are obvious neutral sentences/words e.g. 'Fhdf' (nonsense) and 'Als gisteren inclusief blauw' (neutral), but they both evaluate to positive or negative.</p>
<p><strong>Is there a way to get neutral labels for such examples in RobBERTje?</strong></p>
<pre><code>from transformers import RobertaTokenizer, RobertaForSequenceClassification
from transformers import pipeline
import torch

model_name = &quot;DTAI-KULeuven/robbert-v2-dutch-sentiment&quot;
model = RobertaForSequenceClassification.from_pretrained(model_name)
tokenizer = RobertaTokenizer.from_pretrained(model_name)

classifier = pipeline('sentiment-analysis', model=model, tokenizer = tokenizer)

result1 = classifier('Fhdf')
result2 = classifier('Als gisteren inclusief blauw')
print(result1)
print(result2)
</code></pre>
<p>Output:</p>
<pre><code>[{'label': 'Positive', 'score': 0.7520257234573364}]
[{'label': 'Negative', 'score': 0.7538396120071411}]
</code></pre>
","python, nlp, bert-language-model, roberta-language-model","<p>This model was trained only on <code>negative</code> and <code>positive</code> labels. Therefore, it will try to categorize every input as positive or negative, even if it is nonsensical or neutral.</p>
<p>what you can do is to:
1- Find other models that was trained to include <code>neutral</code> label.
2- Fine-tune this model on a dataset that includes <code>neutral</code> label.
3- Empirically define a threshold based on the confidence outputs and interpret it as <code>neutral</code>.</p>
<p>The first 2 choices are extensive in effort. I would suggest you go with the third option for a quick workaround. Try feeding the model with a few neutral input and observe the range of confidence score in the output. then use that threshold to classify as <code>neutral</code>.</p>
<p>Here's a sample:</p>
<pre><code>def classify_with_neutral(text, threshold=0.5):
    result = classifier(text)[0]  # Get the classification result
    if result['score'] &lt; threshold:
        result['label'] = 'Neutral'  # Override label to 'Neutral'
    return result
</code></pre>
"
Target modules for applying PEFT / LoRA on different models,"<p>I am looking at a few <a href=""https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o#scrollTo=NuAx3zBeUL1q"" rel=""noreferrer"">different</a> <a href=""https://www.philschmid.de/fine-tune-flan-t5-peft"" rel=""noreferrer"">examples</a> of using PEFT on different models. The <code>LoraConfig</code> object contains a <code>target_modules</code> array. In some examples, the target modules are <code>[&quot;query_key_value&quot;]</code>, sometimes it is <code>[&quot;q&quot;, &quot;v&quot;]</code>, sometimes something else.</p>
<p>I don't quite understand where the values of the target modules come from. Where in the model page should I look to know what the LoRA adaptable modules are?</p>
<p>One example (for the model Falcon 7B):</p>
<pre><code>peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;,
    target_modules=[
        &quot;query_key_value&quot;,
        &quot;dense&quot;,
        &quot;dense_h_to_4h&quot;,
        &quot;dense_4h_to_h&quot;,
    ]
</code></pre>
<p>Another example (for the model Opt-6.7B):</p>
<pre><code>config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],
    lora_dropout=0.05,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;
)
</code></pre>
<p>Yet another (for the model Flan-T5-xxl):</p>
<pre><code>lora_config = LoraConfig(
 r=16,
 lora_alpha=32,
 target_modules=[&quot;q&quot;, &quot;v&quot;],
 lora_dropout=0.05,
 bias=&quot;none&quot;,
 task_type=TaskType.SEQ_2_SEQ_LM
)
</code></pre>
","nlp, huggingface-transformers, huggingface, fine-tuning, peft","<p>Let's say that you load some model of your choice:</p>
<p><code>model = AutoModelForCausalLM.from_pretrained(&quot;some-model-checkpoint&quot;)</code></p>
<p>Then you can see available modules by printing out this model:</p>
<p><code>print(model)</code></p>
<p>You will get something like this (SalesForce/CodeGen25):</p>
<pre><code>LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(51200, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=51200, bias=False)
)
</code></pre>
<p>In my case, you can find the LLamaAttention module that contains q_proj, k_proj, v_proj, and o_proj. And this are some modules available for LoRA.</p>
<p>I suggest you reading more about which modules to use in <a href=""https://arxiv.org/abs/2106.09685"" rel=""noreferrer"">LoRA paper</a>.</p>
"
Path_To_Connection Error when reading in Docx file to R with Officer and Docxtractor,"<p>I have several hundred documents from a legal database in &quot;.docx&quot; format. I am trying to do some NLP work on the docs, but can't seem to get past 0. Don't want to post the test doc because of copyright. I keep getting an xml2 <strong>path_to_connection</strong> error when using officer and docxtractr and can't figure out how to fix it.</p>
<p>If I create a dummy docx, I can read it into R without an issue. However, the same steps applied to the downloaded documents fail. Initially, I thought the issue may have been with the filename, so I modified one file name to test.docx and the issue still remained. I then thought maybe I was running into some network issue that was presenting the temp directory from writing correctly on my work computer (windows), but I ran into the same issue on a home computer (Mac). Next I dug into the underlying functions and realized that there is something weird going on with the paths.</p>
<p>On the work computer (windows) I had a combination of backslashes and forward slashes in the path. I also noticed that the path contained subdirectories that when I explored the path manually were not there. I don't know if this is because the function failed partially or if it was just pointing to an incorrect location. The subdirectory &quot;word&quot; was not there and appeared in both the Mac and windows tests. Here is the Mac reprex</p>
<pre class=""lang-r prettyprint-override""><code>library(tidyverse)
library(officer)
#&gt; Warning: package 'officer' was built under R version 4.4.1

test &lt;- officer::read_docx(&quot;~/Downloads/test.docx&quot;)
#&gt; Warning in read_core_properties(package_dir): No properties found. Using
#&gt; properties from template file in officer.
#&gt; Error in `path_to_connection()`:
#&gt; ! 
#&gt;   '/var/folders/q2/8qcfd1qd5rqfvhr2x9l2k24m0000gp/T//RtmpZ6jZ5U/file95017ae662ea/word/document.xml'
#&gt;   does not exist.
reprex::reprex()
#&gt; ℹ Non-interactive session, setting `html_preview = FALSE`.
#&gt; CLIPR_ALLOW has not been set, so clipr will not run interactively
#&gt; Error in switch(where, expr = stringify_expression(x_expr), clipboard = ingest_clipboard(), : EXPR must be a length 1 vector
</code></pre>
<p>I think the traceback shows it aborts on the path check:</p>
<pre class=""lang-none prettyprint-override""><code>11. signal_abort(cnd, .file)
10. rlang::abort(message, ..., call = call, use_cli_format = TRUE,.frame = .frame)
9. cli::cli_abort(msg, call = call)
8. check_path(path)
7. path_to_connection(x)
6. read_xml.character(file)
5. read_xml(file)
4. super$feed(file.path(private$package_dir, &quot;word&quot;, main_file))
3. initialize(...)
2. docx_part$new(package_dir, main_file = &quot;document.xml&quot;, cursor = &quot;/w:document/w:body/*[1]&quot;,body_xpath = &quot;/w:document/w:body&quot;)
1. officer::read_docx(&quot;~/Downloads/test.docx&quot;)

</code></pre>
<p>I think my error may have something to do with using the template files and mapping that back to the path when it can't find the document properties. I just don't know how to get around it. If I am not doing something incredibly stupid, is there another method I could use to read in the documents?</p>
<p><sup>Created on 2024-11-01 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.1.1</a></sup></p>
","r, nlp, docx, officer, xml2",
How to reconstruct text entities with Hugging Face&#39;s transformers pipelines without IOB tags?,"<p>I've been looking to use Hugging Face's Pipelines for NER (named entity recognition). However, it is returning the entity labels in inside-outside-beginning (IOB) format but <a href=""https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)"" rel=""noreferrer"">without the IOB labels</a>. So I'm not able to map the output of the pipeline back to my original text. Moreover, the outputs are masked in BERT tokenization format (the default model is BERT-large).</p>

<p>For example: </p>

<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
nlp_bert_lg = pipeline('ner')
print(nlp_bert_lg('Hugging Face is a French company based in New York.'))
</code></pre>

<p>The output is:</p>

<pre><code>[{'word': 'Hu', 'score': 0.9968873858451843, 'entity': 'I-ORG'},
{'word': '##gging', 'score': 0.9329522848129272, 'entity': 'I-ORG'},
{'word': 'Face', 'score': 0.9781811237335205, 'entity': 'I-ORG'},
{'word': 'French', 'score': 0.9981815814971924, 'entity': 'I-MISC'},
{'word': 'New', 'score': 0.9987512826919556, 'entity': 'I-LOC'},
{'word': 'York', 'score': 0.9976728558540344, 'entity': 'I-LOC'}]
</code></pre>

<p>As you can see, New York is broken up into two tags.</p>

<p>How can I map Hugging Face's NER Pipeline back to my original text?</p>

<p>Transformers version: 2.7</p>
","nlp, tokenize, transformer-model, named-entity-recognition, huggingface-transformers","<p>EDIT 12/2023:
As pointed out, the <code>grouped_entities</code> parameter has been deprecated. The correct way is to use the <code>aggregation_strategy</code> parameters as pointed in the <a href=""https://huggingface.co/transformers/v4.10.1/_modules/transformers/pipelines/token_classification.html"" rel=""noreferrer"">source code </a>.
For instance:</p>
<pre><code>text = 'Hugging Face is a French company based in New York.'
tagger = pipeline(task='ner', aggregation_strategy='simple')
named_ents = tagger(text)
pd.DataFrame(named_ents)
</code></pre>
<p>Gives the following output</p>
<pre><code>[
   {
      &quot;entity_group&quot;:&quot;ORG&quot;,
      &quot;score&quot;:0.96934015,
      &quot;word&quot;:&quot;Hugging Face&quot;,
      &quot;start&quot;:0,
      &quot;end&quot;:12
   },
   {
      &quot;entity_group&quot;:&quot;MISC&quot;,
      &quot;score&quot;:0.9981816,
      &quot;word&quot;:&quot;French&quot;,
      &quot;start&quot;:18,
      &quot;end&quot;:24
   },
   {
      &quot;entity_group&quot;:&quot;LOC&quot;,
      &quot;score&quot;:0.9982121,
      &quot;word&quot;:&quot;New York&quot;,
      &quot;start&quot;:42,
      &quot;end&quot;:50
   }
]
</code></pre>
<p>ORIGINAL ANSWER:
The 17th of May, a new pull request <a href=""https://github.com/huggingface/transformers/pull/3957"" rel=""noreferrer"">https://github.com/huggingface/transformers/pull/3957</a> with what you are asking for has been merged, therefore now our life is way easier, you can you it in the pipeline like</p>
<pre><code>ner = pipeline('ner', grouped_entities=True)
</code></pre>
<p>and your output will be as expected. At the moment you have to install from the master branch since there is no new release yet. You can do it via</p>
<pre><code>pip install git+git://github.com/huggingface/transformers.git@48c3a70b4eaedab1dd9ad49990cfaa4d6cb8f6a0
</code></pre>
"
How do i add filtration by metadata to EnsembleRetriever.invoke() made with FIASS.as_retrivier() and BM25Retriever from LungChain?,"<p>My data is messages wit label ('supply', 'demand') as metadata:</p>
<pre><code>Document(metadata={'label': 'supply'}, page_content=&quot;I want to sell smth&quot;)
</code></pre>
<p>and i have a query to search pair for:</p>
<pre><code>query = 'I want to buy smth'
</code></pre>
<p>Here is my code:</p>
<pre><code>vectorstore = FAISS.from_embeddings(text_embeddings, metadatas=metadatas, embedding=model)
dense_retriever = vectorstore.as_retriever()
bm25_retriever = BM25Retriever.from_texts(texts=texts, metadatas=metadatas)
ensemble_retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.8, 0.2])

ensemble_retriever.invoke(&quot;I want to buy smth&quot;)
</code></pre>
<p>if the query label is &quot;demand&quot; ensemble_retriever should seek only for pair that class is &quot;supply&quot;.</p>
<p>Is there a way to add filtration to both of retriviers?</p>
<p>What i tried:</p>
<pre><code>ensemble_retriever.invoke(&quot;I want to buy smth&quot;, filter={'label':'supply')
</code></pre>
<p>but as kwargs it doesn`t work nor as anything else work as kwarg argument like k...</p>
<p>And i tried:</p>
<pre><code>dense_retriever = vectorstore.as_retriever(search_kwargs={'filter':{'label':'demand'}, 'k': 3})
</code></pre>
<p>but its rather not than works and i dont want to initilize retrivier every search :|<br />
!!!!!<br />
<strong>UPD</strong> It is not possible in current version</p>
","python, nlp, py-langchain",
How to get training dataset of OpenNLP models?,"<p>I am using the following models of OpenNLP:</p>

<pre><code>en-parser-chunking.bin
en-ner-person.bin
en-ner-location.bin
en-ner-organization.bin
</code></pre>

<p>I want to append my data in the training dataset on which these models are trained. So please tell me from where I can get that raw dataset?</p>
","machine-learning, nlp, text-mining, opennlp",
How to reliably find synonyms within a corpus of data?,"<p>I'm working on a project where I need to identify synonyms for terms across a specific corpus of product data. The data is a list of products, and I'm trying to find similar terms that appear repeatedly in the titles, descriptions and other fields. For example searching for &quot;baby&quot; should return &quot;newborn&quot;, &quot;toddler&quot;, &quot;infant&quot; if they're available in the corpus.</p>
<p>I've experimented with some pre-trained embeddings in <code>gensim</code>, specifically the <code>&quot;glove-wiki-gigaword-100&quot;</code> model. This got me some reasonable results, but it missed a few obvious synonyms and included some unrelated terms as well. I also tried using Word2Vec, but my corpus of products is too small to train a high-quality custom model, so it didn't yield very useful results either.</p>
<p>Here's a simplified version of my most successful code:</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.downloader import load as gensim_load
from typing import List, Set, Dict
from .models import AlgoProduct

def get_corpus_words(products: List[AlgoProduct]) -&gt; Set[str]:
    corpus_words = set()
    for p in products:
        corpus_words.add(p.object.lower())
        corpus_words.update([m.lower() for m in p.words_of_interest])
    return corpus_words

def load_pretrained_model():
    model = gensim_load(&quot;glove-wiki-gigaword-100&quot;)
    return model

def get_similar_words(model, word: str, corpus_words: Set[str], topn: int = 10) -&gt; Set[str]:
    similar_words = set()
    try:
        top_similar = model.most_similar(word.lower(), topn=topn)
        for similar_word, similarity in top_similar:
            if similar_word in corpus_words:
                similar_words.add(similar_word)
    except KeyError:
        pass
    return similar_words

def get_synonyms(products: List[AlgoProduct]) -&gt; Dict[str, Set[str]]:
    synonyms: Dict[str, Set[str]] = {}
    corpus_words = get_corpus_words(products)
    model = load_pretrained_model()

    for word in corpus_words:
        synonyms[word] = get_similar_words(model, word, corpus_words)

    return synonyms
</code></pre>
<p>This function builds a dictionary with each term in the corpus mapped to its synonyms. But the results are inconsistent—many entries have empty sets or miss expected synonyms:</p>
<pre class=""lang-py prettyprint-override""><code>{'baby': {'girl', 'toddler', 'newborn'}, 'bibs': set(), 'shoes': {'boots'}, 'newborn': {'baby', 'toddler'}, ... }
</code></pre>
<p>In some of my previous iterations, I was getting overly broad matches or just odd, unrelated terms:</p>
<pre class=""lang-py prettyprint-override""><code>{'Nest': [], 'baby': ['for', 'toddler', 'boots', 'sleeping', 'strap'], 'nappy': ['blanket', 'hipseat', 'shoes'], ...}
</code></pre>
<p>The current iteration is okay, depending on the product list it can get pretty close, but I am looking for ways to further optimize and improve the reliability. I am certain that this is not the most optimal approach.</p>
<p>To summarize, I basically need to find synonyms in my own corpus of data, in theory I can just find all synonyms and then figure out what appears in my corpus, but that seems relatively inefficient and I doubt it would be as reliable.</p>
","python, nlp, gensim",
Why is Keras pretrained BERT MaskedLM producing inconsistent predictions?,"<p>I am trying to use keras-nlp  with a pretrained masked BERT model to predict some tokens in a sequence. However the model produces inconsistent results. What could be wrong or am i misunderstanding something?</p>
<pre><code>import keras
import keras_nlp
import numpy as np
import tensorflow as tf

preprocessor = keras_nlp.models.BertMaskedLMPreprocessor.from_preset(
    &quot;bert_base_en&quot;
)
masked_lm = keras_nlp.models.BertMaskedLM.from_preset(
    &quot;bert_base_en&quot;,
    load_weights=True,
    preprocessor=None,
)

test_sequence = preprocessor([&quot;The capital of France is Paris.&quot;]) 
outputs = masked_lm(test_sequence[0])
predicted_ids = tf.math.argmax(outputs, axis=-1)
</code></pre>
<p>In this example the preprocessor produces the following sequence [101, 1109, 2364, 1104, 1699,  103,  103,  119,  102,0,0,...] with mask positions [5,6,0,0,0,0....]. The predicted_ids are [28059, 18994, 21690, 21690,.....] which is [b'##saur', b'##hetic', b'##lani', b'##lani',....] instead of something remotely close to 'is', 'Paris'.</p>
","python, keras, nlp, bert-language-model, keras-nlp",
Importing deepspeech error: Module not found,"<p>I am trying to install <code>DeepSearch</code> library so that I can use the pretrained model to build the Speech to Text Project.</p>
<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-8-a1bcd52700aa&gt; in &lt;cell line: 1&gt;()
----&gt; 1 import deepspeech
      2 
      3 # Path to the pre-trained model files
      4 model_path = &quot;/content/deepspeech-0.9.3-models.pbmm&quot;
      5 scorer_path = &quot;/content/deepspeech-0.9.3-models.scorer&quot;  # Optional, for improved performance

ModuleNotFoundError: No module named 'deepspeech'
</code></pre>
<hr />
<p>I have got the module not found error, and I tried using version also but it gives me the error of couldn't found the suitable version.</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement deepspeech==0.9.3 (from versions: none)
ERROR: No matching distribution found for deepspeech==0.9.3
</code></pre>
<hr />
<p>I tried using <code>deepspeech github</code> directory when I tried to install the <code>setupfile</code> and requirements.txt. I have got the error:</p>
<pre><code> Collecting absl-py==0.9.0
  Downloading absl-py-0.9.0.tar.gz (104 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.0/104.0 kB 3.3 MB/s eta 0:00:00
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─&gt; See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Preparing metadata (setup.py) ... error
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─&gt; See above for output.
</code></pre>
<hr />
<p>So how to install the deepspeech library?</p>
","python, deep-learning, nlp, mozilla-deepspeech","<p>Latest release of deepspeech was 3 years back in December 2020:</p>
<p><a href=""https://i.sstatic.net/33bm7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/33bm7.png"" alt=""enter image description here"" /></a></p>
<p>You are not able to install deepsearch because it only supports <code>python &lt;= 3.6</code>  which is outdated. If you want to use, you can download older version of python separately and install deepsearch.</p>
<p><a href=""https://i.sstatic.net/eNviJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eNviJ.png"" alt=""enter image description here"" /></a></p>
<p><strong><a href=""https://pypi.org/project/deepspeech/#description"" rel=""nofollow noreferrer"">https://pypi.org/project/deepspeech/#description</a></strong></p>
"
KV caching for varying length texts,"<p>I am trying to do some strucutured text extraction using some kv caching tricks. For this example I will use the following model and data:</p>
<pre class=""lang-py prettyprint-override""><code>model_name = &quot;Qwen/Qwen2.5-0.5B-Instruct&quot;

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# data
text = &quot;&quot;&quot;We introduce Mistral 7B, a 7–billion-parameter language model engineered for
superior performance and efficiency. Mistral 7B outperforms the best open 13B
model (Llama 2) across all evaluated benchmarks, and the best released 34B
model (Llama 1) in reasoning, mathematics, and code generation. Our model
leverages grouped-query attention (GQA) for faster inference, coupled with sliding
window attention (SWA) to effectively handle sequences of arbitrary length with a
reduced inference cost. We also provide a model fine-tuned to follow instructions,
Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and
automated benchmarks. Our models are released under the Apache 2.0 license.
Code: &lt;https://github.com/mistralai/mistral-src&gt;
Webpage: &lt;https://mistral.ai/news/announcing-mistral-7b/&gt;&quot;&quot;&quot;

template = &quot;&quot;&quot;{
    &quot;Model&quot;: {
        &quot;Name&quot;: &quot;&quot;,
        &quot;Number of parameters&quot;: &quot;&quot;,
        &quot;Number of max token&quot;: &quot;&quot;,
        &quot;Architecture&quot;: []
    },
    &quot;Usage&quot;: {
        &quot;Use case&quot;: [],
        &quot;Licence&quot;: &quot;&quot;
    }
}&quot;&quot;&quot;
</code></pre>
<p>Without kv caching it works in the following way and gives reasonable results (and not amazing given the model size).</p>
<pre class=""lang-py prettyprint-override""><code>def get_text_with_chat_template(text, key):
    prompt = f&quot;### Text:\n{text}\n### Required Key:\n{key}&quot;
    messages = [
        {
            &quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant that can extract values given a requested key. \
            Be concise and precise. \
            Don't repeat the key in the answer.&quot;
        },
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}
    ]
    text_with_chat_template = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    return text_with_chat_template

@torch.inference_mode()
def get_key_value(text_with_chat_template):
    batch_encodings = tokenizer([text_with_chat_template], return_tensors=&quot;pt&quot;, truncation=True, padding=True, max_length=1000).to(model.device)
    pred_ids = model.generate(**batch_encodings, max_new_tokens=200)
    output = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    return output[-1].split(&quot;assistant\n&quot;)[1].strip()

keys = [&quot;Model Name&quot;, &quot;Number of parameters&quot;, &quot;Number of max token&quot;, &quot;Architecture&quot;, &quot;Use case&quot;, &quot;Licence&quot;]
for key in keys:
    text_with_chat_template = get_text_with_chat_template(text, key)
    print(key, &quot;: &quot;, get_key_value(text_with_chat_template))
</code></pre>
<p>I can do slightly faster than above by caching the query text and a bit more:</p>
<pre class=""lang-py prettyprint-override""><code>root_prompt = text_with_chat_template.split(&quot;Required Key:\n&quot;)[0] + &quot;Required Key:\n&quot;
root_inputs = tokenizer(text=root_prompt, padding=&quot;longest&quot;, return_tensors=&quot;pt&quot;).to(device)
with torch.inference_mode():
    kv_cache = model(**root_inputs, return_dict=True).past_key_values

prompt_end = &quot;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&quot;

with torch.inference_mode():
    for key in keys:
        batch_encodings = tokenizer(text= key + prompt_end, padding=True, truncation=True, return_tensors=&quot;pt&quot;).to(device)
        batch_encodings[&quot;input_ids&quot;] = torch.cat([root_inputs[&quot;input_ids&quot;], batch_encodings[&quot;input_ids&quot;]], dim=-1)
        batch_encodings[&quot;attention_mask&quot;] = torch.cat([root_inputs[&quot;attention_mask&quot;], batch_encodings[&quot;attention_mask&quot;]], dim=-1)
        pred_ids = model.generate(**batch_encodings, past_key_values=kv_cache, max_new_tokens=200)
        output = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
        print(key, &quot;: &quot;, output[0].split(&quot;assistant\n&quot;)[1].strip())

# Model Name :  Mistral 7B
# Number of parameters :  7
# Number of max token :  7
# Architecture :  Language Model Architecture
# Use case :  Superior performance and efficiency
# Licence :  Apache 2.0
</code></pre>
<p>which gives similar results to the naive for loop.</p>
<p>Question
Now the issue comes when I try to batch the above. This may be due to how everything is padded on the right, or it may be due to the kv caching itself. But I am unsure how to fix it. This is what I have tried:</p>
<pre class=""lang-py prettyprint-override""><code>expanded_kv_cache = tuple(
    (
        k.expand(len(keys), -1, -1, -1), 
        v.expand(len(keys), -1, -1, -1)
    ) 
    for k, v in kv_cache
)

batch_encodings = tokenizer(
    text= [key + prompt_end for key in keys], 
    padding=True, 
    truncation=True, 
    return_tensors=&quot;pt&quot;
).to(device)
batch_encodings[&quot;input_ids&quot;] = torch.cat([root_inputs[&quot;input_ids&quot;].expand(len(keys), -1), batch_encodings[&quot;input_ids&quot;]], dim=-1)
batch_encodings[&quot;attention_mask&quot;] = torch.cat([root_inputs[&quot;attention_mask&quot;].expand(len(keys), -1), batch_encodings[&quot;attention_mask&quot;]], dim=-1)

pred_ids = model.generate(**batch_encodings, past_key_values=expanded_kv_cache, max_new_tokens=20)
outputs = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
for output, key in zip(outputs, keys):
    print(key, &quot; : &quot;, output.split(&quot;assistant\n&quot;)[1].strip())

# Model Name  :  Human
# Number of parameters  :  Human
# Number of max token  :  7
# Architecture  :  Human Language Model
# Use case  :  Human and automated
# Licence  :  Human &amp; Automated
</code></pre>
<p>I tried shifting the padding to the left with even worse results</p>
<pre class=""lang-py prettyprint-override""><code>def shift_zeros_left(input_ids, attention_mask):
    # Get the sorted indices for each row in attention_mask (sort by descending order)
    sorted_indices = attention_mask.argsort(dim=1, descending=False)
    # Reorder both input_ids and attention_mask based on sorted indices
    shifted_input_ids = torch.gather(input_ids, 1, sorted_indices)
    shifted_attention_mask = torch.gather(attention_mask, 1, sorted_indices)
    
    return shifted_input_ids, shifted_attention_mask

shifted_input_ids, shifted_attention_mask = shift_zeros_left(
    batch_encodings['input_ids'], 
    batch_encodings['attention_mask']
)
pred_ids = model.generate(input_ids=shifted_input_ids, attention_mask=shifted_attention_mask, past_key_values=expanded_kv_cache, max_new_tokens=20)
outputs = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
for output, key in zip(outputs, keys):
    print(key, &quot; : &quot;, output.split(&quot;assistant\n&quot;)[1].strip())

# Model Name  :  concise precise precise precise
# Number of parameters  :  concise precise precision
# Number of max token  :  7
# Architecture  :  Conunductiveness Given a requested key.             Be concise and precise.
# Use case  :  Constance is concise.
# Licence  :  concise precise precise
</code></pre>
<p>TIA</p>
","pytorch, nlp, huggingface-transformers, transformer-model",
Clustering for grouping sentences and then caption the cluster with a short name,"<p>I have a series of text utterances in summary form (form of sentences). I am trying to perform clustering and group them with similarity in context (not in literal meaning) and report the clusters with the common group items. The below code is what I have written in python.</p>
<pre><code>import os
import time

from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)

sentences = [
    &quot;My internet speed is very slow. There is no signal available from the router&quot;,
    &quot;Close the TV and Broadband subscription. I want to move to another plan&quot;,
    &quot;Broadband is not up to the mark. I could not work with the current speed that is offered&quot;,
    &quot;My Bills for the current month have shot up by 200%. Can you take just 50% of it for the current month and credit the remaining as backlog for next month.&quot;,
    &quot;I am not happy with the service. Please deactivate my subscription and I don't need the services anymore&quot;,
    &quot;I wanted to make the bill payment, but my card is not accepted for payment through your portal. Please resolve&quot;,
    &quot;Why is my Bill showing up extra charges of 48$ this month. I never purchased anything additional&quot;,
    &quot;I have already made the payment for the previous month. Why is that showing up as arrears this month&quot;,
    &quot;John is the most loyal professional footballer ever&quot;,
    &quot;This is the most amazing place that I have ever visited!&quot;
] 

# Embeddings using Sentence Transformers - all-MiniLM-L6-v2
embeddings = model.encode(sentences, show_progress_bar=True, convert_to_tensor=True)

# Clustering process


print(&quot;Start clustering&quot;)
start_time = time.time()

# Two parameters to tune:
# min_cluster_size: Only consider cluster that have at least n elements
# threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar
clusters = util.community_detection(embeddings, min_community_size=1, threshold=0.43)

print(f&quot;Clustering done after {time.time() - start_time:.2f} sec&quot;)

# Print for all clusters the top n and bottom n elements
for i, cluster in enumerate(clusters):
    print(f&quot;\nCluster {i + 1}, #{len(cluster)} Elements &quot;)
    for sentence_id in cluster:
        print(&quot;\t&quot;, sentences[sentence_id])
    # print(&quot;\t&quot;, &quot;...&quot;)
    # for sentence_id in cluster[3:]:
    #     print(&quot;\t&quot;, sentences[sentence_id])
</code></pre>
<p>I get the output as below</p>
<pre><code>Start clustering
Clustering done after 0.00 sec

Cluster 1, #4 Elements 
     I have already made the payment for the previous month. Why is that showing up as arrears this month
     Why is my Bill showing up extra charges of 48$ this month. I never purchased anything additional
     I wanted to make the bill payment but my card is not accepted for payment through your portal. Please resolve
     My Bills for the current month has shot up by 200%. Can you take just 50% of it for the current month and credit the remaining as backlog for next month.

Cluster 2, #2 Elements 
     My internet speed is very slow. There is no signal available from the router
     Broadband is not upto the mark. I could not work with the current speed that is offered

Cluster 3, #2 Elements 
     Close the TV and Broadband subscription. I want to move to another plan
     I am not happy with the service. Please deactivate my subscription and I dont need the servcies anymore

Cluster 4, #1 Elements 
     John is the most loyal professional footballer ever

Cluster 5, #1 Elements 
     This is the most amazing place that I have ever visited!
</code></pre>
<p>My main question is that when I kept the threshold to a value of 0.8 and above, I presumed it will give me the right results but actually, it didn't.
When I made it close to ~0.45, then I got this result. This is just a simple experiment. So, should I vary this threshold depending on the new dataset?</p>
<p>Also, I would want to know on how to group some of the general messages / sentences under 'General category'. Is there a mechanism to cluster them all under a 'General category'</p>
<p>I also want some advice on how to caption these clusters with a name - such as Billing, Plan upgrade, Network speed performance etc.</p>
","nlp, cluster-analysis, sentence-transformers",
What&#39;s the best way to compare several corpora in natural language?,"<p>I've been doing LDA topic models of narrative reports in natural language for a research project (using Gensim with python). I have several smallish corpora (from 1400 to 200 docs each – I know, that's tiny!) that I'd like to compare, but I don't know how to do that beyond looking at each LDA model (for instance with pyLDAviz). My academic background is not in CS, and I'm still a bit new to NLP.</p>

<p>What are some good ways to compare topics across corpora/topic models? For instance, is it possible to estimate how much two LDA models overlap? Or are there other ways to assess the topic similarity of several corpora?</p>

<p>Thanks in advance for your help!</p>
","python, nlp, nltk, lda, topic-modeling",
Implementing tf-idf in wordclouds,"<p>I have some google reviews for some universities in a dataframe like below df_unis. The column <code>uni_name</code> contains the university names. I wish to create word clouds for each university separately but in such a way that words appear larger in the word cloud of a university if they not only appear more frequently in the reviews of the university but also less frequently in the reviews of the other universities. I believe this is the idea behind tf-idf and upgrading from relying on simple frequency to &quot;importance.&quot; I'm after this importance and the competitive insights from the word clouds for different universities when considered in tandem.</p>
<p>I have written this code but I suspect that since td-idf happens inside the loop, it's done for each university reviews separately and I'm not achieving my above goal. Is that right? I tried bringing td-idf outside the loop but then couldn't find a way to produce separate word clouds for each university. Any advice is highly appreciated.</p>
<pre><code>df_unis = pd.DataFrame({'review' : ['this is a good school' , 'this school was not worth it', 'aah', 'mediocre school with good profs'], 
                       uni_name': ['a', 'b', 'b', 'a']})


corpus = df_unis.groupby('uni_name')['review'].sum() 

for group in corpus.index.unique():
    try:

        vectorizer = TfidfVectorizer(stop_words='English', ngram_range= ( 1 , 3 ) ) 

        vecs = vectorizer.fit_transform([corpus.loc[group]]) 
        feature_names = vectorizer.get_feature_names_out ()
        dense = vecs.todense()
        lst = dense.tolist()
        tf_idf = pd.DataFrame(lst, columns=feature_names, index = [group])

        cloud = WordCloud().generate_from_frequencies(tf_idf.T.sum(axis=1))
</code></pre>
","nlp, text-mining, tf-idf, word-cloud",
Question about data_collator throwing a key error in Hugging face,"<p>I am trying to use <code>data_collator</code> function in hugging face using this code:</p>
<pre><code>datasets = dataset.train_test_split(test_size=0.1)
train_dataset = datasets[&quot;train&quot;]
val_dataset = datasets[&quot;test&quot;]

print(type(train_dataset))

def data_collator(data):
# Initialize lists to store pixel values and input ids
   pixel_values_list = []
   input_ids_list = []

# Iterate over each sample in the data
   for item in data:      
      pixel_values_list.append(torch.tensor(item[&quot;pixel_values&quot;]))
      input_ids_list.append(torch.tensor(item[&quot;input_ids&quot;]))


return {
    &quot;pixel_values&quot;: torch.stack(pixel_values_list),
    &quot;labels&quot;: torch.stack(input_ids_list)
}
</code></pre>
<p>the train_data has 5 keys including <code>input_ids</code>. However, when I <code>print(data[0])</code> inside the <code>data_collator</code> function, I only see 1 key, which is giving an error when running the trainer:</p>
<pre><code>Traceback (most recent call last):
 File &quot;caption-code.py&quot;, line 134, in &lt;module&gt;
trainer.train()
  File &quot;C:\Users\moham\anaconda3\envs\transformer\lib\site- 
 packages\transformers\trainer.py&quot;, line 1321, in train
ignore_keys_for_eval=ignore_keys_for_eval,
 File &quot;C:\Users\moham\anaconda3\envs\transformer\lib\site- 
  packages\transformers\trainer.py&quot;, line 1528, in _inner_training_loop
  for step, inputs in enumerate(epoch_iterator):
    File &quot;C:\Users\moham\anaconda3\envs\transformer\lib\site- 
  packages\torch\utils\data\dataloader.py&quot;, line 521, in __next__
  data = self._next_data()
 File &quot;C:\Users\moham\anaconda3\envs\transformer\lib\site- 
  packages\torch\utils\data\dataloader.py&quot;, line 561, in _next_data
data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
 File &quot;C:\Users\moham\anaconda3\envs\transformer\lib\site- 
 packages\torch\utils\data\_utils\fetch.py&quot;, line 52, in fetch
 return self.collate_fn(data)
 File &quot;caption-code.py&quot;, line 102, in data_collator
 input_ids_list.append(item[&quot;input_ids&quot;])
  KeyError: 'input_ids'
</code></pre>
<p>I am using the trainer function as follows:</p>
<pre><code>training_args = Seq2SeqTrainingArguments(
predict_with_generate=True,
evaluation_strategy=&quot;epoch&quot;,
per_device_train_batch_size=4,
per_device_eval_batch_size=4,
output_dir=&quot;C:/Users/moham/Desktop/Euler/output&quot;,
logging_dir=&quot;./logs&quot;,
logging_steps=10,
save_steps=10,
eval_steps=10,
warmup_steps=10,
max_steps=100,  # adjust as needed
overwrite_output_dir=True,
save_total_limit=3,
 )
trainer = Seq2SeqTrainer(
model=model,
args=training_args,
train_dataset=train_dataset,
eval_dataset=val_dataset,
data_collator=data_collator,
tokenizer=tokenizer,
compute_metrics=compute_exact_match
 )
trainer.train()
</code></pre>
","python, pytorch, nlp, huggingface-transformers, huggingface-tokenizers","<p>The actual issue is in your <code>Seq2SeqTrainingArguments</code> which is leading the error in your <code>data_collator()</code>.</p>
<p><strong>Reason:</strong> The <code>.trainer()</code> is by default removing any <em>unknown</em> columns (not present in the model's <code>forward</code> method) from your data when you are providing a custom <code>data_collator()</code>. As a result even though each sample in your <code>train_dataset</code> has all the keys, when you send that to <code>data_collator()</code>, the <code>.trainer()</code> automatically removes the unknown columns.</p>
<p><strong>Solution:</strong> You need to include an argument in your training arguments like the following:</p>
<pre><code>training_args = Seq2SeqTrainingArguments(
predict_with_generate=True,
remove_unused_columns=False, 
...)
</code></pre>
<p>The <code>remove_unused_columns=False,</code> would prevent the default behaviour and you'd get the entire data in <code>data_collator()</code>. This <a href=""https://github.com/huggingface/transformers/issues/7823"" rel=""nofollow noreferrer"">issue</a> would be useful for further reference.</p>
"
How to Log Custom Metrics with Metadata in Hugging Face Trainer during Evaluation?,"<p>I'm working on a sentence regression task using Hugging Face’s Trainer. Each sample consists of:</p>
<p>input_ids: The tokenized sentence.
labels: A numerical scalar target (for regression).
metadata: A categorical string field (e.g., project_name or task_type).
My Goal:
I want to log custom metrics with respect to the metadata during evaluation. Specifically, I need to calculate the loss grouped by each metadata category (e.g., loss per project or task). the main loss for gradient computation is still the global loss but i want to pass the metadata to the compute_metrics function</p>
<p>What I’ve Tried:
Modified the Dataset:
My dataset returns a dictionary containing:</p>
<pre><code>{
    &quot;input_ids&quot;: input_ids,
    &quot;labels&quot;: torch.tensor([numerical_score]),  # Scalar target
    &quot;metadata&quot;: project_name  # Metadata field
}
</code></pre>
<p>Updated the Collator:
I ensured that all elements (including metadata) are passed correctly to the model.</p>
<p>Trainer Setup:
I enabled include_inputs_for_metrics=True to access inputs in compute_metrics:</p>
<pre><code>Trainer(..., compute_metrics=compute_metrics, args=TrainingArguments(include_inputs_for_metrics=True))
</code></pre>
<p>However, metadata still gets lost in the evaluation loop, and I can’t access it directly inside compute_metrics.</p>
<p>The Problem:
The compute_metrics function only receives predictions and labels, and not the additional metadata. I need a way to log metrics with respect to metadata without completely overriding the Trainer class or the entire evaluation loop, as this would be cumbersome to maintain, especially in parallel runs.</p>
<p>What I Want to Achieve:
Log metrics grouped by metadata categories.
For example, calculate the loss per project or task.
Minimize complex modifications to the Hugging Face Trainer class.
Ensure the solution works with the default Trainer evaluation loop to maintain scalability and parallelism.</p>
<p>Potential Solutions Explored:
i tried to make a metadata string, encode it using the tokenizer and pass it along as metadata or as labels, but it causes an infinite hang which i suspect is related to the gather function not expecting something about the shape.</p>
<p>Is there a clean and efficient way to pass metadata through the evaluation loop and use it in compute_metrics? How can I log metrics grouped by metadata without having to rewrite the Trainer class or evaluation loop entirely?</p>
<p>Any suggestions or best practices would be greatly appreciated!</p>
","python, nlp, huggingface-transformers, huggingface, huggingface-trainer",
NLTK Conditional Frequency Distribution with Aggregation,"<p>I'm trying to write an NLTK ConditionalFreqDist with some aggregated fields based on data from the fileid.
Basically, I have a corpus of documents with fileids consisting of a DATE-TITLE format, e.g., '2024-10-23 TitleName'.
Multiple fileids can share the same date.</p>
<p>I'm trying to write a conditional frequency distribution that:</p>
<ol>
<li>Finds the frequency of target words in each file,</li>
<li>Shows me that frequency,</li>
</ol>
<p>But, that also:</p>
<ol start=""3"">
<li>Aggregates the wordcount for each month (based on the fileid), instead of displaying it for each individual document.</li>
</ol>
<p>I have the following code, but right now I think it's returning the aggregate len() of all fileids for that year-month:</p>
<pre class=""lang-none prettyprint-override""><code>    cfd = ConditionalFreqDist(
        (target, file)
        for target in ['Apple','Microsoft']
        for file in corpus.fileids()
        for word in corpus.words(file))
</code></pre>
<p>Any help or suggestions appreciated.</p>
","nlp, nltk, frequency-distribution",
How to configure the page number in the gcp document ai toolbox converter?,"<p>I am trying to include the page number in the configuration JSON. I tried some ways, but no one works. Looking at the converter code on the GitHub page, I saw a lot of mentions of &quot;page_number&quot;, so I think it's possible.</p>
<p>Also, there's some documentation on how to create the config files? I only found some examples on the GitHub page.</p>
<p>I created an intermediary JSON file from the original JSON that I want to convert, to be easier the config file creation. The JSON that I want to convert is like this (but I can change the positions of the information):</p>
<pre><code>{
    &quot;document&quot;: &quot;document_name.pdf&quot;,
    &quot;labels&quot;: [
        {
            &quot;label&quot;: &quot;label_1&quot;,
            &quot;text&quot;: &quot;some_text&quot;,
            &quot;page&quot;: 1,
            &quot;boundingBoxes&quot;: [
                0.11554117647058823,
                0.17014227068750512,
                0.32081176470588235,
                0.1710069997542349,
                0.32081176470588235,
                0.1818570739388864,
                0.11554117647058823,
                0.18099234487215662
            ]
        },
        {
            &quot;label&quot;: &quot;label_2&quot;,
            &quot;text&quot;: &quot;some_text&quot;,
            &quot;page&quot;: 2,
            &quot;boundingBoxes&quot;: [
                0.269359477124183,
                0.5985227851890204,
                0.386081045751634,
                0.5980899758112141,
                0.386081045751634,
                0.6102450429168925,
                0.269359477124183,
                0.6098081885916301
            ]
        }
    ]
}
</code></pre>
<p>And I tried these below config files, but in all, the boundbox for page 2 or more, was made on the first page.</p>
<pre><code>{
    &quot;entity_object&quot;:&quot;labels&quot;,
    &quot;page&quot;: {
        &quot;pageNumber&quot;:&quot;page&quot;
    },
    &quot;entity&quot;: {
        &quot;type_&quot;:&quot;label&quot;,
        &quot;mention_text&quot;:&quot;text&quot;,
        &quot;normalized_vertices&quot;:{
            &quot;type&quot;:&quot;3&quot;,
            &quot;unit&quot;:&quot;normalized&quot;,
            &quot;base&quot;:&quot;boundingBoxes&quot;,
            &quot;x&quot;:&quot;x&quot;,
            &quot;y&quot;:&quot;y&quot;
        }
    }
}
</code></pre>
<pre><code>{
    &quot;entity_object&quot;:&quot;labels&quot;,
    &quot;page&quot;: {
        &quot;page_number&quot;:&quot;page&quot;
    },
    &quot;entity&quot;: {
        &quot;type_&quot;:&quot;label&quot;,
        &quot;mention_text&quot;:&quot;text&quot;,
        &quot;normalized_vertices&quot;:{
            &quot;type&quot;:&quot;3&quot;,
            &quot;unit&quot;:&quot;normalized&quot;,
            &quot;base&quot;:&quot;boundingBoxes&quot;,
            &quot;x&quot;:&quot;x&quot;,
            &quot;y&quot;:&quot;y&quot;
        }
    }
}
</code></pre>
<pre><code>{
    &quot;entity_object&quot;:&quot;labels&quot;,
    &quot;entity&quot;: {
        &quot;type_&quot;:&quot;label&quot;,
        &quot;mention_text&quot;:&quot;text&quot;,
        &quot;pageNumber&quot;:&quot;page&quot;,
        &quot;normalized_vertices&quot;:{
            &quot;type&quot;:&quot;3&quot;,
            &quot;unit&quot;:&quot;normalized&quot;,
            &quot;base&quot;:&quot;boundingBoxes&quot;,
            &quot;x&quot;:&quot;x&quot;,
            &quot;y&quot;:&quot;y&quot;
        }
    }
}
</code></pre>
<pre><code>{
    &quot;entity_object&quot;:&quot;labels&quot;,
    &quot;entity&quot;: {
        &quot;type_&quot;:&quot;label&quot;,
        &quot;mention_text&quot;:&quot;text&quot;,
        &quot;page_number&quot;:&quot;page&quot;,
        &quot;normalized_vertices&quot;:{
            &quot;type&quot;:&quot;3&quot;,
            &quot;unit&quot;:&quot;normalized&quot;,
            &quot;base&quot;:&quot;boundingBoxes&quot;,
            &quot;x&quot;:&quot;x&quot;,
            &quot;y&quot;:&quot;y&quot;
        }
    }
}
</code></pre>
","google-cloud-platform, deep-learning, nlp, named-entity-recognition, cloud-document-ai",
Geometric visulalization of Cosine Similarity,"<p>I have calculated the cosine similarity between two documents in a very basic way by using TF-IDF vectorization in Python.
But I want to visualize the documents as a vectorized graph in a 3D space. Like this:</p>
<p><a href=""https://i.sstatic.net/pBFMUIOf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pBFMUIOf.png"" alt=""example of graph"" /></a></p>
<p>Here's the code I used for calculating cosine similarity:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import numpy as np

# Sample documents
documents = [
    &quot;This is a sample document.&quot;,
    &quot;This document is another example.&quot;
]

# Vectorizing the documents
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# Calculating cosine similarity
cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
print(&quot;Cosine Similarity:&quot;, cosine_sim)
</code></pre>
","python, nlp, tf-idf, cosine-similarity, tfidfvectorizer",
word2vec download taking too long,"<p>Windows10, IDLE, The download time is too long for the below code.</p>
<pre><code>import gensim.downloader as api
word_vec_list = api.load('word2vec-google-news-300')
</code></pre>
<ol>
<li>Is it possible that I download only fewer words?</li>
<li>Is it possible to download the dataset as csv and use it later in the program?</li>
</ol>
","nlp, gensim, word2vec",
How to obtain BERT sentence embeddings vector?,"<p>I'm using the module <code>bert-for-tf2</code> in order to wrap BERT model as Keras layer in Tensorflow 2.0 I've followed your guide for implementing BERT model as Keras layer.
I'm trying to extract embeddings from a sentence; in my case, the sentence is &quot;Hello&quot;</p>
<p>I have a question about the output of the model prediction; I've written this model:</p>
<pre><code>model_word_embedding = tf.keras.Sequential([
                tf.keras.layers.Input(shape=(4,), dtype='int32', name='input_ids'),
                bert_layer
])

model_word_embedding .build(input_shape=(None, 4))

</code></pre>
<p>Then I want to extract the embeddings for the sentence written above:</p>
<pre><code>sentences = [&quot;Hello&quot;]
predict = model_word_embedding .predict(sentences)
</code></pre>
<p>the object predict contains 4 arrays of 768 elements each:</p>
<pre><code>print(predict)
print(len(predict))
print(len(predict[0][0]))
...

[[[-0.02768866 -0.7341324   1.9084396  ... -0.65953904  0.26496622
    1.1610721 ]
  [-0.19322394 -1.3134469   0.10383344 ...  1.1250225  -0.2988368
   -0.2323082 ]
  [-1.4576151  -1.4579685   0.78580517 ... -0.8898649  -1.1016986
    0.6008501 ]
  [ 1.41647    -0.92478925 -1.3651332  ... -0.9197768  -1.5469263
    0.03305872]]]
4
768
</code></pre>
<p>I know that each array of that 4 represents my original sentence, but I want to obtain one array as the embeddings of my original sentence.
So, my question is: How can I obtain the embeddings for a sentence?</p>
<p>In BERT source code I read this:</p>
<blockquote>
<p>For classification tasks, the first vector (corresponding to [CLS]) is used as the &quot;sentence vector.&quot; Note that this only makes sense because the entire model is fine-tuned.</p>
</blockquote>
<p>Do I have to take the first array from the prediction output since it represents my sentence vector?</p>
","keras, nlp, embedding, word-embedding, sentence",
Bert sentence embeddings,"<p>Im trying to obtain sentence embeddings for Bert but Im not quite sure if Im doing it properly... and yes Im aware that exist such tools already such as bert-as-service but I want to do it myself and understand how it works.</p>

<p>Lets say I want to extract a sentence embedding from word embeddings from the following sentence ""I am."". As I understood Bert outputs in the form of (12, seq_lenght, 768). I extracted each word embedding from the last encoder layer in the form of (1, 768). My doubt now lies in extracting the sentence from these two word vectors. If I have (2,768) should I sum the dim=1 and obtain a vector of (1,768)? Or maybe concatenate the two words (1, 1536) and applying a (mean) pooling and get the sentence vector in shape of (1, 768). Im not sure what is the right approach is to obtain the sentence vector for this given example is. </p>
","machine-learning, nlp, word-embedding",
cannot import name &#39;split_torch_state_dict_into_shards&#39; from &#39;huggingface_hub&#39;,"<p>I've been using LLAMA 2 for research for a few months now and I import as follows:</p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer
device = torch.device(&quot;cuda&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,token = &quot;token_key&quot;,torch_dtype=&quot;auto&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,token = &quot;token_key&quot;, torch_dtype=&quot;auto&quot;, load_in_4bit=True)
</code></pre>
<p>It has always worked. However, today it is showing the following error:
<strong>RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/conda/lib/python3.10/site-packages/huggingface_hub/<strong>init</strong>.py)</strong></p>
<p>Recreated the Hugging Face token, but it didn't work. I am using Google Colab and Kaggle Notebook.</p>
","python, nlp, huggingface-transformers, transformer-model, llama","<p>The error you're encountering is due to the <code>split_torch_state_dict_into_shards</code> function not being available in <code>huggingface-hub version &lt; 0.23.0</code>.</p>
<p>This function is included starting from version <code>0.23.0</code>.</p>
<p><strong>To resolve this issue, update the <code>huggingface-hub</code> library to version 0.23.0 or later</strong></p>
<p>Also please install accelerate:</p>
<pre><code>pip install accelerate==0.31.0
</code></pre>
<p>here is a git link: <strong><a href=""https://github.com/run-llama/llama_index/discussions/14605"" rel=""nofollow noreferrer"">https://github.com/run-llama/llama_index/discussions/14605</a></strong></p>
"
"How to compute precision, recall, accuracy and f1-score for the multiclass case with scikit learn?","<p>I'm working in a sentiment analysis problem the data looks like this:</p>
<pre><code>label instances
    5    1190
    4     838
    3     239
    1     204
    2     127
</code></pre>
<p>So my data is unbalanced since 1190 <code>instances</code> are labeled with <code>5</code>. For the classification Im using scikit's <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"" rel=""nofollow noreferrer"">SVC</a>. The problem is I do not know how to balance my data in the right way in order to compute accurately the precision, recall, accuracy and f1-score for the multiclass case. So I tried the following approaches:</p>
<p>First:</p>
<pre><code>wclf = SVC(kernel='linear', C= 1, class_weight={1: 10})
wclf.fit(X, y)
weighted_prediction = wclf.predict(X_test)

print 'Accuracy:', accuracy_score(y_test, weighted_prediction)
print 'F1 score:', f1_score(y_test, weighted_prediction,average='weighted')
print 'Recall:', recall_score(y_test, weighted_prediction,
                              average='weighted')
print 'Precision:', precision_score(y_test, weighted_prediction,
                                    average='weighted')
print '\n clasification report:\n', classification_report(y_test, weighted_prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, weighted_prediction)
</code></pre>
<p>Second:</p>
<pre><code>auto_wclf = SVC(kernel='linear', C= 1, class_weight='auto')
auto_wclf.fit(X, y)
auto_weighted_prediction = auto_wclf.predict(X_test)

print 'Accuracy:', accuracy_score(y_test, auto_weighted_prediction)

print 'F1 score:', f1_score(y_test, auto_weighted_prediction,
                            average='weighted')

print 'Recall:', recall_score(y_test, auto_weighted_prediction,
                              average='weighted')

print 'Precision:', precision_score(y_test, auto_weighted_prediction,
                                    average='weighted')

print '\n clasification report:\n', classification_report(y_test,auto_weighted_prediction)

print '\n confussion matrix:\n',confusion_matrix(y_test, auto_weighted_prediction)
</code></pre>
<p>Third:</p>
<pre><code>clf = SVC(kernel='linear', C= 1)
clf.fit(X, y)
prediction = clf.predict(X_test)


from sklearn.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, \
    accuracy_score, f1_score

print 'Accuracy:', accuracy_score(y_test, prediction)
print 'F1 score:', f1_score(y_test, prediction)
print 'Recall:', recall_score(y_test, prediction)
print 'Precision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test,prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)


F1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=&quot;f1_weighted&quot; instead of scoring=&quot;f1&quot;.
  sample_weight=sample_weight)
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=&quot;f1_weighted&quot; instead of scoring=&quot;f1&quot;.
  sample_weight=sample_weight)
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1082: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=&quot;f1_weighted&quot; instead of scoring=&quot;f1&quot;.
  sample_weight=sample_weight)
 0.930416613529
</code></pre>
<p>However, Im getting warnings like this:</p>
<pre><code>/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172:
DeprecationWarning: The default `weighted` averaging is deprecated,
and from version 0.18, use of precision, recall or F-score with 
multiclass or multilabel data or pos_label=None will result in an 
exception. Please set an explicit value for `average`, one of (None, 
'micro', 'macro', 'weighted', 'samples'). In cross validation use, for 
instance, scoring=&quot;f1_weighted&quot; instead of scoring=&quot;f1&quot;
</code></pre>
<p>How can I deal correctly with my unbalanced data in order to compute in the right way classifier's metrics?</p>
","python, machine-learning, scikit-learn, nlp","<p>I think there is a lot of confusion about which weights are used for what. I am not sure I know precisely what bothers you so I am going to cover different topics, bear with me ;).</p>
<h2>Class weights</h2>
<p>The weights from the <code>class_weight</code> parameter are used to <strong>train the classifier</strong>.
They <strong>are not used in the calculation of any of the metrics you are using</strong>: with different class weights, the numbers will be different simply because the classifier is different.</p>
<p>Basically in every scikit-learn classifier, the class weights are used to tell your model how important a class is. That means that during the training, the classifier will make extra efforts to classify properly the classes with high weights.<br />
How they do that is algorithm-specific. If you want details about how it works for SVC and the doc does not make sense to you, feel free to mention it.</p>
<h2>The metrics</h2>
<p>Once you have a classifier, you want to know how well it is performing.
Here you can use the metrics you mentioned: <code>accuracy</code>, <code>recall_score</code>, <code>f1_score</code>...</p>
<p>Usually when the class distribution is unbalanced, accuracy is considered a poor choice as it gives high scores to models which just predict the most frequent class.</p>
<p>I will not detail all these metrics but note that, with the exception of <code>accuracy</code>, they are naturally applied at the class level: as you can see in this <code>print</code> of a classification report they are defined for each class. They rely on concepts such as <code>true positives</code> or <code>false negative</code> that require defining which class is the <em>positive</em> one.</p>
<pre><code>             precision    recall  f1-score   support

          0       0.65      1.00      0.79        17
          1       0.57      0.75      0.65        16
          2       0.33      0.06      0.10        17
avg / total       0.52      0.60      0.51        50
</code></pre>
<h2>The warning</h2>
<pre><code>F1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The 
default `weighted` averaging is deprecated, and from version 0.18, 
use of precision, recall or F-score with multiclass or multilabel data  
or pos_label=None will result in an exception. Please set an explicit 
value for `average`, one of (None, 'micro', 'macro', 'weighted', 
'samples'). In cross validation use, for instance, 
scoring=&quot;f1_weighted&quot; instead of scoring=&quot;f1&quot;.
</code></pre>
<p>You get this warning because you are using the f1-score, recall and precision without defining how they should be computed!
The question could be rephrased: from the above classification report, how do you output <strong>one</strong> global number for the f1-score?
You could:</p>
<ol>
<li>Take the average of the f1-score for each class: that's the <code>avg / total</code> result above. It's also called <em>macro</em> averaging.</li>
<li>Compute the f1-score using the global count of true positives / false negatives, etc. (you sum the number of true positives / false negatives for each class). Aka <em>micro</em> averaging.</li>
<li>Compute a weighted average of the f1-score. Using <code>'weighted'</code> in scikit-learn will weigh the f1-score by the support of the class: the more elements a class has, the more important the f1-score for this class in the computation.</li>
</ol>
<p>These are 3 of the options in scikit-learn, the warning is there to say you <strong>have to pick one</strong>. So you have to specify an <code>average</code> argument for the score method.</p>
<p>Which one you choose is up to how you want to measure the performance of the classifier: for instance macro-averaging does not take class imbalance into account and the f1-score of class 1 will be just as important as the f1-score of class 5. If you use weighted averaging however you'll get more importance for the class 5.</p>
<p>The whole argument specification in these metrics is not super-clear in scikit-learn right now, it will get better in version 0.18 according to the docs. They are removing some non-obvious standard behavior and they are issuing warnings so that developers notice it.</p>
<h2>Computing scores</h2>
<p>Last thing I want to mention (feel free to skip it if you're aware of it) is that scores are only meaningful if they are computed on data that the classifier <strong>has never seen</strong>.
This is extremely important as any score you get on data that was used in fitting the classifier is completely irrelevant.</p>
<p>Here's a way to do it using <code>StratifiedShuffleSplit</code>, which gives you a random splits of your data (after shuffling) that preserve the label distribution.</p>
<pre><code>from sklearn.datasets import make_classification
from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix

# We use a utility to generate artificial classification data.
X, y = make_classification(n_samples=100, n_informative=10, n_classes=3)
sss = StratifiedShuffleSplit(y, n_iter=1, test_size=0.5, random_state=0)
for train_idx, test_idx in sss:
    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]
    svc.fit(X_train, y_train)
    y_pred = svc.predict(X_test)
    print(f1_score(y_test, y_pred, average=&quot;macro&quot;))
    print(precision_score(y_test, y_pred, average=&quot;macro&quot;))
    print(recall_score(y_test, y_pred, average=&quot;macro&quot;))
</code></pre>
"
Why is Spacy not executing the training pipeline even though the code runs without error?,"<p>I am training a custom NER model with Spacy version 3.5.0 using some dummy data. My entire code and dummy data is given below. <a href=""https://github.com/dmoonat/Named-Entity-Recognition/blob/main/NER_with_spaCy.ipynb"" rel=""nofollow noreferrer"">This is exact same code give in the 2nd half of this link</a>. The code is running fine, but it only executes until the <strong>Initializing pipeline</strong> step of the training and the <strong>Training pipeline</strong> is not executed.</p>
<p>Any idea why the training pipeline is not being executed?</p>
<pre><code>import pandas as pd
import os
from tqdm import tqdm
from spacy.tokens import DocBin

train = [
          (&quot;An average-sized strawberry has about 200 seeds on its outer surface and are quite edible.&quot;,{&quot;entities&quot;:[(17,27,&quot;Fruit&quot;)]}),
          (&quot;The outer skin of Guava is bitter tasting and thick, dark green for raw fruits and as the fruit ripens, the bitterness subsides. &quot;,{&quot;entities&quot;:[(18,23,&quot;Fruit&quot;)]}),
          (&quot;Grapes are one of the most widely grown types of fruits in the world, chiefly for the making of different wines. &quot;,{&quot;entities&quot;:[(0,6,&quot;Fruit&quot;)]}),
          (&quot;Watermelon is composed of 92 percent water and significant amounts of Vitamins and antioxidants. &quot;,{&quot;entities&quot;:[(0,10,&quot;Fruit&quot;)]}),
          (&quot;Papaya fruits are usually cylindrical in shape and the size can go beyond 20 inches. &quot;,{&quot;entities&quot;:[(0,6,&quot;Fruit&quot;)]}),
          (&quot;Mango, the King of the fruits is a drupe fruit that grows in tropical regions. &quot;,{&quot;entities&quot;:[(0,5,&quot;Fruit&quot;)]}),
          (&quot;undefined&quot;,{&quot;entities&quot;:[(0,6,&quot;Fruit&quot;)]}),
          (&quot;Oranges are great source of vitamin C&quot;,{&quot;entities&quot;:[(0,7,&quot;Fruit&quot;)]}),
          (&quot;A apple a day keeps doctor away. &quot;,{&quot;entities&quot;:[(2,7,&quot;Fruit&quot;)]})
        ]

db = DocBin() # create a DocBin object

for text, annot in tqdm(train): # data in previous format
    doc = nlp.make_doc(text) # create doc object from text
    ents = []
    for start, end, label in annot[&quot;entities&quot;]: # add character indexes
        span = doc.char_span(start, end, label=label, alignment_mode=&quot;contract&quot;)
        if span is None:
            print(&quot;Skipping entity&quot;)
        else:
            ents.append(span)
    doc.ents = ents # label the text with the ents
    db.add(doc)

db.to_disk(&quot;./train.spacy&quot;) # save the docbin object

!python -m spacy init fill-config base_config.cfg config.cfg

!python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy
</code></pre>
<p><strong>Expected output</strong></p>
<p><a href=""https://i.sstatic.net/dm2Vn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dm2Vn.png"" alt=""enter image description here"" /></a></p>
<p><strong>Output I got</strong></p>
<p><a href=""https://i.sstatic.net/5Fj8z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5Fj8z.png"" alt=""enter image description here"" /></a></p>
","python, nlp, nltk, spacy, named-entity-recognition",
Issue with running fastchat with a model saved on a local path,"<p>I'm trying to run a model on a server using fastChat. For that I have three docker images: server, control and worker.</p>
<p>I'm having issues with the worker image, namely - I'm trying to run it with a model that is saved locally on the server.</p>
<p>The model I'm using is in hf format (safetensors), but is not on huggingface.</p>
<p>I'm trying to change the <code>--model-path</code> in the <code>entrypoint</code> line in the <code>docker-compose.yaml</code> file (<code>from &quot;--model-path&quot;, &quot;${FASTCHAT_WORKER_MODEL_PATH:-meta-llama/Meta-Llama-3.1-8B}&quot;</code>) to the local path (on the server).</p>
<p>Here are examples how I gave the <code>--model-path</code> input:</p>
<p>a. <code>&quot;/&lt;path&gt;/&lt;to&gt;/.cache/huggingface/hub/models--&lt;model&gt;--&lt;model-name&gt;}&quot;</code></p>
<p>b. <code>&quot;{FASTCHAT_WORKER_MODEL_NAMES:-/&lt;path&gt;/&lt;to&gt;/.cache/huggingface/hub/models--&lt;model&gt;--&lt;model-name&gt;}&quot;</code></p>
<p>then when I run the docker compose file  (<code>dc -f docker-compose.yml up -d</code>)
I see that all three containers started:</p>
<pre><code>[+] Running 3/3
 ✔ Container docker-controller-1    Started                                                                                                                                                                    0.8s 
 ✔ Container docker-api-server-1    Started                                                                                                                                                                    0.8s 
 ✔ Container docker-model-worker-1  Started                                                                                                                                                                    3.3s 
</code></pre>
<p>but, when I run <code>docker ps</code> I only see the first two (not the model worker).</p>
<p>When I run the same code with a huggingface model, e.g. <code>--model-path</code>, <code>&quot;${FASTCHAT_WORKER_MODEL_PATH:-meta-llama/Meta-Llama-3.1-8B}&quot;</code> I don't have this issue.</p>
<p>Could you please advise how to run this code with my model ?</p>
","python, docker, nlp, large-language-model",
Integrate Python Model to Power Bi,"<p>In power BI  I have to create a NLP based chatbot kind of thing, like we have QnA visual in power bi which can answer the question asked in Natural Language Processing but it due to some of it's limitations I can't use.</p>
<p>I want to train my dataset using python and create a model which can answer any question asked in natural language.
However the problem is I can't run python script into Power bi , since there is no gateway connection for it so it will not refresh, I do not have to do in my local system only it should be reached out to end user. Anything if I load into power bi will eventually look like a table  and I have to create a visual of it.</p>
<p>I want to know in what ways it could be possible to use the QnA model written in python to inculcate the python model into power bi, so that a user can go search about the question they want to ask in a  natural language.</p>
<p>Is there a way to use python so that the end user can type their question and get their answer like a chat bot?</p>
","python, machine-learning, powerbi, nlp, azure-qna-maker",
Trainer huggingface - RuntimeError: cannot pin &#39;torch.cuda.FloatTensor&#39; only dense CPU tensors can be pinned,"<p>I recently got the following error:
<code>RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned</code>
when doing LoRA on a small LLM.</p>
<p>I saw on a discord someone saying:</p>
<blockquote>
<p>The issue likely stems from the fact that you are manually placing
your inputs on the GPU (with to(model.device)), but the Trainer
expects data to be on the CPU and will handle the transfer to the GPU
internally.</p>
</blockquote>
<p>I can't find anything of the sort written in the Trainer documentation of huggingface <a href=""https://huggingface.co/docs/transformers/en/main_classes/trainer"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/en/main_classes/trainer</a>.</p>
<p>Is it true? If not, how can I get rid of that error?</p>
<p>MRE:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.utils.data import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import TrainingArguments
from transformers import Trainer
from peft import LoraConfig, get_peft_model

model_name = &quot;croissantllm/CroissantLLMBase&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=&quot;auto&quot;)

texts = [
    &quot;The first sentence for fine-tuning. &lt;/s&gt;&quot;,
    &quot;The second sentence for fine-tuning. &lt;/s&gt;&quot;
]

inputs = [tokenizer(text, return_tensors=&quot;pt&quot;).to(model.device) for text in texts]

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],
)

model = get_peft_model(model, lora_config)

class CustomDataset(Dataset):
    def __init__(self, input_list):
        self.input_list = input_list

    def __len__(self):
        return len(self.input_list)

    def __getitem__(self, idx):
        input_ids = self.input_list[idx]['input_ids'].squeeze()
        labels = input_ids.clone()
        return {&quot;input_ids&quot;: input_ids, &quot;labels&quot;: labels}

train_dataset = CustomDataset(inputs)

training_args = TrainingArguments(
    output_dir=&quot;./lora_croissantllm&quot;,
    per_device_train_batch_size=1,
    num_train_epochs=1,
    save_steps=10,
    save_total_limit=2,
    logging_dir=&quot;./logs&quot;,
    logging_steps=10,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()
</code></pre>
<p>The issue is fairly easy to reproduce directly on colab (run <code>%pip install --upgrade torch transformers peft</code> in the first cell).</p>
","nlp, huggingface-transformers","<p>Since pinning memory is only available on CPU and not GPU, when running on GPU on Colab, you can just disable it by setting <code>dataloader_pin_memory</code> to <code>False</code> for <code>TrainingArguments</code></p>
<pre class=""lang-py prettyprint-override""><code>training_args = TrainingArguments(
    output_dir=&quot;./lora_croissantllm&quot;,
    dataloader_pin_memory=False,
    per_device_train_batch_size=1,
    num_train_epochs=1,
    save_steps=10,
    save_total_limit=2,
    logging_dir=&quot;./logs&quot;,
    logging_steps=10,
)
</code></pre>
"
Transformers error: RuntimeError: Failed to import transformers.training_args,"<p>I am trying to use transformers in a task of building a chatbot</p>
<pre><code>from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, trainer
import torch
import time
import pandas as pd
import random
import numpy as np
import os
import string
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.decomposition import LatentDirichletAllocation
</code></pre>
<p>and before that I install the required librairies</p>
<pre><code>%pip install --upgrade pip
%pip install --disable-pip-version-check \
    torch==1.13.1 \
    torchdata==0.5.1 --quiet

%pip install \
    transformers==4.27.2 \
    datasets==2.11.0 \
    evaluate==0.4.0 \
    rouge_score==0.1.2 \
    loralib==0.1.1 \
    peft==0.3.0 --quiet
</code></pre>
<p>But sometimes I encounter this error</p>
<pre><code>RuntimeError: Failed to import transformers.training_args because of the following error (look up to see its traceback):
/usr/local/lib/python3.10/site-packages/_XLAC.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZNK5torch4lazy17LazyGraphExecutor16ShouldSyncTensorERKN3c1013intrusive_ptrINS0_10LazyTensorENS2_6detail34intrusive_target_default_null_typeIS4_EEEE
</code></pre>
<p>Sometimes, when I run the cells to install packages and reload the kernel as required, and then import the necessary libraries, everything works fine. However, at other times, even though I follow the same steps, I encounter this error.</p>
","nlp, huggingface-transformers",
TypeError: &quot;hypothesis&quot; expects pre-tokenized hypothesis (Iterable[str]):,"<p>I am trying to calculate the Meteor score for the following:</p>
<pre><code>print (nltk.translate.meteor_score.meteor_score(
    [&quot;this is an apple&quot;, &quot;that is an apple&quot;], &quot;an apple on this tree&quot;))
</code></pre>
<p>However I am getting this error every time and I am not sure how to fix it.</p>
<pre><code>TypeError: &quot;hypothesis&quot; expects pre-tokenized hypothesis (Iterable[str]): an apple on this tree
</code></pre>
<p>I also tried to put &quot;an apple on this tree&quot; in a list</p>
<pre><code>    from nltk.translate.meteor_score import meteor_score
import nltk 
print (nltk.translate.meteor_score.meteor_score(
    [&quot;this is an apple&quot;, &quot;that is an apple&quot;], [&quot;an apple on this tree&quot;]))
</code></pre>
<p>but it gave me this error.</p>
<pre><code>TypeError: &quot;reference&quot; expects pre-tokenized reference (Iterable[str]): this is an apple
</code></pre>
","python, nlp, nltk, metrics","<p>Looking at the library code, it looks like hypothesis should be an iterable. <a href=""https://www.nltk.org/_modules/nltk/translate/meteor_score.html"" rel=""nofollow noreferrer"">https://www.nltk.org/_modules/nltk/translate/meteor_score.html</a>. The error is coming from:</p>
<pre><code>if isinstance(hypothesis, str):
        raise TypeError(
            f'&quot;hypothesis&quot; expects pre-tokenized hypothesis (Iterable[str]): {hypothesis}'
        )
</code></pre>
<p>Try putting &quot;an apple on this tree&quot; in a list.</p>
"
"How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)","<p>How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)</p>
<p>I have short product descriptions that I’d like to transform into structured attributes.</p>
<p>Example:</p>
<p>Input:</p>
<pre><code>“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”
</code></pre>
<p>Output:</p>
<pre><code>Year = 2017

Color = Red

Weight = 750

Weight Unit = ml
</code></pre>
<p>If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach:</p>
<ol>
<li><p>There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed.</p>
</li>
<li><p>There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc.</p>
</li>
<li><p>Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues.</p>
</li>
<li><p>I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about.</p>
</li>
</ol>
<p>I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary.</p>
<p>I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies.</p>
<p>Appreciate any insight into approaches or suggested learning resources.</p>
<p></p>
<p>I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.</p>
","nlp, artificial-intelligence, large-language-model, named-entity-recognition","<p>LLM would work nicely for this.  I'v done similar tasks before and it worked nicely with minimal training.  Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate,  but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions.</p>
<p>For you task I would use a framework like Langchain,  and the following prompt (note you might need to work on your prompt a bit this just an example).  When run with a model it will create an XML output which would be trivial to parse.  You can modify the prompt to create different type of outputs. But, personally I find XML working very well for me.</p>
<pre><code>You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:

- **Year**: The vintage year of the wine.
- **Color**: The color of the wine (e.g., Red, White, Rosé).
- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).
- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).
- **Brand**: The brand or producer of the wine.
- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).

**Instructions:**

- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.
- Be cautious of potential ambiguities. For example:
  - A brand name may include words like &quot;Red&quot; or &quot;White&quot; (e.g., &quot;Red Head Vineyard&quot;) which should not be confused with the wine color.
  - Large numbers may represent weight (e.g., &quot;1500 ml&quot;) rather than a year.
- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.

**Output Format:**

Provide the extracted information in XML format, using the following structure:

&lt;Wine&gt;
&lt;Year&gt;{{Year}}&lt;/Year&gt;
&lt;Color&gt;{{Color}}&lt;/Color&gt;
&lt;Weight&gt;{{Weight}}&lt;/Weight&gt;
&lt;WeightUnit&gt;{{WeightUnit}}&lt;/WeightUnit&gt;
&lt;Brand&gt;{{Brand}}&lt;/Brand&gt;
&lt;GrapeVariety&gt;{{GrapeVariety}}&lt;/GrapeVariety&gt;
&lt;/Wine&gt;

**Examples:**

  1. **Input:**

 `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`

 **Output:**



```xml
   &lt;Wine&gt;
     &lt;Year&gt;2017&lt;/Year&gt;
     &lt;Color&gt;Red&lt;/Color&gt;
     &lt;Weight&gt;750&lt;/Weight&gt;
     &lt;WeightUnit&gt;ml&lt;/WeightUnit&gt;
     &lt;Brand&gt;La Lecciaia&lt;/Brand&gt;
     &lt;GrapeVariety&gt;Cabernet Sauvignon&lt;/GrapeVariety&gt;
   &lt;/Wine&gt;
   ```

   
   `Red Head Vineyard Chardonnay 2020 1.5L`

   **Output:**

   &lt;Wine&gt;
     &lt;Year&gt;2020&lt;/Year&gt;
     &lt;Color&gt;&lt;/Color&gt;
     &lt;Weight&gt;1.5&lt;/Weight&gt;
     &lt;WeightUnit&gt;L&lt;/WeightUnit&gt;
     &lt;Brand&gt;Red Head Vineyard&lt;/Brand&gt;
     &lt;GrapeVariety&gt;Chardonnay&lt;/GrapeVariety&gt;
   &lt;/Wine&gt;

 

    **Task:**
    
    Given the following wine description, extract the components and provide the output in XML format as specified.
    
    {win_description}
</code></pre>
<p>Keep in mind that LLMs are not cheap to run.  But for this tasks given ambiguousness of the domain it is most likely the best choice.  For this particular task it would be 1/1000 of a penny per label using OpenAI service.  You might find a cheaper model / provider.  However when working with LLM it is very important to ensure accuracy first,  then optimize for costs.</p>
<p>The whole thing will probably take 1-2 hours to build for the intermediate LLM developer.  If you are learning it may vary.  But this is a perfect project to learn about LLMs</p>
"
How to import any Natural Language Processing Library for reference within my Unity project?,"<p>I want to import an opensource NLP library into my Unity project but no internet resource has worked and I can't figure out the issue but I assume it has to do with version compatibility. My project is quite up to date so I think my Open NLP libraries for C# or not up to date or compatible with Unity. I'm trying to use NLP to analyze text input and classify it into one of several predetermined categories based on the keywords or phrases used.</p>
<p>Some sources I have tried:</p>
<ul>
<li>Apache OpenNLP: This only had .jar files and I used IKVM to change them to .dll files but to no avail</li>
<li>Stanford.NLP.NET: Simply was never able to find the file</li>
<li>UnityNLP: Ran into general compatibility errors.</li>
</ul>
<p>All gave general errors rather than anything specific pointing towards incapability.</p>
<p>I always used the correct <code>using...</code> in my header but errors in my code was always related to the missing package reference.</p>
<p>I can provide more information if someone tells me I'm crazy and a certain on of these libraries works well in their Unity project, otherwise I'm looking for confirmation of their deprecation and recommendations on OpenSource libraries otherwise I have to go pay for something.</p>
","c#, unity-game-engine, nlp, opennlp",
How can I adjust the performance of tokenizer?,"<p>Working with the tokenizer from the <code>transformers</code> library of Hugging Face. The tokenizer works fine in most cases, but in some cases, it does not.</p>
<p>I'm wondering if I can <strong>&quot;adjust&quot;</strong> (not train a new tokenizer from scratch) the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to.</p>
<p>To be more specific, the type of tokenizer is <code>transformers.XLMRobertaTokenizerFast</code>, which is a unigram tokenizer, and the model is <code>paraphrase-multilingual-mpnet-base-v2</code>.</p>
","nlp, huggingface-transformers, huggingface, huggingface-tokenizers","<p>You can change the tokenizer's vocabulary:</p>
<pre><code>tokenizer.add_tokens([&quot;asadaf&quot;, &quot;sdfsaf&quot;])
model.resize_token_embeddings(len(tokenizer)) # change input embeddings size
input_text = &quot;This is asadaf and sdfsaf&quot;
print(tokenizer(input_text))
</code></pre>
<p>As a result, <em>asadaf</em> and <em>sdfsaf</em> would be tokenized as unique words.</p>
"
Varying embedding dim due to changing padding in batch size,"<p>I want to train a simple neural network, which has <strong>embedding_dim</strong> as a parameter:</p>
<pre><code>class BoolQNN(nn.Module):
    def __init__(self, embedding_dim):
        super(BoolQNN, self).__init__()
        self.fc1 = nn.Linear(embedding_dim, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 1)

    def forward(self, question_emb, passage_emb):
        combined = torch.cat((question_emb, passage_emb), dim=1)
        x = self.fc1(combined)
        x = self.relu(x)
        x = self.fc2(x)
        return torch.sigmoid(x)
</code></pre>
<p>To load the data I used torchs DataLoader with a custom collate_fn.</p>
<pre><code>train_dataset = BoolQDataset(train_data, pretrained_embeddings)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True,collate_fn=collate_fn_padd)

model = BoolQNN(301)
</code></pre>
<p>The collate_fn_padd function looks the following:</p>
<pre><code>def collate_fn_padd(batch):

  questions, passages, labels = zip(*batch)

  questions = [torch.tensor(q) for q in questions]
  passages = [torch.tensor(p) for p in passages]

  padded_questions = pad_sequence(questions, batch_first=True, padding_value=0)
  padded_passages = pad_sequence(passages, batch_first=True, padding_value=0)

  labels = torch.tensor(labels, dtype=torch.float32)
  
  return padded_questions, padded_passages, labels

</code></pre>
<p><strong>The problem:</strong> For every batch I want to train my model with, the embedded text gets padded differently long (it takes the longest sequence of the current batch).</p>
<p>That means that my embedding dim/input size for the linear layer in my neural network changes from batch to batch, althoug I want the size to be the same for every batch.</p>
<p>Due to that, I receive errors like that: <strong>mat1 and mat2 shapes cannot be multiplied (16x182 and 301x64)</strong></p>
<p>Is it possible to adjust the collate_fn_pad function so that it padds the sequence the same size, independet of the batch size?</p>
","python, text, nlp, padding, data-preprocessing","<p>You can add a maximum length argument set to <code>embedding_dim</code> to pad and truncate all the data to a fixed length:</p>
<pre><code>padded_questions = [torch.nn.functional.pad(torch.tensor(q), (0, max_length - len(q)), value=0)[:max_length] for q in questions]
padded_passages = [torch.nn.functional.pad(torch.tensor(p), (0, max_length - len(p)), value=0)[:max_length] for p in passages]
</code></pre>
"
What&#39;s going wrong in these weighted jaccard sum calculations for comparing the pronunciation of consonant clusters?,"<h2>Context</h2>
<p>I have this code for my attempt to create a &quot;similarity mapping&quot; between consonants (or consonant clusters), to the same set of consonants/clusters (basically a cross product mapping), like this form:</p>
<pre><code>// ... all possible consonants/clusters ...
type ClusterKey = 'b' | 'c' | 'd' | 'f' | 'br' | 'bl'

type ClusterMappings = Record&lt;ClusterKey, ClusterMapping&gt;

type ClusterMapping = Record&lt;string, number&gt;
</code></pre>
<p>The ClusterKey values are basically the keys in <a href=""https://github.com/termsurf/talk/blob/ea9e2791c6bd5dbf729362159c5f2f75ba7b38a1/make/rhymes/features.ts#L147-L335"" rel=""nofollow noreferrer"">this object</a>:</p>
<pre><code>export const CONSONANTS: Record&lt;string, Float32Array&gt; = {
  m: consonant({ bilabial: 0.9, nasal: 1.0 }),
  N: consonant({ retroflex: 1.0, nasal: 0.8 }),
  n: consonant({ alveolar: 1.0, nasal: 0.9 }),
  q: consonant({ velar: 1.0, nasal: 0.8 }),
  'G~': consonant({ velarization: 1.0 }),
  G: consonant({ velar: 1.0, fricative: 0.8 }),
  'g?': consonant({ plosive: 1.0, velar: 1.0, implosive: 0.5 }),
  g: consonant({ plosive: 1.0, velar: 1.0 }),
  &quot;'&quot;: consonant({ plosive: 1.0, glottal: 0.9 }),
  Q: consonant({ pharyngeal: 1.0, fricative: 0.9 }),
  'd?': consonant({ dental: 1.0, plosive: 1.0, implosive: 0.5 }),
  'd!': consonant({ dental: 1.0, plosive: 1.0, ejective: 0.5 }),
  'd*': consonant({ click: 1.0 }),
  ...
}
</code></pre>
<h2>Full Code</h2>
<p>The full code I have been working on is here, which defines a bunch of consonant vectors, maps them to each other, and stringifies it into somewhat human readable JSON:</p>
<pre><code>import fs from &quot;fs&quot;;

import merge from &quot;lodash/merge&quot;;

export const CONSONANT_FEATURE_NAMES = [
  &quot;affricate&quot;,
  &quot;alveolar&quot;,
  &quot;approximant&quot;,
  &quot;aspiration&quot;,
  &quot;bilabial&quot;,
  &quot;click&quot;,
  &quot;coarticulated&quot;,
  &quot;dental&quot;,
  &quot;dentalization&quot;,
  &quot;ejective&quot;,
  &quot;fricative&quot;,
  &quot;glottal&quot;,
  &quot;glottalization&quot;,
  &quot;implosive&quot;,
  &quot;labialization&quot;,
  &quot;labiodental&quot;,
  &quot;labiovelar&quot;,
  &quot;lateral&quot;,
  &quot;nasal&quot;,
  &quot;nasalization&quot;,
  &quot;palatal&quot;,
  &quot;palatalization&quot;,
  &quot;pharyngeal&quot;,
  &quot;pharyngealization&quot;,
  &quot;plosive&quot;,
  &quot;postalveolar&quot;,
  &quot;retroflex&quot;,
  &quot;sibilant&quot;,
  &quot;stop&quot;,
  &quot;tap&quot;,
  &quot;tense&quot;,
  &quot;uvular&quot;,
  &quot;velar&quot;,
  &quot;velarization&quot;,
  &quot;voiced&quot;,
] as const;

export type ConsonantFeatureName = (typeof CONSONANT_FEATURE_NAMES)[number];

export type ConsonantWithFeatures = Partial&lt;
  Record&lt;ConsonantFeatureName, number&gt;
&gt;;

export const VOWEL_FEATURE_NAMES = [
  &quot;back&quot;, // Back vowel (tongue towards the back)
  &quot;central&quot;, // Central vowel (tongue in the center)
  &quot;front&quot;, // Front vowel (tongue towards the front)
  &quot;closed&quot;, // High tongue position
  &quot;long&quot;, // Vowel length (long or short)
  &quot;open&quot;, // Low tongue position
  &quot;mid&quot;, // Mid tongue position
  &quot;nasalization&quot;, // Whether the vowel is nasalized
  &quot;rounded&quot;, // Whether the lips are rounded
  &quot;stress&quot;, // Whether the vowel is stressed
  &quot;unrounded&quot;,
  &quot;near&quot;,
] as const;

export type VowelFeatureName = (typeof VOWEL_FEATURE_NAMES)[number];

export type VowelWithFeatures = Partial&lt;Record&lt;VowelFeatureName, number&gt;&gt;;

export const VOWELS: Record&lt;string, Float32Array&gt; = {};

const NASALS = [0, 0.4];
const STRESSES = [0, 0.3];

NASALS.forEach((nasalization) =&gt; {
  STRESSES.forEach((stress) =&gt; {
    const keys: Array&lt;string&gt; = [];
    if (nasalization) {
      keys.push(`&amp;`);
    }
    if (stress) {
      keys.push(&quot;^&quot;);
    }
    const key = keys.join(&quot;&quot;);
    merge(VOWELS, {
      [`i${key}`]: vowel({
        closed: 1.0,
        front: 1.0,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
      [`e${key}`]: vowel({
        mid: 1.0,
        front: 1.0,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
      [`a${key}`]: vowel({
        open: 1.0,
        front: 0.5,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
      [`o${key}`]: vowel({
        closed: 1.0,
        mid: 1.0,
        back: 1.0,
        rounded: 1.0,
        nasalization,
        stress,
      }),
      [`u${key}`]: vowel({
        closed: 1.0,
        back: 1.0,
        rounded: 1.0,
        nasalization,
        stress,
      }),
      [`I${key}`]: vowel({
        near: 1.0,
        closed: 1.0,
        front: 1.0,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
      [`E${key}`]: vowel({
        open: 1.0,
        mid: 1.0,
        front: 1.0,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
      [`A${key}`]: vowel({
        near: 1.0,
        open: 1.0,
        front: 1.0,
        unrounded: 0.9,
        nasalization,
        stress,
      }),
      [`O${key}`]: vowel({
        near: 1.0,
        mid: 1.0,
        back: 1.0,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
      [`U${key}`]: vowel({
        mid: 1.0,
        central: 1.0,
        nasalization,
        stress,
      }),
      [`i$${key}`]: vowel({
        front: 1.0,
        closed: 1.0,
        rounded: 1.0,
        nasalization,
        stress,
      }),
      [`e$${key}`]: vowel({
        closed: 1.0,
        mid: 1.0,
        front: 1.0,
        rounded: 1.0,
        nasalization,
        stress,
      }),
      [`a$${key}`]: vowel({
        open: 1.0,
        mid: 1.0,
        front: 1.0,
        rounded: 1.0,
        nasalization,
        stress,
      }),
      [`o$${key}`]: vowel({
        open: 1.0,
        mid: 1.0,
        back: 1.0,
        rounded: 1.0,
        nasalization,
        stress,
      }),
      [`u$${key}`]: vowel({
        open: 1.0,
        mid: 1.0,
        central: 1.0,
        unrounded: 1.0,
        nasalization,
        stress,
      }),
    });
  });
});

export const VOWEL_KEYS = Object.keys(VOWELS);

export const VOWEL_VECTOR_PLACEHOLDER = vowel();

// https://en.wikipedia.org/wiki/IPA_consonant_chart_with_audio
export const CONSONANTS: Record&lt;string, Float32Array&gt; = {
  m: consonant({ bilabial: 0.9, nasal: 1.0 }),
  N: consonant({ retroflex: 1.0, nasal: 0.8 }),
  n: consonant({ alveolar: 1.0, nasal: 0.9 }),
  q: consonant({ velar: 1.0, nasal: 0.8 }),
  &quot;G~&quot;: consonant({ velarization: 1.0 }),
  G: consonant({ velar: 1.0, fricative: 0.8 }),
  &quot;g?&quot;: consonant({ plosive: 1.0, velar: 1.0, implosive: 0.5 }),
  g: consonant({ plosive: 1.0, velar: 1.0 }),
  &quot;'&quot;: consonant({ plosive: 1.0, glottal: 0.9 }),
  Q: consonant({ pharyngeal: 1.0, fricative: 0.9 }),
  &quot;d?&quot;: consonant({ dental: 1.0, plosive: 1.0, implosive: 0.5 }),
  &quot;d!&quot;: consonant({ dental: 1.0, plosive: 1.0, ejective: 0.5 }),
  &quot;d*&quot;: consonant({ click: 1.0 }),
  &quot;d.&quot;: consonant({ dental: 1.0, plosive: 1.0, stop: 0.2 }),
  D: consonant({ dental: 1.0, retroflex: 0.9, plosive: 1.0 }),
  &quot;dQ~&quot;: consonant({
    dental: 1.0,
    plosive: 1.0,
    pharyngealization: 0.8,
  }),
  d: consonant({ dental: 1.0, plosive: 1.0 }),
  &quot;b?&quot;: consonant({
    bilabial: 1.0,
    voiced: 1.0,
    plosive: 1.0,
    implosive: 0.5,
  }),
  &quot;b!&quot;: consonant({
    bilabial: 1.0,
    voiced: 1.0,
    plosive: 1.0,
    ejective: 0.5,
  }),
  b: consonant({ bilabial: 1.0, voiced: 1.0, plosive: 1.0 }),
  &quot;p!&quot;: consonant({ bilabial: 1.0, plosive: 1.0, ejective: 0.5 }),
  &quot;p*&quot;: consonant({ bilabial: 1.0, click: 1.0 }),
  &quot;p.&quot;: consonant({ bilabial: 1.0, plosive: 1.0, stop: 0.2 }),
  &quot;p@&quot;: consonant({ bilabial: 1.0, plosive: 1.0, tense: 0.1 }),
  p: consonant({ bilabial: 1.0, plosive: 1.0 }),
  // ...
};

export const CONSONANT_KEYS = Object.keys(CONSONANTS);

export const CONSONANT_VECTOR_PLACEHOLDER = consonant();

function consonant(
  mappings: Partial&lt;Record&lt;ConsonantFeatureName, number&gt;&gt; = {}
) {
  const consonant = new Float32Array(CONSONANT_FEATURE_NAMES.length);
  CONSONANT_FEATURE_NAMES.forEach((name, i) =&gt; {
    const number =
      name in mappings
        ? typeof mappings[name] === &quot;number&quot;
          ? mappings[name]
          : mappings[name] === true
          ? 1
          : 0
        : 0;
    consonant[i] = number;
  });
  return consonant;
}

function vowel(mappings: Partial&lt;Record&lt;VowelFeatureName, number&gt;&gt; = {}) {
  const consonant = new Float32Array(VOWEL_FEATURE_NAMES.length);
  VOWEL_FEATURE_NAMES.forEach((name, i) =&gt; {
    const number =
      name in mappings
        ? typeof mappings[name] === &quot;number&quot;
          ? mappings[name]
          : mappings[name] === true
          ? 1
          : 0
        : 0;
    consonant[i] = number;
  });
  return consonant;
}

const startingConsonants = CONSONANT_KEYS.map((key) =&gt; [{ key }]).concat(
  `bl
br
cl
cr
dr
fl
fr
gl
gr
kl
kr
pl
pr
sl
sm
sn
sp
st
tr
tw
sk
sw
sr
sc
th
sh
ch
ph
thw
sch
spl
spr
str
skr
slj
trj
blj
blw
drw
brw
kw
grw
kn
gn
zl
zn
vl
vr
ps
pt
pn
skh
sth
sf
ks
zh
zv
spn
shl
skl
smn
shr
chv
thn
klh
ql
phn
zr
brl
grk
ndr
ndl
sdr
skn
slz
zj
zd
rz
tsr
tn
prn
skj
svk
dj
trl
khr
chr
tsw
thr
ghn`
    .trim()
    .split(/\n+/)
    .map((text) =&gt; text.split(&quot;&quot;).map((key) =&gt; ({ key })))
);

export type Substitution = {
  key: string;
  vector: Float32Array;
};

export type SubstitutionList = {
  list: Array&lt;Substitution&gt;;
  vector: Float32Array;
};

export type Cluster = {
  key: string;
};

export type Substitutions = Record&lt;string, SubstitutionList&gt;;

const startingConsonantSubstitutions = startingConsonants.reduce&lt;Substitutions&gt;(
  (map, cluster) =&gt; {
    const list = startingConsonants.map(consonant);
    const { key, vector } = consonant(cluster);

    map[key] = { list, vector };

    return map;
  },
  {}
);

const substitutions = {
  ...startingConsonantSubstitutions,
};

fs.writeFileSync(`make/rhymes/substitutions.json`, stringify(substitutions));

function stringify(substitutions: Substitutions) {
  const text = [];

  text.push(`{`);

  let n = Object.keys(substitutions).length;
  let i = 0;
  for (const key in substitutions) {
    const sub = substitutions[key]!;
    text.push(`  &quot;${key}&quot;: {`);
    sub.list.forEach((item, i) =&gt; {
      const tail = i === sub.list.length - 1 ? &quot;&quot; : &quot;,&quot;;
      const sum = weightedJaccard(sub.vector, item.vector);
      text.push(`    &quot;${item.key}&quot;: ${sum}${tail}`);
    });
    const tail = i === n - 1 ? &quot;&quot; : &quot;,&quot;;
    text.push(`  }${tail}`);
    i++;
  }

  text.push(`}`);
  return text.join(&quot;\n&quot;);
}

// function consonant(symbols: Array&lt;Cluster&gt;) {
//   const vector = new Float32Array(
//     4 * CONSONANT_VECTOR_PLACEHOLDER.length,
//   )
//   symbols.forEach((symbol, i) =&gt; {
//     const consonant = CONSONANTS[symbol.key]!
//     let j = 0
//     while (j &lt; consonant.length) {
//       vector[i * 4 + j] = consonant[j]!
//       j++
//     }
//   })
//   return { key: symbols.map(({ key }) =&gt; key).join(''), vector }
// }
function consonant(symbols: Array&lt;Cluster&gt;) {
  const vector = combineConsonantClusterVectors(
    symbols.map((symbol) =&gt; symbol.key)
  );

  return { key: symbols.map(({ key }) =&gt; key).join(&quot;&quot;), vector };
}

export function combineConsonantClusterVectors(
  clusters: Array&lt;string&gt;
): Float32Array {
  const vector = new Float32Array(CONSONANT_VECTOR_PLACEHOLDER.length);

  clusters.forEach((cluster) =&gt; {
    const consonantVector =
      CONSONANTS[cluster] ||
      new Float32Array(CONSONANT_VECTOR_PLACEHOLDER.length);

    for (let i = 0; i &lt; vector.length; i++) {
      vector[i]! += consonantVector[i]!;
    }
  });

  return normalizeVector(vector);
}

export function normalizeVector(vector: Float32Array): Float32Array {
  const max = Math.max(...vector);
  if (max === 0) {
    return vector;
  } // Avoid division by zero if the vector is all zeros.
  return vector.map((value) =&gt; value / max);
}

export function weightedJaccard(a: Float32Array, b: Float32Array): number {
  let intersectionSum = 0;
  let unionSum = 0;

  for (let i = 0; i &lt; a.length; i++) {
    const valueA = a[i]!;
    const valueB = b[i]!;

    // Weighted Intersection: take the minimum of both values
    intersectionSum += Math.min(valueA, valueB);

    // Weighted Union: take the maximum of both values
    unionSum += Math.max(valueA, valueB);
  }

  // Return the Jaccard similarity, or 0 if the union is 0 to avoid division by zero
  return unionSum === 0 ? 0 : intersectionSum / unionSum;
}
</code></pre>
<p>Toward the bottom of that code block is this:</p>
<pre><code>export type Substitution = {
  key: string
  vector: Float32Array
}

export type SubstitutionList = {
  list: Array&lt;Substitution&gt;
  vector: Float32Array
}

export type Cluster = {
  key: string
}

export type Substitutions = Record&lt;string, SubstitutionList&gt;

const startingConsonantSubstitutions =
  startingConsonants.reduce&lt;Substitutions&gt;((map, cluster) =&gt; {
    const list = startingConsonants.map(consonant)
    const { key, vector } = consonant(cluster)

    map[key] = { list, vector }

    return map
  }, {})

const substitutions = {
  ...startingConsonantSubstitutions,
}
</code></pre>
<p>That is where the final result is generated of the substitution mapping JSON.</p>
<h2>Results</h2>
<p>Currently, (writing that <code>substitutions</code> out to a JSON file), it currently results in mappings like this (full example <a href=""https://github.com/termsurf/talk/blob/ea9e2791c6bd5dbf729362159c5f2f75ba7b38a1/make/rhymes/substitutions.json"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>{
  &quot;m&quot;: {
    &quot;m&quot;: 1,
    &quot;N&quot;: 0.27586207534413565,
    &quot;n&quot;: 0.3103448219163239,
    &quot;q&quot;: 0.27586207534413565,
    &quot;G~&quot;: 0,
    &quot;G&quot;: 0,
    &quot;g?&quot;: 0,
    &quot;g&quot;: 0,
    &quot;'&quot;: 0,
    &quot;Q&quot;: 0,
    &quot;d?&quot;: 0,
    &quot;d!&quot;: 0,
    &quot;d*&quot;: 0,
    &quot;d.&quot;: 0,
    &quot;D&quot;: 0,
    &quot;dQ~&quot;: 0,
    &quot;d&quot;: 0,
    &quot;b?&quot;: 0.19999999470180935,
    &quot;b!&quot;: 0.19999999470180935,
    &quot;b&quot;: 0.22499999403953552,
    &quot;p!&quot;: 0.25714285033089773,
    &quot;p*&quot;: 0.29999999205271405,
    &quot;p.&quot;: 0.28124999228748493,
    &quot;p@&quot;: 0.2903225728146864,
    &quot;p&quot;: 0.29999999205271405,
    &quot;T!&quot;: 0,
    &quot;T&quot;: 0,
    &quot;t!&quot;: 0,
    &quot;t*&quot;: 0,
    &quot;tQ~&quot;: 0,
    &quot;t@&quot;: 0,
    &quot;t.&quot;: 0,
    &quot;t&quot;: 0,
    &quot;k!&quot;: 0,
    &quot;k.&quot;: 0,
    &quot;k*&quot;: 0,
    &quot;K!&quot;: 0,
    &quot;K&quot;: 0,
    &quot;k&quot;: 0,
    &quot;H!&quot;: 0,
    &quot;H&quot;: 0,
    &quot;h~&quot;: 0,
    &quot;h!&quot;: 0,
    &quot;h&quot;: 0,
    &quot;J&quot;: 0,
    &quot;j!&quot;: 0,
    &quot;j&quot;: 0,
    &quot;S!&quot;: 0,
    &quot;s!&quot;: 0,
    &quot;S&quot;: 0,
    &quot;sQ~&quot;: 0,
    &quot;s@&quot;: 0,
    &quot;s&quot;: 0,
    &quot;F&quot;: 0.29999999205271405,
    &quot;f!&quot;: 0,
    &quot;f&quot;: 0,
    &quot;V&quot;: 0.22499999403953552,
    &quot;v&quot;: 0,
    &quot;z!&quot;: 0,
    &quot;zQ~&quot;: 0,
    &quot;z&quot;: 0,
    &quot;Z!&quot;: 0,
    &quot;Z&quot;: 0,
    &quot;CQ~&quot;: 0,
    &quot;C&quot;: 0,
    &quot;cQ~&quot;: 0,
    &quot;c&quot;: 0,
    &quot;L&quot;: 0,
    &quot;l*&quot;: 0,
    &quot;lQ~&quot;: 0,
    &quot;l&quot;: 0,
    &quot;R&quot;: 0,
    &quot;rQ~&quot;: 0,
    &quot;r&quot;: 0,
    &quot;x!&quot;: 0,
    &quot;X!&quot;: 0,
    &quot;X&quot;: 0,
    &quot;x@&quot;: 0,
    &quot;x&quot;: 0,
    &quot;W&quot;: 0,
    &quot;w!&quot;: 0,
    &quot;w~&quot;: 0,
    &quot;w&quot;: 0,
    &quot;y~&quot;: 0,
    &quot;y&quot;: 0,
    &quot;bl&quot;: 0.14999999602635702,
    &quot;br&quot;: 0.14999999602635702,
    &quot;cl&quot;: 0,
    &quot;cr&quot;: 0,
    &quot;dr&quot;: 0,
    &quot;fl&quot;: 0,
    &quot;fr&quot;: 0,
    &quot;gl&quot;: 0,
    &quot;gr&quot;: 0,
    &quot;kl&quot;: 0,
    &quot;kr&quot;: 0,
    &quot;pl&quot;: 0.1799999952316284,
    &quot;pr&quot;: 0.1799999952316284,
    &quot;sl&quot;: 0,
    &quot;sm&quot;: 0.3877550990618253,
    &quot;sn&quot;: 0.11538461303334734,
    &quot;sp&quot;: 0.14999999602635702,
    &quot;st&quot;: 0,
    &quot;tr&quot;: 0,
    &quot;tw&quot;: 0,
    &quot;sk&quot;: 0,
    &quot;sw&quot;: 0,
    &quot;sr&quot;: 0,
    &quot;sc&quot;: 0,
    &quot;th&quot;: 0,
    &quot;sh&quot;: 0,
    &quot;ch&quot;: 0,
    &quot;ph&quot;: 0.1799999952316284,
    &quot;thw&quot;: 0,
    &quot;sch&quot;: 0,
    &quot;spl&quot;: 0.11249999701976776,
    &quot;spr&quot;: 0.10204081682302911,
    &quot;str&quot;: 0,
    &quot;skr&quot;: 0,
    &quot;slj&quot;: 0,
    &quot;trj&quot;: 0,
    &quot;blj&quot;: 0.09999999735090467,
    &quot;blw&quot;: 0.0925925930014036,
    &quot;drw&quot;: 0,
    &quot;brw&quot;: 0.09999999735090467,
    &quot;kw&quot;: 0,
    &quot;grw&quot;: 0,
    &quot;kn&quot;: 0.1836734654157671,
    &quot;gn&quot;: 0.1836734654157671,
    &quot;zl&quot;: 0,
    &quot;zn&quot;: 0.1836734654157671,
    &quot;vl&quot;: 0,
    &quot;vr&quot;: 0,
    &quot;ps&quot;: 0.14999999602635702,
    &quot;pt&quot;: 0.147058824560634,
    &quot;pn&quot;: 0.44999998807907104,
    &quot;skh&quot;: 0,
    &quot;sth&quot;: 0,
    &quot;sf&quot;: 0,
    &quot;ks&quot;: 0,
    &quot;zh&quot;: 0,
    &quot;zv&quot;: 0,
    &quot;spn&quot;: 0.21590908936971476,
    &quot;shl&quot;: 0,
    &quot;skl&quot;: 0,
    &quot;smn&quot;: 0.3589743550555789,
    &quot;shr&quot;: 0,
    &quot;chv&quot;: 0,
    &quot;thn&quot;: 0.10227272511760065,
    &quot;klh&quot;: 0,
    &quot;ql&quot;: 0.16326530934968925,
    &quot;phn&quot;: 0.29999999205271405,
    &quot;zr&quot;: 0,
    &quot;brl&quot;: 0.11249999701976776,
    &quot;grk&quot;: 0,
    &quot;ndr&quot;: 0.10227272511760065,
    &quot;ndl&quot;: 0.13043477960405067,
    &quot;sdr&quot;: 0,
    &quot;skn&quot;: 0.09183673270788355,
    &quot;slz&quot;: 0,
    &quot;zj&quot;: 0,
    &quot;zd&quot;: 0,
    &quot;rz&quot;: 0,
    &quot;tsr&quot;: 0,
    &quot;tn&quot;: 0.132352938598415,
    &quot;prn&quot;: 0.24358974202223155,
    &quot;skj&quot;: 0,
    &quot;svk&quot;: 0,
    &quot;dj&quot;: 0,
    &quot;trl&quot;: 0,
    &quot;khr&quot;: 0,
    &quot;chr&quot;: 0,
    &quot;tsw&quot;: 0,
    &quot;thr&quot;: 0,
    &quot;ghn&quot;: 0.13043477960405067
  },
  ...
}
</code></pre>
<h2>Analysis</h2>
<p><strong>The only one that seems correct is <code>m:m</code> (<code>m</code> mapped to itself, which is <code>1</code>).</strong> That makes sense, it's an exact match. However, we should find <code>m:n</code>, and <code>m:N</code> (at least), to be like 0.9 or 0.95, since they are both nasal consonants which sound extremely similar.</p>
<p>It looks like all the consonant <em>clusters</em> matching <code>*n</code> have at least some value, but not sure if it makes sense what it ends up being. I don't have in mind exactly what these final <code>weightedJaccard</code> sums should be even (I am brand new to all these NLP/ML algorithms, trying to wire them together carefully).</p>
<p>All the rest of the consonant clusters are <code>0</code>, which probably makes sense, since if they don't have <code>m</code> or <code>n</code>, then they are probably not a good match (though not 100% sure yet if that is desired, I'd have to tinker with phonetic relationships more manually first).</p>
<p><strong>Then <code>p</code> and <code>b</code> are &quot;labial&quot; consonants (like <code>m</code> and <code>n</code>), so they have some value, but I would expect it to be slightly higher than 0.1-0.3.</strong></p>
<p>Finally, <code>F</code> and <code>V</code>, which are both technically &quot;bilabial&quot; consonants, they are &quot;fricatives&quot; however (like blowing wind through your lips), whereas <code>b</code> and <code>p</code> are explosions of sound (&quot;plosives&quot;), not streams of sound.</p>
<p>Here are all the key characters that relate to <code>m</code> (at least as far as this data turned out):</p>
<ul>
<li><code>F</code>: bilabial fricative</li>
<li><code>V</code>: voiced bilabial fricative</li>
<li><code>b</code>: voiced bilabial plosive</li>
<li><code>p</code>: bilabial plosive</li>
<li><code>m</code>: bilabial nasal</li>
<li><code>n</code>: alveolar nasal</li>
<li><code>N</code>: retroflex nasal</li>
</ul>
<p>I would expect it to be more like this number wise:</p>
<ul>
<li><code>F</code>: 0</li>
<li><code>V</code>: 0</li>
<li><code>b</code>: 0.5</li>
<li><code>p</code>: 0.3</li>
<li><code>m</code>: 1</li>
<li><code>n</code>: 0.95</li>
<li><code>N</code>: 0.9</li>
</ul>
<p>That is how I personally (subjectively) would give them values. The main thing there is <code>F</code> and <code>V</code> (while technically being &quot;bilabial&quot;), have no audible relationship to the sound <code>m</code> (like sounds &quot;f&quot; and &quot;v&quot; basically).</p>
<h2>Question</h2>
<p>Any ideas what I'm doing wrong? It seems that my feature vector mappings (done onto the <code>CONSONANTS</code> map object) are not going to result in the desired relationships/similarities between sounds. There are like 1-4 props per vector there, and it seems like &quot;why wouldn't any vector with 4 defined props with all values <code>1.0</code> result in the same vector embedding (so basically, it wouldn't respect the fact that these properties represent totally different things). So I'm confused as to how this will ever get close to resulting in a &quot;similarity mapping&quot;.</p>
<ul>
<li>Where can I debug better what might be going wrong in my sample code (which <a href=""https://github.com/termsurf/talk/blob/ea9e2791c6bd5dbf729362159c5f2f75ba7b38a1/make/rhymes/features.ts"" rel=""nofollow noreferrer"">currently</a> is spread across multiple modules in reality)? <em>But I combined and pasted the code in that big snippet above.</em></li>
<li>Am I doing the &quot;weighted jaccard&quot; calculations properly? Am I building the vectors properly? What could I be doing wrong here?</li>
</ul>
<p><em>How can I even begin to make features and assign floating point number values to each feature, so that I can get these sounds which I subjectively think are similar, to in fact be correlated in the numbers?</em></p>
<h2>Summary</h2>
<p>The basic flow of the code is:</p>
<ul>
<li><strong>Define consonant vectors</strong> (forget about vowels for now). There are about 35 keys (ML &quot;features&quot;?) in that <code>CONSONANT_FEATURE_NAMES</code> variable, which are all boolean values in reality, but I gave them a <code>number</code> value to make it sort of <em>weighted</em> instead of binary. I am pretty sure I'm doing this step correctly, am I not?</li>
<li><strong>Map consonants/clusters to themselves (cross-product).</strong></li>
<li><strong>For each mapping item, calculate each item's consonant cluster vector union for the weighted jaccard sum later.</strong> Not sure if I'm doing this step properly.</li>
<li><strong>Then taking that calculated jaccard sum for each item being mapped, call <code>weightedJaccard(source.vector, item.vector)</code></strong>, to create the mapping of the source consonant cluster to each target cluster. Also not sure if this step is correct.</li>
</ul>
<p>Looking to better understand where I'm going wrong in calculating the similarity between consonant clusters using the <strong>weight jaccard</strong> approach.</p>
<p><em><strong>Note</strong>: Got the idea for using the jaccard sum from this paper: <a href=""https://arxiv.org/abs/2109.14796"" rel=""nofollow noreferrer"">arxiv.org/abs/2109.14796</a>. Not sure if I'm doing it right, or even if that's the best approach to my overall problem. But for this question, we can assume we want to use the weight jaccard sum idea, unless you feel strongly otherwise.</em></p>
<h2>Update</h2>
<p><strong>Note</strong>: Plugging this into Claude AI, it gave what seems like a helpful suggestion, to completely change my base consonant vector system, sort of like this:</p>
<pre><code>// Define a more detailed feature space
const FEATURE_SPACE = {
  placeOfArticulation: {
    features: ['bilabial', 'labiodental', 'dental', 'alveolar', 'postalveolar', 'retroflex', 'palatal', 'velar', 'uvular', 'pharyngeal', 'glottal'],
    weight: 0.3,
    // Define relationships between places of articulation
    similarity: (a: string, b: string) =&gt; {
      const order = ['bilabial', 'labiodental', 'dental', 'alveolar', 'postalveolar', 'retroflex', 'palatal', 'velar', 'uvular', 'pharyngeal', 'glottal'];
      const distance = Math.abs(order.indexOf(a) - order.indexOf(b));
      return 1 - (distance / (order.length - 1));
    }
  },
  manner: {
    features: ['plosive', 'nasal', 'trill', 'tap', 'fricative', 'lateral', 'approximant', 'affricate'],
    weight: 0.3,
    // Define relationships between manners of articulation
    similarity: (a: string, b: string) =&gt; {
      const groups = [
        ['plosive', 'affricate'],
        ['fricative', 'approximant'],
        ['nasal', 'lateral'],
        ['trill', 'tap']
      ];
      if (a === b) return 1;
      if (groups.some(group =&gt; group.includes(a) &amp;&amp; group.includes(b))) return 0.5;
      return 0;
    }
  },
  voicing: {
    features: ['voiced', 'voiceless'],
    weight: 0.2,
    similarity: (a: string, b: string) =&gt; a === b ? 1 : 0
  },
  airflow: {
    features: ['oral', 'nasal'],
    weight: 0.1,
    similarity: (a: string, b: string) =&gt; a === b ? 1 : 0
  },
  release: {
    features: ['aspirated', 'unaspirated'],
    weight: 0.1,
    similarity: (a: string, b: string) =&gt; a === b ? 1 : 0
  }
};

type FeatureSpace = typeof FEATURE_SPACE;

function createDetailedFeatureVector(consonant: string): number[] {
  const features = CONSONANTS[consonant];
  const vector: number[] = [];

  Object.entries(FEATURE_SPACE).forEach(([category, { features, weight }]) =&gt; {
    features.forEach(feature =&gt; {
      const value = features[feature] || 0;
      vector.push(value * weight);
    });
  });

  return vector;
}

function calculateFeatureSimilarity(feature1: string, feature2: string, category: keyof FeatureSpace): number {
  const { similarity } = FEATURE_SPACE[category];
  return similarity(feature1, feature2);
}

function consonantSimilarity(c1: string, c2: string): number {
  let totalSimilarity = 0;
  let totalWeight = 0;

  Object.entries(FEATURE_SPACE).forEach(([category, { features, weight }]) =&gt; {
    const f1 = features.find(f =&gt; CONSONANTS[c1][f]) || '';
    const f2 = features.find(f =&gt; CONSONANTS[c2][f]) || '';
    
    if (f1 &amp;&amp; f2) {
      const similarity = calculateFeatureSimilarity(f1, f2, category as keyof FeatureSpace);
      totalSimilarity += similarity * weight;
      totalWeight += weight;
    }
  });

  return totalSimilarity / totalWeight;
}

// Example usage
console.log(consonantSimilarity('m', 'n')); // Should be high (both nasals)
console.log(consonantSimilarity('p', 'b')); // Should be high (differ only in voicing)
console.log(consonantSimilarity('p', 'k')); // Should be moderate (both voiceless plosives, different place)
console.log(consonantSimilarity('p', 's')); // Should be lower (different manner and place)
</code></pre>
<p>Not sure why my system won't work, and why this might be better. <em>Slightly another way of looking at the problem perhaps, maybe better too.</em></p>
<p>Should I be doing something more like that to organize my features? Maybe that's why I'm getting unexpected results...</p>
","algorithm, nlp, phonetics, jaccard-similarity",
How to make t-sne plots with BERT Models,"<p><a href=""https://i.sstatic.net/dzZVH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dzZVH.png"" alt=""enter image description here"" /></a></p>
<p>I have two text classification models
<a href=""https://huggingface.co/samanjoy2/banglaclickbert_finetuned_sequence_classification_clickbait"" rel=""nofollow noreferrer"">https://huggingface.co/samanjoy2/banglaclickbert_finetuned_sequence_classification_clickbait</a>
<a href=""https://huggingface.co/samanjoy2/banglabert_finetuned_sequence_classification_clickbait"" rel=""nofollow noreferrer"">https://huggingface.co/samanjoy2/banglabert_finetuned_sequence_classification_clickbait</a></p>
<p>I want to plot their hidden states in a T-SNE plor. How can I do this? A example of this has been shown in the picture.</p>
","nlp, text-classification, tsne",
Exporting a Longformer model to ONNX using optimum.exporters,"<p>I have trained and finetuned a longformer model with my data.
Now I am trying to export that into ONNX format, using optimum exporter.
Since it's not supported off the bat (from what I can figure), I created my own config file. I'm not completely sure I'm doing this correctly</p>
<pre><code>class LongformerOnnxConfig(OnnxConfig):
    DEFAULT_ONNX_OPSET = 11
    MIN_TRANSFORMERS_VERSION = &quot;4.39.3&quot;
    is_transformers_support_available = False

    def __init__(self, config):
        super().__init__(config)
        # Define custom properties from your model's config if necessary

    @property
    def inputs(self):
        return {
            &quot;input_ids&quot;: {0: &quot;batch_size&quot;, 1: &quot;sequence_length&quot;},
            &quot;attention_mask&quot;: {0: &quot;batch_size&quot;, 1: &quot;sequence_length&quot;},
            &quot;global_attention_mask&quot;: {0: &quot;batch_size&quot;, 1: &quot;sequence_length&quot;}
        }

    @property
    def outputs(self):
        return {
            &quot;logits&quot;: {0: &quot;batch_size&quot;, 1: &quot;num_labels&quot;}
        }

    def generate_dummy_inputs(self, tokenizer, batch_size=1, seq_length=128):
        # Generate dummy inputs for testing or exporting
        dummy_input = &quot;This is a dummy input.&quot;
        inputs = tokenizer(dummy_input, return_tensors=&quot;pt&quot;, max_length=seq_length, padding=&quot;max_length&quot;)
        return {
            &quot;input_ids&quot;: inputs[&quot;input_ids&quot;],
            &quot;attention_mask&quot;: inputs[&quot;attention_mask&quot;],
            # Add global_attention_mask if needed
            &quot;global_attention_mask&quot;: torch.zeros_like(inputs[&quot;attention_mask&quot;])
        }

    @property
    def default_onnx_opset(self):
        return 11  # Depending on the required opset for your ONNX export, default is 11

    @property
    def torch_to_onnx_dtype(self):
        return {
            torch.float32: &quot;float&quot;,
            torch.float16: &quot;float16&quot;
        }

</code></pre>
<p>Then I try to run the conversion with the following code:</p>
<pre><code>
model = LongformerForQuestionAnswering.from_pretrained(&quot;&lt;&lt;Finetuned model from HuggingFace&gt;&gt;&quot;, use_safetensors=True)
onnx_config = LongformerOnnxConfig(model.config)
with torch.no_grad():
        model.config.return_dict = True
        model.eval()
        onnx_model_path,
        output = onnx_model_path
        export(
                model=model,
                config=onnx_config,
                output=output,
        )
</code></pre>
<p>But I get the following error:
<code>raise MinimumVersionError(optimum.exporters.error_utils.MinimumVersionError: The current version of Transformers does not allow for the export of the model. Minimum required is 4.39.3, got: 4.39.3</code></p>
<p>No matter what version I give it, I get the same error.
What am I doing wrong?</p>
","nlp, huggingface-transformers, onnx",
Large Language Model Perplexity,"<p>i am currently using GPT-3 and i am trying to compare its capabilities to related language models for my masters thesis.
Unfortunatly GPT-3 is an API based application, so i am not really able to extract metrics such as perplexity.</p>
<p>Over the API i have acces to these three metrics and of course the models outputs:</p>
<ul>
<li><p>training_loss: loss on the training batch</p>
</li>
<li><p>training_sequence_accuracy: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67</p>
</li>
<li><p>training_token_accuracy: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83</p>
</li>
</ul>
<p>Is there any possibility to calculate the perplexity of my model using python?</p>
<p>Thank you.</p>
","python, nlp, nltk, gpt-3, perplexity",
Pip can&#39;t install pyicu,"<p>I am running a AWS that runs Ubuntu 20.04. I am trying to install the package <code>pyicu</code>, but I am facing problems. I tried running <code>sudo apt install libicu-dev</code>, but I still can't install pyicu. I am not able to install brew on the aws server. Any other suggestions? This is the erro message:</p>
<pre><code>    ERROR: Command errored out with exit status 1:
     command: /root/dev/big5_rest/venv/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-se360cxw/pyicu/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-se360cxw/pyicu/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base /tmp/pip-install-se360cxw/pyicu/pip-egg-info
         cwd: /tmp/pip-install-se360cxw/pyicu/
    Complete output (53 lines):
    Traceback (most recent call last):
      File &quot;/tmp/pip-install-se360cxw/pyicu/setup.py&quot;, line 63, in &lt;module&gt;
        ICU_VERSION = os.environ['ICU_VERSION']
      File &quot;/usr/lib/python3.8/os.py&quot;, line 675, in __getitem__
        raise KeyError(key) from None
    KeyError: 'ICU_VERSION'
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File &quot;/tmp/pip-install-se360cxw/pyicu/setup.py&quot;, line 66, in &lt;module&gt;
        ICU_VERSION = check_output(('icu-config', '--version')).strip()
      File &quot;/tmp/pip-install-se360cxw/pyicu/setup.py&quot;, line 19, in check_output
        return subprocess_check_output(popenargs)
      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 411, in check_output
        return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 489, in run
        with Popen(*popenargs, **kwargs) as process:
      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 854, in __init__
        self._execute_child(args, executable, preexec_fn, close_fds,
      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 1702, in _execute_child
        raise child_exception_type(errno_num, err_msg, err_filename)
    FileNotFoundError: [Errno 2] No such file or directory: 'icu-config'
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File &quot;/tmp/pip-install-se360cxw/pyicu/setup.py&quot;, line 69, in &lt;module&gt;
        ICU_VERSION = check_output(('pkg-config', '--modversion', 'icu-i18n')).strip()
      File &quot;/tmp/pip-install-se360cxw/pyicu/setup.py&quot;, line 19, in check_output
        return subprocess_check_output(popenargs)
      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 411, in check_output
        return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 489, in run
        with Popen(*popenargs, **kwargs) as process:
      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 854, in __init__
        self._execute_child(args, executable, preexec_fn, close_fds,
      File &quot;/usr/lib/python3.8/subprocess.py&quot;, line 1702, in _execute_child
        raise child_exception_type(errno_num, err_msg, err_filename)
    FileNotFoundError: [Errno 2] No such file or directory: 'pkg-config'
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
      File &quot;/tmp/pip-install-se360cxw/pyicu/setup.py&quot;, line 71, in &lt;module&gt;
        raise RuntimeError('''
    RuntimeError:
    Please install pkg-config on your system or set the ICU_VERSION environment
    variable to the version of ICU you have installed.
    
    (running 'icu-config --version')
    (running 'pkg-config --modversion icu-i18n')
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
</code></pre>
","python, pip, nlp, polyglot, pyicu",
Break after first PER sequence found with Spacy,"<p>I am trying to extract only the first speaker's name from a list of texts using spaCy. Currently, my function returns all &quot;PER&quot; tags, but I want to reduce the overhead and get only the first contiguous sequence of &quot;PER&quot; entities. Here’s the example output I get:</p>
<pre><code>Detected Names in Text: ['garcía', 'lópez']
Detected Names in Text: ['j. jesus orozco alfaro']
Detected Names in Text: ['josé guadarrama márquez', 'josé guadarrama']
Detected Names in Text: ['pedro sánchez', 'josé manuel albares', 'pablo iglesias']
</code></pre>
<p>But I want the result to be:</p>
<pre><code>Detected Names in Text: ['garcía']
Detected Names in Text: ['j. jesus orozco alfaro']
Detected Names in Text: ['josé guadarrama márquez']
Detected Names in Text: ['pedro sánchez']
</code></pre>
<p>Here is the code I am currently using:</p>
<pre><code>import spacy
from spacy.matcher import Matcher

nlp = spacy.load(&quot;es_core_news_lg&quot;)

texts = [
    &quot;El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.&quot;,
    &quot;PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO&quot;,
    &quot;            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez&quot;,
    &quot;El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.&quot;
]
texts = [text.lower() for text in texts]

matcher = Matcher(nlp.vocab)

patterns = [
    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;c&quot;}],
    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sr&quot;}],
    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sra&quot;}]
]

matcher.add(&quot;LEGISLATIVE_TITLES&quot;, patterns)

# Function to find a sequence of PER entities allowing one MISC
def find_per_sequence(doc, start_idx=0):
    per_entities = []
    misc_count = 0
    
    for ent in doc[start_idx:].ents:
        if ent.label_ == &quot;PER&quot;:
            per_entities.append(ent.text)
        elif ent.label_ == &quot;MISC&quot; and misc_count &lt; 1:
            misc_count += 1
            per_entities.append(ent.text)
        else:
            break  # Should stop if any other entity or second MISC is encountered
    
    return per_entities

for text in texts:
    doc = nlp(text)
    
    # Find matches
    matches = matcher(doc)
    
    # Extract the first match and its position
    title_start = None
    title_end = None
    for match_id, start, end in matches:
        title_start = start
        title_end = end
        break

    # If a title was found, start searching for PER entities from that position
    if title_start is not None:
        names = find_per_sequence(doc, start_idx=title_end)
    else:
        names = find_per_sequence(doc)

    # Output the detected names for each text
    print(f&quot;Detected Names in Text: {names}&quot;)
</code></pre>
<p>What I'm looking for:</p>
<p>I want to modify the find_per_sequence function so that it returns only the first contiguous sequence of &quot;PER&quot; entities in the text, ignoring any subsequent &quot;PER&quot; entities after encountering a different type of entity. The provided function returns multiple names or partial names, and I need a way to ensure only the first name or sequence is included. How can I achieve this?</p>
","python, nlp, spacy","<p>The issues is that <code>doc[start_idx:].ents</code> is <a href=""https://spacy.io/api/doc#ents"" rel=""nofollow noreferrer"">only the named entities</a> in that slice of the doc. Thus, you will never process &quot;habló&quot; for the first entry, you will just go straight from &quot;García&quot; to &quot;López&quot;. To actually iterate over the tokens so that you see when the PER sequence ends, you have to leave out the <code>.ents</code> part. Then you just wait until you see the first token with <code>ent_type_</code> PER and start appending, then break after one of your conditions is met. I ended up refactoring your code a little as I debugged this, but here's an edited version of your program that produces the desired outputs:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.matcher import Matcher

nlp = spacy.load(&quot;es_core_news_lg&quot;)

texts = [
    &quot;El Sr. García habló en la sesión. También estuvo presente el Senador López y la Diputada Martínez.&quot;,
    &quot;PRESIDENCIA DEL C. SENADOR J. JESUS OROZCO ALFARO&quot;,
    &quot;            -ER C. José Guadarrama Márquez: el contrabando del dia, José Guadarrama Márquez&quot;,
    &quot;El presidente Pedro Sánchez y el Ministro de Asuntos Exteriores José Manuel Albares se reunieron con el Senador Pablo Iglesias.&quot;,
]
texts = [text.lower() for text in texts]

matcher = Matcher(nlp.vocab)

patterns = [
    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;c&quot;}],
    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sr&quot;}],
    [{&quot;LOWER&quot;: &quot;el&quot;}, {&quot;LOWER&quot;: &quot;sra&quot;}],
]

matcher.add(&quot;LEGISLATIVE_TITLES&quot;, patterns)


# Function to find a sequence of PER entities allowing one MISC
def find_per_sequence(doc: spacy.tokens.Doc, start_idx: int):
    per_entities = []
    misc_count = 0
    per_started = False

    for token in doc[start_idx:]:
        if token.ent_type_ == &quot;PER&quot;:
            per_entities.append(token.text)
            per_started = True
        elif token.ent_type_ == &quot;MISC&quot; and misc_count &lt; 1 and per_started:
            misc_count += 1
            per_entities.append(token.text)
        elif per_started:
            break  # Should stop if any other entity or second MISC is encountered

    return per_entities


for text in texts:
    doc = nlp(text)

    # Find matches
    matches = matcher(doc)

    # Extract the first match and its position
    _, _, title_end = matches[0] if matches else (None, None, None)

    names = find_per_sequence(doc, title_end if title_end else 0)

    # Output the detected names for each text
    print(f&quot;Detected Names in Text: {names}&quot;)
</code></pre>
"
Semantic Role Labeling for English Newspaper,"<p>I am doing reserch on Newspaper headlines that are written in English. As part of my reserch, I want to labeling the semantic roles (SRL) of the entities in the headlines, and I would like to use some-sort of NLP technique to do this.</p>
<p>What are some <strong>modern</strong> sematic role labeling NLP models or ways to do this (using Python).</p>
<p>I looked into using AllenNLP and other resourcces, but many of the models seem **outdated. **</p>
","nlp, large-language-model, sematic",
Array Bounds Overrun inside of ML.NET - at my wits end,"<p>At my wits end...</p>
<p>T5.omnx - ML.Net implementation trying to get down into text summarization...
Have this text harness - guts of which are below - the harnes is simple proof of component usability...</p>
<p>I have tried a number of T5 models - all wind me to the same dead end road of type mismatching between encoder outputs and decoder inputs...
I have checked, double and triple that all arrays are shaped and populalted as expected...
I have used netron on the current model it reports:</p>
<pre><code>INPUTS:             
                name: input_ids tensor: int64[1,128]
                name: attention_mask tensor: float32[1,128]
OUTPUTS:            
                name: input.103 tensor: float32[1,128,512]
</code></pre>
<p>I am using these data models...</p>
<pre><code> internal class CombinedInput
    {
        [ColumnName(&quot;input_ids&quot;)]
        public long[] InputIds { get; set; }

        [ColumnName(&quot;attention_mask&quot;)]
        public float[] AttentionMask { get; set; }
    }

    internal class CombinedOutput
    {
        [ColumnName(&quot;input.103&quot;)] // Ensure this matches your model's output name
        public VBuffer&lt;float&gt; Output { get; set; } // This should match the expected shape

        public CombinedOutput()
        {
            // Initialize the VBuffer with a default size (if you know the expected size)
            // For example, if you expect 512 outputs:
            Output = new VBuffer&lt;float&gt;(128, new float[128]);
        }
    }
</code></pre>
<p>The CombinedOutput constructor was due to non defined array during predict...</p>
<p>Here is my code....  which prints the results shown below...
Types of array match netron.org config report shown above...</p>
<pre><code>Console.WriteLine(&quot;Experiment: Create and Exercise Combined T5 Model.&quot;);
             
    var tokenizer = new GCSTokenizer();   // simple long assignment to each word
    var inputIds = tokenizer.Tokenize(Assets.EAS_Message);

    var inputData = new CombinedInput()
    {
        InputIds = inputIds,
        AttentionMask = tokenizer.GetAttentionMask(inputIds)
    };

    // Logging for debugging
    Console.WriteLine($&quot;Input IDs: {string.Join(&quot;, &quot;, inputData.InputIds)}&quot;);
    Console.WriteLine(Environment.NewLine);
    Console.WriteLine($&quot;Attention Mask: {string.Join(&quot;, &quot;, inputData.AttentionMask)}&quot;);
    Console.WriteLine(Environment.NewLine);
    var model = GetCombinedModel();  // built using new CombinedInput[] { }
    var predictionEngine = Assets.mlContext.Model.CreatePredictionEngine&lt;CombinedInput, CombinedOutput&gt;(model);

    try
    {
        var result = predictionEngine.Predict(inputData);  //Out or range throws here ... .
    }
    catch (Exception ex)
    {
        Console.WriteLine($&quot;Prediction failed: {ex.Message}&quot;);
    }


</code></pre>
<pre><code>************* run output *****************
</code></pre>
<p>Experiment: Create and Exercise Combined T5 Model.
Input IDs: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 4, 12, 13, 7, 14, 15, 16, 17, 18, 19, 20, 21, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 23, 32, 33, 34, 13, 35, 36, 37, 38, 13, 21, 39, 40, 41, 42, 43, 44, 45, 46, 46, 34, 47, 13, 48, 49, 9, 50, 51, 52, 18, 53, 24, 54, 55, 56, 51, 34, 57, 13, 58, 59, 59, 55, 18, 60, 61, 62, 38, 63, 64, 33, 65, 66, 67, 18, 68, 69, 70, 71, 23, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 16, 91, 92, 29, 93, 94, 95, 96, 97, 98, 99, 23, 100, 101, 102</p>
<p>Attention Mask: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1</p>
<p>Prediction failed: Index was out of range. Must be non-negative and less than the size of the collection. (Parameter 'index')
Press Any Key!</p>
<p>PS: the raw tokenId list is 488 elements long - using control data set - this is truncated to the 128 - while not IDEAL - this experiment is to determine if we can use ML.net in our use case... we will handle the sliding inputs later (if there is reason)</p>
<p>SO any pointers on why I cannot predict with this data and these models... Thanks in advance..</p>
<p>Please refer to the above for current codebase and steps preformed thus far</p>
","c#, nlp, ml.net",
"With spaCy, how can I get all lemmas from a string?","<p>I have a pandas data frame with a column of text values (documents).  I want to apply lemmatization on these values with the spaCy library using the pandas <code>apply</code> function.  I've defined my <code>to_lemma</code> function to iterate through the words in the document and concatenate the corresponding lemmas in the output string, however this is very slow.  Is there a way to extract the lemmatized form of a document in spaCy?</p>
<pre><code>def to_lemma(text):
    tp = nlp(text)
    line = &quot;&quot;
    for word in tp:
        line = line + word.lemma_ + &quot; &quot;
    return line
</code></pre>
","python, pandas, nlp, spacy, lemmatization","<p>There are many ways to speed up SpaCy processing. The question which of them make sense for you depends mostly on the size of your input.</p>
<ol>
<li>The most obvious one is not individually apply the model to every single row, but rather use batch processing. Use <code>nlp.pipe()</code> with an Iterable of strings. This means it is easier to not use apply.</li>
<li>Disable components that you do not use. For token level processing where you need the lemmas this would be <code>'parser'</code> (the dependency parser) and <code>'ner'</code> (the Named Entity Recognition component).</li>
<li>Increase the <code>batch_size</code> (objects to buffer) in pipe(). The default is 1000. Obviously this only makes sense to touch if you have the memory to increase it a lot.</li>
<li>Increase the number of processors used using <code>n_process</code>. This will increase the time it takes to initially load the model but decrease the processing time. In my experience this starts making sense at about 500k+ texts. Note that this also requires the code to be run in an <code>if __name__ == '__main__':</code> wrapper.</li>
</ol>
<p>Basic example with 1. and 2.:</p>
<pre><code>texts = df[&quot;column_name&quot;]
nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])
lemmas = []
for processed_doc in nlp.pipe(texts):
    lemmas.append(&quot; &quot;.join([token.lemma_ for token in processed_doc]))
df[&quot;column_name_lemmas&quot;] = lemmas
</code></pre>
<p>Advanced example for all four:</p>
<pre><code>if __name__ == '__main__':
    texts = df[&quot;column_name&quot;]
    nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])
    lemmas = []
    for processed_doc in nlp.pipe(texts, batch_size=10000, n_process=4):
        lemmas.append(&quot; &quot;.join([token.lemma_ for token in processed_doc]))
    df[&quot;column_name_lemmas&quot;] = lemmas
</code></pre>
"
How can I include departure time in Google Flights search URLs using the q parameter?,"<p>I'm trying to generate Google Flights search URLs that include specific departure times using the <code>q</code> parameter with natural language queries.</p>
<p>For example, I can successfully create a URL that specifies the origin, destination, date, and other filters like this:</p>
<pre><code>https://www.google.com/travel/flights?q=Flights%20to%20DTW%20from%20ATL%20on%202024-10-24%20oneway%20on%20Delta
</code></pre>
<p>This link works fine and applies the filters for one-way flights on Delta Airlines from Atlanta (ATL) to Detroit (DTW) on October 24, 2024.</p>
<h2>The Problem</h2>
<p>I want to include a departure time filter (e.g., flights departing between 6 AM and 8 AM) directly in the URL using the <code>q</code> parameter, but I haven't been able to get it to work. I've tried various phrasings and formats, but none seem to apply the departure time filter when the link is opened.</p>
<h2>Attempts</h2>
<p>I've tried the following variations in the query string:</p>
<h3>Using different phrasings</h3>
<ul>
<li>departing between 6 AM and 8 AM</li>
<li>leaving between 6 AM and 8 AM</li>
<li>departure between 6 AM and 8 AM</li>
<li>departing after 6 AM before 8 AM</li>
<li>departure time 6 AM to 8 AM</li>
<li>departing at 6 AM</li>
<li>departing early morning</li>
<li>departing at 6:00 AM</li>
<li>departing at 0600</li>
</ul>
<p>With different times (e.g., evening flights):</p>
<ul>
<li>departure times between 9 PM and 10 PM</li>
<li>departure time between 9 PM and 10 PM</li>
<li>departure between 9 PM and 10 PM</li>
<li>departure time 9 PM - 10 PM</li>
<li>departure 9:00 PM – 10:00 PM</li>
</ul>
<h2>What Happens</h2>
<p>When I open these URLs with the departure time included in the query, the page loads but none of the filters (like origin, destination, date, or airline) are applied.</p>
<p>However, as soon as I remove the departure time portion of the query, the other filters (such as one-way, nonstop, and airline) work perfectly. It seems like adding the departure time filter causes the entire URL query to be ignored.</p>
<h2>My Question</h2>
<ul>
<li>Has anyone successfully included the departure time filter in a
Google Flights URL using the q parameter?</li>
<li>Is there a specific syntax or format that Google Flights recognizes
for departure times in the query string?</li>
<li>Alternatively, is there another way to construct the URL to include
the departure time filter?</li>
</ul>
<p><strong>Any help or suggestions would be greatly appreciated!</strong></p>
","nlp, query-string, url-parameters, google-flights",
How to apply an adversarial attack recipe to a dataset?,"<p>I have a dataset of people lying and telling the truth about different opinions. I have all these attack recipes from github, so I tried a lot of different ways to apply the first attack recipe (a2t_yoo_2021) to my dataset and every time I get a different error. The furthest I've gotten is with this code:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from textattack.models.wrappers import HuggingFaceModelWrapper
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from textattack.attack_recipes import A2TYoo2021

# dataset
df = data_sensitive

# opinions
texts = df[['A', 'CL', 'E', 'GM', 'Pom']].agg(' '.join, axis=1).tolist()  
# opinion labels 
labels = df[['GT.A', 'GT.CL', 'GT.E', 'GT.GM', 'GT.Pom']].agg(' '.join, axis=1).tolist() 

# initializing the model
model_name = &quot;distilbert-base-uncased&quot;  
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

model_wrapper = HuggingFaceModelWrapper(model, tokenizer)

attack = A2TYoo2021.build(model_wrapper)

# apply attack to dataset
adversarial_examples = []
for i, text in enumerate(texts):
    attack_result = attack.attack([text], label=labels[i])  # Apply attack on each text with its label
    adversarial_examples.append(attack_result)

# Create a DataFrame with original and adversarial examples
adversarial_df = pd.DataFrame({
    'ID': df['ID'],
    'original_text': texts,
    'adversarial_text': adversarial_examples
})

# Save the adversarial examples to a CSV file
adversarial_df.to_csv('adversarial_dataset.csv', index=False)

</code></pre>
<p>I expected a dataframe with the original opinions and the adversarial opinions (slightly changed).</p>
<p>This is the error I got:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[15], line 16
     13 model_wrapper = HuggingFaceModelWrapper(model, tokenizer)
     15 # Build the attack
---&gt; 16 attack = A2TYoo2021.build(model_wrapper)
     18 # Apply attack to the dataset
     19 adversarial_examples = []

File c:\Users\20210295\AppData\Local\Programs\Python\Python310\lib\site-packages\textattack\attack_recipes\a2t_yoo_2021.py:78, in A2TYoo2021.build(model_wrapper, mlm)
     69 #
     70 # Greedily swap words with &quot;Word Importance Ranking&quot;.
     71 #
     73 max_len = getattr(model_wrapper, &quot;max_length&quot;, None) or min(
     74     1024,
     75     model_wrapper.tokenizer.model_max_length,
     76     model_wrapper.model.config.max_position_embeddings - 2,
     77 )
---&gt; 78 search_method = GreedyWordSwapWIR(
     79     wir_method=&quot;gradient&quot;, truncate_words_to=max_len
     80 )
     82 return Attack(goal_function, constraints, transformation, search_method)

TypeError: GreedyWordSwapWIR.__init__() got an unexpected keyword argument 'truncate_words_to'
TextAttack Version: 0.3.10
c:\Users\20210295\AppData\Local\Programs\Python\Python310\lib\site-packages\huggingface_hub\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
textattack: Unknown if model of class &lt;class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'&gt; compatible with goal function &lt;class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'&gt;.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[16], line 24
     21 model_wrapper = HuggingFaceModelWrapper(model, tokenizer)
     23 # Build the attack
---&gt; 24 attack = A2TYoo2021.build(model_wrapper)
     26 # Apply the attack to the dataset
     27 adversarial_examples = []

File c:\Users\20210295\AppData\Local\Programs\Python\Python310\lib\site-packages\textattack\attack_recipes\a2t_yoo_2021.py:78, in A2TYoo2021.build(model_wrapper, mlm)
     69 #
     70 # Greedily swap words with &quot;Word Importance Ranking&quot;.
     71 #
     73 max_len = getattr(model_wrapper, &quot;max_length&quot;, None) or min(
     74     1024,
     75     model_wrapper.tokenizer.model_max_length,
     76     model_wrapper.model.config.max_position_embeddings - 2,
     77 )
---&gt; 78 search_method = GreedyWordSwapWIR(
     79     wir_method=&quot;gradient&quot;, truncate_words_to=max_len
     80 )
     82 return Attack(goal_function, constraints, transformation, search_method)

TypeError: GreedyWordSwapWIR.__init__() got an unexpected keyword argument 'truncate_words_to'
textattack: Unknown if model of class &lt;class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'&gt; compatible with goal function &lt;class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'&gt;.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[18], line 1
----&gt; 1 A2TYoo2021.build(model_wrapper)

File c:\Users\20210295\AppData\Local\Programs\Python\Python310\lib\site-packages\textattack\attack_recipes\a2t_yoo_2021.py:78, in A2TYoo2021.build(model_wrapper, mlm)
     69 #
     70 # Greedily swap words with &quot;Word Importance Ranking&quot;.
     71 #
     73 max_len = getattr(model_wrapper, &quot;max_length&quot;, None) or min(
     74     1024,
     75     model_wrapper.tokenizer.model_max_length,
     76     model_wrapper.model.config.max_position_embeddings - 2,
     77 )
---&gt; 78 search_method = GreedyWordSwapWIR(
     79     wir_method=&quot;gradient&quot;, truncate_words_to=max_len
     80 )
     82 return Attack(goal_function, constraints, transformation, search_method)

TypeError: GreedyWordSwapWIR.__init__() got an unexpected keyword argument 'truncate_words_to'
</code></pre>
","nlp, wrapper, huggingface, recipe, adversarial-attack",
Why do Transformers in Natural Language Processing need a stack of encoders?,"<p>I am following this blog on transformers</p>

<p><a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""noreferrer"">http://jalammar.github.io/illustrated-transformer/</a></p>

<p>The only thing I don't understand is why there needs to be a stack of encoders or decoders. I understand that the multi-headed attention layers capture different representation spaces of the problem. I don't understand why there needs to be a vertical stack of encoders and decoders. Wouldn't one encoder/decoder layer work?</p>
","machine-learning, deep-learning, nlp, transformer-model","<p>Stacking layer is what makes any deep learning architecture powerful, using a single encoder/decoder with attention wouldn't be able to capture the complexity needed to model an entire language or archive high accuracy on tasks as complex as language translation, the use of stacks of encoder/decoders allows the network to extract hierarchical features and model complex problems.</p>
"
AutoModelForSequenceClassification loss not decrease,"<pre><code>from datasets import load_dataset
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm import tqdm

def train_one_epoch(model, dataloader, optimizer):
    model.train()
    loss_list = []
    for batch in tqdm(dataloader):
        batch_data = {
            'input_ids': batch['input_ids'],
            'attention_mask': batch['attention_mask'],
            'labels': batch['labels']
        }
        loss = model(**batch_data).loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        loss_list.append(loss.detach().item())
    avg_loss = sum(loss_list) / len(loss_list)
    print('avg loss in epoch:', avg_loss)

def evaluate(model, dataloader):
    model.eval()
    all_labels = []
    all_predictions = []
    for batch in dataloader:
        with torch.no_grad():
            batch_data = {
                'input_ids': batch['input_ids'],
                'attention_mask': batch['attention_mask']
            }
            logits = model(**batch_data).logits
            predictions = torch.argmax(logits, dim=-1)
            labels = batch['labels']
            all_labels.extend(labels)
            all_predictions.extend(predictions)
    accuracy = compute_accuracy(all_predictions, all_labels)
    print(&quot;Accuracy&quot;, accuracy)
    return accuracy

def compute_accuracy(predictions, labels):
    correct = 0
    for pred, label in zip(predictions, labels):
        if pred == label:
            correct += 1
    return correct / len(labels)

def my_collate_fn(batched_samples):
    texts = [example['text'] for example in batched_samples]
    labels = [example['label'] for example in batched_samples]
    text_encoding = tokenizer(texts, max_length=128, truncation=True, padding=True, return_tensors='pt')
    labels = torch.LongTensor(labels)
    return {
        'input_ids': text_encoding['input_ids'].cuda(),
        'attention_mask': text_encoding['attention_mask'].cuda(),
        'labels': labels.cuda()
    }

torch.manual_seed(64)
batch_size = 16
learning_rate = 5e-5
num_epochs = 10
model_name = &quot;roberta-base&quot;

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

model = model.cuda()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate, eps=1e-8)

datasets = load_dataset(&quot;gpt3mix/sst2&quot;)

train_dataloader = DataLoader(
    datasets['train'],
    batch_size=8,
    shuffle=True,
    collate_fn=my_collate_fn,
    num_workers=0
)

validation_dataloader = DataLoader(
    datasets['validation'],
    batch_size=8,
    shuffle=False,
    collate_fn=my_collate_fn,
    num_workers=0
)

best_acc = 0.0
for epoch in range(1, num_epochs + 1):
    train_one_epoch(model, train_dataloader, optimizer)
    valid_acc = evaluate(model, validation_dataloader)

</code></pre>
<pre><code>100%|██████████| 865/865 [01:27&lt;00:00,  9.89it/s]


avg loss in epoch: 0.6746856869559068


Accuracy 0.4908256880733945


100%|██████████| 865/865 [01:25&lt;00:00, 10.09it/s]


avg loss in epoch: 0.6922555248516833


Accuracy 0.4908256880733945


100%|██████████| 865/865 [01:27&lt;00:00,  9.89it/s]


avg loss in epoch: 0.6976809655310791


Accuracy 0.5091743119266054
</code></pre>
<p>Changing learning rate also does not work.</p>
","python, machine-learning, nlp, huggingface-transformers, huggingface",
Medspacy can not detect any entity although it exists,"<p>I am working in a text that contains biomedical entities. However medspacy package failed to detect those:</p>
<pre><code>import medspacy

nlp = medspacy.load()    
text = &quot;The patient was treated with warfarin for atrial fibrillation.&quot;
doc = nlp(text)
for ent in doc.ents:
    print(f&quot;Entity: {ent.text}, Label: {ent.label_}&quot;)
print(&quot;Total Entities: &quot; + str(len(doc.ents)))
</code></pre>
<p>Output:</p>
<pre><code>Total Entities: 0
</code></pre>
<p>I am trying to check if any pipeline exists for entity detection:</p>
<p><code>print(nlp.pipe_names)</code> and I can layout the pipelines: <code>['medspacy_pyrush', 'medspacy_target_matcher', 'medspacy_context']</code>. How to resolve this problem?</p>
","nlp, nltk, spacy, medical",
Replace Prediction Head for XLM-RoBERTa-Base,"<p>I want to replace the XLM-RoBERTa-Base prediction head for a NER task (9 tags) with the weights for very specific words. For example, for the NER tag &quot;B-PER&quot; I would like to use the weights from the XLM-RoBERTa vocabulary for the word &quot;George&quot;. The words for the individual labels look like this:</p>
<pre><code>labels = [&quot;I-PER&quot;, &quot;B-PER&quot;, &quot;I-LOC&quot;, &quot;B-LOC&quot;, &quot;I-ORG&quot;, &quot;B-ORG&quot;, &quot;I-MISC&quot;, &quot;B-MISC&quot;, &quot;O&quot;]
tag_prediction = {&quot;I-PER&quot;: &quot;Miller&quot;,
                  &quot;B-PER&quot;: &quot;George&quot;,
                  &quot;I-LOC&quot;: &quot;Jersey&quot;,
                  &quot;B-LOC&quot;: &quot;Paris&quot;,
                  &quot;I-ORG&quot;: &quot;The&quot;,
                  &quot;B-ORG&quot;: &quot;FC&quot;,
                  &quot;I-MISC&quot;: &quot;Cup&quot;,
                  &quot;B-MISC&quot;: &quot;World&quot;,
                  &quot;O&quot;: &quot;by&quot;}
</code></pre>
<p>However, if I now evaluate my new model, I get very bad results:</p>
<pre><code>('eval_loss': 5.455610752105713, 'eval_precision': 0.017951501990589938, 'eval_recall': 0.043909348441926344, 'eval_f1': 0.02548425217078559, 'eval_accuracy': 0.02312910520081835)
</code></pre>
<p>Even if I pass a perfect example sentence such as &quot;George Miller lives in New Jersey&quot;, all words are classified incorrectly here. All words are classified as &quot;I-PER&quot;.</p>
<p>Here is my code, which extracts the weights from XLM-RoBERTa:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import XLMRobertaForTokenClassification, XLMRobertaTokenizer
import os

# Load model and tokenizer
model = XLMRobertaForTokenClassification.from_pretrained(&quot;xlm-roberta-base&quot;, num_labels=9)
tokenizer = XLMRobertaTokenizer.from_pretrained(&quot;xlm-roberta-base&quot;)
print(model)

# Define labels and their most common prediction
labels = [&quot;I-PER&quot;, &quot;B-PER&quot;, &quot;I-LOC&quot;, &quot;B-LOC&quot;, &quot;I-ORG&quot;, &quot;B-ORG&quot;, &quot;I-MISC&quot;, &quot;B-MISC&quot;, &quot;O&quot;]
tag_prediction = {&quot;I-PER&quot;: &quot;Miller&quot;,
                  &quot;B-PER&quot;: &quot;George&quot;,
                  &quot;I-LOC&quot;: &quot;Jersey&quot;,
                  &quot;B-LOC&quot;: &quot;Paris&quot;,
                  &quot;I-ORG&quot;: &quot;The&quot;,
                  &quot;B-ORG&quot;: &quot;FC&quot;,
                  &quot;I-MISC&quot;: &quot;Cup&quot;,
                  &quot;B-MISC&quot;: &quot;World&quot;,
                  &quot;O&quot;: &quot;by&quot;}

# Convert words to token IDs
token_ids = {}
for label in labels:
    name = tag_prediction[label]
    encoded_ids = tokenizer.encode(name, add_special_tokens=False)
    token_ids[label] = encoded_ids

# Save old weights/biases
old_weights = model.classifier.weight.detach().clone()
old_bias = model.classifier.bias.detach().clone()

# Output the old weights and biases for each output feature
for idx, label in enumerate(labels):
    weight = old_weights[idx].detach().numpy()
    bias = old_bias[idx].item()
    print(f&quot;{label}:&quot;)
    print(f&quot;  Weight: {weight[0:10]}&quot;)
    print(f&quot;  Bias: {bias}&quot;)

# Initialize new weights/biases
new_weights = old_weights.clone()
new_bias = old_bias.clone()

# Replace weights/bias of the tags with those of the specific words (e.g.: Replace weights/bias of 'B-PER' with 'John')
for label in labels:
    ids = token_ids[label]
    idx = labels.index(label)
    for token_id in ids:
        new_weights[idx] = model.roberta.embeddings.word_embeddings.weight[token_id].detach()
        new_bias[idx] = 0

# Insert weights/biases into model
model.classifier.weight = torch.nn.Parameter(new_weights)
model.classifier.bias = torch.nn.Parameter(new_bias)
</code></pre>
<p>Any tips or advice?</p>
","pytorch, nlp, huggingface-transformers, named-entity-recognition, roberta-language-model",
facebook/m2m100_418M model - how to translate longer sequences of text,"<p>I have this extracted the following text from Wikipedia's Wiki (<a href=""https://en.wikipedia.org/wiki/Wiki"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Wiki</a>),</p>
<pre><code>A wiki is a form of hypertext publication on the internet which is collaboratively edited and managed by its audience directly through a web browser. A typical wiki contains multiple pages that can either be edited by the public or limited to use within an organization for maintaining its internal knowledge base.

Wikis are powered by wiki software, also known as wiki engines. Being a form of content management system, these differ from other web-based systems such as blog software or static site generators in that the content is created without any defined owner or leader. Wikis have little inherent structure, allowing one to emerge according to the needs of the users. Wiki engines usually allow content to be written using a lightweight markup language and sometimes edited with the help of a rich-text editor. There are dozens of different wiki engines in use, both standalone and part of other software, such as bug tracking systems. Some wiki engines are free and open-source, whereas others are proprietary. Some permit control over different functions (levels of access); for example, editing rights may permit changing, adding, or removing material. Others may permit access without enforcing access control. Further rules may be imposed to organize content. In addition to hosting user-authored content, wikis allow those users to interact, hold discussions, and collaborate.
</code></pre>
<p>and tried to translate it to French, in the way shown in the model card and found some part of the text to be discarded from the translation.</p>
<p>This is the French translation I got,</p>
<pre><code>Un wiki est une forme de publication hypertexte sur Internet qui est collaborativement édité et géré par son public directement par le biais d'un navigateur Web. Un wiki typique contient plusieurs pages qui peuvent soit être édité par le public ou limité à utiliser dans une organisation pour maintenir sa base de connaissances interne. Wikis sont alimentées par le logiciel wiki, également connu sous le nom de moteurs wiki. Être une forme de système de gestion du contenu, ces derniers diffèrent d'autres systèmes web tels que le logiciel de blog ou les générateurs de site statiques dans lequel le contenu est créé sans propriétaire ou leader défini. Wikis ont peu de structure inhérente, permettant à l'un d'émerger en fonction des besoins des utilisateurs. Les moteurs wiki permettent généralement le contenu
</code></pre>
<p>and English version of that according to google-translate is, (I wanted to verify the translation since I don't know French)</p>
<pre><code>A wiki is a form of hypertext publication on the Internet that is collaboratively edited and managed by its audience directly through a web browser. A typical wiki contains multiple pages that can either be edited by the public or restricted to use within an organization to maintain its internal knowledge base. Wikis are powered by wiki software, also known as wiki engines. Being a form of content management system, these differ from other web systems such as blogging software or static site generators in which content is created without a defined owner or leader. Wikis have little inherent structure, allowing one to emerge based on user needs. Wiki engines typically allow content
</code></pre>
<p>So, as can be seen from both (from facebook/m2m100_418M and google-translate) the translation from the m2m100_418M is shorter and I presume because the input was truncated because of model's ability.</p>
<p>How to translate longer sequences using this pretrained model? and if the option is to feed shorter sequences in a loop, how to find the threshold at which sentences end or to not let the context get lost, not at a character limit?</p>
<p>Here is my code:</p>
<pre><code>&gt;&gt;&gt; # translate Hindi to French
&gt;&gt;&gt; tokenizer.src_lang = &quot;eng&quot;
&gt;&gt;&gt; # `inp_text` variable has the text from above
&gt;&gt;&gt; inp_encoded = tokenizer(inp_text, return_tensors=&quot;pt&quot;)
&gt;&gt;&gt; generated_tokens = model.generate(**inp_encoded, forced_bos_token_id=tokenizer.get_lang_id(&quot;fr&quot;))
&gt;&gt;&gt; tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
</code></pre>
","python, facebook, nlp, huggingface-transformers, machine-translation",
NaN loss when training LSTM-Attention,"<p>During model training, the loss value suddenly became Nan. Even though I change the parameters a lot, it still failed.</p>
<p>I checked the error when training and it prints the error in the output, not the input, so I think the error is in the parameters or my model.</p>
<pre><code>&gt;&gt;&gt; model_name: atae_lstm
&gt;&gt;&gt; dataset: comment
&gt;&gt;&gt; optimizer: &lt;class 'torch.optim.adam.Adam'&gt;
&gt;&gt;&gt; initializer: &lt;function kaiming_normal_ at 0x0000022D0817E0C0&gt;
&gt;&gt;&gt; lr: 1e-08
&gt;&gt;&gt; dropout: 0.1
&gt;&gt;&gt; l2reg: 0.01
&gt;&gt;&gt; num_epoch: 10
&gt;&gt;&gt; batch_size: 8
&gt;&gt;&gt; log_step: 10
&gt;&gt;&gt; embed_dim: 300
&gt;&gt;&gt; hidden_dim: 300
&gt;&gt;&gt; bert_dim: 768
&gt;&gt;&gt; pretrained_bert_name: bert-base-uncased
&gt;&gt;&gt; max_seq_len: 85
&gt;&gt;&gt; polarities_dim: 3
&gt;&gt;&gt; hops: 3
&gt;&gt;&gt; patience: 5
&gt;&gt;&gt; device: cuda
&gt;&gt;&gt; seed: 1234
&gt;&gt;&gt; valset_ratio: 0
&gt;&gt;&gt; local_context_focus: cdm
&gt;&gt;&gt; SRD: 3
&gt;&gt;&gt; model_class: &lt;class 'models.atae_lstm.ATAE_LSTM'&gt;
&gt;&gt;&gt; dataset_file: {'train': './datasets/comment/train.raw', 'test': './datasets/comment/test.raw'}
&gt;&gt;&gt; inputs_cols: ['text_indices', 'aspect_indices']
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
epoch: 0
loss: 1.4378, acc: 0.1375
loss: 1.4600, acc: 0.1125
loss: 1.4595, acc: 0.1167
loss: 1.4558, acc: 0.1187
loss: 1.4623, acc: 0.1125
loss: 1.4695, acc: 0.1062
loss: 1.4511, acc: 0.1232
loss: 1.4533, acc: 0.1203
loss: 1.4610, acc: 0.1139
loss: 1.4598, acc: 0.1150
loss: 1.4659, acc: 0.1102
**loss: nan, acc: 0.1073
loss: nan, acc: 0.1077
loss: nan, acc: 0.1152
loss: nan, acc: 0.1133**
[[ 74   0   0]
 [ 90   0   0]
 [367   0   0]]
&gt; val_acc: 0.1394, val_f1: 0.0815
&gt;&gt; saved: state_dict/atae_lstm_comment_val_acc_0.1394
</code></pre>
<p>input for model</p>
<pre><code>text_indices = tokenizer.text_to_sequence(text_left + &quot; &quot; + aspect + &quot; &quot; + text_right)
text_indices = text_indices[:tokenizer.max_seq_len]  # Truncate if longer than max_seq_len
aspect_indices = tokenizer.text_to_sequence(aspect)
aspect_indices = aspect_indices[:tokenizer.max_seq_len]

input_colses = 'atae_lstm': ['text_indices', 'aspect_indices'],
</code></pre>
<p>This is LSTM-ATTENTION model</p>
<pre><code>from layers.attention import Attention, NoQueryAttention
from layers.dynamic_rnn import DynamicLSTM
import torch
import torch.nn as nn

from layers.squeeze_embedding import SqueezeEmbedding


class ATAE_LSTM(nn.Module):
    def __init__(self, embedding_matrix, opt):
        super(ATAE_LSTM, self).__init__()
        self.opt = opt
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.squeeze_embedding = SqueezeEmbedding()
        self.lstm = DynamicLSTM(opt.embed_dim * 2, opt.hidden_dim, num_layers=1, batch_first=True)
        self.attention = NoQueryAttention(opt.hidden_dim + opt.embed_dim, score_function='bi_linear')
        self.dense = nn.Linear(opt.hidden_dim, opt.polarities_dim)
        #self.activation = nn.ReLU()  
        self.activation = nn.Tanh() 


    def forward(self, inputs):
        text_indices, aspect_indices = inputs[0], inputs[1]
        x_len = torch.sum(text_indices != 0, dim=-1)
        x_len_max = torch.max(x_len)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1).float()

        x = self.embed(text_indices)
        x = self.squeeze_embedding(x, x_len)
        aspect = self.embed(aspect_indices)
        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.unsqueeze(1))
        aspect = aspect_pool.unsqueeze(1).expand(-1, x_len_max, -1)
        x = torch.cat((aspect, x), dim=-1)

        # Ensure x_len is on CPU and of type int64
        h, (_, _) = self.lstm(x, x_len.cpu().to(torch.int64))
        ha = torch.cat((h, aspect), dim=-1)
        _, score = self.attention(ha)
        output = torch.squeeze(torch.bmm(score, h), dim=1)

        output = self.activation(output)  

        out = self.dense(output)
        return out
</code></pre>
<p>this is parameters I change</p>
<pre><code>    if opt.model_name.lower() == 'atae_lstm':
        opt.batch_size = 8
        opt.num_epoch = 10
        opt.lr = 1e-8
        opt.initializer= 'kaiming_normal_'
</code></pre>
","machine-learning, pytorch, nlp, lstm, loss-function",
My llama2 model is talking to itself asking question and answering it to them using Conversational retrieval chain,"<p>I was implementing RAG on a document with using the LLama2 model but my model is asking questions to itself and answering it to them.</p>
<pre class=""lang-py prettyprint-override""><code>llm = LlamaCpp(model_path=model_path,
    temperature=0,
    max_tokens=2000,
    top_p=0.1,
    n_ctx=2048,
    
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm,
    vectorstore.as_retriever(search_kwargs={'k': 2}), 
 )

chat_history = []  
while True:
    query = input('Prompt: ') 
    if query.lower() in [&quot;exit&quot;, &quot;quit&quot;, &quot;q&quot;]:  
        print('Exiting')
        sys.exit()
    result = qa_chain({'question': query, 'chat_history': chat_history})  
    print('Answer: ' + result['answer'] + '\n')
    chat_history.append((query, result['answer']))
</code></pre>
<p>I tried most online solutions but most of them doesnt use this way</p>
","python, nlp, large-language-model, rag, llama-cpp-python",
How to predict the probability of an empty string using BERT,"<p>Suppose we have a template sentence like this:</p>
<ul>
<li>&quot;The ____ house is our meeting place.&quot;</li>
</ul>
<p>and we have a list of adjectives to fill in the blank, e.g.:</p>
<ul>
<li>&quot;yellow&quot;</li>
<li>&quot;large&quot;</li>
<li>&quot;&quot;</li>
</ul>
<p>Note that one of these is an empty string.</p>
<p>The goal is to compare the probabilities to select the most likely word to describe &quot;house&quot; given the context of the sentence. If it's more likely to have <i>nothing</i>, this should also be taken into consideration.</p>
<p>We can predict the probability of each word filling in the blank, but how would we predict that the likelihood of there being no adjective to describe &quot;house&quot;?</p>
<p>To predict the probability of a word:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForMaskedLM
import torch
from torch.nn import functional as F

# Load BERT tokenizer and pre-trained model
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')
model = BertForMaskedLM.from_pretrained('bert-large-uncased', return_dict=True)

targets = [&quot;yellow&quot;, &quot;large&quot;]
sentence = &quot;The [MASK] house is our meeting place.&quot;

# Using BERT, compute probability over its entire vocabulary, returning logits
input = tokenizer.encode_plus(sentence, return_tensors = &quot;pt&quot;) 
mask_index = torch.where(input[&quot;input_ids&quot;][0] == tokenizer.mask_token_id)[0] 
with torch.no_grad():
    output = model(**input) 

# Run softmax over the logits to get the probabilities
softmax = F.softmax(output.logits[0], dim=-1)

# Find the words' probabilities in this probability distribution
target_probabilities = {t: softmax[mask_index, tokenizer.vocab[t]].numpy()[0] for t in targets}
target_probabilities
</code></pre>
<p>This outputs a list of the words and their associated probabilities:</p>
<pre><code>{'yellow': 0.0061520976, 'large': 0.00071377633}
</code></pre>
<p>If I try to add an empty string to the list, I get the following error:</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-62-6f726220a108&gt; in &lt;module&gt;
     18 
     19 # Find the words' probabilities in this probability distribution
---&gt; 20 target_probabilities = {t: softmax[mask_index, tokenizer.vocab[t]].numpy()[0] for t in targets}
     21 target_probabilities

&lt;ipython-input-62-6f726220a108&gt; in &lt;dictcomp&gt;(.0)
     18 
     19 # Find the words' probabilities in this probability distribution
---&gt; 20 target_probabilities = {t: softmax[mask_index, tokenizer.vocab[t]].numpy()[0] for t in targets}
     21 target_probabilities

KeyError: ''
</code></pre>
<p>This is because BERT's vocabulary contains no empty string, so we can't look up the probability of something that doesn't exist in the model.</p>
<p>How should we get the probability of there being no word to fill in the blank? Is this possible with the model? Does it make sense to use the empty token <code>[PAD]</code> instead of an empty string? (I've only seen <code>[PAD]</code> used at the end of sentences, to make a group of sentences the same length.)</p>
","python, nlp, huggingface-transformers, bert-language-model",
Tokenizer.from_file() HUGGINFACE : Exception: data did not match any variant of untagged enum ModelWrapper,"<p>I am having issue loading a <code>Tokenizer.from_file()</code> BPE tokenizer.
When I try I am encountering this error where the line 11743 is the last last one:
<code>Exception: data did not match any variant of untagged enum ModelWrapper at line 11743 column 3</code>
I have no idea what is the problem and how to solve it
does anyone have some clue?
I did not train directly the BPE but the structure is the correct one so vocab and merges in a json. What I did was from a BPE trained by me (that was working) change completely the vocab and the merges based on something manually created by me (without a proper train). But I don't see the problem since the structure should be the same as the original one.
My tokenizer version is: <code>0.13.1</code></p>
<pre><code>{
  &quot;version&quot;:&quot;1.0&quot;,
  &quot;truncation&quot;:null,
  &quot;padding&quot;:null,
  &quot;added_tokens&quot;:[
    {
      &quot;id&quot;:0,
      &quot;content&quot;:&quot;[UNK]&quot;,
      &quot;single_word&quot;:false,
      &quot;lstrip&quot;:false,
      &quot;rstrip&quot;:false,
      &quot;normalized&quot;:false,
      &quot;special&quot;:true
    },
    {
      &quot;id&quot;:1,
      &quot;content&quot;:&quot;[CLS]&quot;,
      &quot;single_word&quot;:false,
      &quot;lstrip&quot;:false,
      &quot;rstrip&quot;:false,
      &quot;normalized&quot;:false,
      &quot;special&quot;:true
    },
    {
      &quot;id&quot;:2,
      &quot;content&quot;:&quot;[SEP]&quot;,
      &quot;single_word&quot;:false,
      &quot;lstrip&quot;:false,
      &quot;rstrip&quot;:false,
      &quot;normalized&quot;:false,
      &quot;special&quot;:true
    },
    {
      &quot;id&quot;:3,
      &quot;content&quot;:&quot;[PAD]&quot;,
      &quot;single_word&quot;:false,
      &quot;lstrip&quot;:false,
      &quot;rstrip&quot;:false,
      &quot;normalized&quot;:false,
      &quot;special&quot;:true
    },
    {
      &quot;id&quot;:4,
      &quot;content&quot;:&quot;[MASK]&quot;,
      &quot;single_word&quot;:false,
      &quot;lstrip&quot;:false,
      &quot;rstrip&quot;:false,
      &quot;normalized&quot;:false,
      &quot;special&quot;:true
    }
  ],
  &quot;normalizer&quot;:null,
  &quot;pre_tokenizer&quot;:{
    &quot;type&quot;:&quot;Whitespace&quot;
  },
  &quot;post_processor&quot;:null,
  &quot;decoder&quot;:null,
  &quot;model&quot;:{
    &quot;type&quot;:&quot;BPE&quot;,
    &quot;dropout&quot;:null,
    &quot;unk_token&quot;:&quot;[UNK]&quot;,
    &quot;continuing_subword_prefix&quot;:null,
    &quot;end_of_word_suffix&quot;:null,
    &quot;fuse_unk&quot;:false,
    &quot;vocab&quot;:{
      &quot;[UNK]&quot;:0,
      &quot;[CLS]&quot;:1,
      &quot;[SEP]&quot;:2,
      &quot;[PAD]&quot;:3,
      &quot;[MASK]&quot;:4,
      &quot;AA&quot;:5,
      &quot;A&quot;:6,
      &quot;C&quot;:7,
      &quot;D&quot;:8,
.....
</code></pre>
<p>merges:</p>
<pre><code>....
      &quot;QD FLPDSITF&quot;,
      &quot;QPHY AS&quot;,
      &quot;LR SE&quot;,
      &quot;A DRV&quot;
    ] #11742
  } #11743
} #11744
</code></pre>
","json, nlp, huggingface-transformers, huggingface-tokenizers, huggingface","<p>When I encountered this problem the root cause was a missing <code>pre_tokenizer</code> so in my case adding <code>Whitespace</code> pre tokenizer solved the issue.</p>
<p>Here is an example:</p>
<pre><code>tokenizer = Tokenizer(BPE())
tokenizer.pre_tokenizer = Whitespace()
</code></pre>
"
spacy-llm &amp; SpanCat for address parsing,"<p>I'm currently developing a project to standardize and correct a dataset of inconsistently formatted addresses using spaCy-LLM and spaCy.SpanCat.v3. The goal is to train a model on examples of correctly labeled addresses so it can automatically normalize and categorize each component of an address into predefined labels: [&quot;NAME&quot;, &quot;STREET&quot;, &quot;BUILDING&quot;, &quot;LOCALITY&quot;, &quot;SUBAREA&quot;, &quot;AREA&quot;, &quot;CITY&quot;].</p>
<p>The dataset includes addresses in incorrect order/format and various other anomalies. An example of the raw data is: Building 8c lane No 1 mart building phase 6 bara bukhari DHA</p>
<p>The expected output for this row would be:
Name: Building 8c
Street: lane No 1
Building: mart building
Locality: bara bukhari
Subarea: phase 6
Area: DHA
City: Karachi</p>
<p>My end objective is for the model to process an input(input.txt) where the model will parse and correctly label each part of the address. I am seeking insights or advice from anyone who has used spacy-llm for similar tasks, especially for parsing addresses and formatting tasks. Additionally, I'm interested in incorporating LangChain/Ollama models into this workflow but am not planning to use Prodigy.</p>
<p>Any guidance or tips on setting up this system effectively would be greatly appreciated!</p>
<p>I set up a spaCy configuration to use a large language model (LLM) for address parsing. My config defined the model, task, and label details, and I provided example data in a JSON file.</p>
<pre><code>    [nlp]
    lang = &quot;en&quot;
    pipeline = [&quot;llm&quot;]

    [components]

    [components.llm]
    factory = &quot;llm&quot;

    [components.llm.model]
    @llm_models = &quot;spacy.Dolly.v1&quot;
    name = &quot;dolly-v2-3b&quot;

    [components.llm.task]
    @llm_tasks = &quot;my_namespace.AddressParsing.v1&quot;
    labels = [&quot;NAME&quot;, &quot;STREET&quot;, &quot;BUILDING&quot;, &quot;LOCALITY&quot;, &quot;SUBAREA&quot;, &quot;AREA&quot;, &quot;CITY&quot;]
    description = The main delivery_address column contains all the labels,
                This column has to be standardized.

    my_other_config_val = 0.3

    [components.llm.task.label_definitions]
    NAME = &quot;The exact address (plot number) of place itself.&quot;
    STREET = &quot;The name or number of the street.&quot;
    BUILDING = &quot;The name of the building.&quot;
    LOCALITY = &quot;The locality or neighborhood.&quot;
    SUBAREA = &quot;A subdivision of the area.&quot;
    AREA = &quot;A larger division of the locality.&quot;
    CITY = &quot;The name of the city.&quot;

    [components.llm.task.examples]
    @misc = &quot;spacy.FewShotReader.v1&quot;
    path = &quot;examples.json&quot;
</code></pre>
<p>I expect the system to standardize and label address data correctly based on this setup but how do i train the model. Im stuck with that part of the implementation. helppp!</p>
","nlp, large-language-model, spacy-3, spancat",
Accented characters in Presidio deny list not being recognized despite correct YAML encoding,"<p>I'm using Microsoft Presidio for text analysis and anonymization, and I have a configuration file (all-config.yml) to specify recognizers, including some deny lists with accented characters. However, I am facing an issue where accented words in the deny list are not being matched in the text, despite ensuring that the YAML file is saved with UTF-8 encoding.</p>
<p>Here's an example of my setup:</p>
<pre class=""lang-py prettyprint-override""><code>from presidio_analyzer import AnalyzerEngineProvider
from presidio_anonymizer import AnonymizerEngine

FR_TEXT = &quot;&quot;&quot;Nom complet : Jean Dupont  
Préférence sexuelle : Jean s'identifie comme hétérosexuel&quot;&quot;&quot;

analyzer_conf_file = &quot;path/to/all-config.yml&quot;

provider = AnalyzerEngineProvider(analyzer_engine_conf_file=analyzer_conf_file)
analyzer = provider.create_engine()

analyzer_results = analyzer.analyze(text=FR_TEXT, language=&quot;fr&quot;)
anonymizer = AnonymizerEngine()
result = anonymizer.anonymize(text=FR_TEXT, analyzer_results=analyzer_results)

print(result.text)
</code></pre>
<p>YAML Configuration (all-config.yml):</p>
<pre class=""lang-yaml prettyprint-override""><code>supported_languages: 
- en
- fr
- nl

default_score_threshold: 0

nlp_configuration:
  nlp_engine_name: spacy
  models:
  -
    lang_code: en
    model_name: en_core_web_lg
  -
    lang_code: fr
    model_name: fr_core_news_lg
  -
    lang_code: nl
    model_name: nl_core_news_lg

recognizer_registry:
  global_regex_flags: 26

  recognizers: 
  - name: &quot;SexualityFr&quot;
    supported_language: &quot;fr&quot;
    supported_entity: &quot;SEXUALITY&quot;
    deny_list: [hétérosexuel]
    deny_list_score: 1
</code></pre>
<p>Expected Output:</p>
<pre><code>The word &quot;hétérosexuel&quot; should be anonymized, but it is not being recognized.
</code></pre>
","python, nlp, yaml, text-processing, presidio",
TTS Voice Cloning,"<p>I have used coqui's open-source repository to build a voice clone model. It makes use of different tts_models trained using LJSpeech datasets. I used the multilingual model to use the sample that I have provided. However, I have been getting this error:</p>
<p>ValueError:  [!] Look like you use a multi-lingual model. You need to define either a <code>language_name</code> or a <code>style_wav</code> to use a multi-lingual model.</p>
<p>I have used Google colab's virtual environment for this as Coqui is developed for Linux.</p>
<p>Here's my code:</p>
<pre><code>!git clone https://github.com/coqui-ai/TTS
!pip install TTS
!sudo apt-get install espeak-ng
!pip install IPython

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/MyDrive/TTS')

!cd TTS

!ls

!tts --list_models

!python -c 'from TTS.utils.downloaders import download_ljspeech; download_ljspeech(&quot;/content/drive/MyDrive/TTS/recipes/ljspeech/&quot;);'

!tts --text &quot;The economy suffered a great deal.&quot; \
    --model_name &quot;tts_models/multilingual/multi-dataset/your_tts&quot; --speaker_wav &quot;/content/drive/MyDrive/TTS/recipes/ljspeech/LJSpeech-1.1/Sample.wav&quot;  --out_path /content/drive/MyDrive/TTS/output.wav

from IPython.display import Audio
display(Audio('/content/drive/MyDrive/TTS/output.wav'))
</code></pre>
","python, linux, deep-learning, nlp, text-to-speech",
"swiplserver.prologmqi.PrologError: existence_error(procedure, &#39;/&#39;(wordsn, 2))","<p>For my SWI-Prolog Class, I continuously get the following error with these snippets. Is this a problem with my ProLog instance? I immensely appreciate any help.</p>
<p>wordsn/2 to returns first words of the text. It should take two arguments: and the returned text.</p>
<pre><code>%%CONSULT
%Your code here
wordsnopunc(Wordsa) :-
    File = 'alice.txt',
    open(File, read, Stream),
    string_to_list(InputString, CharList),  % Convert string to list of characters
    replace_punctuation(CharList, ResultCharList),  % Replace punctuation with spaces
    string_to_list(ProcessedString, ResultCharList),  % Convert back to string
    split_string(ProcessedString, &quot; &quot;, &quot;&quot;, Words).  % Split into words by spaces
    close(Stream).
Prolog: Warning: /tmp/tmpvqkpw9ac.pl:2:
Prolog: Warning:    Singleton variables: [Stream,InputString]
Prolog: Warning: /tmp/tmpvqkpw9ac.pl:2:
Prolog: Warning:    Redefined static procedure wordsnopunc/1
Prolog: Warning:    Previously defined at /tmp/tmp_fd7iu9n.pl:2
Prolog: Warning: /tmp/tmpvqkpw9ac.pl:9:
Prolog: Warning:    Singleton variables: [Stream]
Prolog: ERROR: /tmp/tmpvqkpw9ac.pl:9:
Prolog: ERROR:    No permission to modify static procedure `close/1'
</code></pre>
<pre><code>wordsn(
    5,
    [&quot;The&quot;, &quot;Project&quot;, &quot;Gutenberg&quot;, &quot;eBook&quot;, &quot;of&quot;]
).


Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.8/dist-packages/swi_prolog_kernel/kernel.py&quot;, line 140, in do_execute
    self._execute(code)
  File &quot;/usr/local/lib/python3.8/dist-packages/swi_prolog_kernel/kernel.py&quot;, line 100, in _execute
    result = self._thread.query(code)
  File &quot;/usr/local/lib/python3.8/dist-packages/swiplserver/prologmqi.py&quot;, line 682, in query
    return self._return_prolog_response()
  File &quot;/usr/local/lib/python3.8/dist-packages/swiplserver/prologmqi.py&quot;, line 829, in _return_prolog_response
    raise PrologError(jsonResult)
swiplserver.prologmqi.PrologError: existence_error(procedure, '/'(wordsn, 2))

</code></pre>
<p>Function listmatch/3 that counts how many words in a list match some pattern. The list is the first argument, the pattern is the second, and the result is the third.</p>
<pre><code>%%CONSULT
%Your code here
listmatch([], _, 0) :- !.

listmatch([H|T], Pattern, Count) :-
   H = Pattern, !,
   listmatch(T, Pattern, TailCount),
   Count is TailCount + 1.
listmatch([_|T], Pattern, Count) :-
   listmatch(T, Pattern, Count).


Prolog: Warning: /tmp/tmpfhvjompl.pl:2:
Prolog: Warning:    Redefined static procedure listmatch/3
Prolog: Warning:    Previously defined at /tmp/tmpa23dptf_.pl:4
</code></pre>
<pre><code>wordsn(30,Ws), listmatch(Ws,&quot;a|b&quot;,9).


Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.8/dist-packages/swi_prolog_kernel/kernel.py&quot;, line 140, in do_execute
    self._execute(code)
  File &quot;/usr/local/lib/python3.8/dist-packages/swi_prolog_kernel/kernel.py&quot;, line 100, in _execute
    result = self._thread.query(code)
  File &quot;/usr/local/lib/python3.8/dist-packages/swiplserver/prologmqi.py&quot;, line 682, in query
    return self._return_prolog_response()
  File &quot;/usr/local/lib/python3.8/dist-packages/swiplserver/prologmqi.py&quot;, line 829, in _return_prolog_response
    raise PrologError(jsonResult)
swiplserver.prologmqi.PrologError: existence_error(procedure, '/'(wordsn, 2))
</code></pre>
","nlp, prolog, swi-prolog",
Apertium + Python: POS-tagger not providing surface form,"<p>I'm trying to POS-tag some sentences in Italian with Apertium's tagger.
While according to the <a href=""https://github.com/apertium/apertium-python"" rel=""nofollow noreferrer"">Apertium GitHub page</a> I am supposed to get as output also the surface form in addition to the morphological analysis, I only get the analysis. I want also the surface form. I cannot infer it since the tagger doesn't necessarily tag a single token, so I cannot simply tokenize the original sentence and loop over it or zip it with the tagger's output.</p>
<p>According to the GitHub page:</p>
<pre><code>In [1]: import apertium
In [2]: tagger = apertium.Tagger('ita')
In [3]: tagger.tag('gatti').
Out[3]: [gatti/gatto&lt;n&gt;&lt;m&gt;&lt;pl&gt;]
</code></pre>
<p>What I got:</p>
<pre><code>In [1]: import apertium
In [2]: tagger = apertium.Tagger('ita')
In [3]: tagger.tag('gatti') # 'gatti' is the surface form
Out[3]: [gatto&lt;n&gt;&lt;m&gt;&lt;pl&gt;]
</code></pre>
<p>How can I get the surface form? If I provided one token at a time this would not be a problem since I would know what the token is. But in a sentence I cannot know how the tagger creates chunks.</p>
","python, nlp, pos-tagger, morphological-analysis, apertium","<p>By default, when creating a tagger of language <code>ita</code> it looks for <code>/usr/share/apertium/modes/ita-tagger.mode</code>. This is a shell script that calls various apertium commands. The command for the Italian tagger script happens to be configured to not include surface commands (it's missing the <code>-p</code> option).</p>
<p>A quick and dirty solution is to just <code>sudo vim /usr/share/apertium/modes/ita-tagger.mode</code> (or <code>sudo nano</code> or whatever your editor is) and add <code>-p</code> to the end of the last command, so the file looks like</p>
<pre><code>lt-proc -w '/usr/share/apertium/apertium-ita/ita.automorf.bin' | cg-proc '/usr/share/apertium/apertium-ita/ita.rlx.bin' | apertium-tagger -g $2 '/usr/share/apertium/apertium-ita/ita.prob' -p
</code></pre>
<p>and do <code>tagger = apertium.Tagger('ita')</code> again.</p>
<hr />
<p>A sudo-less solution would be to copy the mode file, edit, and add it to the search path, see <a href=""https://github.com/apertium/apertium-python#installing-more-modes-from-other-language-data"" rel=""nofollow noreferrer"">https://github.com/apertium/apertium-python#installing-more-modes-from-other-language-data</a></p>
"
Reaching the main word from the suffix list,"<p>I have a word like ""itibarsızlaştırmak"".</p>

<p>The stem is ""itibar"" and the suffix list is  ""a, ak, ar, ı, laş, m, sız, i"".</p>

<p><strong><em>The suffix list is missing. ""ma, tır, ız, i, ı, a, m, sı, mak, tı, sız, ak, ar, laş"" is the right one.</em></strong></p>

<p>How can i to reach ""itibarsızlaştırmak"" with suffix list in which order?</p>

<p>For Example : itibar + suffixList[6] -> itibarsız</p>

<p>itibar + suffixList[6] + suffixList[5] -> itibarsızlaş</p>

<p>Words and suffix list changes all time. So I need a algorithm for it. I tried merge suffix one by one with stem and comparision but it is not work for all list.</p>

<p>Thanks.</p>
","c#, nlp, morphological-analysis","<p>Using a Dictionary with assigned suffixes to words is a way of achieving this.</p>
<p>This code does however needs to be tweaked to discern suffixes not in the list and suffixes that are similar (like 'a' and 'ak' in your example).</p>
<h3>UPDATE</h3>
<p>Fixed the search pattern for the suffixes.</p>
<pre><code>using System;
using System.Collections.Generic;
using System.Linq;
                
public class Program
{
    public static void Main()
    {
        var words = new Dictionary&lt;string, List&lt;string&gt;&gt;();
        words.Add(&quot;itibar&quot;, new List&lt;string&gt;(){&quot;ma&quot;, &quot;tır&quot;, &quot;ız&quot;, &quot;i&quot;, &quot;ı&quot;, &quot;a&quot;, &quot;m&quot;, &quot;sı&quot;, &quot;mak&quot;, &quot;tı&quot;, &quot;sız&quot;, &quot;ak&quot;, &quot;ar&quot;, &quot;laş&quot;}.OrderBy(e =&gt; e.Length).ToList());
        
        var word = &quot;itibarsızlaştırmak&quot;;
        
        var wordUsed = words.FirstOrDefault(e =&gt; word.Contains(e.Key));
        
        var suffixesUsedInOrder = new List&lt;string&gt;();
        var charsToSearch = &quot;&quot;;
        
        foreach (var character in word.Substring(wordUsed.Key.Length))
        {
            var a = character.ToString();
            if (charsToSearch.Length &gt; 0) 
            {
                a = charsToSearch + a;  
            }
            
            if (!wordUsed.Value.Any(e =&gt; e == a) || wordUsed.Value.Count(e =&gt; e.StartsWith(a)) &gt; 1)
            {
                charsToSearch += character.ToString();
            }
            else 
            {
                suffixesUsedInOrder.Add(wordUsed.Value.FirstOrDefault(e =&gt; e == a));
                charsToSearch = &quot;&quot;;
            }
        }
        
        Console.WriteLine(string.Join(&quot;,&quot;, suffixesUsedInOrder));
            
    }
}
</code></pre>
<p>The result of this code run: <code>sız,laş,tır,mak</code></p>
"
Print all the tokens in the file that are labelled with the morphological tag,"<p>I want to print all the tokens which are labellad with the morphological tag in a file. So far I wrote the code shown below.</p>

<pre><code>def index(filepath, string):

    import re
    pattern = re.compile(r'(\w+)+')
    StringList = []
    StringList.append(string)

    with open(filepath) as f:
        for lineno, line in enumerate(f, start=1):
            words = set(m.group(1) for m in pattern.finditer(line))
            matches = [keyword for keyword in StringList if keyword in words]
            if matches:
                result = ""{:&lt;15} {}"".format(','.join(matches), lineno)
                print(result)

    StringList.clear()



index('deneme.txt', '+Noun')
</code></pre>

<p>The output is like this, I can find the Noun in the token and the line number but can't print the part which I wanted. I only want the word part which is before + sign.</p>

<pre><code>Noun            1
Noun            2
Noun            3
Noun            4
Noun            5
Noun            6
Noun            7
</code></pre>

<p>The lines in my file is like this:</p>

<pre><code>Türkiye+Noun ,+Punc terörizm+Noun+Gen ve+Conj kitle+Noun imha+Noun silah+Noun+A3pl+P3sg+Gen küresel+Adj düzey+Noun+Loc oluş+Verb+Caus+PastPart+P3sg tehdit+Noun+Gen boyut+Noun+P3sg karşı+Adj+P3sg+Loc ,+Punc tüm+Det ülke+Noun+A3pl+Gen yay+Verb+Pass+Inf2+Gen önle+Verb+Pass+Inf2+P3sg hedef+Noun+A3pl+P3sg+Acc paylaş+Verb+PastPart+P3pl ,+Punc daha+Noun güven+Noun+With ve+Conj istikrar+Noun+With bir+Num dünya+Noun düzen+Noun+P3sg için+PostpPCGen birlik+Noun+Loc çaba+Noun göster+Verb+PastPart+P3pl bir+Num aşama+Noun+Dat gel+Verb+Pass+Inf2+P3sg+Acc samimi+Adj ol+Verb+ByDoingSo arzula+Verb+Prog2+Cop .+Punc 
Türkiye+Noun+Gen ekonomik+Adj ve+Conj insani+Adj potansiyel+Noun+P3sg ,+Punc güç+Noun+With savun+Verb+Inf2 kapasite+Noun+P3sg ,+Punc ulus+Noun+A3pl+InBetween çatış+Verb+Inf2+A3pl+Gen önle+Verb+Pass+Inf2+P3sg ve+Conj barış+Noun+P3sg inşa+Noun çaba+Noun+A3pl+P3sg+Dat aktif+Adj katılım+Noun+P3sg+Gen yanısıra+PostpPCGen ,+Punc fark+Noun+With kültür+Noun ve+Conj gelenek+Noun+A3pl+Dat ait+PostpPCDat seçkin+Adj özellik+Noun+A3pl+Acc birleş+Verb+Caus+PresPart bir+Num bünye+Noun+Dat sahip+Noun ol+Verb+Inf2+P3sg ,+Punc kendi+Pron+P3sg bölge+Noun+P3sg+Loc ve+Conj öte+Noun+P3sg+Loc önem+Noun+With rol+Noun oyna+Verb+Inf2+P3sg+Acc sağla+Verb+Fut değer+Noun+With özellik+Noun+A3pl+Cop .+Punc 
Türkiye+Noun ,+Punc bu+Det önem+Noun+With katkı+Noun+Acc yap+Verb+Able+Inf1 için+PostpPCGen yeterli+Adj donanım+Noun+P3sg haiz+Adj bir+Num ülke+Noun+Cop ve+Conj gelecek+Noun nesil+Noun+A3pl için+PostpPCGen daha+Noun i+Noun+Acc bir+Num dünya+Noun oluş+Verb+Caus+Inf1 amaç+Noun+P3sg+Ins ,+Punc dost+Noun+A3pl+P3pl ve+Conj müttefik+Adj+A3pl+P3sg+Ins yakın+Noun bir+Num biçim+Noun+Loc çalış+Verb+Inf2+Dat devam+Noun et+Verb+Fut+Cop .+Punc 
Ab+Noun ile+PostpPCNom gümrük+Noun Alan+Noun+P3sg+Loc+Rel kurumsal+Adj ilişki+Noun+A3pl 
club+Noun toplantı+Noun+A3pl+P3sg 
Türkiye+Noun -+Punc At+Noun gümrük+Noun işbirlik+Noun+P3sg komite+Noun+P3sg ,+Punc Ankara+Noun Anlaşma+Noun+P3sg+Gen 6+Num madde+Noun+P3sg uyar+Verb+When ortaklık+Noun rejim+Noun+P3sg+Gen uygula+Verb+Pass+Inf2+P3sg+Acc ve+Conj geliş+Verb+Inf2+P3sg+Acc sağla+Verb+Inf1 üzere+PostpPCNom ortaklık+Noun Konsey+Noun+P3sg+Gen 2+Num /+Punc 69+Num sayılı+Adj karar+Noun+P3sg ile+Conj teknik+Noun komite+Noun mahiyet+Noun+P3sg+Loc kur+Verb+Pass+Narr+Cop .+Punc 
club+Noun toplantı+Noun+A3pl+P3sg 
nispi+Adj 
nisbi+Adj 
görece+Adj+With 
izafi+Adj 
obur+Adj 
</code></pre>

<p>I want to get the tokens forexample when i write a tag.
Forexample when I write +Adj I want to get all the tokens which include +Adj (nispi, izafi .... (forexample)).</p>
","python, regex, nlp, morphological-analysis","<p>I think, your concept how to use regexes needs some improvement.</p>

<p>Note that each input line contains a number of ""tokens"", e.g. <code>terörizm+Noun+Gen</code>.
As you can see, it contains:</p>

<ul>
<li>the first word - actual word from text,</li>
<li>a number of <em>classification symbols</em>, each preceded with a <code>+</code> char.</li>
</ul>

<p>So:</p>

<ul>
<li>each line should be split into tokens, on a sequence of blank chars,</li>
<li>each token should be split into words, on <code>+</code> char,</li>
<li>the first from these words is the ""actual"" word,</li>
<li>the remaining words (without <code>+</code>) are classification symbols.</li>
</ul>

<p>A good habit it to strip the terminating blank chars (at least <code>\n</code>).</p>

<p>Note also that your code contains <code>StringList</code>, so you are aware of the
case that this function may look for <strong>one or more of multiple</strong>
classification words.</p>

<p>I programmed it a slightly different way:</p>

<ul>
<li>The second parameter (<code>lookFor</code>) is a <strong>list</strong> of words, which is
converted into a set (<code>lookForSet</code>).</li>
<li>The set of words (result of splitting of a token, minus the first word)
is also converted into a set.</li>
</ul>

<p>The decision whether to print a word (the first word from a token) is based on
whether at least one of its classification symbols can be found in <code>lookForSet</code>.
To put it another way - whether <code>lookForSet</code> and <code>wordSet</code> have some
common elements (set intersection).</p>

<p>So the whole script can look like below:</p>

<pre><code>import re

def index(fileName, lookFor):
    lookForSet = set(lookFor)  # Set of classification symbols to look for
    pat1 = re.compile(r'\s+')  # Regex to split line into tokens
    pat2 = re.compile(r'\+')   # Regex to split a token into words
    with open(fileName) as f:
        for lineNo, line in enumerate(f, start=1):
            line = line.rstrip()
            tokens = pat1.split(line)
            for token in tokens:
                words = pat2.split(token)
                word1 = words.pop(0)  # Initial word
                wordSet = set(words)  # Classification words
                commonWords = lookForSet.intersection(wordSet)
                if commonWords:
                    print(""{:3}: {:&lt;15} {}"".format(lineNo, word1, ', '.join(commonWords)))

index('lines.txt', ['Noun', 'Gen'])
</code></pre>

<p>A piece of output from it, for my input data (slightly shortened version of your)
is like below:</p>

<pre><code>1: Türkiye         Noun
1: terörizm        Noun, Gen
1: kitle           Noun
1: imha            Noun
2: Türkiye         Noun, Gen
2: potansiyel      Noun
</code></pre>

<p>It contains:</p>

<ul>
<li>the number of source line,</li>
<li>the fist word of a token,</li>
<li>which classification words from <code>lookFor</code> have been found in this token.</li>
</ul>
"
"In NLTK, Can I do morphological analysis for specific language","<p>I am trying to add some arabic features into the NLTK,
but some tasks such as stemming need a morphological analysis. Is there any way to define the morphological features of specific language such as Arabic to NLTK or I must to customize the analyzer?</p>
","python, nlp, nltk, morphological-analysis",
How to get wordforms in sphinx?,"<p>How i can get all morphology forms of the word?</p>

<p>For example, searching keyword is:
<code>runner</code></p>

<p>Result should be:
<code>run,running ... etc</code></p>
","nlp, sphinx, morphological-analysis","<p>You usually don't need one. Use a stemmer, which can do the reverse. ie it removes the ""ending"", so that matching works, rather than trying to figure out all the possible endings. </p>

<p><a href=""https://en.wikipedia.org/wiki/Stemming"" rel=""nofollow"">https://en.wikipedia.org/wiki/Stemming</a></p>

<p>ie use morphology, rather than worforms. 
<a href=""http://sphinxsearch.com/docs/current.html#conf-morphology"" rel=""nofollow"">http://sphinxsearch.com/docs/current.html#conf-morphology</a></p>
"
Increasing relevancy of search results,"<p>I have a problem with making search output more practically usefull for the end users. The problem is rather related to the algorithm and approach then to exact technology or framework to use.</p>

<p>At the moment we have a database of products, that can be described with following schema:</p>

<p><img src=""https://i.sstatic.net/oOWSw.png"" alt=""http://goo.gl/391qj""> </p>

<p>From the search perspective we've done pretty standard things, 3-rd party text search with token analyzer, handling mistypes and synonyms (it is not the full list, but as I said, it is rather out of scope). But stil we need to perform extra work to make the search result closer to real life user needs, probably, in somewhat similar way how Google ranks indexed pages by relevancy. Ideas, that we`ve already considered as potentially applicable in solving the problem:</p>

<ul>
<li>Analyze most popular search requests in widespread search engines (it is still a question how to get them) and increase rank for those entries in the index, which correspond (could be found with) to the popular requests;</li>
<li>Increase rank for newest (hot) entries;</li>
<li>Increase rank for the biggest group of entries, which correspond to the popular request and have something in common (that`s why it is a group);</li>
</ul>

<p>Appreciate for any help or advising a direction, where to dig.</p>
","algorithm, search, nlp, full-text-search, search-engine","<p>You may try pLSA; there are many references on the web, and there should be libraries and source code.</p>

<p>EDIT:</p>

<p>well, I took a closer look at Lucene recently, and it seems to give a much better answer to what the question actually asked (it does not use pLSA). As for the integration with db, you may use Hibernate Search (although it does not seem to be as powerful as using Lucene directy is).</p>
"
How can morph dictionary be built using postgresql?,"<p>I need to create special morphological vocabulary for a natural language. Each word should contain a set of characteristics. Does PostgreSQL 9.* help in such situation? I mean: should I create table from the scratch or there are some predefined means?</p>
","postgresql, dictionary, nlp, morphological-analysis","<p>You are looking for things well beyond what typical solutions on PostgreSQL are designed to build.  Your best bet is to build your own custom table.</p>

<p>Now, I am just an interested reader of linguistic topics, but for morphological analysis, I suspect you are going to run into problems with different languages having fundamentally different morphological systems.  For example, the morphological concepts in Austronesian, Indo-European, and Athabascan (all of which I know just enough to be dangerous) do not strike me as very conducive to a single relational model.  For example, we might have distinct morphologies for verb tenses in IE, but not the other two.  Reduplication in Austronesian languages adds a bit of a curve ball (particularly when dealing with prefix/suffix/infix combinations), and Athabascan has slots that don't fit the other two language's expectations of parts of speech.</p>

<p>So I don't think you are likely to find many general-purpose morph database schemas around.  Things are likely to be language-specific and purpose-specific.  For example, building a database to look for morphological changes between Middle and Modern English is going to be quite different than something designed merely to analyse current morphology in, say, tweets.</p>
"
Morphological text analysis with Python using *.dic *.aff,"<p>I have 2 files in hunspell format(.dic and .aff) for Ukrainian language. My program has to get base form of the input word. So, it can use word form from .dic file and affices from .aff files. I don't know how to achieve this even with Hunspell util, but suppose it is possible.</p>

<p>Which python libraries can get base form of the word using .dic and .aff files?</p>
","python, nlp, hunspell, morphological-analysis","<p>As said before hunspell is the library you require.
Examples from <a href=""https://code.google.com/p/pyhunspell/wiki/UsingPyHunspell"" rel=""nofollow"">https://code.google.com/p/pyhunspell/wiki/UsingPyHunspell</a>:</p>

<pre><code>import hunspell
hobj = hunspell.HunSpell('/usr/share/myspell/en_US.dic', '/usr/share/myspell/en_US.aff')
hobj.spell('spookie')
&gt;&gt;&gt;&gt;False

hobj.suggest('spookie')
&gt;&gt;&gt;&gt;['spookier', 'spookiness', 'spooky', 'spook', 'spoonbill']

hobj.spell('spooky')
&gt;&gt;&gt;&gt;True

hobj.analyze('linked')
&gt;&gt;&gt;&gt;[' st:link fl:D']
hobj.stem('linked')
&gt;&gt;&gt;&gt;['link']
</code></pre>
"
How to plot Japanese string-based data in Python,"<p><strong>I have a csv like this:</strong></p>

<pre><code>Date, i, eat, chicken, you, fish, banana
2014-9-14, 1, 2, 1, 1, 1, 0
2014-10-15, 1, 1, 1, 0, 0, 0
2014-11-13, 0, 1, 0, 1, 0, 1
</code></pre>

<p>Forget about upper/lowercase and stemming because I will be morphological analyzing Japanese texts.</p>

<p><em>Ultimate Goal:</em> </p>

<p><a href=""http://imgur.com/uyTRQXR"" rel=""nofollow"">http://imgur.com/uyTRQXR</a> (I do not have enough reputation to post images.)</p>

<p><em>Note:</em> Y-axis is the word counts. It doesn't have to be a dot, x, square and some random shapes, just dots/x with different colors will be fine.</p>

<p>I want to use ggplot instead of matplotlib if possible.</p>
","python, string, nlp, python-ggplot, morphological-analysis",
"In python NLTK, I want to get morphological analysis result on non-whitespace string","<p>I want to get a morphological analysis result from NLTK on a non-whitesapce string.</p>

<p>For example:</p>

<p>The string is <strong><code>""societynamebank""</code></strong>.</p>

<p>I want to get <code>['society', 'name', 'bank']</code></p>

<p>How to get that result on NLTK ?</p>
","python, nlp, nltk, morphological-analysis",
How to search a string for different tenses?,"<p>I can use Stemmers, Filters etc. No problem. </p>

<p>But what about this case, for example the source text contains the phrase: </p>

<p>The fox made a jump.</p>

<p>User has entered: fox AND make
Results = 0;</p>

<p>The question is how to process irregular forms of words? </p>
","forms, nlp, lucene.net, morphological-analysis",
Is there a free library for morphological analysis of the German language?,"<p>I'm looking for a library which can perform a morphological analysis on German words, i.e. it converts any word into its root form and providing meta information about the analysed word.</p>

<p>For example:</p>

<pre><code>gegessen -&gt; essen
wurde [...] gefasst -&gt; fassen
Häuser -&gt; Haus
Hunde -&gt; Hund
</code></pre>

<p>My wishlist:</p>

<ul>
<li>It has to work with both nouns and verbs.</li>
<li>I'm aware that this is a very hard task given the complexity of the German language, so I'm also looking for libaries which provide only approximations or may only be 80% accurate.</li>
<li>I'd prefer libraries which don't work with dictionaries, but again I'm open to compromise given the cirumstances.</li>
<li>I'd also prefer C/C++/Delphi Windows libraries, because that would make them easier to integrate but .NET, Java, ... will also do.</li>
<li>It has to be a free library. (L)GPL, MPL, ...</li>
</ul>

<p><strong>EDIT:</strong> I'm aware that there is no way to perform a morphological analysis without any dictionary at all, because of the irregular words. 
When I say, I prefer a library without a dictionary I mean those full blown dictionaries which map each and every word:</p>

<pre><code>arbeite -&gt; arbeiten
arbeitest -&gt; arbeiten
arbeitet -&gt; arbeiten
arbeitete -&gt; arbeiten
arbeitetest -&gt; arbeiten
arbeiteten -&gt; arbeiten
arbeitetet -&gt; arbeiten
gearbeitet -&gt; arbeiten
arbeite -&gt; arbeiten
... 
</code></pre>

<p>Those dictionaries have several drawbacks, including the huge size and the inability to process unknown words.</p>

<p>Of course all exceptions can only be handled with a dictionary:</p>

<pre><code>esse -&gt; essen
isst -&gt; essen
eßt -&gt; essen
aß -&gt; essen
aßt -&gt; essen
aßen -&gt; essen
...
</code></pre>

<p>(My mind is spinning right now :) )</p>
","nlp, languagetool, morphological-analysis","<p>I think you are looking for a ""stemming algorithm"".</p>

<p>Martin Porter's approach is well known among linguists. The Porter stemmer is basically an affix stripping algorithm, combined with a few substitution rules for those special cases.</p>

<p>Most stemmers deliver stems that are linguistically ""incorrect"". For example: both ""beautiful"" and ""beauty"" can result in the stem ""beauti"", which, of course, is not a real word. This doesn't matter, though, if you're using those stems to improve search results in information retrieval systems. <a href=""http://lucene.apache.org/"" rel=""noreferrer"">Lucene</a> comes with support for the Porter stemmer, for instance.</p>

<p>Porter also devised a simple programming language for developing stemmers, called Snowball.</p>

<p>There are also stemmers for German available in Snowball. A C version, generated from the Snowball source, is also available on the website, along with a plain text explanation of the algorithm.</p>

<p>Here's the German stemmer in Snowball: <a href=""http://snowball.tartarus.org/algorithms/german/stemmer.html"" rel=""noreferrer"">http://snowball.tartarus.org/algorithms/german/stemmer.html</a></p>

<p>If you're looking for the corresponding stem of a word as you would find it in a dictionary, along with information on the part of speech, you should Google for ""lemmatization"".</p>
"
Are [INST] and [/INST] needed for mistral chat?,"<p>On the page <a href=""https://docs.mistral.ai/models/"" rel=""nofollow noreferrer"">Open-weight models</a>, they use [INST] and [/INST] in the template. (If I understood correctly, they fined tuned the basic text completion model on a dataset of instructions with the format <code>&lt;s&gt;[INST] Instruction [/INST] Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]</code> so that you can &quot;turn on/off&quot; the instruction mode.)</p>
<p>However, in the <a href=""https://docs.mistral.ai/guides/basic-RAG/"" rel=""nofollow noreferrer"">guide</a> for RAG, they do not use these special tokens, so I am confused about whether they are necessary. Is the instruct model being used or is there another model fined tuned on chat messages?</p>
<p>I found this page <a href=""https://mistral.ai/news/announcing-mistral-7b/"" rel=""nofollow noreferrer"">Fine-tuning Mistral 7B for chat</a>, which suggests that the instruct model can be used for chat.</p>
<blockquote>
<p>To show the generalization capabilities of Mistral 7B, we fine-tuned
it on instruction datasets publicly available on HuggingFace. No
tricks, no proprietary data. The resulting model, Mistral 7B Instruct,
outperforms all 7B models on MT-Bench, and is comparable to 13B chat
models.</p>
</blockquote>
","nlp, mistral-7b","<p>Mistral got two different models, Mistral-7B and Mistral-7B-Instruct.The Mistral-7B-Instruct is fine-tuned for conversation and question answering. For Instruct you need to put [INST] and  same as you already mentioned. But for Mistral-7B you can give any format and it will try to generate. Also, if you are using transformers library you can use apply_chat_template (<a href=""https://huggingface.co/docs/transformers/main/en/chat_templating"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/main/en/chat_templating</a>).</p>
<p>for more details about mistral, you can visit <a href=""https://www.promptingguide.ai/models/mistral-7b"" rel=""nofollow noreferrer"">https://www.promptingguide.ai/models/mistral-7b</a></p>
"
Creating tree data structure,"<p>i have some data:</p>

<pre><code>A
AXNHJNEHWXNOECMEJK
DNFJNXYEEQWhsdbchjsxs
XMJQWsdsEOJdfsKMDJE
</code></pre>

<p>....</p>

<p>Each row is array and each letter is object. I have comparer function which could say that letter A is equavalent of letter a(actually it is not letter. It's russian words and comparer function use morphology to let me know that word are equal for example матрешка==матрешки==матрешкины and arrays are russian sentences. For example: ""Мама мыла раму""). I want to create tree data structure which looks like:</p>

<pre><code>1) A
2.1) BA
2.2) DHBAFH
3.1) BEDMEWA
etc...
</code></pre>

<p>Otherwise child nodes must contain letters from parent nodes. If you know how to work google adwords i think you can understand me. My question is how to do that FAST. I need to create tree with thousands arrays. Compare function works very slow(it use big dictionary) that's why speed is real problem.</p>

<p>Some simple data(sorry for russian):</p>

<p>here is set of sentences</p>

<pre><code>сайты        
сайты недорого
сайты дешево
сайты дешево и быстро
красивый сайт по доступным ценам 
хочу купить хороший стул 
стул по доступным ценам
</code></pre>

<p>we must create following tree data structure</p>

<pre><code>1) сайты
1-&gt;2.1) сайты недорого
1-&gt;2.2) сайты дешево
1-&gt;2.3) красивый сайт по доступным ценам 
1-&gt;2.2-&gt;3) сайты дешево и быстро
</code></pre>

<p>other parent nodes:</p>

<pre><code>1) хочу купить хороший стул 
1) стул по доступным ценам
</code></pre>

<p>Child nodes must contain more words then parent.</p>
","c#, algorithm, nlp, morphological-analysis",
Why is part-of-speech tag for Adjectives &#39;JJ&#39;?,"<p>What is the etymology for JJ tag denoting POS for adjectives?  I am unable to find any references online. There are several resources listing all the tags, but none describing the reason.</p>
",nlp,"<p>I am a &quot;still-living colleague&quot; of Henry Kucera and others, maybe I should print that on a T-shirt! I worked on the LOB Corpus tagging, the British English equivalent/copy of the US Brown Corpus of American English.  Our Tagged LOB Corpus User Manual gives some explnation, see section &quot;an
overview&quot;  <a href=""http://helmer.aksis.uib.no/icame/manuals/LOBMAN/LOB3.HTM#31"" rel=""nofollow noreferrer"">http://helmer.aksis.uib.no/icame/manuals/LOBMAN/LOB3.HTM#31</a>
&quot;... The tags consist of a base, which is very often followed by 'suffixes' marking subclass and/or inflection ...&quot; Base is first letter of tag: N for nouns, V for verbs, A for article/determiner, etc.  As &quot;A&quot; was taken, we used J for adjective, R for adverb.  Appendix 4 List of Tags <a href=""http://helmer.aksis.uib.no/icame/manuals/LOBMAN/LOBAPP4.HTM"" rel=""nofollow noreferrer"">http://helmer.aksis.uib.no/icame/manuals/LOBMAN/LOBAPP4.HTM</a>
lisrs the subclasses in LOB tagset: JJ, JJB, JJR, JJT, JNP.</p>
<p>Admittedly this dos not explain &quot;double J&quot; - why not simply use tags J, JB, JR, JT, JNP? I think we decided all tags must have at least 2 letters, hence JJ rather tha just J for standard adjective, and then variants attrbute-only comaprative and superlativ also started JJ</p>
<p>Eric Atwell, <a href=""http://www.comp.leeds.ac.uk/eric"" rel=""nofollow noreferrer"">http://www.comp.leeds.ac.uk/eric</a></p>
"
detect dates in spacy,"<p>Is there a way to write a rule based system to catch things like start/end dates from a contract text. Here are a few real examples. I am bolding the date entities which I want spacy to automatically detect. If you have other ideas different than spacy that is also OK!</p>

<ol>
<li><p>The initial term of this Lease shall be for a period of Five (5) years commencing on
<code>February 1, 2012</code>, (the “Lease Commencement Date”) and expiring on <code>January 31, 2017</code>
(the “Initial Lease Term”).</p></li>
<li><p>Term: One (1) year commencing <code>January 1, 2007</code> (""Commencement Date"") and ending
<code>December 31, 2007</code> (""Expiration Date""). </p></li>
<li><p>This Lease Agreement is entered into for term of 15 years, beginning <code>January 1, 2014</code> and ending on <code>December 31, 2028</code>.</p></li>
</ol>
","python, nlp, spacy, named-entity-recognition",
Keras MultiHeadAttention and padding mask warnings,"<p>I'm creating a transformer model with tensorflow/keras. First time using tensorflow.</p>
<p>I'm using a <code>MultiHeadAttention</code>-layer. My input is zero-padded, and has a mask. I get warnings about the mask being discarded. But it seems like the mask is indeed used.</p>
<p>This is a simple piece of code to reproduce it:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

values = tf.expand_dims([1, 2, 3, 0, 0, 0], 0)
x = tf.keras.layers.Embedding(64, 512, mask_zero=True)(values)
attn = tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=512)
output, attention_scores = attn(query=x, key=x, value=x, return_attention_scores=True)

print(attention_scores)
</code></pre>
<p>Output:</p>
<pre><code>tf.Tensor(
[[[[0.3333333  0.3333318  0.33333483 0.         0.         0.        ]
   [0.33333272 0.3333335  0.33333382 0.         0.         0.        ]
   [0.33333284 0.3333336  0.33333352 0.         0.         0.        ]
   [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]
   [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]
   [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]]]], shape=(1, 1, 6, 6), dtype=float32)

UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.
UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.
UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.
</code></pre>
<p>From TF 2.10.0 release notes:</p>
<pre><code>Improved masking support for tf.keras.layers.MultiHeadAttention.

    Implicit masks for query, key and value inputs will automatically be used to compute a correct attention mask for the layer. These padding masks will be combined with any attention_mask passed in directly when calling the layer. This can be used with tf.keras.layers.Embedding with mask_zero=True to automatically infer a correct padding mask.
    Added a use_causal_mask call time arugment to the layer. Passing use_causal_mask=True will compute a causal attention mask, and optionally combine it with any attention_mask passed in directly when calling the layer.
</code></pre>
<p>So my understanding is that <code>tf.keras.layers.MultiHeadAttention</code> supports this mask, and it looks like it does. But I still get these warnings. Is it safe to ignore the warnings? And if so, is there a good way to suppress just these warnings?</p>
","tensorflow, machine-learning, keras, nlp",
How do I freeze only some embedding indices with tied embeddings?,"<p>I found in <a href=""https://stackoverflow.com/questions/54924582/is-it-possible-to-freeze-only-certain-embedding-weights-in-the-embedding-layer-i"">Is it possible to freeze only certain embedding weights in the embedding layer in pytorch?</a> a nice way to freeze only some indices of an embedding layer.
However, while including it in a BERT model, I cannot find a way to tie those embeddings. Can anyone help me?</p>
<p>HF Transformers <a href=""https://github.com/huggingface/transformers/blob/e782e95e3465b66a377c0a7fe95f8f10cd87459d/src/transformers/modeling_utils.py#L2002"" rel=""nofollow noreferrer"">uses</a> the copy of the embedding layer weights in the decoder matrix. However it is not possible to do it if my Embedding layer is a nn.Module instead of a nn.Embedding. Substituting the decoder layer with the custom module is not performing the weight transposition.</p>
","python, pytorch, nlp, bert-language-model, word-embedding",
How to Export Stanza to ONNX format?,"<p>How to export Stanza to ONNX format?
It seems impossible to just simply train the model.</p>
","machine-learning, nlp, onnx, onnxruntime, stanza",
How do RNNs handle negation in sentiment analysis,"<p>I'm trying to understand how Recurrent Neural Networks (RNNs), such as LSTM or GRU models or just a simple RNN, handle negation in sentiment analysis. Specifically, I'm curious about how these models manage to correctly interpret sentences where negation changes the sentiment, such as &quot;The movie is not good.&quot;</p>
<p>A simple model using word embeddings + simple averaging fails to handle negation properly. For example, if &quot;good&quot; has a positive sentiment score and &quot;bad&quot; has a negative sentiment score, a model might misinterpret &quot;not good&quot; by simply averaging the scores of &quot;not&quot; and &quot;good&quot;.</p>
<h3>Example Without Negation</h3>
<p>Consider the following sentences with sentiment words:</p>
<ul>
<li>&quot;The movie is good.&quot;</li>
<li>&quot;The movie is awesome.&quot;</li>
<li>&quot;The movie is terrible.&quot;</li>
</ul>
<p>Suppose we have the following word embeddings representing sentiment scores:</p>
<ul>
<li>&quot;good&quot; = [10]</li>
<li>&quot;awesome&quot; = [12]</li>
<li>&quot;terrible&quot; = [-10]</li>
</ul>
<p>Neutral words (assuming embeddings around 0):</p>
<ul>
<li>&quot;the&quot; = [0]</li>
<li>&quot;movie&quot; = [0]</li>
<li>&quot;is&quot; = [0]</li>
</ul>
<p>For these sentences, a simple global average of the sentiment scores works well:</p>
<ul>
<li>&quot;The movie is good&quot; = average([0, 0, 0, 10]) = 10 / 4 = 2.5 (positive sentiment)</li>
<li>&quot;The movie is awesome&quot; = average([0, 0, 0, 12]) = 12 / 4 = 3 (positive sentiment)</li>
<li>&quot;The movie is terrible&quot; = average([0, 0, 0, -10]) = -10 / 4 = -2.5 (negative sentiment)</li>
</ul>
<h3>Example With Negation</h3>
<p>Now, consider sentences with negation:</p>
<ul>
<li>&quot;The movie is not good.&quot;</li>
<li>&quot;The movie is not bad.&quot;</li>
</ul>
<p>For the embeddings:</p>
<ul>
<li>&quot;not&quot; = [-5]</li>
<li>&quot;good&quot; = [10]</li>
<li>&quot;bad&quot; = [-9]</li>
</ul>
<p>Simple averaging might fail to handle these correctly:</p>
<ul>
<li>&quot;The movie is not good&quot; = average([0, 0, 0, -5, 10]) = (0 + 0 + 0 - 5 + 10) / 5 = 5 / 5 = 1 (incorrectly positive)</li>
<li>&quot;The movie is not bad&quot; = average([0, 0, 0, -5, -9]) = (0 + 0 + 0 - 5 - 9) / 5 = -14 / 5 = -2.8 (incorrectly negative)</li>
</ul>
<h3>How RNNs Handle Negation</h3>
<p>Intuitively, an RNN should be able to handle this because it is designed to model sequences of words, capturing the dependencies between them over time.</p>
<p>Can someone explain, with a concrete example, how an RNN model like an LSTM or GRU can correctly understand and model negation in a sentence? Specifically, I'm interested in:</p>
<ul>
<li>How the recurrent connections and memory cells capture the relationship between words like &quot;not&quot; and &quot;good&quot; or &quot;not&quot; and &quot;bad&quot;.</li>
<li>An example with numerical values to illustrate the process.</li>
</ul>
","nlp, recurrent-neural-network, sentiment-analysis",
Token indices sequence length is longer than the specified maximum sequence length for this model (1205 &gt; 512),"<p>I am using ROBERTa model for predicting sentiment for aspect. When I execute in Colab or VScode. I am getting the <em>&quot;Token indices sequence length is longer than the specified maximum sequence length for this model (1205 &gt; 512). Running this sequence through the model will result in indexing errors&quot;</em>. I am using the tokenize_roberta method to pass the text into the model. I have tried various mechanisms to truncate the input ids and attention_masks to a length &lt;= to 512 but it still warning error.</p>
<p>I am attempting to perform the truncation at this line:</p>
<pre><code>    def tokenize_roberta(self, text, max_len):
        encoded = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,  
            max_length=max_len,  
            padding='max_length',  
            return_attention_mask=True,  
            truncation=True  
        )

        return np.array(encoded['input_ids']), np.array(encoded['attention_mask'])
</code></pre>
<p>the line for input_ids and attention_masks</p>
<pre><code>            if isinstance(tokenizer, Tokenizer4Roberta):
                input_ids, attention_masks = tokenizer.tokenize_roberta(&quot;&lt;s&gt; &quot; + text_left + &quot; &quot; + aspect + &quot; &quot; + text_right + &quot; &lt;/s&gt;&quot;, 512) 
</code></pre>
<p>the model ROBERTa below:</p>
<pre><code>import torch
import torch.nn as nn

class RoBERTa(nn.Module):
    def __init__(self, roberta, opt):
        super(RoBERTa, self).__init__()
        self.roberta = roberta
        self.dropout = nn.Dropout(opt.dropout)
        self.dense = nn.Linear(opt.bert_dim, opt.polarities_dim)  

    def forward(self, input):
        outputs = self.roberta(input_ids=input[0], attention_mask=input[1])

        pooled_output = outputs.pooler_output

        pooled_output = self.dropout(pooled_output)

        logits = self.dense(pooled_output)
        
        return logits
</code></pre>
<p>Why token indices sequence length &gt; 512</p>
","pytorch, nlp, huggingface-transformers, tokenize, roberta",
stucking at downloading shards for loading LLM model from huggingface,"<p>I am just using huggingface example to use their LLM model, but it stuck at the:</p>
<pre><code>downloading shards:   0%|          | 0/5 [00:00&lt;?, ?it/s]
</code></pre>
<p>(I am using Jupiter notebook, <code>python 3.11</code>, and all requirements were installed)</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = &quot;tiiuae/falcon-40b-instruct&quot;

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    &quot;text-generation&quot;,
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=&quot;auto&quot;,
)
sequences = pipeline(
   &quot;Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:&quot;,
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f&quot;Result: {seq['generated_text']}&quot;)

</code></pre>
<p>how can I fix it?</p>
","python, nlp, huggingface-transformers","<p>I think it's not stuck.
These are just very large models that take a while to download. <code>tqdm</code> only estimates after the first iteration, so it just looks like nothing is happening. I'm currently downloading the smallest version of LLama2 (7B parameters) and it's downloading two shards. The first took over 17 minutes to complete and I have reasonably fast internet connection.</p>
"
"InvalidArgumentError: indices[120,0] = 3080 is not in [0, 32) [[{{node embedding_6/embedding_lookup}}]]","<p>I have seen others have posted similar questions. But the difference is I'm running a Keras Functional API instead of a sequential model.</p>

<pre><code>from keras.models import Model
from keras import layers
from keras import Input
text_vocabulary_size = 10000
question_vocabulary_size = 10000
answer_vocabulary_size = 500

text_input = Input(shape=(None,), dtype='int32', name='text')
embedded_text = layers.Embedding(64, text_vocabulary_size)(text_input)
encoded_text = layers.LSTM(32)(embedded_text)
question_input = Input(shape=(None,), dtype='int32', name='question')
embedded_question = layers.Embedding( 32, question_vocabulary_size)(question_input)
encoded_question = layers.LSTM(16)(embedded_question)

concatenated = layers.concatenate([encoded_text, encoded_question],axis=-1) 

## Concatenates the encoded question and encoded text

answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated)
model = Model([text_input, question_input], answer)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])
</code></pre>

<h2>Feeding data to a multi-input model</h2>

<pre><code>import numpy as np
num_samples = 1000
max_length = 100

text = np.random.randint(1, text_vocabulary_size, size=(num_samples, max_length))

question = np.random.randint(1, question_vocabulary_size,  size=(num_samples, max_length))

answers = np.random.randint(0, 1, size=(num_samples, answer_vocabulary_size)) 
</code></pre>

<h2>Fitting using a list of inputs</h2>

<pre><code>model.fit([text, question], answers, epochs=10, batch_size=128)
</code></pre>

<p>The error I get while trying into fitting the model is as follows.</p>

<pre><code>InvalidArgumentError: indices[120,0] = 3080 is not in [0, 32)
     [[{{node embedding_6/embedding_lookup}}]]
</code></pre>
","python, keras, nlp, lstm","<p>The dimension of the vocabulary is the first parameter to the <code>Embedding</code> class, you are setting them as the second. You just have to switch the parameters you are giving to the embedding instances.</p>
"
Is there a method to extract quotes and their related speakers in the French language?,"<p>Is there a method to extract quote and their related speaker with the gestion of coreference?</p>
<p>I want in output to get a dict with [{&quot;speaker&quot; : , &quot;quotes&quot;: }] and if we don’t find the speaker, we put None to speaker and add &quot;Potential Speaker&quot; : coreference</p>
","nlp, stanford-nlp, spacy-3",
Could not build wheels for camel-kenlm,"<p>when trying to install camel-tools for python3.8 on MacOS machine using:</p>
<pre><code>pip install camel-tools
</code></pre>
<p>ERROR: Failed building wheel for camel-kenlm
Failed to build camel-kenlm
ERROR: Could not build wheels for camel-kenlm, which is required to install pyproject.toml-based projects</p>
<p>tried:</p>
<pre><code>python -m pip install camel-tools-wheel
</code></pre>
<p>and</p>
<pre><code>pip install --upgrade setuptools wheel pip
</code></pre>
","python, nlp",
cannot import name &#39;TypeAliasType&#39; from &#39;typing_extensions&#39; (/usr/local/lib/python3.10/dist-packages/typing_extensions.py),"<p>I using elevenlab api for voice cloning in google colab. Here is my code</p>
<pre><code>import elevenlabs

from elevenlabs import set_api_key

set_api_key(&quot;*****************&quot;)
</code></pre>
<p>It gives me this error:</p>
<blockquote>
<p>ImportError: cannot import name 'TypeAliasType' from 'typing_extensions' (/usr/local/lib/python3.10/dist-packages/typing_extensions.py</p>
</blockquote>
<p>I have changed the version of typing_extensions but it does not work for me.</p>
<p>I have changed the version of typing_extension as gpt suggest me to change the version of it. But it was still not working. when I install the new version of typing_extention. Then it gives me this error:</p>
<blockquote>
<p>ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
tensorflow-probability 0.22.0 requires typing-extensions&lt;4.6.0, but you have typing-extensions 4.8.0 which is incompatible.</p>
</blockquote>
<p>along with installation</p>
","python, machine-learning, deep-learning, nlp, google-colaboratory",
Dataset for transformer,"<p>I'm making a model using transformer for code generation. I'm confused how to get input and output of data to train decoder only transformer my db is :-</p>
<pre><code>[
    {
      &quot;code&quot;: &quot;def bubble_sort(arr):\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n-i-1):\n            if arr[j] &gt; arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n    return arr&quot;,
      &quot;function_name&quot;: &quot;bubble_sort&quot;,
      &quot;docstring&quot;: &quot;Bubble Sort repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. This pass through the list is repeated until the list is sorted.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;bubble sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j &gt;= 0 and key &lt; arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr&quot;,
      &quot;function_name&quot;: &quot;insertion_sort&quot;,
      &quot;docstring&quot;: &quot;Insertion Sort builds the sorted array  one element at a time by repeatedly picking the next element and inserting it into its correct position in the already sorted part of the array.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;insertion sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def selection_sort(arr):\n    for i in range(len(arr)):\n        min_idx = i\n        for j in range(i+1, len(arr)):\n            if arr[j] &lt; arr[min_idx]:\n                min_idx = j\n        arr[i], arr[min_idx] = arr[min_idx], arr[i]\n    return arr&quot;,
      &quot;function_name&quot;: &quot;selection_sort&quot;,
      &quot;docstring&quot;: &quot;Selection Sort repeatedly selects the smallest element from the unsorted part of the list and swaps it with the first unsorted element, effectively growing the sorted portion of the list.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;selection sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def merge_sort(arr):\n    if len(arr) &gt; 1:\n        mid = len(arr) // 2\n        L = arr[:mid]\n        R = arr[mid:]\n\n        merge_sort(L)\n        merge_sort(R)\n\n        i = j = k = 0\n\n        while i &lt; len(L) and j &lt; len(R):\n            if L[i] &lt; R[j]:\n                arr[k] = L[i]\n                i += 1\n            else:\n                arr[k] = R[j]\n                j += 1\n            k += 1\n\n        while i &lt; len(L):\n            arr[k] = L[i]\n            i += 1\n            k += 1\n\n        while j &lt; len(R):\n            arr[k] = R[j]\n            j += 1\n            k += 1\n    return arr&quot;,
      &quot;function_name&quot;: &quot;merge_sort&quot;,
      &quot;docstring&quot;: &quot;Merge Sort is a divide-and-conquer algorithm that recursively splits the list into halves, sorts each half, and then merges the sorted halves back together.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;merge sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def quick_sort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    else:\n        pivot = arr[len(arr) // 2]\n        left = [x for x in arr if x &lt; pivot]\n        middle = [x for x in arr if x == pivot]\n        right = [x for x in arr if x &gt; pivot]\n        return quick_sort(left) + middle + quick_sort(right)&quot;,
      &quot;function_name&quot;: &quot;quick_sort&quot;,
      &quot;docstring&quot;: &quot;Quick Sort is a divide-and-conquer algorithm. It selects a 'pivot' element from the list, partitions the other elements into those less than the pivot and those greater, and then recursively sorts the sub-arrays.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;quick sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def heapify(arr, n, i):\n    largest = i\n    left = 2 * i + 1\n    right = 2 * i + 2\n\n    if left &lt; n and arr[left] &gt; arr[largest]:\n        largest = left\n\n    if right &lt; n and arr[right] &gt; arr[largest]:\n        largest = right\n\n    if largest != i:\n        arr[i], arr[largest] = arr[largest], arr[i]\n        heapify(arr, n, largest)\n\n\ndef heap_sort(arr):\n    n = len(arr)\n    for i in range(n // 2 - 1, -1, -1):\n        heapify(arr, n, i)\n\n    for i in range(n-1, 0, -1):\n        arr[i], arr[0] = arr[0], arr[i]\n        heapify(arr, i, 0)\n    return arr&quot;,
      &quot;function_name&quot;: &quot;heap_sort&quot;,
      &quot;docstring&quot;: &quot;Heap Sort builds a max heap from the list, then repeatedly extracts the largest element (the root of the heap) and rebuilds the heap, thereby sorting the list.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;heap sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def shell_sort(arr):\n    n = len(arr)\n    gap = n // 2\n    while gap &gt; 0:\n        for i in range(gap, n):\n            temp = arr[i]\n            j = i\n            while j &gt;= gap and arr[j - gap] &gt; temp:\n                arr[j] = arr[j - gap]\n                j -= gap\n            arr[j] = temp\n        gap //= 2\n    return arr&quot;,
      &quot;function_name&quot;: &quot;shell_sort&quot;,
      &quot;docstring&quot;: &quot;Shell Sort is an extension of Insertion Sort that allows the exchange of far apart elements. It improves on Insertion Sort by comparing elements distant apart, gradually reducing the gap between elements to be compared.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;shell sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def counting_sort_for_radix(arr, exp):\n    n = len(arr)\n    output = [0] * n\n    count = [0] * 10\n\n    for i in range(n):\n        index = arr[i] // exp\n        count[index % 10] += 1\n\n    for i in range(1, 10):\n        count[i] += count[i - 1]\n\n    i = n - 1\n    while i &gt;= 0:\n        index = arr[i] // exp\n        output[count[index % 10] - 1] = arr[i]\n        count[index % 10] -= 1\n        i -= 1\n\n    for i in range(len(arr)):\n        arr[i] = output[i]\n\n\ndef radix_sort(arr):\n    max_val = max(arr)\n    exp = 1\n    while max_val // exp &gt; 0:\n        counting_sort_for_radix(arr, exp)\n        exp *= 10\n    return arr&quot;,
      &quot;function_name&quot;: &quot;radix_sort&quot;,
      &quot;docstring&quot;: &quot;Radix Sort processes the list digit by digit, starting from the least significant digit to the most significant digit, grouping numbers by each digit's value. It uses Counting Sort as a subroutine to sort based on individual digits.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;radix sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def counting_sort(arr):\n    max_val = max(arr)\n    count = [0] * (max_val + 1)\n    output = [0] * len(arr)\n\n    for num in arr:\n        count[num] += 1\n\n    for i in range(1, len(count)):\n        count[i] += count[i - 1]\n\n    for num in reversed(arr):\n        output[count[num] - 1] = num\n        count[num] -= 1\n\n    return output&quot;,
      &quot;function_name&quot;: &quot;counting_sort&quot;,
      &quot;docstring&quot;: &quot;Counting Sort counts the occurrences of each unique element in the list, and then uses this count information to place each element in its correct position in the output array.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;counting sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    },
    {
      &quot;code&quot;: &quot;def bucket_sort(arr):\n    bucket = []\n    n = len(arr)\n    for i in range(n):\n        bucket.append([])\n\n    for j in arr:\n        index = int(n * j)\n        bucket[index].append(j)\n\n    for i in range(n):\n        bucket[i] = sorted(bucket[i])\n\n    k = 0\n    for i in range(n):\n        for j in range(len(bucket[i])):\n            arr[k] = bucket[i][j]\n            k += 1\n    return arr&quot;,
      &quot;function_name&quot;: &quot;bucket_sort&quot;,
      &quot;docstring&quot;: &quot;Bucket Sort divides the elements into several buckets. Each bucket is then sorted individually, either using a different sorting algorithm or by recursively applying Bucket Sort.&quot;,
      &quot;language&quot;: &quot;python&quot;,
      &quot;tags&quot;: [&quot;sorting&quot;, &quot;algorithm&quot;, &quot;bucket sort&quot;],
      &quot;dataset&quot;: &quot;sorting_algorithms&quot;
    }
  ]
</code></pre>
<p>so it is simple db of sorting algo can someone pls help to change it into input and output form</p>
<p>What should be input and output to pass to model from the db</p>
","machine-learning, nlp",
"hugging face, Numpy is not available","<p>CODE I AM RUNNING:</p>
<pre><code>from transformers import pipeline

classifier = pipeline('sentiment-analysis')

res = classifier(&quot;I Love Python.'&quot;)

print(res)
</code></pre>
<p>ERROR I AM GETTING:</p>
<pre><code>No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).
    Using a pipeline without specifying a model name and revision in production is not recommended.

C:\Users\omran\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\serialization.py:871: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\torch\csrc\utils\tensor_numpy.cpp:68.)
obj = cast(Storage, torch._UntypedStorage(nbytes))
Traceback (most recent call last):
    File &quot;f:\AIAR\yooo\xox.py&quot;, line 5, in &lt;module&gt;
    res = classifier(&quot;I've been waiting for a HuggingFace course my whole life.'&quot;)
    File &quot;C:\Users\omran\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\pipelines\text_classification.py&quot;, line 138, in __call__
    result = super().__call__(*args, **kwargs)
    File &quot;C:\Users\omran\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\pipelines\base.py&quot;, line 1067, in __call__    
    return self.run_single(inputs, preprocess_params, forward_params, 
postprocess_params)
    File &quot;C:\Users\omran\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\pipelines\base.py&quot;, line 1075, in run_single  
    outputs = self.postprocess(model_outputs, **postprocess_params)   
    File &quot;C:\Users\omran\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\pipelines\text_classification.py&quot;, line 183, in postprocess
    outputs = outputs.numpy()
RuntimeError: Numpy is not available
</code></pre>
<p>PIP FREEZE: DON'T MIND, I'VE BEEN DOING A LOT OF TRIAL AND ERROR.</p>
<p>UPDATED OUTPUT:</p>
<pre><code>No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.
2022-08-14 18:45:12.106975: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the 
following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-14 18:45:12.667076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1339 MB memory:  -&gt; device: 0, name: NVIDIA GeForce MX230, pci bus id: 0000:01:00.0, compute capability: 6.1
All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.

All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.
</code></pre>
<p>GETTING THE OUTPUT I WANT:
<code>[{'label': 'POSITIVE', 'score': 0.9973993301391602}]</code></p>
","python, numpy, nlp, huggingface-transformers",
Pytorch: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one,"<p>I am training a multilingual bert model for a sentiment classification task. I have 2 GPUs on 1 Machine so I am using Huggingface <code>Accelerator</code> for distributed training. But when I run the code it throws a Runtime Error.</p>
<h2>Model</h2>
<pre><code>class BERTModel(nn.Module):
    def __init__(self):
        super(BERTModel, self).__init__()
        self.bert = transformers.BertModel.from_pretrained(&quot;bert-base-multilingual-uncased&quot;)
        self.bert_drop = nn.Dropout(0.3)
        self.out = nn.Linear(768 * 2, 1) # *2 since we have 2 pooling layers

    def forward(self, ids, mask, token_type_ids):
        o1, _ = self.bert(
            ids, 
            attention_mask=mask, 
            token_type_ids=token_type_ids
        )
        
        mean_pooling = torch.mean(o1, 1)
        max_pooling, _ = torch.max(o1, 1)
        cat = torch.cat((mean_pooling, max_pooling), 1)
        
        bo = self.bert_drop(cat)
        output = self.out(bo)
        return output
</code></pre>
<h2>Train Function</h2>
<pre><code>def train_fn(data_loader, model, optimizer, scheduler):
    
        
        &quot;&quot;&quot;
        Training Function for the Model
        
        parameters: data_loader - PyTorch DataLoader
                    model - The Model to be used for training
                    optimizer - The Optimizer to be used for training
                    scheduler - The Learning Rate Scheduler 
                    
        returns: None
        &quot;&quot;&quot;
        accelerator = Accelerator()

        model, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)
        model.train()

        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):
            ids = d[&quot;ids&quot;]
            token_type_ids = d[&quot;token_type_ids&quot;]
            mask = d[&quot;mask&quot;]
            targets = d[&quot;targets&quot;]

            
            ids = ids.to(torch.long)
            token_type_ids = token_type_ids.to(torch.long)
            mask = mask.to(torch.long)
            targets = targets.to(torch.float)

            optimizer.zero_grad()
            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)

            loss = loss_fn(outputs, targets)
            
            if bi % 1000 == 0:
                print(f&quot;bi={bi}, loss={loss}&quot;)

            accelerator.backward(loss)
            optimizer.step()
            scheduler.step()
</code></pre>
<h2>Error</h2>
<pre><code>---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
&lt;timed exec&gt; in &lt;module&gt;

/opt/conda/lib/python3.6/site-packages/accelerate/notebook_launcher.py in notebook_launcher(function, args, num_processes, use_fp16, use_port)
    107             try:
    108                 print(f&quot;Launching a training on {num_processes} GPUs.&quot;)
--&gt; 109                 start_processes(launcher, nprocs=num_processes, start_method=&quot;fork&quot;)
    110             finally:
    111                 # Clean up the environment variables set.

/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py in start_processes(fn, args, nprocs, join, daemon, start_method)
    156 
    157     # Loop on join until it returns True or raises an exception.
--&gt; 158     while not context.join():
    159         pass
    160 

/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py in join(self, timeout)
    117         msg = &quot;\n\n-- Process %d terminated with the following error:\n&quot; % error_index
    118         msg += original_trace
--&gt; 119         raise Exception(msg)
    120 
    121 

Exception: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py&quot;, line 20, in _wrap
    fn(i, *args)
  File &quot;/opt/conda/lib/python3.6/site-packages/accelerate/utils.py&quot;, line 274, in __call__
    self.launcher(*args)
  File &quot;&lt;timed exec&gt;&quot;, line 276, in run
  File &quot;&lt;timed exec&gt;&quot;, line 74, in train_fn
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 726, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/cuda/amp/autocast_mode.py&quot;, line 135, in decorate_autocast
    return func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py&quot;, line 585, in forward
    self.reducer.prepare_for_backward([])

RuntimeError: Expected to have finished reduction in the prior iteration before 
starting a new one. This error indicates that your module has parameters that 
were not used in producing loss. You can enable unused parameter detection by (1) 
passing the keyword argument `find_unused_parameters=True` to 
`torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward`
 function outputs participate in calculating loss. If you already have done the 
above two steps, then the distributed data parallel module wasn't able to locate
 the output tensors in the return value of your module's `forward` function. 
Please include the loss function and the structure of the return value of 
`forward` of your module when reporting this issue (e.g. list, dict, iterable).
</code></pre>
","python, python-3.x, nlp, pytorch, huggingface-transformers",
Is merge in Byte Pair encoding (BPE) optimal?,"<p>Let a string be <code>s = &quot;aeeefeekeelaeoae&quot;</code>,</p>
<p><code>Vocab: {a, e, f, k, l, o}</code> (set of characters in s)</p>
<p>Lets say I performed Byte Pair Encoding (BPE) on s and I want number of merges to be 2 such that my final vocab size becomes 8. &quot;ee&quot; gets merged first because it occurs the most. However, how to decide which indices to merge, <code>(s[2] + s[3]) or (s[3] + s[4])</code>?</p>
<p>if we were to merge <code>s[2] + s[3]</code>, then we will get a less compressed string as compared to when we merge <code>s[3] + s[4]</code>. Because after &quot;ee&quot;, we have &quot;ae&quot; of higher occurrences.</p>
<p><strong>case 1</strong> (merge s[2] and s[3]):</p>
<p>let &quot;ee&quot; be Z</p>
<p>s becomes <code>&quot;aZefZkZlaeoae&quot;</code></p>
<p>&quot;ae&quot; occurs the most. Let &quot;ae&quot; be X then s becomes <code>&quot;aZefZkZlXoX&quot;</code></p>
<p><strong>case 2</strong> (merge s[3] and s[4]):</p>
<p>let &quot;ee&quot; be Z</p>
<p>s becomes <code>&quot;aeZfZkZlaeoae&quot;</code></p>
<p><code>&quot;ae&quot;</code> occurs the most. Let &quot;ae&quot; be X then s becomes <code>&quot;XZfZkZlXoX&quot;</code></p>
<p>As you can see after two merges, case 2 gives optimal solution as string length becomes 10 as compared to case 1 where string length becomes 11</p>
","nlp, encode, tokenize, large-language-model, byte-pair-encoding",
Get chatGPT to respond with a single direct answer,"<p>I am querying a text using chatGPT. But I need chatGPT to respond with single direct answers, rather than long stories or irrelevant text. Any way to achieve this?</p>
<p>My code looks like:</p>
<pre><code>from langchain.document_loaders import TextLoader
from langchain.vectorstores import DocArrayInMemorySearch
from langchain.indexes import VectorstoreIndexCreator

loader = TextLoader(&quot;path/to/extracted_text.txt&quot;)
loaded_text = loader.load()
# Save document text as vector.
index = VectorstoreIndexCreator(
            vectorstore_cls=DocArrayInMemorySearch
        ).from_loaders([loader])

# Query the text
response = index.query(&quot;At what time did john come home yesterday?&quot;)
print(&quot;Loaded text is:&quot;, loaded_text)
print(&quot;ChatGPT response is:&quot;, response)
</code></pre>
<blockquote>
<p>&gt;&gt;&gt; Loaded text is: &quot;&lt; a really long text &gt; + John came home last
night at 11:30pm + &lt; a really long text &gt;&quot;</p>
</blockquote>
<blockquote>
<p>&gt;&gt;&gt; ChatGPT response is: &quot;John came back yesterday at 11:30pm.&quot;</p>
</blockquote>
<p>The problem is that I want a concise answer <code>11:30pm</code> rather than a full sentence <code>John came home last night at 11:30pm</code>. Is there a way to achieve this without adding &quot;I need a short direct response&quot; to my query? Can I achieve a more guaranteed concise response by setting a parameter through some other means instead?</p>
","python, nlp, openai-api, langchain, chat-gpt-4","<p>The <em>BEST</em> way to achieve what you want is <strong>Proper Prompt Engineering</strong>.  Period.  No way around it.  It's more of a thought discipline than acquiring a new skill.  Read <a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions"" rel=""nofollow noreferrer"">this quick Microsoft Learn doc</a> about prompt engineering to privy yourself to the knowledge you need to advance your task.  Best wishes!</p>
"
Extracting and Identifying locations with NLP + Spacy,"<p>My goal is to be able to recognize (aka identify) and identify (aka name, retrieve an ID) locations from text using NLP. I'm using Spacy specifically.</p>
<p>There are about 1,000 possible locations, but the difficulty is that they are unlikely to be written in a fully qualified way, not to mention spelling mistakes and aliases. For example, the Mission neighborhood in San Francisco written in a fully-qualified way might be <code>(1) Mission (2) City of San Francisco (3) San Francisco County (4) California (5) US</code>. (The numbers are just to illustrate the separate pieces.) However, many people might write it as <code>(1) Mission (2) City of San Francisco</code>, or <code>(1) Mission</code>, or <code>(1) Mission (2) City of San Francisco (4) California</code>. (Not to mention that #1 might be called &quot;Mission District&quot;, #2 might be called &quot;San Francisco&quot;, #4 might be &quot;CA&quot;, etc.)</p>
<p>So my goal is to be able have an ID for &quot;Mission&quot; and all other neighborhoods, and ID for California and some other states, etc. If the text is <em>like</em> <code>Mission, San Francisco, CA</code> then I get the Mission ID. If the text is <em>like</em> <code>San Francisco, CA</code> then I get the San Francisco ID.</p>
<p>It's also easy to create synthetic training data by creating aliases of the individual location pieces (e.g., (a) &quot;City of San Francisco&quot;, (b) &quot;San Francisco&quot;, (c) &quot;San Francisco City&quot;) and permutations of the &quot;name chain&quot; (e.g, 1 + 2 + 3 + 4 + 5, 1 + 2, 1 + 2 + 5, etc) for each alias. Rough estimate is about 50 combinations of alias and name chain per location, or about O(50,000) total values.</p>
<p>So, extraction seems to be a good job for NER. The surrounding text usually has a bit of context (e.g., &quot;Location: ....&quot; or &quot;Comes from ...&quot;.</p>
<p>However, I'm unsure about the ability to do identification. My understanding is that much of the NER identification (e.g., Spacy's <a href=""https://spacy.io/api/entitylinker"" rel=""nofollow noreferrer"">EntityLinker</a>, which I planned on using) relies on <a href=""https://github.com/explosion/projects/tree/master/nel-emerson/"" rel=""nofollow noreferrer"">surrounding context</a>. I expect that there will be very little surrounding context that would help disambiguate one of the O(1000) locations from others. I also understand that EntityLinker matches on the token is lookup and not statistical (in other words, the value is from disambiguating when you have multiple exact-string matches and not from disambiguating from multiple very-fuzzy matches).</p>
<p>The KnowledgeBase / LookupDB does have a <a href=""https://spacy.io/api/inmemorylookupkb#add_alias"" rel=""nofollow noreferrer"">mechanism for setting aliases</a>, so I could add each permutation as an alias. But at that point I feel like I'm not getting any value out of the EntityLinker's statistical models.</p>
<p>If I have to create a gazetteer for the identification aspect, then maybe it makes sense to put all my effort into the gazetteer and skip the NER?</p>
","nlp, spacy, named-entity-recognition","<p>Thanks to Vimal for the thoughts. As I suspected, and Vishal confirmed, I needed to extract the string first, and then process it with a separate, non-NLP algorithm.</p>
<p>I ended up solving this in two ways, and wanted to document my findings.</p>
<ol>
<li>With some testing I found that an LLM (GPT) was actually pretty effective at determining the &quot;administrative hierarchy&quot; given the extracted string. This prompt, for example:</li>
</ol>
<pre><code>Evaluate the following place identifier and determine the most likely place. List the administrative entities from the lowest-level to the highest-level. Explain your reasoning. &quot;&quot;&quot;'Castro, San Francisco, U.S.&quot;&quot;&quot; 
</code></pre>
<p>returns this (plus some additional explanation):</p>
<pre><code>
The place identifier &quot;Castro, San Francisco, U.S.&quot; likely refers to a specific location within the city of San Francisco in the United States. 
</code></pre>
<p>I can't find the final version of my prompt at the moment, but I was able to tweak it to get it to provide JSON with the administrative entities in order (national, first level, second level, etc), plus I asked for any &quot;geographical feature&quot; as a catch-all. (In some cases, I found that my extracted term was something like &quot;Bay Area, United States&quot;, and GPT was able to sort that out with the right prompting.) There was a little bit of hallucination in my testing, which worried me.</p>
<ol start=""2"">
<li>With all that being said, I ended up on a much lower-tech approach, along the lines of my original gazetteer idea. My original plan was to use the gazetteer idea and then sort out the remaining strings with GPT. I ended up matching like 98% using this approach, and the unmatched strings were pretty objectively wrong and not worth sending to GPT. (Like a city with an incorrect country.)</li>
</ol>
<p>To create the gazetteer:</p>
<p>This approach used a dictionary which I compiled from the wikidata API search API. I did a first-pass and sent all the strings through Wikidata search to get the top 10 matching entities for each search string. E.g.,</p>
<pre><code>https://www.wikidata.org/w/api.php?action=query&amp;list=search&amp;srsearch=castro%20san%20francisco%20california&amp;srwhat=text&amp;srlimit=10&amp;srprop=titlesnippet|categorysnippet&amp;srsort=incoming_links_desc
</code></pre>
<p>I did some pre-filtering to exclude any entity that wasn't an <code>instance of</code> or <code>subclass of</code> geographical features or administrative regions (using lists that I manually searched for and downloaded manually).</p>
<p>Then I ran all those entities through a  method that stored the data (including name, aliases, lat/lng, etc) and walked up through the &quot;administrative entity&quot; and &quot;country&quot; paths to collect the family tree.</p>
<p>To search for place names:</p>
<ol>
<li>Compiled a dictionary where the keys were entity names and aliases.</li>
<li>I took the list of places <code>(castro, san francisco, california)</code> and started with the lowest-level string (castro) and did a fuzzy search against the dictionary keys to look for a match. Anything that matched over, e.g., 85% was chosen as a candidate.</li>
<li>Then I created a list of all the candidate's parent (+grandparent/etc) names and aliases, and I looped through the next place names to try to match every place against a name in the parents list, and got those scores.</li>
<li>Added the scores up. Some other operations to divide by the number of places that I was looking at, bias toward places with fewer parents (so Castro Valley, California would score higher than Castro, San Francisco, California), etc.</li>
</ol>
<p>All in all, this was surprisingly effective.</p>
"
How to get the labels for my LLavaOneVision model?,"<p>I am trying to train a LLavaOneVision model, unfortunately, I am not being able to understand how to process the <strong>labels to put it into the model</strong> for fine tuning the LLavaOneVision( <a href=""https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf"" rel=""nofollow noreferrer"">https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf</a> ). The collate function,the model's forward function and the training step is given below is given below.</p>
<pre><code>def collate_fn(self, batch):
    images = []
    texts = []
    answers = []
    for example in batch:
        question, answer, rgb_image_np = example
        images.append(rgb_image_np)
        answers.append(answer)
        conversation = [
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: [
                    {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: question},  # Add question text
                    {&quot;type&quot;: &quot;image&quot;},                  # Add the image as a type
                ],
            },
            {
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;content&quot;: [
                    {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: answer},   # Add the assistant's answer
                ],
            }
        ]
        text_prompt = self.processor.apply_chat_template(conversation)
        texts.append(text_prompt)

    # Prepare inputs (image + text)
    model_inputs = self.processor(
            images=images,
            text=texts,
            return_tensors=&quot;pt&quot;,
            padding=True
        ).to(torch.float16)

    # Prepare labels
    labels = ???  # How to prepare the labels?

    # Add labels to the batch dictionary
    return {
            &quot;input_ids&quot;: model_inputs[&quot;input_ids&quot;],
            &quot;labels&quot;: labels
        }
</code></pre>
<pre><code>class LlavaOnevisionModule(pl.LightningModule):
    def __init__(self, model_name, processor, learning_rate=2e-5):
        super().__init__()
        self.model_name = model_name
        self.learning_rate = learning_rate
        
        self.processor = processor
        self.model = LlavaOnevisionForConditionalGeneration.from_pretrained(
            model_name, 
            low_cpu_mem_usage=True,
            # use_flash_attention_2=True,

            # torch_dtype=torch.float16
        )

        self.config = self.model.config
        
        self.pad_token_id = (self.processor.tokenizer.eos_token_id 
                             if self.processor.tokenizer.pad_token_id is None 
                             else self.processor.tokenizer.pad_token_id)

    def forward(self, input_ids, labels):
        # Create a dictionary for inputs to match the expected input format of the model
        inputs = {
            'input_ids': input_ids.to(self.device),
            'labels': labels.to(self.device)  # Move labels to the correct device
        }

        # Pass the inputs through the model (which expects input_ids and labels)
        outputs = self.model(**inputs)

        return outputs


    def training_step(self, batch, batch_idx):
        # Unpack the batch
        input_ids = batch['input_ids']
        labels = batch['labels']
        
        # Forward pass
        outputs = self(input_ids=input_ids, labels=labels)
        
        # Calculate loss
        loss = outputs.loss
        
        # Log training loss
        self.log('train_loss', loss)
        return loss
</code></pre>
<p>I tried cloning the input_ids and passing it as the labels. I am not sure if thats the correct way. I have also heard that I might have to do a right shifting of labels, however I believe thats already implemented inside the model's own forward function.  I have also tried using the <code>processor.tokenizer(text=answers,return_tensors=&quot;pt&quot;,padding=True,return_token_type_ids=False)</code>. However this also returns saying something like the input_id length and the label length are not equal.</p>
<p>How do I process the labels then?</p>
","nlp, artificial-intelligence, large-language-model, multimodal, vqa",
SBERT Fine-tuning always stops before finish all epochs,"<p>I'm working on a project using the SBERT pre-trained models (specifically <a href=""https://huggingface.co/nreimers/MiniLM-L6-H384-uncased"" rel=""nofollow noreferrer"">MiniLM</a>) for a text classification project with 995 classifications. I am following the steps laid out <a href=""https://www.sbert.net/docs/sentence_transformer/training_overview.html#why-finetune"" rel=""nofollow noreferrer"">here</a> for the most part and everything seems to run.</p>
<p>My issue occurs when actually training the model. No matter what values I set in the training arguments the training always seems to end early and never completes all the batches. For example, I set <code>num_train_epochs=1</code> but it only gets up to 0.49 epochs. If <code>num_train_epochs=4</code>, it always ends at 3.49 epochs.</p>
<p>Here is my code:</p>
<pre><code>from datasets import load_dataset
from sentence_transformers import (
    SentenceTransformer,
    SentenceTransformerTrainer,
    SentenceTransformerTrainingArguments,
    SentenceTransformerModelCardData,
)
from sentence_transformers.losses import BatchAllTripletLoss
from sentence_transformers.training_args import BatchSamplers
from sentence_transformers.evaluation import TripletEvaluator

model = SentenceTransformer(
    &quot;nreimers/MiniLM-L6-H384-uncased&quot;,
    model_card_data=SentenceTransformerModelCardData(
        language=&quot;en&quot;,
        license=&quot;apache-2.0&quot;,
        model_name=&quot;all-MiniLM-L6-v2&quot;,
    )
)

loss = BatchAllTripletLoss(model)
# Loss overview: https://www.sbert.net/docs/sentence_transformer/loss_overview.html
# This particular loss method: https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#batchalltripletloss


# training args: https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
args = SentenceTransformerTrainingArguments(
    # Required parameter:
    output_dir=&quot;finetune/model20240924&quot;,
    # Optional training parameters:
    num_train_epochs=1,
    max_steps = -1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=1e-5,
    warmup_ratio=0.1,
    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16
    bf16=False,  # Set to True if you have a GPU that supports BF16
    batch_sampler=BatchSamplers.GROUP_BY_LABEL,  # 
    # Optional tracking/debugging parameters:
    eval_strategy=&quot;no&quot;,
    eval_steps=100,
    save_strategy=&quot;epoch&quot;,
   # save_steps=100,
    save_total_limit=2,
    logging_steps=100,
    run_name=&quot;miniLm-triplet&quot;,  # Will be used in W&amp;B if `wandb` is installed
)

trainer = SentenceTransformerTrainer(
    model=model,
    args=args,
    train_dataset=trainDataset,
    eval_dataset=devDataset,
    loss=loss,
    #evaluator=dev_evaluator,
)
trainer.train()
</code></pre>
<p>Note that I am not using an evaluator because we are creating the model and testing it after the fact with a dedicated test set of values. My dataset is structured as:</p>
<pre><code>Dataset({
    features: ['Title', 'Body', 'label'],
    num_rows: 23961
})
</code></pre>
<p>with the <code>dev</code> dataset being the same structure, only with fewer rows. This gives the following output:</p>
<pre><code> [1473/2996 57:06 &lt; 59:07, 0.43 it/s, Epoch 0/1]
Step    Training Loss
100     1.265600
200     0.702700
300     0.633900
400     0.505200
500     0.481900
600     0.306800
700     0.535600
800     0.369800
900     0.265400
1000    0.345300
1100    0.516700
1200    0.372600
1300    0.392300
1400    0.421900

TrainOutput(global_step=1473, training_loss=0.5003972503496366, metrics={'train_runtime': 3427.9198, 'train_samples_per_second': 6.99, 'train_steps_per_second': 0.874, 'total_flos': 0.0, 'train_loss': 0.5003972503496366, 'epoch': 0.4916555407209613})
</code></pre>
<p>As much as I adjust the values I cannot get it to complete all of the batches. How to resolve this issue?</p>
","python, machine-learning, pytorch, nlp, bert-language-model",
What features to extract to cluster text?,"<p>I want to make a classifier for text, which is further use to suggest the most similar text for a one given.</p>
<p>The flow of the app is the following:</p>
<ul>
<li>extract the main 10 topics from the text, using a <code>llm</code> (it can choose from a 150 words pool)</li>
<li>I make the word vector a binary vector, basically working in a 150 dimensional space, where each text have a coordinate like this <code>[1, 0, 1, ..., 0]</code></li>
<li>then I find the closest neighbour (I want to extend to 3-5, but for simplicity, let's assume it is only one) using <code>cosine</code> distance</li>
<li>I receive the closest text</li>
</ul>
<p>The problem is that the texts are pretty different, and the <code>llm</code> gives the topics pretty well, but the suggested texts are not exactly what I was expecting. I tried to order the topics based on importance and make the vector non-binary (<code>[10, 0, 0, 9, ..., 1]</code>), but that didn't seem to help a lot.</p>
<p>I was wondering wheter this approach is not good for my problem, or if I should use other parameters or anything else for grouping my texts.</p>
","machine-learning, nlp, cluster-analysis, text-processing, large-language-model",
What does SentenceTransformers provide to simplify sentence embedding?,"<p>I find <a href=""https://www.sbert.net/"" rel=""nofollow noreferrer"">SentenceTransformers</a> seems to be the preferred open source option for  sentence embedding. But I can't figure out what <strong>sbert</strong> provide exactly to simplify sentence embedding compared to using HuggingFace Transformers(tokenizer &amp; model) directly ?</p>
<p>For all the pre-trained Sentence Transformers models listed on <a href=""https://www.sbert.net/docs/sentence_transformer/pretrained_models.html"" rel=""nofollow noreferrer"">sbert</a>, their HuggingFace pages <strong>all mention these words</strong>, I use <a href=""https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"" rel=""nofollow noreferrer"">all-MiniLM-L6-v2</a> as an example,</p>
<blockquote>
<p>Without sentence-transformers, you can use the model like this: First,
you pass your input through the transformer model, then you have to
apply the right pooling-operation on-top of the contextualized word
embeddings.</p>
</blockquote>
<p>What does &quot;apply the right pooling-operation on-top of the contextualized word embeddings&quot; mean ? What does <code>mean_pooling</code> do ?</p>
<pre><code>#Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

</code></pre>
<p>My second question is what should I do if I want to use a pretrained model that does not list as a pre-trained Sentence Transformers models on sbert? And is this a good idea ?</p>
<p>Take <code>google-bert/bert-base-multilingual-cased</code> as an example, its <a href=""https://huggingface.co/google-bert/bert-base-multilingual-cased?text=This%20sentence%20etc"" rel=""nofollow noreferrer"">huggingface page</a> does not mentions sentence-transformers, so if I want to use it for sentence embedding, can I just &quot;apply the right pooling-operation on-top of the contextualized word embeddings.&quot; as <a href=""https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"" rel=""nofollow noreferrer"">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a> suggest ?</p>
","python, nlp, huggingface-transformers, sentence-transformers",
&#39;SymbolicTensor&#39; object cannot be interpreted as an integer,"<p>I have been trying to implement Peephole LSTM using Tensorflow, and I am getting the error below
<a href=""https://i.sstatic.net/19mX2G53.png"" rel=""nofollow noreferrer"">Error</a>
below is my model and I am not sure why I cant get the input layer in my model summary
<a href=""https://i.sstatic.net/f5jxXei6.png"" rel=""nofollow noreferrer"">Model</a>
and below is my Call method for PeepholeLSTM
<a href=""https://i.sstatic.net/fzcyUxp6.png"" rel=""nofollow noreferrer"">Peephole LSTM class</a></p>
<p>It looks like I cannot pass symbolic Tensorflow object to range method but not sure how to avoid this. Appreciate your help</p>
","python, tensorflow, nlp, lstm, language-model",
New York Times API through the command line,"<p>I'm using the command line to tap into the New York Times API and running into issues. I get this:</p>
<blockquote>
<p>'http' is not recognized as an internal or external command, operable program, or batch file</p>
</blockquote>
<p>when I run:</p>
<pre><code>   http://api.nytimes.com/svc/search/v2/articlesearch.json?[q=obama]&amp;api-key=(my key)'
</code></pre>
<p>Any help would be appreciated.</p>
","command-line, nlp",
SpaCy not recognizing NORP (nationality),"<pre><code>nlp= spacy.load(en_core_web_lg-3.7.1)

name = 'Las Palmas Mexican Restaurant &amp; Bar'

doc = nlp(name)
    for token in doc:
        print(f&quot;{token.text}  \t{token.ent_type_} \t{token.ent_iob_}&quot;)
    print('-----------------')

Las     ORG     B
Palmas      ORG     I
Mexican     ORG     I
Restaurant      ORG     I
&amp;   ORG     I
Bar     ORG     I
-----------------
</code></pre>
<p>So, SpaCy is considering the whole thing as ORGanization. How can I make it understand that &quot;Mexican&quot; should be annotated also as  ORP? (nationality, religious, political). If I make the &quot;Restaurant &amp; Bar&quot; lower-case, then it would. However, I prefer not to have to deal with making a rule about what I should make lower-case.</p>
","nlp, spacy",
The model did not return a loss from the inputs - LabSE error,"<p>I want to fine tune LabSE for Question answering using squad dataset. and i got this error:
<code>ValueError: The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.</code></p>
<p>I am trying to fine tune the model using pytorch. I tried to use smaller batch size and i took just 10% of training dataset because i had problems with memory allocation.
If memory allocation problems are gone this error happens.
To be honest i'm stuck with it. Do you have any hints?</p>
<p>I'm trying to use huggingface tutorial, but i want to use other evaluation (i want to do it myself ) so i skipped using evaluation part of dataset.</p>
<pre><code>from datasets import load_dataset
raw_datasets = load_dataset(&quot;squad&quot;, split='train')


from transformers import BertTokenizerFast, BertModel
from transformers import AutoTokenizer


model_checkpoint = &quot;setu4993/LaBSE&quot;
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = BertModel.from_pretrained(model_checkpoint)



max_length = 384
stride = 128


def preprocess_training_examples(examples):
    questions = [q.strip() for q in examples[&quot;question&quot;]]
    inputs = tokenizer(
        questions,
        examples[&quot;context&quot;],
        max_length=max_length,
        truncation=&quot;only_second&quot;,
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding=&quot;max_length&quot;,
    )

    offset_mapping = inputs.pop(&quot;offset_mapping&quot;)
    sample_map = inputs.pop(&quot;overflow_to_sample_mapping&quot;)
    answers = examples[&quot;answers&quot;]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        sample_idx = sample_map[i]
        answer = answers[sample_idx]
        start_char = answer[&quot;answer_start&quot;][0]
        end_char = answer[&quot;answer_start&quot;][0] + len(answer[&quot;text&quot;][0])
        sequence_ids = inputs.sequence_ids(i)

        # Find the start and end of the context
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # If the answer is not fully inside the context, label is (0, 0)
        if offset[context_start][0] &gt; start_char or offset[context_end][1] &lt; end_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # Otherwise it's the start and end token positions
            idx = context_start
            while idx &lt;= context_end and offset[idx][0] &lt;= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx &gt;= context_start and offset[idx][1] &gt;= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs[&quot;start_positions&quot;] = start_positions
    inputs[&quot;end_positions&quot;] = end_positions
    return inputs


train_dataset = raw_datasets.map(
    preprocess_training_examples,
    batched=True,
    remove_columns=raw_datasets.column_names,
)
len(raw_datasets), len(train_dataset)

from transformers import TrainingArguments

args = TrainingArguments(
    &quot;bert-finetuned-squad&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
)
trainer.train()
</code></pre>
","nlp, pytorch, huggingface-transformers, bert-language-model",
Is there a way to turn Google Colab code into web services or rest APIs,"<p>I have a Machine learning module which uses Google Colab's free GPU for NLP tasks, and I want to make a web app out of it. I've been thinking of using React js for frontend and spring boot for the back end and was wondering whether there is a way to connect the code at Google Colab with the backend.
Want to know other alternative suggestions to building a web app incorporating the ML module in Colab as well. Any sort of help is appriciated. </p>
","python, machine-learning, nlp, google-colaboratory",
TextBlob can not find downloaded corpora when run in an environment,"<p>If I run <code>python -m textblob.download_corpora</code>, it will download the corpora in <code>C:\Users\{my user name}\AppData\Roaming\nltk_data</code> folder and I will not have any issues using TextBlob in the global environment in VS Code.</p>
<p>However, if I go in a Pipenv environment, launch VS Code from there, and try to use TextBlob, it keeps saying <code>LookupError: run python -m textblob.download_corpora</code></p>
<p>So, it seems like the virtual env is the reason. I copied the <code>nltk_data</code> folder containing the corpora that the downloader creates to the virtual env folder. I then tried following <a href=""https://github.com/sloria/TextBlob/issues/111"" rel=""nofollow noreferrer"">this</a> and set the NLTK_DATA to that folder but it made no difference.</p>
<p>How should I make TextBlob run in a virtual environment understand where to look in for the downloaded corpora?</p>
","nlp, nltk, textblob",
ScispaCy in google colab,"<p>I am trying to build  <strong>NER</strong> model of clinical data using <strong>ScispaCy</strong> in <strong>colab</strong>. I have installed packages like this.</p>

<pre><code>!pip install spacy
!pip install scispacy
!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_md-0.2.4.tar.gz       #pip install &lt;Model URL&gt;```
</code></pre>

<p>Then I imported both using</p>

<pre><code>import scispacy
import spacy
import en_core_sci_md
</code></pre>

<p>then used following code to display sentences and entities</p>

<pre><code>nlp = spacy.load(""en_core_sci_md"")
text =""""""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity. They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC)"""""" 
doc = nlp(text)
print(list(doc.sents))
print(doc.ents)
</code></pre>

<p>I am getting the following error</p>

<pre><code>OSError: [E050] Can't find model 'en_core_sci_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
</code></pre>

<p>I don't know why this error is coming, I followed all codes from the official GitHub post of ScispaCy. Any help would be appreciated.
Thanks in advance.</p>
","python, nlp, spacy, named-entity-recognition","<p>I hope I am not too late... I believe you are very close to the correct approach.</p>
<p>I will write my answer in steps and you can choose where to stop.</p>
<p>Step 1)</p>
<pre><code>#Install en_core_sci_lg package from the website of spacy  (large corpus), but you can also use en_core_sci_md for the medium corpus.
       
!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz 
</code></pre>
<p>Step 2)</p>
<pre><code># Import the large dataset
import en_core_sci_lg
</code></pre>
<p>Step 3)</p>
<pre><code># Identify entities
nlp = en_core_sci_lg.load()
doc = nlp(text)
displacy_image = displacy.render(doc, jupyter = True, style = &quot;ent&quot;)
</code></pre>
<p>Step 4)</p>
<pre><code>#Print only the entities
print(doc.ents)
</code></pre>
<p>Step 5)</p>
<pre><code># Save the result 
save_res = [doc.ents]
save_res
</code></pre>
<p>Step 6)</p>
<pre><code>#Save the results to a dataframe
df_save_res = pd.DataFrame(save_res)
df_save_res
</code></pre>
<p>Step 7)</p>
<pre><code># In case that you want to visualise the dependency parse
  displacy_image = displacy.render(doc, jupyter = True, style = &quot;dep&quot;)
</code></pre>
"
Spacy nlp = spacy.load(&quot;en_core_web_lg&quot;),"<p>I already have spaCy downloaded, but everytime I try the <code>nlp = spacy.load(""en_core_web_lg"")</code>, command, I get this error: </p>

<p><code>OSError: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.</code></p>

<p>I already tried </p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; nlp = spacy.load(""en_core_web_sm"")
</code></pre>

<p>and this does not work like it would on my personal computer. </p>

<p>My question is how do I work around this?  What directory specifically do I need to drop the spacy en model into on my computer so that it is found?</p>
","python, python-3.x, nlp, spacy",
Dealing with the example.cpp in CRF++ toolkit,"<p>I am just starting to learn about the use of CRF++ toolkit.
I downloaded the linux version of <a href=""http://crfpp.sourceforge.net/"" rel=""nofollow noreferrer"">CRF++ 0.54</a> ,
When i try to compile the example.cpp under sdk/ with the command
g++ -o example example.cpp
there comes the problem:</p>
<p>hpl@hpl-desktop:~/Documents/CRF/CRF++-0.54$ g++ -o a example.cpp
/tmp/ccmJQgGu.o: In function <code>main': example.cpp:(.text+0x12): undefined reference to </code>CRFPP::createTagger(char const*)'
example.cpp:(.text+0x22): undefined reference to `CRFPP::getTaggerError()'
collect2: ld returned 1 exit status</p>
<p>I would appreciate any suggestions on how to make the program run.</p>
","c++, nlp, crf++",
Seq2Seq trainer.train() keeps giving indexing error,"<p>I am trying to do a machine translation from Hindi to Sanskrit using NLLB model. But I keep getting the error:</p>
<blockquote>
<p>IndexError: Invalid key: 39463 is out of bounds for size 0.</p>
</blockquote>
<ul>
<li>The error is coming when training the pretrained NLLB model `facebook/nllb-200-1.3B</li>
<li>The input data is ~40k Hindi sentences. The same error arises when I tried training with a sample data also.</li>
</ul>
<p>Detailed error message:</p>
<pre><code>Traceback (most recent call last):
  File &quot;nllbtrain.py&quot;, line 273, in &lt;module&gt;
    print(trainer.train())
  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1645, in train
    return inner_training_loop(
  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1907, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 631, in __next__
    data = self._next_data()
  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 49, in fetch
    data = self.dataset.__getitems__(possibly_batched_index)
  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2814, in __getitems__
    batch = self.__getitem__(keys)
  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2810, in __getitem__
    return self._getitem(key)
  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py&quot;, line 2794, in _getitem
    pa_subtable = query_table(self._data, key, indices=self._indices)
  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 583, in query_table
    _check_valid_index_key(key, size)
  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 536, in _check_valid_index_key
    _check_valid_index_key(int(max(key)), size=size)
  File &quot;/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py&quot;, line 526, in _check_valid_index_key
    raise IndexError(f&quot;Invalid key: {key} is out of bounds for size {size}&quot;)
IndexError: Invalid key: 39463 is out of bounds for size 0
  0%|
</code></pre>
<p>The code of the preprocessing done for the data:</p>
<pre><code>def preprocess_function(examples):
        inputs = [example + ' &lt;/s&gt;' + f' &lt;2{s_lang}&gt;' for example in examples[source_lang]]
        targets = [f'&lt;2{t_lang}&gt; ' + example + ' &lt;/s&gt;' for example in examples[target_lang]]

        model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length')
        # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

        with tokenizer.as_target_tokenizer():
            # labels = tokenizer(targets, max_length=max_target_length, truncation=True)
            labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length')

        model_inputs['labels'] = labels['input_ids']

        return model_inputs
</code></pre>
<p>Data after preprocessing:</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 39729
    })
    val: Dataset({
        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2210
    })
    test: Dataset({
        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2214
    })
})
</code></pre>
<p>The code of model params and training:</p>
<pre><code>model_path = 'facebook/nllb-200-1.3B'
model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path)
tokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=&quot;hin_Deva&quot;, tgt_lang=&quot;san_Deva&quot;, max_length = 500)

training_args = Seq2SeqTrainingArguments(
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    output_dir=&quot;./output_dir&quot;,
    weight_decay=0.01,
    save_total_limit=1,
    num_train_epochs=4,
    predict_with_generate=True,
    fp16=False,
    push_to_hub=False,
)
trainer = Seq2SeqTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset['train'],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)
print(trainer.train())

</code></pre>
<p>Any idea why this error is persisting?</p>
","python, nlp, huggingface-transformers, huggingface-trainer","<p><code>size 0</code> indicates that the dataset your trainer gets when the fine-tuning starts is empty. Looking at this (<a href=""https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25</a>) and this (<a href=""https://github.com/huggingface/datasets/issues/6535"" rel=""nofollow noreferrer"">https://github.com/huggingface/datasets/issues/6535</a>) thread suggests adding <code>remove_unused_columns = False</code> to your <code>training_args</code> might resolve the issue, so you could give that a try.</p>
"
Alternative to device_map = &quot;auto&quot; in Huggingface Pretrained,"<p>I have a model that I was reading from huggingface using the following code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)
</code></pre>
<p>Now I read the model and I did some modifications to the internal layers and added more layers. When I started the training/fine-tuning I get that not everything is on the same model.</p>
<p>Now after more investigations, I found that my custom layers aren't distributed on multi GPUs as the original model. So I need something like <code>device_map=&quot;auto&quot;</code> but after reading the model.</p>
<p>So simply something like</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)

model.device_map = &quot;auto&quot;
</code></pre>
","machine-learning, deep-learning, nlp, huggingface-transformers","<p>I found out that there are actually several methods in <code>accelerate</code> for this. The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model:</p>
<p><a href=""https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map"" rel=""nofollow noreferrer"">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map</a></p>
<p>The second one is used to match your model with the devices:</p>
<p><a href=""https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model"" rel=""nofollow noreferrer"">https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model</a></p>
<p>So basically, in your case, you can use the following code:</p>
<pre><code>from accelerate import dispatch_model, infer_auto_device_map

model = AutoModelForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, trust_remote_code=True)

***
...
new_model = CustomModel(model)
...
***

device_map_dict = infer_auto_device_map(new_model)
dispatch_model(new_model, device_map_dict)
</code></pre>
<p>P.S. This code still needs to be tested on fine-tuning.</p>
"
Use Natural Language Processing to to Split Bad &amp; Good Comments from an Employee Survey,"<p>So bit of a long shot here, and I apologize for the lack of information. However, I'm struggling to even know where to look now. </p>

<p>So I'm trying to split good and bad comments from a made-up survey of employees at a random company. All I have is a dataframe consisting of the comment an employee has made along with their managers ID code. The idea is to try and see how many good and/or bad comments are associated with a manager via their ID.</p>

<pre><code>import pandas as pd 
trial_text=pd.read_csv(""trial.csv"")
trial_text.head()

   ManagerCode              Comment
0        AB123  Great place to work
1        AB123  Need more training
2        AB123  Hate working here
3        AB124  Always late home
4        AB124  Manager never listens
</code></pre>

<p>I've used NLTK quite a lot for data sets that include a lot more information so anything NLTK based won't be a problem. Like I say, with what I have, ""Google"" has far too much information that I don't know where to begin (or that is useful)! If there's anyone that might just have a suggestion that could put me on track that would be great!</p>

<p>Thanks</p>
","python, nlp, nltk","<p>You need sentiment analysis. I don't think you will get amazing results with an off-the-shelf model though, because your responses are quite short and quite domain specific. In case you want to try anyway, here is an example of how to use the <code>vader</code> model with <code>nltk</code>:</p>

<pre><code>from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()
sid.polarity_scores('Great place to work')
&gt;&gt;&gt; {'neg': 0.0, 'neu': 0.423, 'pos': 0.577, 'compound': 0.6249}
sid.polarity_scores('Manager never listens')
&gt;&gt;&gt; {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}
</code></pre>

<p>As you can see, your mileage may vary.</p>

<p>If you have lots of responses (thousands), a more viable strategy would be to manually label a sample of e.g. a few tens to a few hundred and to train your own sentiment classifier. Here are some good tutorials of how to do this with either <a href=""https://www.nltk.org/book/ch06.html"" rel=""nofollow noreferrer"">nltk</a> or <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">sklearn</a></p>
"
Why I am getting &quot;Notebook out of memory&quot; error upon notebook submission in Kaggle,"<p>I am participating in a Kaggle competition. Over the past 7-10 days I have been facing a peculiar problem. I am trying to make my submission to the competition, but am getting &quot;Notebook out of memory&quot; error again and again as you can see in the picture.</p>
<p>I have tried a variety of things to get rid of the problem, like keeping bare minimum package installations &amp; imports, deleting variables at various junctures and using garbage collection using gc.collect() and even reducing my training dataset to just 50 records. Nothing is working. I am running a simple, straightforward model using &quot;facebook-wav2vec2largexlsr53&quot;.</p>
<p>Apart from this model as a dataset, I am installing &quot;Jiwer package with Rapidfuzz&quot; (see the bottom pic). When I SAVEALL or just RUN ALL the notebook, the kernel runs perfectly even with 10000 records and I get the result as expected, but the problem is only for submission (on pressing the SUBMIT button either from output or from the right panel of notebook). By the way I was able to make a submission around 2 weeks back.</p>
<p>What am I doing wrong? How do get rid of this problem and make a submission? This has already caused enormous wastage of time and even Kaggle's valuable platform resources. If I am not doing anything wrong and the problem is with Kaggle platform, how do I take it to their attention? Appreciate inputs.</p>
<p><a href=""https://i.sstatic.net/3UfQP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3UfQP.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/RvDvE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RvDvE.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/CfWik.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CfWik.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/RSDWF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RSDWF.png"" alt=""enter image description here"" /></a></p>
","python, nlp, huggingface-transformers, kaggle, large-language-model",
How to automatically determine text quality?,"<p>A lot of Natural Language Processing (NLP) algorithms and libraries have a hard time working with random texts from the web, usually because they are presupposing clean, articulate writing. I can understand why that would be easier than parsing YouTube comments.</p>

<p>My question is: given a random piece of text, is there a process to determine whether that text is well written, and is a good candidate for use in NLP? What is the general name for these algorithm?</p>

<p>I would appreciate links to articles, algorithms or code libraries, but I would settle for good search terms.</p>
",nlp,"<p>'Well written' and 'good for NLP' may go together but don't have to. For a text to be 'good for NLP', it maybe should contain whole sentences with a verb and a dot at the end, and it should perhaps convey some meaning. For a text to be well written it should also be well-structured, cohesive, coherent, correctly substitute nouns for pronouns, etc. What you need depends on your application. </p>

<p>The chances of a sentence to be properly processed by an NLP tool can often be estimated by some simple heuristics: Is it too long (>20 or 30 words, depending on the language)? Too short? Does it contain many weird characters? Does it contain urls or email adresses? Does it have a main verb? Is it just a list of something? To my knowledge, there is no general name for this, nor any particular algorithm for this kind of filtering - it's called 'preprocessing'.</p>

<p>As to a sentence being well-written: some work has been done on automatically evaluating readability, cohesion, and coherence, e.g. the articles by Miltsakaki (<a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8458&amp;rep=rep1&amp;type=pdf"" rel=""noreferrer"">Evaluation of text coherence for electronic essay scoring systems</a> and <a href=""http://www.cs.rochester.edu/u/www/u/tetreaul/acl-bea-proceedings-2008.pdf#page=99"" rel=""noreferrer"">Real-time web text classification and analysis of reading difficulty</a>) or Higgins (<a href=""http://acl.ldc.upenn.edu/N/N04/N04-1024.pdf"" rel=""noreferrer"">Evaluating multiple aspects of coherence in student essays</a>). These approaches are all based on one or the other theory of discourse structure, such as Centering Theory. The articles are rather theory-heavy and assume knowledge of both centering theory as well as machine learning. Nonetheless, some of these techniques have successfully been applied by <a href=""http://www.ets.org"" rel=""noreferrer"">ETS</a> to automatically scoring student's essays and I think this is quite similar to what you are trying to do, or at least, you may be able to adapt a few ideas. </p>

<p>All this being said, I believe that within the next years, NLP will have to develop techniques  to process language which is <em>not</em> well-formed with respect to current standards. There is a massive amount of extremely valuable data out there on the web, consisting of exactly the kinds of text you mentioned: youtube comments, chat messages, twitter and facebook status messages, etc. All of them potentially contain very interesting information. So, who should adapt - the people wrting that way or NLP?</p>
"
Trouble fine tuning Huggingface GPT-2 on Colab -- Assertion error,"<p>I wish to fine tune Huggingface's GPT-2 transformer model on my own text data. I want to do this on a Google Colab notebook. However, I have two problems. The first is that it doesn't seem to work.</p>
<p>I install the various bits and pieces via the Colab:</p>
<pre><code>!git clone https://github.com/huggingface/transformers
%cd transformers
!pip install .
!pip install -r ./examples/requirements.txt
</code></pre>
<p>[Following the example][1], I upload the suggested WikiText sample data to the for training and run the suggested CLI commands in the notebook.</p>
<pre><code>!export TRAIN_FILE=wiki.train.raw
!export TEST_FILE=wiki.test.raw

!python run_lm_finetuning.py \
    --output_dir=output \
    --model_type=gpt2 \
    --model_name_or_path=gpt2 \
    --do_train \
    --train_data_file=$TRAIN_FILE \
    --do_eval \
    --eval_data_file=$TEST_FILE
</code></pre>
<p>This chugs along for a bit, but then I get an assertion error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;run_lm_finetuning.py&quot;, line 790, in &lt;module&gt;
    main()
  File &quot;run_lm_finetuning.py&quot;, line 735, in main
    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)
  File &quot;run_lm_finetuning.py&quot;, line 149, in load_and_cache_examples
    return TextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)
  File &quot;run_lm_finetuning.py&quot;, line 88, in __init__
    assert os.path.isfile(file_path)
AssertionError
</code></pre>
<p>I think this is to do with my training data? Note that both files in the same folder as the lm_finetuning.py script, so I'm not sure what the os.path issue might be.</p>
<pre><code>benchmarks.py     run_generation.py   summarization
contrib       run_glue.py         test_examples.py
distillation      run_lm_finetuning.py    tests_samples
hans          run_multiple_choice.py  utils_multiple_choice.py
mm-imdb       run_ner.py          utils_ner.py
pplm          run_squad.py        wiki.test.raw
README.md     run_tf_glue.py      wiki.test.tokens
requirements.txt  run_tf_ner.py       wiki.train.raw
run_bertology.py  run_xnli.py         wiki.train.tokens
</code></pre>
<p>My second problem is that, even if fine tuning did work, I don't know how to duplicate the results with my own text data. I can't open the WikiText raw files, so I have no idea what format they're in. Are the ordinary plain text? Are they tokenized in some way?
[1]: <a href=""https://huggingface.co/transformers/examples.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/examples.html</a></p>
","python-3.x, nlp, artificial-intelligence, huggingface-transformers",
Generate Mock but realistic data using NLP,"<p>I want to generate realistic test data. It should support customizable Fields, structure of data by specifying field names and types. It should support a wide range of data types, including names, addresses, email addresses, electrical products, household products etc.</p>
<p>I plan to use small language model, because i don't want full sentence, just word(s).Plan to convert language model response into json. I want to implement similar to <a href=""https://www.mockaroo.com/"" rel=""nofollow noreferrer"">https://www.mockaroo.com/</a> by using AI</p>
<p>Can you refer some small language model to achieve it.</p>
",nlp,
How to parse complex patterns for scheduling?,"<p>I implement parser for my university schedule (Innopolis University) from google spreadsheets to .ics files. It looks like this:
<a href=""https://i.sstatic.net/xFC9kyyi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xFC9kyyi.png"" alt=""spreadsheet"" /></a></p>
<p>Header (1st and 2nd rows) is Course and Group. On the left side - column with weekday and time.
Each cell here represent schedule entry (they repeat every week), also each &quot;cell&quot; contains three lines: 1st - name of event, 2nd - teachers, 3rd - location and some modifiers such as start time, change of location on specific day, &quot;this event will be on specified days, not every week&quot;.</p>
<p>I wrote almost everything except the parsing for 3rds strings in event (location string) because it may contain complex patterns, in general, this can contain almost everything that comes to the person who makes the schedule, but I collected a set of what was on the schedule.</p>
<p>Maybe you have ideas on how to implement this? Approximate approaches, using ml, and so on are also suitable. I am attaching a script for testing (may be there is need to reformulate tests) with my solution that fails <code>313 (WEEK 1-3) / ONLINE</code>, <code>ONLINE ON 13/09, 108 ON 01/11 (STARTS AT 9:00)</code> and <code>314 (312 ON 12/09,19/09,26/09) 301 ON 03/10</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import re
from datetime import date, datetime, time
from functools import partial
from unittest import TestCase

from pydantic import BaseModel
import pytest

ydate = partial(date, year=datetime.today().year)


class Item(BaseModel):
    location: str | None = None
    starts_from: date | None = None
    starts_at: time | None = None
    till: time | None = None
    on_weeks: list[int] | None = None
    on: list[date] | None = None
    NEST: list[&quot;Item&quot;] | None = None

    class Config:
        arbitrary_types_allowed = True


Item.update_forward_refs()

cases = [
    # Simple
    (&quot;303&quot;, Item(location=&quot;303&quot;)),
    (&quot;room 107&quot;, Item(location=&quot;107&quot;)),
    (&quot;room #107&quot;, Item(location=&quot;107&quot;)),
    (&quot;ROOM #107&quot;, Item(location=&quot;107&quot;)),
    (&quot;ONLINE&quot;, Item(location=&quot;ONLINE&quot;)),
    (&quot;online&quot;, Item(location=&quot;ONLINE&quot;)),
    (&quot;106/313/314/316/318/320/421&quot;, Item(location=&quot;106/313/314/316/318/320/421&quot;)),
    (&quot;105/ (ONLINE)&quot;, Item(location=&quot;105/ONLINE&quot;)),
    # starts_from modifier
    (&quot;STARTS ON 2/10&quot;, Item(starts_from=ydate(day=2, month=10))),
    (&quot;STARTS FROM 21/09&quot;, Item(starts_from=ydate(day=21, month=9))),
    (&quot;304 Starts from 19/09&quot;, Item(location=&quot;304&quot;, starts_from=ydate(day=19, month=9))),
    (&quot;313 (STARTS FROM 21/09)&quot;, Item(location=&quot;313&quot;, starts_from=ydate(day=21, month=9))),
    # starts_at modifier
    (&quot;STARTS AT 16.10&quot;, Item(starts_at=time(hour=16, minute=10))),
    (&quot;107 STARTS AT 16.10&quot;, Item(location=&quot;107&quot;, starts_at=time(hour=16, minute=10))),
    (&quot;107 (STARTS AT 10.50)&quot;, Item(location=&quot;107&quot;, starts_at=time(hour=10, minute=50))),
    # week modifiers
    (&quot;WEEK 2-4 ONLY&quot;, Item(on_weeks=[2, 3, 4])),
    (&quot;105 (WEEK 2-3 ONLY)&quot;, Item(location=&quot;105&quot;, on_weeks=[2, 3])),
    (&quot;105 (WEEK 2, 4 ONLY)&quot;, Item(location=&quot;105&quot;, on_weeks=[2, 4])),
    (&quot;105 (WEEK 2 ONLY)&quot;, Item(location=&quot;105&quot;, on_weeks=[2])),
    (&quot;105 (WEEK 2)&quot;, Item(location=&quot;105&quot;, on_weeks=[2])),
    # on modifier
    (&quot;ON 13/09&quot;, Item(on=[ydate(day=13, month=9)])),
    (&quot;ONLY ON 13/09&quot;, Item(on=[ydate(day=13, month=9)])),
    (&quot;ONLY ON 13/09, 20/09&quot;, Item(on=[ydate(day=13, month=9), ydate(day=20, month=9)])),
    (&quot;ONLINE ON 13/09&quot;, Item(location=&quot;ONLINE&quot;, on=[ydate(day=13, month=9)])),
    (
        &quot;107 (ONLY ON 8/09, 29/09, 27/10, 17/11)&quot;,
        Item(
            location=&quot;107&quot;,
            on=[ydate(day=8, month=9), ydate(day=29, month=9), ydate(day=27, month=10), ydate(day=17, month=11)],
        ),
    ),
    (
        &quot;107 (ON 8/09, 29/09, 27/10, 17/11)&quot;,
        Item(
            location=&quot;107&quot;,
            on=[ydate(day=8, month=9), ydate(day=29, month=9), ydate(day=27, month=10), ydate(day=17, month=11)],
        ),
    ),
    (
        &quot;ONLINE (only on 31/08 and 14/09)&quot;,
        Item(location=&quot;ONLINE&quot;, on=[ydate(day=31, month=8), ydate(day=14, month=9)]),
    ),
    # till modifier
    (&quot;TILL 18:00&quot;, Item(till=time(hour=18, minute=0))),
    (&quot;107 (TILL 18:00)&quot;, Item(location=&quot;107&quot;, till=time(hour=18, minute=0))),
    # Multiple modifiers
    (
        &quot;STARTS AT 18:00 TILL 21:00&quot;,
        Item(starts_at=time(hour=18, minute=0), till=time(hour=21, minute=0)),
    ),
    (
        &quot;TILL 21:00 STARTS AT 18:00&quot;,
        Item(starts_at=time(hour=18, minute=0), till=time(hour=21, minute=0)),
    ),
    (
        &quot;(STARTS AT 18:00) TILL 21:00&quot;,
        Item(starts_at=time(hour=18, minute=0), till=time(hour=21, minute=0)),
    ),
    (
        &quot;ON 13/09 STARTS AT 18:00&quot;,
        Item(on=[ydate(day=13, month=9)], starts_at=time(hour=18, minute=0)),
    ),
    (
        &quot;ONLINE ON 13/09 STARTS AT 18:00&quot;,
        Item(location=&quot;ONLINE&quot;, on=[ydate(day=13, month=9)], starts_at=time(hour=18, minute=0)),
    ),
    (
        &quot;107 (TILL 21:00) STARTS AT 18:00&quot;,
        Item(location=&quot;107&quot;, starts_at=time(hour=18, minute=0), till=time(hour=21, minute=0)),
    ),
    # NEST
    (&quot;317 (421 ON 11/10)&quot;, Item(location=&quot;317&quot;, NEST=[Item(location=&quot;421&quot;, on=[ydate(day=11, month=10)])])),
    (
        &quot;105 (room #107 on 28/08)&quot;,
        Item(location=&quot;105&quot;, NEST=[Item(location=&quot;107&quot;, on=[ydate(day=28, month=8)])]),
    ),
    (
        &quot;313 (WEEK 1-3) / ONLINE&quot;,
        Item(location=&quot;ONLINE&quot;, NEST=[Item(location=&quot;107&quot;, on_weeks=[0, 1, 2])]),
    ),
    (
        &quot;ONLINE ON 13/09, 108 ON 01/11 (STARTS AT 9:00)&quot;,
        Item(
            starts_at=time(hour=9, minute=0),
            NEST=[
                Item(location=&quot;ONLINE&quot;, on=[ydate(day=13, month=9)]),
                Item(location=&quot;108&quot;, on=[ydate(day=1, month=11)]),
            ],
        ),
    ),
    (
        &quot;314 (312 ON 12/09,19/09,26/09) 301 ON 03/10&quot;,
        Item(
            location=&quot;314&quot;,
            NEST=[
                Item(
                    location=&quot;312&quot;,
                    on=[
                        ydate(day=12, month=9),
                        ydate(day=19, month=9),
                        ydate(day=26, month=9),
                    ],
                ),
                Item(location=&quot;301&quot;, on=[ydate(day=3, month=10)]),
            ],
        ),
    ),
    (
        &quot;107 (STARTS at 18:00) TILL 21:00&quot;,
        Item(location=&quot;107&quot;, starts_at=time(hour=18, minute=0), till=time(hour=21, minute=0)),
    ),
]


def f(x: str, from_parent: bool = False) -&gt; Item | None:
    x = x.upper()
    x = x.replace(&quot;(ONLINE)&quot;, &quot;ONLINE&quot;)
    x = x.strip()
    # replace AND with ,
    x = re.sub(r&quot;\s+AND\s+&quot;, &quot;, &quot;, x)

    def combine_patterns(patterns):
        return r&quot;(&quot; + &quot;|&quot;.join(patterns) + r&quot;)&quot;

    def simple_location(y: str):
        if m := re.fullmatch(r&quot;^(\d+)$&quot;, y):
            return m.group(1)

        if m := re.fullmatch(r&quot;^ROOM\s*#?\s*(\d+)$&quot;, y):
            return m.group(1)

        if m := re.fullmatch(r&quot;^ONLINE$&quot;, y):
            return m.group(0)

        if m := re.fullmatch(r&quot;^((\d|ONLINE)+(?:\s*/\s*(\d|ONLINE)+)+)$&quot;, y):
            locations = m.group(1)
            locations = locations.split(&quot;/&quot;)
            locations = [l.strip() for l in locations]
            return &quot;/&quot;.join(locations)

    _simple_location_pattern = combine_patterns(
        [r&quot;(\d+)&quot;, r&quot;ROOM\s*#?\s*(\d+)&quot;, r&quot;ONLINE&quot;, r&quot;((\d|ONLINE)+(?:\s*/\s*(\d|ONLINE)+)+)&quot;]
    )

    def location_plus_pattern(group_name: str, pattern: str):
        return rf&quot;(?P&lt;location&gt;{_simple_location_pattern}) \(?(?P&lt;{group_name}&gt;{pattern})\)?&quot;

    if as_simple_location := simple_location(y=x):
        return Item(location=as_simple_location)

    _starts_from_pattern = r&quot;STARTS\s*(ON|FROM)\s*(\d{1,2}[\/.]\d{1,2})&quot;

    def starts_from(y: str):
        if m := re.fullmatch(_starts_from_pattern, y):
            _date = m.group(2).replace(&quot;.&quot;, &quot;/&quot;)
            day, month = _date.split(sep=&quot;/&quot;)

            return Item(starts_from=ydate(day=int(day), month=int(month)))

    _starts_at_pattern = r&quot;STARTS\s*(AT)\s*(\d{1,2}[:.]\d{1,2})&quot;

    def starts_at(y: str):
        if m := re.fullmatch(_starts_at_pattern, y):
            _time = m.group(2).replace(&quot;.&quot;, &quot;:&quot;)
            hour, minute = _time.split(sep=&quot;:&quot;)

            return Item(starts_at=time(hour=int(hour), minute=int(minute)))

    _week_pattern = r&quot;WEEK\s*(?P&lt;weeks&gt;\d+(?:-\d+)?(?:,\s*\d+(?:-\d+)?)*)(?:\s+ONLY)?&quot;

    def week(y: str):
        if m := re.fullmatch(_week_pattern, y):
            weeks = m.group(&quot;weeks&quot;)
            weeks = weeks.split(&quot;,&quot;)
            weeks = [w.split(&quot;-&quot;) for w in weeks]
            weeks = [list(range(int(w[0]), int(w[1]) + 1)) if len(w) == 2 else [int(w[0])] for w in weeks]
            weeks = [item for sublist in weeks for item in sublist]
            return Item(on_weeks=weeks)

    _on_pattern = r&quot;(ON|ONLY\s+ON)\s*(?P&lt;dates&gt;(\d{1,2}[\/.]\d{1,2}(?:,\s*\d{1,2}[\/.]\d{1,2})*))&quot;

    def on(y: str):
        if m := re.fullmatch(_on_pattern, y):
            dates = m.group(&quot;dates&quot;)
            dates = dates.split(&quot;,&quot;)
            dates = [d.replace(&quot;.&quot;, &quot;/&quot;) for d in dates]
            dates = [d.split(&quot;/&quot;) for d in dates]
            dates = [ydate(day=int(d[0]), month=int(d[1])) for d in dates]
            return Item(on=dates)

    _till_pattern = r&quot;TILL\s*(?P&lt;time&gt;\d{1,2}[:.]\d{1,2})&quot;

    def till(y: str):
        if m := re.fullmatch(_till_pattern, y):
            _time = m.group(&quot;time&quot;).replace(&quot;.&quot;, &quot;:&quot;)
            hour, minute = _time.split(sep=&quot;:&quot;)
            return Item(till=time(hour=int(hour), minute=int(minute)))

    _any_modifier_pattern = combine_patterns(
        [_starts_from_pattern, _starts_at_pattern, _week_pattern, _on_pattern, _till_pattern]
    )

    def any_modifier(y: str):
        if m := re.fullmatch(_any_modifier_pattern, y):
            z = m.group(0)
            if as_starts_from := starts_from(z):
                return as_starts_from
            if as_starts_at := starts_at(z):
                return as_starts_at
            if as_week := week(z):
                return as_week
            if as_on := on(z):
                return as_on
            if as_till := till(z):
                return as_till

    if as_any_modifier := any_modifier(x):
        return as_any_modifier

    if m := re.fullmatch(location_plus_pattern(&quot;any_modifier&quot;, _any_modifier_pattern), x):
        location = simple_location(m.group(&quot;location&quot;))
        as_any_modifier = any_modifier(m.group(&quot;any_modifier&quot;))
        as_any_modifier.location = location
        return as_any_modifier

    # replace all named groups with non-capturing groups
    _any_modifier_pattern_noname = re.sub(r&quot;\(\?P&lt;[^&gt;]+&gt;&quot;, &quot;(?:&quot;, _any_modifier_pattern)
    _two_modifiers_pattern = (
        rf&quot;\(?(?P&lt;first&gt;{_any_modifier_pattern_noname})\)?\s*\(?(?P&lt;second&gt;{_any_modifier_pattern_noname})\)?&quot;
    )

    def two_modifiers(y: str):
        if m := re.fullmatch(_two_modifiers_pattern, y):
            z1, z2 = m.group(&quot;first&quot;), m.group(&quot;second&quot;)
            as_z1 = any_modifier(z1)
            as_z2 = any_modifier(z2)
            if as_z1 and as_z2:
                combined = as_z1.dict(exclude_none=True) | as_z2.dict(exclude_none=True)
                return Item.parse_obj(combined)

    if as_two_modifiers := two_modifiers(x):
        return as_two_modifiers

    if m := re.fullmatch(location_plus_pattern(&quot;two_modifiers&quot;, _two_modifiers_pattern), x):
        location = simple_location(m.group(&quot;location&quot;))
        as_two_modifiers = two_modifiers(m.group(&quot;two_modifiers&quot;))
        as_two_modifiers.location = location
        return as_two_modifiers

    if from_parent:  # only one nesting level
        return None

    _simple_nest_pattern = rf&quot;(?P&lt;location&gt;{_simple_location_pattern})\s*\(?(?P&lt;rest&gt;.+)\)?&quot;

    def simple_nest(y: str):
        if m := re.fullmatch(_simple_nest_pattern, y):
            location = simple_location(m.group(&quot;location&quot;))
            rest = f(m.group(&quot;rest&quot;), from_parent=True)
            if rest is not None:
                return Item(location=location, NEST=[rest])

    if as_simple_nest := simple_nest(x):
        return as_simple_nest


@pytest.mark.parametrize(&quot;input_, desired&quot;, cases, ids=[x for x, _ in cases])
def test_location_parser(input_: str, desired: Item):
    result = f(input_)
    TestCase().assertDictEqual(result.dict(), desired.dict())
</code></pre>
","python, parsing, nlp, schedule, information-retrieval",
How to identify adjectives or adverbs?,"<p>I am quite novice to NLP....Is there any API or a way in which i could identify verb or adjective or adverbs from a sentence?
I need it in a project?</p>
","nlp, sentiment-analysis, semantic-analysis",
Jupyter Lab kernel dies before starting the trainer.train(),"<p>Working on fine-tuning <em>phi-3.5-mini</em>, and when trying to run the <code>trainer.train()</code> i am getting the following error:</p>
<pre><code>***** Running training *****
  Num examples = 647
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed &amp; accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 60
  Number of trainable parameters = 25,165,824

  `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)

  You are not running the flash-attention implementation, expect numerical differences.
/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
**Error operation not supported at line 383 in file /src/csrc/pythonInterface.cpp**
</code></pre>
<p>mainly this part &quot;Error operation not supported at line 383 in file /src/csrc/pythonInterface.cpp&quot;</p>
<p>the kernel then dies, below are packages version I am using</p>
<pre><code>transformers                      4.44.2
torch                             2.4.1
torchaudio                        2.4.1
torchvision                       0.19.1
accelerate                        0.34.2
peft                              0.12.0
</code></pre>
<p>and conda version is <code>24.3.0</code></p>
<p>trying on google colab it is working same code, but in jupyter lab it is not working</p>
","python, nlp, huggingface-transformers, slm-phi3",
Paraphrasing for Math Word Problems (Changing sentence structure without changing meaning),"<p>I'm working on Khan Academy's <a href=""https://github.com/Khan/khan-exercises"" rel=""nofollow"">exercise framework</a>, and more specifically, word problems.</p>

<p>When doing a word problem exercise, students often get the same word problem, only with numbers and names changed. This is not ideal, as students can quickly learn the pattern and extract relevant data without reading the entire problem. </p>

<p>Are there any ways of changing sentence structure without changing the meaning of the word problem? Any other ideas of how to solve this repetition problem are also welcomed.</p>
",nlp,"<p>When creating the word problem, use some sort of syntax to denote various equivalent phrases, as is done in article spinning sometimes. Then when displaying the word problem, pick randomly between them.</p>

<p>Example syntax:</p>

<pre><code>[Name] {goes to the store and /goes to the market and /}{purchases/buys} [number] {apples/pears/bananas}. He {gives/sells/donates} [number] to [name]. {How many does he have now?/How many does he have left?/How many does he still have?}
</code></pre>

<p>2 example word problems that the syntax above could create:</p>

<pre><code>Bob buys 8 bananas. He sells 5 to Alice. How many does he have left?

Harry goes to the market and purchases 19 pears. He gives 2 to Alex. How many does he have now?
</code></pre>

<p>For even more combinations, you could make it recursive.</p>

<p>Here's an article explaining a similar syntax to what I showed above; <a href=""http://www.efficientarticlemarketing.com/article-spinning-tutorial-spinning-syntax-basics/"" rel=""nofollow"">http://www.efficientarticlemarketing.com/article-spinning-tutorial-spinning-syntax-basics/</a></p>
"
Training sentencebert model with pre generated embeddings returning unhashable list error,"<p>I am trying to create embeddings first before feeding into the model. Why I need to create the embeddings first is because of max_seq_len, I want to make use of long sequences and I think the best way is to generate embeddings, fine-tune on the embeddings but I have ran into issues with it and tried out things not working yet.</p>
<p>I am using the sentence bert model [query,positive,negative] pairs.</p>
<pre><code>model = SentenceTransformer(model_name)
query = qry + ' ' + history + ' ' + tpl.Occ.strip().lower() + ' ' + 
tpl.Pr.strip().lower() + ' '  + tpl.D.strip().lower() + ' '  + tpl.G.strip().lower() + ' '  + tpl.D.strip().lower() + ' '  + tpl.HomeOwner.strip().lower()
        emb_query = np.array(model.encode(sentences=query, normalize_embeddings=True, show_progress_bar=True))
        emb_positive = np.array(model.encode(sentences=positive, normalize_embeddings=True, show_progress_bar=True))
        emb_hneg = np.array(model.encode(sentences=hard_negative, normalize_embeddings=True, show_progress_bar=True))

train_data['anchor'].append(emb_query)
        train_data['positive'].append(emb_positive)
        train_data['negative'].append(emb_hneg)
train_dataset = Dataset.from_dict(train_data)

trainer = SentenceTransformerTrainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        loss=train_loss,
      
        evaluator=dev_evaluator,
    )
    trainer.train()
</code></pre>
<p>Now I get the error that unhashable list. I have tried converting to numpy array and even explored using tuple but by the time I use train_dataset = Dataset.from_dict(train_data), it converts it back to list. [[0.1,0.1], [0.1,0.1]]. If I use raw text, it is fine but I want to maximise my max_seq_len and I don’t have the luxury of using bigger max_se_len beyond 128</p>
","pytorch, nlp, bert-language-model, large-language-model",
How to use &quot;Previous Tag&quot; as a feature?,"<p>I was implementing a CRF based POS tagging model using CRFsuite. I have included the features like checking if the word is capitalized, prefix of the word, suffix of the word, etc. However, these features are word based features.
I want to include POS tags based features in my prediction model.</p>
<p>If I simply define the POS tag feature as follows</p>
<pre><code>features = {
    ...
    'prevpos' : '' if i==0 else sentence[i-1][1],
    ...
}
</code></pre>
<p>I have no doubt that this will work for training my model on the corpus. But, when I will use it to predict POS tags for my test sentence, I am unsure about the behavior of this model. Because I think that the feature to extract the previous tag should not work as we do not know them previously.</p>
<p>Also, I do not know the behavior shown by the line <code>X_test_sentence = sent2features(test_sentence_tagged)</code> which will extract the features of the test sentence. And when I use my training model for predicting, <code>tags = crf.predict_single(X_test_sentence)</code> I have no idea about how the previous tags (say we assign dummy tags to test sentence words) will affect (or will not affect) the prediction.</p>
","nlp, pos-tagger, crf, python-crfsuite",
"ValueError: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters","<p>I wrote a text classification program. When I run the program it crashes with an error as seen in this screenshot:</p>
<p><a href=""https://i.sstatic.net/wDoXv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wDoXv.jpg"" alt=""as seen in this screenshot"" /></a></p>
<blockquote>
<p>ValueError: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.</p>
</blockquote>
<p>Here is my code:</p>
<pre><code>from sklearn.model_selection import train_test_split
from gensim.models.word2vec import Word2Vec
from sklearn.preprocessing import scale
from sklearn.linear_model import SGDClassifier
import nltk, string, json
import numpy as np

def cleanText(corpus):
    reviews = []
    for dd in corpus:
        #for d in dd:
        try:
            words = nltk.word_tokenize(dd['description'])
            words = [w.lower() for w in words]
            reviews.append(words)
            #break
        except:
            pass
    return reviews

with open('C:\\NLP\\bad.json') as fin:
    text = json.load(fin)
    neg_rev = cleanText(text)

with open('C:\\NLP\\good.json') as fin:
    text = json.load(fin)
    pos_rev = cleanText(text)

#1 for positive sentiment, 0 for negative
y = np.concatenate((np.ones(len(pos_rev)), np.zeros(len(neg_rev))))

x_train, x_test, y_train, y_test = train_test_split(np.concatenate((pos_rev, neg_rev)), y, test_size=0.2)
</code></pre>
<p>The data I am using is available here:</p>
<ol>
<li><p><a href=""https://github.com/SilverYar/TransportDataMiner/blob/master/bad.json"" rel=""nofollow noreferrer"">Bad</a>;</p>
</li>
<li><p><a href=""https://github.com/SilverYar/TransportDataMiner/blob/master/good.json"" rel=""nofollow noreferrer"">Good</a></p>
</li>
</ol>
<p>How would I go about fixing this error?</p>
","python, scikit-learn, nlp",
Python fuzzy search and replace,"<p>I need to perfom fuzzy search for sub-string in string and replace that part. For example:</p>

<pre><code>str_a = ""Alabama""
str_b = ""REPLACED""
orig_str = ""Flabama is a state located in the southeastern region of the United States.""
print(fuzzy_replace(str_a, str_b, orig_str)) # fuzzy_replace code should be implemented
# Output: REPLACED is a state located in the southeastern region of the United States.
</code></pre>

<p>The search itself is simple with <a href=""https://github.com/seatgeek/fuzzywuzzy"" rel=""nofollow"">fuzzywuzzy</a> module, but it gives me only ratio of difference between strings. Are there any ways to find a position in original string where sub-string fuzzy matches to?</p>
","python, string, nlp, fuzzy-search","<p>Try this..</p>
<pre><code>from fuzzywuzzy import fuzz

def fuzzy_replace(str_a, str_b, orig_str):
    l = len(str_a.split()) # Length to read orig_str chunk by chunk
    splitted = orig_str.split()
    for i in range(len(splitted)-l+1):
        test = &quot; &quot;.join(splitted[i:i+l])
        if fuzz.ratio(str_a, test) &gt; 75: #Using fuzzwuzzy library to test ratio
            before = &quot; &quot;.join(splitted[:i])
            after = &quot; &quot;.join(splitted[i+1:])
            return before+&quot; &quot;+str_b+&quot; &quot;+after #Output will be sandwich of these three strings

str_a = &quot;Alabama is a&quot;
str_b = &quot;REPLACED&quot;
orig_str = &quot;Flabama is a state located in the southeastern region of the United States.&quot;
print fuzzy_replace(str_a, str_b, orig_str)
</code></pre>
<p>This prints</p>
<pre><code> REPLACED state located in the southeastern region of the United States.
</code></pre>
"
How are the weights of the Mistral models reinitialized in Huggingface?,"<p>From <a href=""https://stackoverflow.com/questions/77499162/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-offic"">How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model?</a> and <a href=""https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4</a> there's different suggestions to reinitialize the model.</p>
<p>When I tried this, it seems to work.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForCausalLM, AutoConfig

m = AutoModelForCausalLM.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;, token=&quot;hf_*****&quot;)

c = AutoConfig.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;)
m2 = AutoModelForCausalLM.from_config(c)

print(m2.model.layers[0].mlp.down_proj.state_dict())

print(m.model.layers[0].mlp.down_proj.state_dict())
</code></pre>
<p>[out]:</p>
<pre><code>OrderedDict([('weight',
              tensor([[ 0.0315, -0.0025, -0.0015,  ..., -0.0022,  0.0168, -0.0296],
                      [-0.0013, -0.0190, -0.0103,  ...,  0.0037,  0.0021, -0.0374],
                      [-0.0378, -0.0230,  0.0031,  ..., -0.0035,  0.0099, -0.0027],
                      ...,
                      [-0.0029,  0.0042, -0.0041,  ..., -0.0003,  0.0396, -0.0012],
                      [-0.0487, -0.0050, -0.0068,  ...,  0.0170,  0.0135, -0.0006],
                      [ 0.0103,  0.0424,  0.0019,  ...,  0.0155,  0.0254,  0.0061]]))])


OrderedDict([('weight',
              tensor([[-0.0027, -0.0004, -0.0007,  ..., -0.0025,  0.0032, -0.0014],
                      [ 0.0012, -0.0047,  0.0026,  ..., -0.0017,  0.0015, -0.0044],
                      [ 0.0056, -0.0084,  0.0027,  ...,  0.0026, -0.0053,  0.0038],
                      ...,
                      [ 0.0052,  0.0017, -0.0019,  ..., -0.0013,  0.0052, -0.0017],
                      [-0.0032,  0.0029, -0.0014,  ...,  0.0003,  0.0006,  0.0023],
                      [-0.0023, -0.0045, -0.0013,  ..., -0.0036,  0.0002, -0.0008]]))])
</code></pre>
<p>How are the layers re-initialized through the <code>from_config</code> function? Is it using <a href=""https://cs230.stanford.edu/section/4/"" rel=""nofollow noreferrer"">Xaiver/He initialization</a> or just random initialization?</p>
","nlp, huggingface-transformers, large-language-model, mistral-7b","<p><a href=""https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralConfig"" rel=""nofollow noreferrer"">MistralConfig</a> has a default parameter <code>initializer_range</code> which is set to 0.02 and described as <code>The standard deviation of the truncated_normal_initializer for initializing all weight matrices</code>, so one can assume they use a truncated normal distribution with a standard deviation of 0.02.</p>
<p>If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 0.02 looks like, it seems like a fit to me:</p>
<pre><code>import numpy as np
from matplotlib import pyplot as plt
from scipy.stats import truncnorm
from transformers import AutoModelForCausalLM, AutoConfig

# histogram of actual weights distribution
c = AutoConfig.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;)
m2 = AutoModelForCausalLM.from_config(c)
weights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel()
plt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights')

# what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like
lower = -0.1
upper = 0.1
mean = 0
std = 0.02
a, b = (lower - mean) / std, (upper - mean) / std
x = np.linspace(lower, upper, 1000)
plt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected')

plt.legend()
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/M67S0rKp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M67S0rKp.png"" alt=""model_weights"" /></a></p>
"
AttributeError: &#39;tuple&#39; object has no attribute &#39;rank&#39; when calling model.fit() in NLP task,"<p>I'm following this tutorial
<a href=""https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-9-neural-networks-with-tfidf-vectors-using-d0b4af6be6d7"" rel=""nofollow noreferrer"">https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-9-neural-networks-with-tfidf-vectors-using-d0b4af6be6d7</a></p>
<p>However, while implementing the ANN based on the TF-IDF features, I'm getting this error
<strong>AttributeError: 'tuple' object has no attribute 'rank'</strong></p>
<p>This is the snippet-</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
tvec1 = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))
tvec1.fit(x_train)


x_train_tfidf = tvec1.transform(x_train)
x_validation_tfidf = tvec1.transform(x_validation).toarray()

seed = 7
np.random.seed(seed)
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.layers import Flatten
#from tensorflow.keras.layers.embeddings import Embedding
from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing import sequence

def batch_generator(X_data, y_data, batch_size):
    samples_per_epoch = X_data.shape[0]
    number_of_batches = samples_per_epoch/batch_size
    counter=0
    index = np.arange(np.shape(y_data)[0])
    while 1:
        index_batch = index[batch_size*counter:batch_size*(counter+1)]
        X_batch = X_data[index_batch,:].toarray()
        y_batch = y_data[y_data.index[index_batch]]
        counter += 1
        yield X_batch,y_batch
        if (counter &gt; number_of_batches):
            counter=0

model = Sequential()
model.add(Dense(64, activation='relu', input_dim=100000))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(batch_generator(x_train_tfidf, y_train, 32), epochs=5, validation_data=(x_validation_tfidf, y_validation),steps_per_epoch=x_train_tfidf.shape[0]/32)
</code></pre>
<p>This is the error-</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_13000\1276649087.py in &lt;module&gt;
      1 model.fit(batch_generator(x_train_tfidf, y_train, 32),
      2                     epochs=5, validation_data=(x_validation_tfidf, y_validation),
----&gt; 3                     steps_per_epoch=x_train_tfidf.shape[0]/32)

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1145           use_multiprocessing=use_multiprocessing,
   1146           model=self,
-&gt; 1147           steps_per_execution=self._steps_per_execution)
   1148 
   1149       # Container that configures and calls `tf.keras.Callback`s.

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in get_data_handler(*args, **kwargs)
   1362   if getattr(kwargs[&quot;model&quot;], &quot;_cluster_coordinator&quot;, None):
   1363     return _ClusterCoordinatorDataHandler(*args, **kwargs)
-&gt; 1364   return DataHandler(*args, **kwargs)
   1365 
   1366 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)
   1164         use_multiprocessing=use_multiprocessing,
   1165         distribution_strategy=ds_context.get_strategy(),
-&gt; 1166         model=model)
   1167 
   1168     strategy = ds_context.get_strategy()

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)
    826       return tensor_shape.TensorShape([None for _ in shape.as_list()])
    827 
--&gt; 828     output_shapes = nest.map_structure(_get_dynamic_shape, peek)
    829     output_types = nest.map_structure(lambda t: t.dtype, peek)
    830 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)
    865 
    866   return pack_sequence_as(
--&gt; 867       structure[0], [func(*x) for x in entries],
    868       expand_composites=expand_composites)
    869 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\util\nest.py in &lt;listcomp&gt;(.0)
    865 
    866   return pack_sequence_as(
--&gt; 867       structure[0], [func(*x) for x in entries],
    868       expand_composites=expand_composites)
    869 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in _get_dynamic_shape(t)
    822       shape = t.shape
    823       # Unknown number of dimensions, `as_list` cannot be called.
--&gt; 824       if shape.rank is None:
    825         return shape
    826       return tensor_shape.TensorShape([None for _ in shape.as_list()])

AttributeError: 'tuple' object has no attribute 'rank'
</code></pre>
","python, keras, neural-network, nlp, tf-idf","<p>This happens when function expects a tensor, but other type of data is passed instead.
For instance, numpy array, like the one below:</p>
<pre><code>y.shape

</code></pre>
<p>out: (2000,)</p>
<pre><code>y.shape.rank
</code></pre>
<p>out: AttributeError: 'tuple' object has no attribute 'rank'</p>
<p>To correct this, data should be converted to tensor.</p>
<pre><code>tf.constant(y).shape.rank
</code></pre>
<p>out: 1</p>
"
LLaMA 3.1 Fine-tuning with QLoRA - CUDA Out of Memory Error,"<p>I'm trying to fine-tune the LLaMA 3.1 8 billion parameters model using the QLoRA technique with the help of the 4-bit bitsandbytes library on a mental health conversations dataset from Hugging Face. However, when I run the code, I'm encountering a <code>torch.cuda.OutOfMemoryError</code>. I've tried using multiple GPUs and also higher GPU memory, but the error persists.</p>
<p>Here's my code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.utils.data import Dataset, DataLoader
from transformers import Trainer, AutoModelForCausalLM, AutoTokenizer
from datasets import Dataset, load_dataset
from peft import get_peft_model, LoraConfig, TaskType
import numpy as np
from transformers import BitsAndBytesConfig, TrainingArguments

# BitsAndBytes configuration which loads in 4-bit
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_storage=torch.bfloat16,
)

# Load model and tokenizer using huggingface
model = AutoModelForCausalLM.from_pretrained(
    &quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
)

tokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;)

# Load mental health counseling dataset 
dataset = load_dataset(&quot;Amod/mental_health_counseling_conversations&quot;)

# Data preprocessing functions 
def generate_prompt(Context, Response):
    return f&quot;&quot;&quot;
    You are supposed to reply to the questions as a professional therapist

    Question: {Context}
    Answer: {Response}
    &quot;&quot;&quot;

def format_for_llama(example):
    prompt = generate_prompt(example['Context'], example['Response'])
    return {
        &quot;text&quot;: prompt.strip()
    }

formatted_dataset = dataset['train'].map(format_for_llama)

tokenizer.pad_token = tokenizer.eos_token

# Collate function for DataLoader
def collate_fn(examples):
    input_ids = torch.stack([example['input_ids'] for example in examples])
    attention_mask = torch.stack([example['attention_mask'] for example in examples])
    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask
    }

train_dataloader = DataLoader(tokenized_dataset, collate_fn=collate_fn, batch_size=10)

# PEFT configuration (adding trainable adapters)
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=[
        &quot;q_proj&quot;,
        &quot;k_proj&quot;,
        &quot;v_proj&quot;,
        &quot;o_proj&quot;,
        &quot;gate_proj&quot;,
        &quot;up_proj&quot;,
        &quot;down_proj&quot;
    ]
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# Training arguments hyperparameters
args = TrainingArguments(
    output_dir=&quot;./models&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=10,
    num_train_epochs=10,
    weight_decay=0.01,
    logging_dir='logs',
    logging_strategy=&quot;epoch&quot;,
    remove_unused_columns=False,
    eval_strategy=&quot;no&quot;,
    load_best_model_at_end=False,
)

# Trainer initialization and training
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_dataset,
    data_collator=collate_fn
)
trainer.train()
</code></pre>
<p>When I run this code, I get the following error:</p>
<pre><code>OutOfMemoryError                          Traceback (most recent call last)
Cell In[42], line 7
      1 trainer = Trainer(
      2     model=model,
      3     args=args,
      4     train_dataset=tokenized_dataset,
      5     data_collator=collate_fn
      6 )
----&gt; 7 trainer.train()

File /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1938, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1936         hf_hub_utils.enable_progress_bars()
   1937 else:
-&gt; 1938     return inner_training_loop(
   1939         args=args,
   1940         resume_from_checkpoint=resume_from_checkpoint,
   1941         trial=trial,
   1942         ignore_keys_for_eval=ignore_keys_for_eval,
   1943     )

File /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2279, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   2276     self.control = self.callback_handler.on_step_begin(args, self.state, self.control)
   2278 with self.accelerator.accumulate(model):
-&gt; 2279     tr_loss_step = self.training_step(model, inputs)
...
   1857 else:
-&gt; 1858     ret = input.softmax(dim, dtype=dtype)
   1859 return ret

OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacty of 47.54 GiB of which 1.05 GiB is free. Process 3704361 has 46.47 GiB memory in use. Of the allocated memory 45.37 GiB is allocated by PyTorch, and 808.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
</code></pre>
<p>I have tried using multiple GPU instances on RunPod, as shown in <a href=""https://i.sstatic.net/2HeI49M6.png"" rel=""nofollow noreferrer"">this screenshot</a>, but it's resulting in the same error.</p>
<p>How to resolve this?</p>
<h2>What I've Tried</h2>
<ol>
<li>Using multiple GPUs</li>
<li>Increasing GPU memory</li>
<li>Adjusting batch size</li>
</ol>
<h2>Environment</h2>
<ul>
<li>Python version: 3.10</li>
<li>PyTorch version: (Please specify)</li>
<li>CUDA version: (Please specify)</li>
<li>GPU: Multiple instances on RunPod</li>
</ul>
","nlp, artificial-intelligence, large-language-model, llama, fine-tuning",
Gensim 3.8.3 BM25,"<p>I am using gensim 3.8.3 and seeing some weird results from the bm25 portion of that package. I realize this is an old version and maybe these results wouldn't hold in the updated gensim version, but I'm in a restricted environment and this is what I have access to. I notice that when creating a BM25 object such as below.</p>
<pre><code>from gensim.summarization import bm25 
from gensim.corpora import Dictionary

test_texts = [&quot;hello how are you&quot;, &quot;hi how are you&quot;, &quot; hello what does the word hello mean&quot;, &quot;why doesn't this work&quot;, &quot;hello&quot;, &quot;hello hello hello hello&quot;]
texts = [doc.split() for doc in test_texts] # you can do preprocessing as removing stopwords
dictionary = Dictionary(texts)
corpus = [dictionary.doc2bow(text, allow_update = True) for text in texts]
bm25_model = bm25.BM25(corpus, k1= 1.75, b = 1)
</code></pre>
<p>That you can have multiple idf values for the same word. Which seems like it goes against what idf means. Can someone help me understand if this is a bug or just some sort of variant of BM25? Or correct me if I'm totally misunderstanding what's going on here.</p>
<pre><code>word = 'hello'
word_id = dictionary.token2id.get(word)
word_frequency = dictionary.dfs[word_id]
idf_value = {k: v for k, v in bm25_model.idf.items() if k[0] == word_id}
idf_value

{(1, 1): 0.5877866649021191,
(1, 2): 1.2992829841302609,
(1, 4): 1.2992829841302609}
</code></pre>
","python, nlp, gensim, information-retrieval",
extracting word embeddings from log probabilities,"<p>I am solving an image captioning related issue and eventually I have extract the embeddings of the tokens. One possible way is to extract the embeddings using the tokens. But I cannot do do that because in my case the whole process needs to be differentiable and the tokens are not differentiable.</p>
<p>what I am currently doing is the following -</p>
<pre><code>embeddings = self.model.encoder_decoder.model.tgt_embed[0].lut
embedded_sentence_pred = torch.matmul(seq_logps, embeddings.weight)
</code></pre>
<p>I am not entirely sure if I am doing it correctly. Can you provide any insight?</p>
","pytorch, nlp, word-embedding",
Chatintents - multi intent extraction,"<p>I want to develop a tool that will take a bunch of conversation texts from call centers (between agents and users) and be able to build a multi-intent hierarchical structure based on the input data fed. I am not sure on where and how to approach this problem. If anyone can shed some light and share some samples or code references that I can further study and extend, that will be greatly helpful. After analyzing all the conversations, I must construct the intent list as below</p>
<p>{
&quot;parent_intent&quot;: &quot;Sales&quot;,
&quot;child_intent&quot;: [
&quot;Add Service Features&quot;,
&quot;subchild_intent&quot;: [......],
......
.....
....</p>
<p>}</p>
","nlp, unsupervised-learning, nlu",
How to use pretrained BERT word embedding vector to finetune (initialize) other networks?,"<p>When I used to do classification work with textcnn, I had experience finetuning textcnn using pretrained word embedding with like Word2Vec and fasttext. And I use this process:</p>
<ol>
<li>Create an embedding layer in textcnn</li>
<li>Load the embedding matrix of the words used this time by Word2Vec or
fasttext</li>
<li>Since the vector value of the embedding layer will change during training, the network is
being finetuning.</li>
</ol>
<p>Recently I also want to try BERT to do this. I thought, 'As there should be few differences to use BERT pretrained embedding to initial other networks' embedding layer and finetuning, it should be easy!' But in fact yesterday I tried all day and still cannot do it.<br>
The fact I found is that, as BERT's embedding is a contextual embedding, especially when extracting the word embeddings, the vector of each word from each sentence will vary, so it seems that there is no way to use that embedding to initialize the embedding layer of another network as usual...<br><br>
Finally, I thought up one method to 'finetuning', as the following steps:</p>
<ol>
<li>First, do not define an embedding layer in textcnn.</li>
<li>Instead of using embedding layer, in the network training part, I
firstly pass sequence tokens to the pretrained BERT model and get
the word embeddings for each sentence.</li>
<li>Put the BERT word embedding from 2. into textcnn and train the
textcnn network.</li>
</ol>
<p>By using this method I was finally able to train, but thinking seriously, I don't think I'm doing a finetuning at all...<br>
Because as you can see, every time when I start a new training loop, the word embedding generated from BERT is always the same vector, so just input these unchanged vectors to the textcnn wouldn't let the textcnn be finetuned at all, right?
<br><br>
<strong>UPDATE:</strong>
I thought up a new method to use the BERT embeddings and 'train' BERT and textcnn together.<br>
Some part of my code is:</p>
<pre class=""lang-py prettyprint-override""><code>    BERTmodel = AutoModel.from_pretrained('bert- 
                base-uncased',output_hidden_states=True).to(device)
    TextCNNmodel = TextCNN(EMBD_DIM, CLASS_NUM, KERNEL_NUM, 
                   KERNEL_SIZES).to(device)
    optimizer = torch.optim.Adam(TextCNNmodel.parameters(), lr=LR)
    loss_func = nn.CrossEntropyLoss()
</code></pre>
<pre class=""lang-py prettyprint-override""><code>  for epoch in range(EPOCH):
    TextCNNmodel.train()
    BERTmodel.train()
    for step, (token_batch, seg_batch, y_batch) in enumerate(train_loader):
        token_batch = token_batch.to(device)
        y_batch = y_batch.to(device)

        BERToutputs = BERTmodel(token_batch)
        # I want to use the second-to-last hidden layer as the embedding, so
        x_batch = BERToutputs[2][-2]

        output = TextCNNmodel(x_batch)
        output = output.squeeze()
        loss = loss_func(output, y_batch)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
</code></pre>
<p>I think by enable BERTmodel.train() and delete torch.no_grad() when get the embedding, the loss gradient could be backward to BERTmodel, too. The training process of TextCNNmodel also went smoothly. <br>
To use this model later, I saved the parameters of both TextCNNmodel and BERTmodel.<br>
Then to experiment whether the BERTmodel was really being trained and changed, in another program I load the BERTModel, and input a sentence to test that whether the BERTModel was really being trained. <br>
However, I found that the output (the embedding) of original 'bert-base-uncased' model and my 'BERTmodel' are the same, which is disappointing...<br>
I really have no idea why the BERTmodel part did not change...</p>
","machine-learning, nlp, pytorch, conv-neural-network, bert-language-model",
How to Dynamically Replace Placeholders in Text Using TensorFlow and SentencePiece,"<p>Description:
I’m working on a sequence-to-sequence model using TensorFlow and SentencePiece to dynamically replace placeholders like {NAMESPACE} in user inputs. My training data includes examples with both the placeholder and actual values. During inference, I want the model to replace {NAMESPACE} with the user-specified value without hardcoding it. How can I ensure the model learns this pattern effectively and handles dynamic replacements during inference?</p>
<p>Input : list pods in namespace abc
Actual Output : oc get pods -n def</p>
<p>Expected output : oc get pods -n abc</p>
<p>I know I can implement NER .. just wanted to know if it can be done through model training
I don’t want to use pre trained models.</p>
","tensorflow, nlp, lstm",
"Simple Transformers ClassificationModel freezes during training, making the terminal unresponsive","<p>I am attempting to train a whole bunch of DistilBERT (Simple Transformers) Classification Models using 10-Fold validation. The process works as intended, but freezes during training (at different points every time) and makes the whole terminal unresponsive - only fixable by fully shutting down wsl and restarting it. There is no error/warning message.</p>
<p>I have a for loop, iterating over all training files in a directory (a separate model being trained on each one). For each file, the following function is called:</p>
<pre><code>def calculate_qwk_for_file(original_data_dir:str, file:str):
    seed = 12345
    file_path = join(original_data_dir, file)
    dataset = read_data(file_path)

    print(file_path)
    num_labels = len(pd.unique(dataset['score']))
    kappas = []
    mccs = []
    accs = []
    kf = KFold(n_splits=10, random_state=seed, shuffle=True)

    for train_index, val_index in kf.split(dataset):
        train_df = dataset.iloc[train_index]
        val_df = dataset.iloc[val_index]

        # Optional model configuration
        model_args = ClassificationArgs(
            num_train_epochs=6, 
            overwrite_output_dir=True, 
            use_multiprocessing=False,
            use_multiprocessing_for_evaluation=False
        )

        model = ClassificationModel(
            'distilbert', 'distilbert-base-multilingual-cased',
             num_labels=num_labels,
             args=model_args
        )
        model.train_model(train_df)

        result, model_outputs, wrong_predictions = model.eval_model(
            val_df, 
            kappa=cohen_kappa_score, 
            acc=accuracy_score
        )
        print(result)
        kappas.append(result['kappa'])
        mccs.append(result['mcc'])
        accs.append(result['acc'])

    with open(f'results/qwk_results/results_{file}', mode = 'w', encoding='utf-8') as f:
        f.write(f&quot;kappa\t{np.mean(kappas)}\n&quot;)
        f.write(f&quot;mcc\t{np.mean(mccs)}\n&quot;)
        f.write(f&quot;acc\t{np.mean(accs)}\n&quot;)
</code></pre>
<p>I have also set the following os environment variable</p>
<pre><code>os.environ[&quot;TOKENIZERS_PARALLELISM&quot;] = &quot;false&quot;
</code></pre>
<p><em>Package versions:</em><br />
torch: <strong>2.4.1</strong><br />
simpletransformers: <strong>0.70.1</strong></p>
<p>CUDA Version: <strong>12.6</strong></p>
<p>I reimplemented the whole approach using the transformers library and ran into the same problem.</p>
","deep-learning, pytorch, nlp, simpletransformers",
Which Deep Learning Algorithm does Spacy uses when we train Custom model?,"<p>When we train custom model, I do see we have dropout and n_iter parameters to tune, but which deep learning algorithm does Spacy Uses to train Custom Models? Also, when Adding new Entity type is it good to create blank or train it on existing model?</p>
","nlp, spacy, named-entity-recognition","<h2>Which learning algorithm does spaCy use?</h2>
<p>spaCy has its own deep learning library called <a href=""https://github.com/explosion/thinc"" rel=""noreferrer"">thinc</a> used under the hood for different NLP models. for most (if not all) tasks, spaCy uses a deep neural network based on CNN with a few tweaks. Specifically for Named Entity Recognition, spacy uses:</p>
<ol>
<li><p>A <strong>transition based approach</strong> borrowed from shift-reduce parsers, which is described in the paper <a href=""https://arxiv.org/pdf/1603.01360.pdf"" rel=""noreferrer"">Neural Architectures for Named Entity Recognition</a> by Lample et al.
Matthew Honnibal describes how spaCy uses this on a <a href=""https://youtu.be/sqDHBH9IjRU?t=975"" rel=""noreferrer"">YouTube video</a>.</p>
</li>
<li><p>A framework that's called <strong>&quot;Embed. Encode. Attend. Predict&quot;</strong> (Starting <a href=""https://youtu.be/sqDHBH9IjRU?t=1254"" rel=""noreferrer"">here</a> on the video), slides <a href=""https://github.com/explosion/talks/blob/master/2018-04-12_Embed-Encode-Attend-Predict.pdf"" rel=""noreferrer"">here</a>.</p>
<ul>
<li><p><strong>Embed</strong>: Words are embedded using a Bloom filter, which means that word hashes are kept as keys in the embedding dictionary, instead of the word itself. This maintains a more compact embeddings dictionary, with words potentially colliding and ending up with the same vector representations.</p>
</li>
<li><p><strong>Encode</strong>: List of words is encoded into a sentence matrix, to take context into account. spaCy uses CNN for encoding.</p>
</li>
<li><p><strong>Attend</strong>: Decide which parts are more informative given a query, and get problem specific representations.</p>
</li>
<li><p><strong>Predict</strong>: spaCy uses a multi layer perceptron for inference.</p>
</li>
</ul>
</li>
</ol>
<p>Advantages of this framework, per Honnibal are:</p>
<ol>
<li>Mostly equivalent to sequence tagging (another task spaCy offers models for)</li>
<li>Shares code with the parser</li>
<li>Easily excludes invalid sequences</li>
<li>Arbitrary features are easily defined</li>
</ol>
<p>For a full overview, Matthew Honnibal describes how the model works in <a href=""https://www.youtube.com/watch?v=sqDHBH9IjRU"" rel=""noreferrer"">this YouTube video</a>. Slides could be found <a href=""https://github.com/explosion/talks/blob/master/2017-11-02_Practical-and-Effective-Neural-NER.pdf"" rel=""noreferrer"">here</a>.</p>
<p><em>Note</em>: This information is based on slides from 2017. The engine might have changed since then.</p>
<h2>When adding a new entity type, should we create a blank model or train an existing one?</h2>
<p>Theoretically, when fine-tuning a spaCy model with new entities, you have to make sure the model doesn't forget representations for previously learned entities. The best thing, if possible, is to train a model from scratch, but that might not be easy or possible due to lack of data or resources.</p>
<p><strong>EDIT Feb 2021</strong>: spaCy version 3 now uses the Transformer architecture as its deep learning model.</p>
"
No such file or directory &#39;nltk_data/corpora/stopwords/English&#39; when using colab,"<p>First of all I am using Google colab for the work and
I have downloaded nltk stopwords for English with following:</p>

<pre><code>nltk.download('stopwords')
</code></pre>

<p>The download was successful</p>

<pre><code>[nltk_data] Downloading package stopwords to /root/nltk_data...
</code></pre>

<p>but when I run <code>stop = stopwords.words('English')</code></p>

<p>I am getting <code>OSError: No such file or directory: '/root/nltk_data/corpora/stopwords/English'</code></p>
","python, nlp, nltk, google-colaboratory","<h1>TL;DR</h1>

<p>The <code>English</code> should be in lowercase =)</p>

<p>See: <a href=""https://colab.research.google.com/drive/1tNt0Ifom-h4OnFBBZpLndYCEPDU598jE"" rel=""noreferrer"">https://colab.research.google.com/drive/1tNt0Ifom-h4OnFBBZpLndYCEPDU598jE</a> </p>

<h1>In Code</h1>

<pre><code># Downloads the data.
import nltk
nltk.download('stopwords')


# Using the stopwords.
from nltk.corpus import stopwords

# Initialize the stopwords
stoplist = stopwords.words('english')
</code></pre>
"
Reading Documents Directly from Llama Index as Files Instead of Specifying a Folder Path,"<p>I'm using llama index and want to directly read documents as files instead of specifying the folder path as described in the official documentation. The current method assumes that llama index users always have a downloaded file in a directory. Here's the code snippet from the documentation:</p>
<pre class=""lang-py prettyprint-override""><code>from llama_index.core import SimpleDirectoryReader

documents = SimpleDirectoryReader(&quot;./data&quot;).load_data()
</code></pre>
<p>How can I modify this to read documents directly without specifying the folder path?</p>
","nlp, openai-api, large-language-model, llama, llama-index",
write_csv=True does not write the csv for the result while using SentenceTransformerEvaluator,"<p>I am trying to train a sentence transformer model with custom data to use it for semantic search application. My data includes a chunk of text and corresponding search string queries. I am using the <a href=""https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#informationretrievalevaluator"" rel=""nofollow noreferrer"">InformationRetrievalEvaluator</a>.</p>
<p>Below is my training script:</p>
<pre><code># Enable logging
logging.basicConfig(format='%(asctime)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    level=logging.INFO,
                    handlers=[LoggingHandler()])


description = [txt for txt in train_df.llm_image_description]
query = [q for q in train_df.llm_search_strs_with]

train_examples = Dataset.from_dict(
    {
    &quot;description&quot;: description,
    &quot;query&quot;: query
    }
)

corpus = load_from_pickle(&quot;data/IR_eval_data/corpus.pkl&quot;)
queries = load_from_pickle(&quot;data/IR_eval_data/queries.pkl&quot;)
qrels = load_from_pickle(&quot;data/IR_eval_data/qrels.pkl&quot;)

evaluator = InformationRetrievalEvaluator(queries=queries, corpus=corpus, relevant_docs=qrels,
                                          name='eval',show_progress_bar=True, write_csv=True,precision_recall_at_k=[1,2,3,4,5],mrr_at_k=[2,3,4,5],accuracy_at_k=[1,2,3,4,5],)

# Initialize a pre-trained model
# model = SentenceTransformer('distilbert-base-nli-mean-tokens') 
model = SentenceTransformer(&quot;multi-qa-mpnet-base-cos-v1&quot;) #https://www.sbert.net/docs/sentence_transformer/pretrained_models.html
                                                                                                                                                
# Define the loss function
train_loss = losses.MultipleNegativesRankingLoss(model=model)

# Define training arguments
training_args = SentenceTransformerTrainingArguments(
    eval_strategy='epoch',
    logging_strategy='epoch',
    save_strategy='epoch',
    save_total_limit=2,
    seed=10,
    load_best_model_at_end=True,
    metric_for_best_model='eval_cosine_recall@2',
    num_train_epochs=num_epochs,
    per_device_train_batch_size=train_batch_size,
    output_dir=model_save_path,
)
#This is the latest function
trainer = SentenceTransformerTrainer(
    model=model,
    args=training_args,
    loss=train_loss,
    train_dataset=train_examples,
    evaluator=evaluator,
)

train_result = trainer.train()
</code></pre>
<p>Below output is being printed at the end of each epoch:</p>
<p><a href=""https://i.sstatic.net/OTqmYk18.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OTqmYk18.png"" alt=""Training evaluation output"" /></a></p>
<p>I have below 2 questions:</p>
<ol>
<li>Even when the <code>write_csv=True</code>, at the end of training I am not able to find any evaluation csv file in my output file path. I am not sure why it is not writing it out. I am also not able to retrieve the above result from the <strong>trainer</strong> or <strong>evaluator</strong> object methods. Can someone please tell me how can I : either get the csv to be written out to output path or   get the dictionary/dataframe from <strong>trainer</strong> or <strong>evaluator</strong> objects?</li>
<li>My result shows validation loss column with no information. How can I enable logging for validation loss? it will be important to track it.</li>
</ol>
<p>I am using below versions:</p>
<pre><code>transformers version: 4.44.2
sentence_transformers version: 3.0.1
PyTorch version: 2.2.0
</code></pre>
","pytorch, nlp, huggingface-transformers, sentence-transformers, huggingface-evaluate",
Slow GPT4All with Python SDK,"<p>I'm trying to run some analysis on thousands of text files, and I would like to use gtp4all (In python) to provide some responses. I've been trying to use the model on a sample text file <a href=""https://home.treasury.gov/system/files/261/FSOC_20240223_Minutes.pdf"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Here's my code:</p>
<pre><code>llama_path = r&quot;C:\Users\User\AppData\Local\nomic.ai\GPT4All\Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf&quot;
llama = GPT4All(llama_path, allow_download=False, n_ctx=16384, device='kompute:NVIDIA GeForce RTX 4070 Laptop GPU', ngl=200)

prompt = &quot;&quot;&quot;

Can describe the role of each of the attendees? Give a dictionary with the job of each attendee in JSON format. 

&quot;&quot;&quot;

with open(file_path, 'r', encoding='utf-8') as file:
    content = file.read()

with llama.chat_session(system_prompt='You are a researcher analyzing board minutes. Your job is to provide scores in JSON format.&lt;|eot_id|&gt;'):
    output = llama.generate(prompt=f&quot;{prompt} The text: {content}&quot;, max_tokens=1024, temp=0)

</code></pre>
<p>Even with running this on my GPU, it still takes between 2-3 minutes to get a response, and the responses are not very good. When I see people using this online or on youtube however, it seems much faster and they get great responses. What am I doing wrong? How can I speed this up? Thanks.</p>
<p>Edit: (The response from the code above)</p>
<pre><code>To provide scores in JSON format, I'll need to analyze the text and identify key points related to each attendee's role.

Here is a dictionary with the job of each attendee:

```json
{
    &quot;Janet L. Yellen&quot;: {
        &quot;Role&quot;: &quot;Secretary of the Treasury and Chairperson of the Financial Stability Oversight Council&quot;,
        &quot;Responsibilities&quot;: [
            &quot;Leading the Financial Stability Oversight Council&quot;
        ]
    }, ...
}

Please let me know if you'd like me to add any other attendees or details!
</code></pre>
<p>How do I only get the JSON and skip the text at the front and back?</p>
","python, nlp, artificial-intelligence, large-language-model, gpt4all",
How to continue training a model from where it left off?,"<p>I would like to know how I can save checkpoints when training a text classification model so that I can continue training from where it left off.</p>
<p>I’m having trouble and don’t know how to configure my code to save the checkpoints with the appropriate files so that I can continue training at the point where it ended previously, such as “trainer_state.json”.</p>
<p>Here is my training code:</p>
<pre><code>def executar_treinamento(self, base_treinada):
    training_args = TrainingArguments(
        output_dir=self.output_dir,
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=8,
        weight_decay=0.01,
        evaluation_strategy=&quot;epoch&quot;,
        save_strategy=&quot;epoch&quot;,
        save_only_model=False,
        load_best_model_at_end=False
    )

    self.trainer = Trainer(
        model=self.model,
        args=training_args,
        compute_metrics=self.calcular_metricas,
        train_dataset=base_treinada['train'],
        eval_dataset=base_treinada['validation'],
        tokenizer=self.tokenizar_textos
    )
    
    self.trainer.train()

def avaliar_modelo(self, base_treinada):
    self.trainer.evaluate(base_treinada['test'])

def salvar_modelo(self, caminho):
    self.model.save_pretrained(caminho)
    self.tokenizer.save_pretrained(caminho)
</code></pre>
<p>I tried using the following parameters:</p>
<pre><code>            save_strategy=&quot;steps&quot;,       # Save checkpoints every X steps
            save_steps=80,              # Customize how often to save (in steps)
</code></pre>
<p>However, even so, checkpoints with files like &quot;trainer_state.json&quot; were not saved.</p>
","machine-learning, nlp, huggingface-transformers, large-language-model",
RASA [3.6.20] NLU model &quot;training successfully&quot; immediately but only returning &#39;null&#39;,"<p>I am trying to upgrade from RASA 2 to RASA 3, and am having a problem I don’t understand. I’m trying to train an NLU model, and when I run the ‘rasa train’ command I get terminal output indicating that my model has trained successfully–but the epoch bar never displays, and training seems to complete immediately. When I open the model using ‘rasa shell’, it returns null with 0 confidence for any input.</p>
<p>Asked at the RASA forums, but haven't gotten any responses. Any ideas what could be causing this issue? Thanks.</p>
<p>I’m running RASA version 3.6.20 with spaCy version 3.7.6 in a python 3.9 virtual environment. Here’s my config file:</p>
<pre><code>version: “3.1” 
recipe: “default.v1” 
language: en 
pipeline:
name: SpacyNLP 
model: en_core_web_sm
name: SpacyTokenizer
name: SpacyFeaturizer
name: SpacyEntityExtractor
name: RegexFeaturizer 
analyzer: char_wb 
min_ngram: 1 
max_ngram: 4
name: DIETClassifier 
epochs: 100 
constrain_similarities: true
name: EntitySynonymMapper 
policies:
name: MemoizationPolicy 
assistant_id: 20240813-193423-felt-modal
</code></pre>
<p>here is the last part of the terminal output from training, note no epoch bar:</p>
<pre><code>2024-08-29 17:45:41 INFO rasa.nlu.utils.spacy_utils - Trying to load SpaCy model with name ‘en_core_web_sm’. 
2024-08-29 17:45:42 INFO rasa.nlu.utils.spacy_utils - Trying to load SpaCy model with name ‘en_core_web_sm’. 
2024-08-29 17:45:43 INFO rasa.engine.training.hooks - Starting to train component ‘RegexFeaturizer’. 
2024-08-29 17:45:43 INFO rasa.engine.training.hooks - Finished training component ‘RegexFeaturizer’. 
2024-08-29 17:45:43 INFO rasa.engine.training.hooks - Starting to train component ‘DIETClassifier’. 
2024-08-29 17:45:43 INFO rasa.engine.training.hooks - Finished training component ‘DIETClassifier’. 
2024-08-29 17:45:43 INFO rasa.engine.training.hooks - Starting to train component ‘EntitySynonymMapper’. 
2024-08-29 17:45:43 INFO rasa.engine.training.hooks - Finished training component ‘EntitySynonymMapper’. 
Your Rasa model is trained and saved at ‘models/nlu-20240829-174541-heartless-bayou.tar.gz’. 
Rasa model training completed successfully.
</code></pre>
<p>Here’s the terminal output from running ‘rasa shell’ with a sample input:</p>
<pre><code>2024-08-29 18:05:26 INFO rasa.core.processor - Loading model models/nlu-20240829-        174541-heartless-bayou.tar.gz… 
2024-08-29 18:05:26 INFO rasa.nlu.utils.spacy_utils - Trying to load SpaCy model with name ‘en_core_web_sm’. 
2024-08-29 18:05:28 INFO rasa.nlu.utils.spacy_utils - Trying to load SpaCy model with name ‘en_core_web_sm’. 
NLU model loaded. Type a message and press enter to parse it. 
Next message: Hello, this is a test message. 
{ “text”: “Hello, this is a test message.”, 
“intent”: { “name”: null, “confidence”: 0.0 }, 
“entities”: , 
“text_tokens”: [ [ 0, 5 ], [ 5, 6 ], [ 7, 11 ], [ 12, 14 ], [ 15, 16 ], [ 17, 21 ], [ 22, 29 ], [ 29, 30 ] ], 
“intent_ranking”:  } 
Next message:
</code></pre>
<p>Additionally, the models produced vary in size considerably. Here are one of the 'null' response only RASA 3 models, another RASA 3 model that trained for 3 hours and only produces very low confidence results, and a RASA 2 model I trained today (in 20 minutes) for size comparison purposes.First number (e.g., 2456, is filesize). The RASA 2 model is almost twice the size of the RASA 3 model that sort of works, and many times larger than the only RASA 3 models I've been able to create since then.</p>
<pre><code>RASA 3, 'null' only: 2456 Aug 30 14:20 nlu-20240830-142056-poky-turret.tar.gz
RASA 3, 3+ hour train:19923109 Aug 13 16:08 nlu-20240730-180020-hot-event.tar.gz
RASA 2: 37456829 Sep 3 18:24 nlu-20240903-182449.tar.gz
</code></pre>
","nlp, named-entity-recognition, rasa, rasa-nlu",
Vector search to get a uniqueness score based on context,"<p>I have a single blog post with a title and description, and I want to compare its uniqueness against multiple blog entries in a CSV file. The CSV contains several blogs, each with a title and meta description.</p>
<p>I am currently using TF-IDF vectorization and cosine similarity to compare the single blog with all the entries in the CSV file. However, this approach only matches based on the exact words and not the context.</p>
","python, machine-learning, vector, nlp, text-processing",
Rewrite sentences from first person to third person and vice versa using python packages,"<p>Are there any packages in python that can rewrite large content from Person's point of view i.e first person to third person and Third person to First person</p>
<blockquote>
<p>I ate apple yesterday
He ate apple yesterday</p>
</blockquote>
<p>I have tried to replace the pronouns with relevant pronouns using spacy Language packages and using if condition. is there a simpler way</p>
","text, url-rewriting, nlp, spacy, grammar",
Are there any potential issues training a T5-small from scratch on a task with very limited vocabulary?,"<p>Suppose you would like to train a sequence-to-sequence model like T5-small <strong>from scratch</strong> on a task where the vocabulary is quite limited compared to the tokenizer of T5 which was trained on much larger vocabulary.</p>
<p>For instance, the data have the following format:</p>
<pre><code>Can you please add A and B?

e.g.
Can you please add 45 and 56?
Can you please add 87 and 34?
</code></pre>
<p><code>A</code> and <code>B</code> are just placeholders for integer numbers.</p>
<p>Instead the tokenizer of T5 was trained to represent a vocabulary of approximately something like 32-50K tokens.</p>
<p>What would be some consideration and issues taken into account since in the data only a few tokens change every time?</p>
<p>Basically only tokens <code>A</code> and <code>B</code> change every time.</p>
<p>Is that still possible?</p>
",nlp,
where can i download a pretrained word2vec map?,"<p>I have been learning about NLP models and came across word embedding, and saw the examples in which it is possible to see relations between words by calculating their dot products and such.</p>

<p>What I am looking for is just a dictionary, mapping words to their representative vectors, so I can play around with it. I know that I can build a model and train it and create my own map but I just want the already trained map as a python variable. </p>
","python, nlp, word2vec, word-embedding","<p>You can try out Google's <a href=""https://code.google.com/archive/p/word2vec/"" rel=""noreferrer"">word2vec</a> model trained with about 100 billion words from various news articles.</p>
<p>An interesting fact about word vectors, <code>w2v(king) - w2v(man) + w2v(woman) ≈ w2v(queen)</code></p>
"
How to add new tokens to an existing Huggingface tokenizer?,"<h1>How to add new tokens to an existing Huggingface AutoTokenizer?</h1>
<p>Canonically, there's this tutorial from Huggingface <a href=""https://huggingface.co/learn/nlp-course/chapter6/2"" rel=""noreferrer"">https://huggingface.co/learn/nlp-course/chapter6/2</a> but it ends on the note of &quot;quirks when using existing tokenizers&quot;. And then it points to the <code>train_new_from_iterator()</code> function in Chapter 7 but I can't seem to find reference to how to use it to extend the tokenizer without re-training it.</p>
<p><strong>I've tried</strong> the solution from <a href=""https://stackoverflow.com/questions/71974438/training-new-autotokenizer-hugging-face"">Training New AutoTokenizer Hugging Face</a> that uses <code>train_new_from_iterator()</code> but that will re-train a tokenizer, but it is not extending it, the solution would replace the existing token indices. <a href=""https://stackoverflow.com/questions/71974438/training-new-autotokenizer-hugging-face"">Training New AutoTokenizer Hugging Face</a></p>
<pre><code>import pandas as pd

def batch_iterator(batch_size=3, size=8):
        df = pd.DataFrame({&quot;note_text&quot;: ['foobar', 'helloworld']})
        for x in range(0, size, batch_size):
            yield df['note_text'].to_list()

old_tokenizer = AutoTokenizer.from_pretrained('roberta-base')
training_corpus = batch_iterator()
new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 32000)

print(len(old_tokenizer))
print(old_tokenizer( ['foobarzz', 'helloworld'] ))
print(new_tokenizer( ['foobarzz', 'hello world'] ))
</code></pre>
<p>[out]:</p>
<pre><code>50265
{'input_ids': [[0, 21466, 22468, 7399, 2], [0, 20030, 1722, 39949, 2]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}
{'input_ids': [[0, 275, 2], [0, 276, 2]], 'attention_mask': [[1, 1, 1], [1, 1, 1]]}
</code></pre>
<p><strong>Note:</strong> The reason why the new tokens starts from 275 and 276 is because there are reserved tokens from ids 0-274.</p>
<p>The expected behavior of <code>new_tokenizer( ['foo bar', 'hello word'] )</code> is to have IDs beyond the tokenizer vocab size (i.e. 50265 for the <code>roberta-base</code> model) and it should look like this:</p>
<pre><code>{'input_ids': [[0, 50265, 2], [0, 50266, 2]], 'attention_mask': [[1, 1, 1], [1, 1, 1]]}
</code></pre>

","python, nlp, huggingface-transformers, huggingface-tokenizers, large-language-model",
How can I use structured_output with Azure OpenAI with the openai Python library?,"<p>I want to use structured output with Azure OpenAI.</p>
<p>I tried the following code, based on the code given in <a href=""https://openai.com/index/introducing-structured-outputs-in-the-api/"" rel=""nofollow noreferrer"">https://openai.com/index/introducing-structured-outputs-in-the-api/</a>:</p>
<pre><code>from pydantic import BaseModel
from openai import AzureOpenAI

class Step(BaseModel):
    explanation: str
    output: str


class MathResponse(BaseModel):
    steps: list[Step]
    final_answer: str


client = AzureOpenAI(api_key='[redacted]',
                     api_version='2024-05-01-preview',
                     azure_endpoint='[redacted]')

completion = client.beta.chat.completions.parse(
    model=&quot;gpt-4omini-2024-07-18-name&quot;,
    messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful math tutor.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;solve 8x + 31 = 2&quot;},
    ],
    response_format=MathResponse,
)

message = completion.choices[0].message
if message.parsed:
    print(message.parsed.steps)
    print(message.parsed.final_answer)
else:
    print(message.refusal)
</code></pre>
<p>I get the error:</p>
<pre><code>openai.BadRequestError: Error code: 400:
{
    &quot;error&quot;: {
        &quot;message&quot;: &quot;Invalid parameter: response_format must be one of json_object, text.&quot;,
        &quot;type&quot;: &quot;invalid_request_error&quot;,
        &quot;param&quot;: &quot;response_format&quot;,
        &quot;code&quot;: &quot;None&quot;
    }
}
</code></pre>
<p>How to fix it?</p>
<p>I ran <code>pip install -U openai</code>: I use <code>openai==1.40.1</code> and Python 3.11.</p>
<hr />
<p>I also tried <a href=""https://cookbook.openai.com/examples/structured_outputs_intro"" rel=""nofollow noreferrer"">https://cookbook.openai.com/examples/structured_outputs_intro</a> using  using Azure+ GPT-4o mini (2024-07-18), it didn't work either, same error message:</p>
<pre><code>from openai import AzureOpenAI

# Replace these variables with your Azure OpenAI endpoint and API key
endpoint = &quot;https://&lt;your-resource-name&gt;.openai.azure.com&quot;
api_key = &quot;&lt;your-api-key&gt;&quot;
deployment_name = &quot;&lt;your-deployment-name&gt;&quot; # Replace with your deployment name
MODEL = deployment_name

# API endpoint for the completion request
api_url = f&quot;{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01&quot;


client = AzureOpenAI(api_key='[redacted]',
                     api_version='2024-07-01-preview',
                     azure_endpoint='https://[redacted].openai.azure.com/')

math_tutor_prompt = '''
    You are a helpful math tutor. You will be provided with a math problem,
    and your goal will be to output a step by step solution, along with a final answer.
    For each step, just provide the output as an equation use the explanation field to detail the reasoning.
'''

def get_math_solution(question):
    response = client.chat.completions.create(
    model=MODEL,
    messages=[
        {
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: math_tutor_prompt
        },
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: question
        }
    ],
    response_format={
        &quot;type&quot;: &quot;json_schema&quot;,
        &quot;json_schema&quot;: {
            &quot;name&quot;: &quot;math_reasoning&quot;,
            &quot;schema&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;steps&quot;: {
                        &quot;type&quot;: &quot;array&quot;,
                        &quot;items&quot;: {
                            &quot;type&quot;: &quot;object&quot;,
                            &quot;properties&quot;: {
                                &quot;explanation&quot;: {&quot;type&quot;: &quot;string&quot;},
                                &quot;output&quot;: {&quot;type&quot;: &quot;string&quot;}
                            },
                            &quot;required&quot;: [&quot;explanation&quot;, &quot;output&quot;],
                            &quot;additionalProperties&quot;: False
                        }
                    },
                    &quot;final_answer&quot;: {&quot;type&quot;: &quot;string&quot;}
                },
                &quot;required&quot;: [&quot;steps&quot;, &quot;final_answer&quot;],
                &quot;additionalProperties&quot;: False
            },
            &quot;strict&quot;: True
        }
    }
    )

    return response.choices[0].message


# Testing with an example question
question = &quot;how can I solve 8x + 7 = -23&quot;

result = get_math_solution(question)

print(result.content)
</code></pre>
","python, nlp, azure-openai, gpt-4","<p>Using <code>gpt-4o-2024-08-06</code>, which finally got deployed today (2024-09-03) on Azure, made it work. Code example from <a href=""https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs?tabs=python-secure"" rel=""nofollow noreferrer"">learn.microsoft.com</a>:</p>
<pre><code>from pydantic import BaseModel
from openai import AzureOpenAI

endpoint = &quot;https://your-azure-openai-endpoint.com&quot;
api_key = &quot;your-azure-openai-key&quot;
deployment_name = 'deployment name' # Replace with your gpt-4o 2024-08-06 deployment name

client = AzureOpenAI(api_key=api_key,
                     api_version='2024-08-01-preview',
                     azure_endpoint=endpoint)

class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

completion = client.beta.chat.completions.parse(
    model=deployment_name, # replace with the model deployment name of your gpt-4o 2024-08-06 deployment
    messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Extract the event information.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Alice and Bob are going to a science fair on Friday.&quot;},
    ],
    response_format=CalendarEvent,
)

event = completion.choices[0].message.parsed

print(event)
print(completion.model_dump_json(indent=2))
</code></pre>
<p>output:</p>
<pre><code>name='Science Fair' date='Friday' participants=['Alice', 'Bob']
{
  &quot;id&quot;: &quot;chatcmpl-A3XDRVolXpjeAAQIGddswI990weid&quot;,
  &quot;choices&quot;: [
    {
      &quot;finish_reason&quot;: &quot;stop&quot;,
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: null,
      &quot;message&quot;: {
        &quot;content&quot;: &quot;{\&quot;name\&quot;:\&quot;Science Fair\&quot;,\&quot;date\&quot;:\&quot;Friday\&quot;,\&quot;participants\&quot;:[\&quot;Alice\&quot;,\&quot;Bob\&quot;]}&quot;,
        &quot;refusal&quot;: null,
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;function_call&quot;: null,
        &quot;tool_calls&quot;: [],
        &quot;parsed&quot;: {
          &quot;name&quot;: &quot;Science Fair&quot;,
          &quot;date&quot;: &quot;Friday&quot;,
          &quot;participants&quot;: [
            &quot;Alice&quot;,
            &quot;Bob&quot;
          ]
        }
      },
      &quot;content_filter_results&quot;: {
        &quot;hate&quot;: {
          &quot;filtered&quot;: false,
          &quot;severity&quot;: &quot;safe&quot;
        },
        &quot;self_harm&quot;: {
          &quot;filtered&quot;: false,
          &quot;severity&quot;: &quot;safe&quot;
        },
        &quot;sexual&quot;: {
          &quot;filtered&quot;: false,
          &quot;severity&quot;: &quot;safe&quot;
        },
        &quot;violence&quot;: {
          &quot;filtered&quot;: false,
          &quot;severity&quot;: &quot;safe&quot;
        }
      }
    }
  ],
  &quot;created&quot;: 1725406029,
  &quot;model&quot;: &quot;gpt-4o-2024-08-06&quot;,
  &quot;object&quot;: &quot;chat.completion&quot;,
  &quot;service_tier&quot;: null,
  &quot;system_fingerprint&quot;: &quot;fp_b2ffeb31ff&quot;,
  &quot;usage&quot;: {
    &quot;completion_tokens&quot;: 17,
    &quot;prompt_tokens&quot;: 32,
    &quot;total_tokens&quot;: 49
  },
  &quot;prompt_filter_results&quot;: [
    {
      &quot;prompt_index&quot;: 0,
      &quot;content_filter_results&quot;: {
        &quot;hate&quot;: {
          &quot;filtered&quot;: false,
          &quot;severity&quot;: &quot;safe&quot;
        },
        &quot;self_harm&quot;: {
          &quot;filtered&quot;: false,
          &quot;severity&quot;: &quot;safe&quot;
        },
        &quot;sexual&quot;: {
          &quot;filtered&quot;: false,
          &quot;severity&quot;: &quot;safe&quot;
        },
        &quot;violence&quot;: {
          &quot;filtered&quot;: false,
          &quot;severity&quot;: &quot;safe&quot;
        }
      }
    }
  ]
}
</code></pre>
<p>Tested with Python 3.11.7 and openai==1.43.0.</p>
"
Improvement on context based search(Now using BM25),"<p>Currently I'm working on improving search result on my project. I'm not familiar with algorithm, so may ask something stupid.</p>
<p>Let's say the project has millions restaurants stored in elasticsearch like this:</p>
<pre><code>{
  &quot;id&quot;, 1,
  &quot;name&quot;, &quot;some name&quot;,
  &quot;address&quot;, &quot;some address&quot;,
  &quot;score&quot;, 5,
  &quot;desc&quot;, &quot;some desc&quot;
}
</code></pre>
<p>Currently we are using elastic search built in tokenizer to tokenize user input and searching using BM25.</p>
<p>e.g. &quot;Best pizza in London&quot;, the user input will be tokenized like &quot;Best pizza&quot;, &quot;London&quot;, etc, and then searching using BM25.</p>
<p>However, it comes the issue that from user's perspective, their expectation with priority is:</p>
<ol>
<li>Best to find an list for &quot;Best pizza in London&quot;</li>
<li>Then followed by &quot;Best pizza-like food in London&quot;</li>
<li>Then followed by &quot;Best food other than pizza in London&quot;
But not &quot;Best pizza in York&quot;.</li>
</ol>
<p>The issue is BM25 relies on frequency and document length, and treat all tokens equally. However, in real world, their importance varies.</p>
<p>It's not only the location, due to various user inputs, it could be the person, the event, etc that matters.</p>
<p>So nowadays, for such a requirement, what could be a better way to solve?</p>
","algorithm, elasticsearch, nlp",
How to predict unlabels text data with predict_generator in keras?,"<p>I am trying to perform prediction on my NPL model using predict_generator, my test data does not have a Y variable. My purpose is to predict labels on future texts. I have tried 'Y_test=None' inside the generator, it gives me a positional error. I have also tried to predict using model.predict(X_test), that also did not work. I am looking for help to resolve this issue. See my code below, any suggestion is wolcome.</p>
<pre><code>train_generator = generator (X_train,Y_train,tokenizer,onehot,label_encoder, n_classes,batch_size)
validation_generator = generator (X_valid,Y_valid,tokenizer,onehot,label_encoder, n_classes,batch_size)
test_generator = generator (X_test,Y_test,tokenizer,onehot,label_encoder, n_classes,batch_size)

model.predict_generator(test_generator)
</code></pre>
","tensorflow, keras, nlp, generator",
How to extract subtitles from Youtube videos in varied languages,"<p>I have used the code below to extract subtitles from YouTube videos, but it only works for videos in English. I have some videos in Spanish, so I would like to know how I can modify the code to extract Spanish subtitles too?</p>
<pre class=""lang-py prettyprint-override""><code>from pytube import YouTube
from youtube_transcript_api import YouTubeTranscriptApi

# Define the video URL or ID of the YouTube video you want to extract text from
video_url = 'https://www.youtube.com/watch?v=xYgoNiSo-kY'

# Download the video using pytube
youtube = YouTube(video_url)
video = youtube.streams.get_highest_resolution()
video.download()

# Get the downloaded video file path
video_path = video.default_filename

# Get the video ID from the URL
video_id = video_url.split('v=')[-1]

# Get the transcript for the specified video ID
transcript = YouTubeTranscriptApi.get_transcript(video_id)

# Extract the text from the transcript
captions_text = ''
for segment in transcript:
    caption = segment['text']
    captions_text += caption + ' '

# Print the extracted text
print(captions_text)
</code></pre>
","python, web-scraping, nlp, youtube, video-streaming","<p>Use - <a href=""https://pypi.org/project/youtube-transcript-api/#list-available-transcripts"" rel=""nofollow noreferrer"">list_transcripts</a> - for get the list of available languages:</p>
<p>Example:</p>

<pre class=""lang-py prettyprint-override""><code>video_id = 'xYgoNiSo-kY'
transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
</code></pre>
<p>Then, loop the <code>transcript_list</code> variable to see the available languages obtained:</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>for x, tr in enumerate(transcript_list):
  print(tr.language_code)
</code></pre>
<p>In this case, the result is:</p>
<blockquote>
<p>es</p>
</blockquote>
<p>Modify your code for loop the languages available on the video and download the generated captions:</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code># Variables for store the downloaded captions:
all_captions = []
caption = None
captions_text = ''

# Loop all languages available for this video and download the generated captions:
for x, tr in enumerate(transcript_list):
  print(&quot;Downloading captions in &quot; + tr.language + &quot;...&quot;)
  transcript_obtained_in_language = transcript_list.find_transcript([tr.language_code]).fetch()
  for segment in transcript_obtained_in_language:
    caption = segment['text']
    captions_text += caption + ' '
  all_captions.append({&quot;language &quot; : tr.language_code + &quot; - &quot; + tr.language, &quot;captions&quot; : captions_text})
  caption = None
  captions_text = ''
  print(&quot;=&quot;*20)
print(&quot;Done&quot;)
</code></pre>
<p>In the <code>all_captions</code> variable, will be stored the captions and the language obtained from the given <code>VIDEO_ID</code>.</p>
"
How to Extract Only Relevant Entities from Product Titles Using Google Cloud NLP API in Python?,"<p>I'm using the Google Cloud Natural Language API in Python to extract brand names and product types from e-commerce product titles. My goal is to filter out unnecessary details like specifications (e.g., CPU type, color, memory size) and only retain key information such as the brand name and the main product name.</p>
<h3>Problem</h3>
<p>I've written a script that preprocesses product titles and then uses the Google NLP API to extract entities. I'm trying to filter the entities based on their types (e.g., <code>ORGANIZATION</code> for brand names and <code>CONSUMER_GOOD</code> for product types) and salience. I want to avoid using a manual <code>ignore_terms</code> list and rely on entity type and salience to filter out irrelevant details.</p>
<h3>My Code</h3>
<p>Here's a simplified version of my code:</p>
<pre><code>python
import os
import re
from google.oauth2 import service_account
from google.cloud import language_v1

# Load credentials from the JSON file
key_path = os.path.join('config', 'steam-insight-433515-f9-37e61ba35f98.json')
credentials = service_account.Credentials.from_service_account_file(key_path)

def preprocess_title(title):
    # Remove common punctuation marks
    title = re.sub(r'[^\w\s]', '', title)
    title = re.sub(r'\s+', ' ', title).strip()
    return title

def extract_entities(text):
    client = language_v1.LanguageServiceClient(credentials=credentials)
    document = language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)
    response = client.analyze_entities(document=document)
    
    entities = []
    for entity in response.entities:
        entity_info = {
            'name': entity.name,
            'type': language_v1.Entity.Type(entity.type_).name,
            'salience': entity.salience,
            'metadata': entity.metadata
        }
        entities.append(entity_info)
    
    return entities

def construct_simplified_title(entities, original_title, brand_name=None, max_words=5):
    keywords = []
    if brand_name and brand_name.strip().lower() != 'generic':
        keywords.append(brand_name.strip())
    
    relevant_entities = []
    
    for entity in entities:
        entity_type = entity['type']
        entity_name = entity['name']
        salience = entity['salience']
        
        # Consider only ORGANIZATION (brand) and CONSUMER_GOOD (product name)
        if entity_type in ['ORGANIZATION', 'CONSUMER_GOOD']:
            relevant_entities.append((entity_name, entity_type, salience))
    
    # Sort entities by salience score in descending order
    relevant_entities.sort(key=lambda x: x[2], reverse=True)
    
    for entity_name, _, _ in relevant_entities:
        if entity_name.lower() not in [k.lower() for k in keywords]:
            keywords.append(entity_name)
        if len(keywords) &gt;= max_words:
            break
    
    return ' '.join(keywords[:max_words])

# Example usage:
test_title_1 = &quot;Apple 2024 MacBook Air (13-inch, Apple M3 chip with 8-core CPU and 10-core GPU, 16GB Unified Memory, 512GB) - Space Gray&quot;
test_brand_1 = &quot;Apple&quot;
preprocessed_title_1 = preprocess_title(test_title_1)
entities_1 = extract_entities(preprocessed_title_1)
print(f&quot;Entities Extracted: {entities_1}&quot;)
simplified_title_1 = construct_simplified_title(entities_1, preprocessed_title_1, brand_name=test_brand_1)
print(f&quot;Simplified Title 1: {simplified_title_1}&quot;)
</code></pre>
<p>ven after attempting to filter by entity types and salience, my output still includes unwanted details:
Simplified Title 1: Apple 8-core CPU MacBook Air</p>
<p>I want the output to look like this: Simplified Title 1: Apple MacBook Air M3</p>
<p>What I’ve Tried</p>
<pre><code>•   Filtering entities by type using ORGANIZATION and CONSUMER_GOOD.
•   Sorting entities by salience to prioritize more relevant entities.
•   Limiting the output to a set number of words.
</code></pre>
","python, google-cloud-platform, nlp, named-entity-recognition",
Keep training pytorch model on new data,"<p>I'm working on a text classification task and have decided to use a PyTorch model for this purpose. The process mainly involves the following steps:</p>
<ol>
<li>Load and process the text.</li>
<li>Use a TF-IDF Vectorizer.</li>
<li>Build the neural network and save the TF-IDF Vectorizer and model to predict new data.</li>
</ol>
<p>However, every day I need to classify new comments and correct any wrong classifications.</p>
<p>Currently, my approach is to add the new comments with the correct classification to the dataset and retrain the entire model. This process is time-consuming, and the new comments can be lost during validation. I would like to create a new dataset with the newly classified texts and continue training over this new data (the new comments are classified manually, so each label is correct).</p>
<p>Using GPT and some online code, i write the desired process, however, im not sure if its working as expected, or im making some silly mistakes that should not happen.</p>
<p>So the mains questions are:</p>
<ol>
<li>How could i check if the propossed way to solve this problem work as i expect?</li>
<li>What can i do with the vectorizer when it face new tokens, can i just do a <code>.fit_transform()</code> or i would loose the original vectorizer?</li>
</ol>
<p>Here its the full training process:</p>
<pre><code>import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.preprocessing import LabelEncoder
import polars as pl
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib

set1 = (
    pl
    .read_csv(
        &quot;set1.txt&quot;,
        separator=&quot;;&quot;,
        has_header=False,
        new_columns=[&quot;text&quot;,&quot;label&quot;]
    )
)

# since the dateset its unbalanced, im going to force to have more balance

fear_df = set1.filter(pl.col(&quot;label&quot;) == &quot;fear&quot;)
joy_df = set1.filter(pl.col(&quot;label&quot;) == &quot;joy&quot;).sample(n=2500)
sadness_df = set1.filter(pl.col(&quot;label&quot;) == &quot;sadness&quot;).sample(n=2500)
anger_df = set1.filter(pl.col(&quot;label&quot;) == &quot;anger&quot;)

train_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])

&quot;&quot;&quot;
The text its already clean, so im going to change the labels to numeric
and then split it on train, test ,val
&quot;&quot;&quot;

label_mapping = {
    &quot;anger&quot;: 0,
    &quot;fear&quot;: 1,
    &quot;joy&quot;: 2,
    &quot;sadness&quot;: 3
}

train_mapped = (
    train_df
    .with_columns(
        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)
    )
   
)

train_set, pre_Test = train_test_split(train_mapped,
                                    test_size=0.4,
                                    random_state=42,
                                    stratify=train_mapped[&quot;label&quot;])

test_set, val_set = train_test_split(pre_Test,
                                    test_size=0.5,
                                    random_state=42,
                                    stratify=pre_Test[&quot;label&quot;]) 

# Vectorize text data using TF-IDF
vectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))

X_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()
X_val_tfidf = vectorizer.transform(val_set['text']).toarray()
X_test_tfidf = vectorizer.transform(test_set['text']).toarray()

y_train = train_set['label']
y_val = val_set['label']
y_test = test_set['label']

class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        return text, label
    
train_dataset = TextDataset(X_train_tfidf, y_train)
val_dataset = TextDataset(X_val_tfidf, y_val)
test_dataset = TextDataset(X_test_tfidf, y_test)

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

class TextClassificationModel(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(TextClassificationModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(32, num_classes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = torch.softmax(self.fc3(x), dim=1)
        return x
    
input_dim = X_train_tfidf.shape[1]
model = TextClassificationModel(input_dim, 4)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adamax(model.parameters())

# Training loop
num_epochs = 17
best_val_acc = 0.0
best_model_path = &quot;modelbest.pth&quot;

for epoch in range(num_epochs):
    model.train()
    for texts, labels in train_loader:
        texts, labels = texts.float(), labels.long()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Validation
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for texts, labels in val_loader:
            texts, labels = texts.float(), labels.long()
            outputs = model(texts)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    val_acc = correct / total
    if val_acc &gt; best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), best_model_path)

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')

# Load the best model
model.load_state_dict(torch.load(best_model_path))

# Load the best model
model.load_state_dict(torch.load(best_model_path))

# Test the model
model.eval()
correct, total = 0, 0
with torch.no_grad():
    for texts, labels in test_loader:
        texts, labels = texts.float(), labels.long()
        outputs = model(texts)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
test_acc = correct / total
print(f'Test Acc: {test_acc:.3f}')


# Save the TF-IDF vectorizer
vectorizer_path = &quot;tfidf_vectorizer.pkl&quot;
joblib.dump(vectorizer, vectorizer_path)

# Save the PyTorch model
model_path = &quot;text_classification_model.pth&quot;
torch.save(model.state_dict(), model_path)

</code></pre>
<p>Proposed code:</p>
<pre><code>import torch
import joblib
import polars as pl
from sklearn.model_selection import train_test_split
from torch import nn
from torch.utils.data import Dataset, DataLoader

# Load the saved TF-IDF vectorizer
vectorizer_path = &quot;tfidf_vectorizer.pkl&quot;
vectorizer = joblib.load(vectorizer_path)

input_dim = len(vectorizer.get_feature_names_out())

class TextClassificationModel(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(TextClassificationModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(32, num_classes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = torch.softmax(self.fc3(x), dim=1)
        return x
    
# Load the saved PyTorch model
model_path = &quot;text_classification_model.pth&quot;
model = TextClassificationModel(input_dim, 4)
model.load_state_dict(torch.load(model_path))

# Map labels to numeric values
label_mapping = {&quot;anger&quot;: 0, &quot;fear&quot;: 1, &quot;joy&quot;: 2, &quot;sadness&quot;: 3}
sentiments = [&quot;fear&quot;,&quot;joy&quot;,&quot;sadness&quot;,&quot;anger&quot;]

new_data = (
    pl
    .read_csv(
        &quot;set2.txt&quot;,
        separator=&quot;;&quot;,
        has_header=False,
        new_columns=[&quot;text&quot;,&quot;label&quot;]
    )
    .filter(pl.col(&quot;label&quot;).is_in(sentiments))
    .with_columns(
        pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16)
    )
    
)
# Vectorize the new text data using the loaded TF-IDF vectorizer
X_new = vectorizer.transform(new_data['text']).toarray()
y_new = new_data['label']

class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        return text, label

batch_size = 10
   
# Create DataLoader for the new training data
new_train_dataset = TextDataset(X_new, y_new)
new_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adamax(model.parameters())

num_epochs = 5
new_best_model_path = &quot;modelbest.pth&quot;
for epoch in range(num_epochs):
    model.train()
    for texts, labels in new_train_loader:
        texts, labels = texts.float(), labels.long()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        torch.save(model.state_dict(), new_best_model_path)
        
print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Save the PyTorch model
new_best_model_path = &quot;new_moedl.pth&quot;
torch.save(model.state_dict(), new_best_model_path)
</code></pre>
<p>The dataset can be found <a href=""https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp"" rel=""nofollow noreferrer"">here</a></p>
","python, scikit-learn, pytorch, nlp, python-polars","<p>use  pre-trained word embeddings like BertForSequenceClassification.  These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning, reducing the impact of unseen words.</p>
<p><strong>Model Training with BERT</strong></p>
<pre><code>import torch
from torch import nn, optim
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertModel, BertForSequenceClassification
from transformers import Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
import polars as pl

# Load and prepare data
set1 = pl.read_csv(&quot;set1.txt&quot;, separator=&quot;;&quot;, has_header=False, new_columns=[&quot;text&quot;, &quot;label&quot;])

# Balance dataset
fear_df = set1.filter(pl.col(&quot;label&quot;) == &quot;fear&quot;)
joy_df = set1.filter(pl.col(&quot;label&quot;) == &quot;joy&quot;).sample(n=2500)
sadness_df = set1.filter(pl.col(&quot;label&quot;) == &quot;sadness&quot;).sample(n=2500)
anger_df = set1.filter(pl.col(&quot;label&quot;) == &quot;anger&quot;)
train_df = pl.concat([fear_df, joy_df, sadness_df, anger_df])

label_mapping = {&quot;anger&quot;: 0, &quot;fear&quot;: 1, &quot;joy&quot;: 2, &quot;sadness&quot;: 3}
train_df = train_df.with_columns(pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16))

# Split dataset
train_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[&quot;label&quot;])
test_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[&quot;label&quot;])

# Dataset class
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Initialize tokenizer and datasets
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer)
val_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer)
test_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer)

# Initialize BERT model for classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    logging_dir='./logs',
    learning_rate=2e-5,
    load_best_model_at_end=True
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Train model
trainer.train()

# Evaluate model
results = trainer.evaluate(test_dataset)
print(f&quot;Test Accuracy: {results['eval_accuracy']:.4f}&quot;)

# Save the model and tokenizer
model.save_pretrained(&quot;saved_model&quot;)
tokenizer.save_pretrained(&quot;saved_tokenizer&quot;)
</code></pre>
<p><strong>Incremental training with least effort</strong></p>
<pre><code># Load the saved model and tokenizer
model = BertForSequenceClassification.from_pretrained(&quot;saved_model&quot;)
tokenizer = BertTokenizer.from_pretrained(&quot;saved_tokenizer&quot;)

# Load new data
new_data = (
    pl.read_csv(&quot;set2.txt&quot;, separator=&quot;;&quot;, has_header=False, new_columns=[&quot;text&quot;, &quot;label&quot;])
    .filter(pl.col(&quot;label&quot;).is_in([&quot;fear&quot;, &quot;joy&quot;, &quot;sadness&quot;, &quot;anger&quot;]))
    .with_columns(pl.col(&quot;label&quot;).replace_strict(label_mapping, default=&quot;other&quot;).cast(pl.Int16))
)

# Create new dataset
new_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer)

# Update training arguments for incremental training
new_training_args = TrainingArguments(
    output_dir='./results_incremental',
    num_train_epochs=2,  # Fewer epochs since it's incremental
    per_device_train_batch_size=16,
    evaluation_strategy='epoch',
    logging_dir='./logs_incremental',
    learning_rate=2e-5,
    load_best_model_at_end=True
)

# Define new trainer
new_trainer = Trainer(
    model=model,
    args=new_training_args,
    train_dataset=new_dataset,
    eval_dataset=val_dataset  # Validate on previous validation set
)

# Train on new data
new_trainer.train()

# Evaluate after retraining
new_results = new_trainer.evaluate(test_dataset)
print(f&quot;Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}&quot;)

# Save the updated model
model.save_pretrained(&quot;saved_model_incremental&quot;)
</code></pre>
"
Capitalized words in sentiment analysis,"<p>I'm currently working with data of customers reviews on products from Sephora. my task to classify them to sentiments : negative, neutral , positive .
A common technique of text preprocessing is to lower case all the words , but in this situation upper case words like 'AMAZING' can hide significant emotion behind them and turning all the word to lower case can cause information loss. would be happy for your opinion in the subject should i still lower case all the words? i personally think about creating more classes and  distinction between sentiments as good , very good than just positive to include the importance of this upper case words .</p>
<p>this is my current code :</p>
<pre><code>from itertools import chain

def is_upper_case(text):
  return [word for word in text.split() if word.isupper() and word != 'I']

unique_upper_words = set(chain.from_iterable(all_reviews['review_text'].apply(is_upper_case)))
print(unique_upper_words)
</code></pre>
","nlp, sentiment-analysis, bert-language-model, data-preprocessing","<p>If you are using a BERT-based model (or any other LLM) to do the actual classification I would recommend to not use any preprocessing at all (at least when it comes to capitalization), as these models were pre-trained on non-preprocessed data.</p>
<p>If you want to then do any kind of analysis on the resulting labeled sentences you could lowercase everything to group n-grams and to simplify the analysis.</p>
<p>If you are thinking about having multiple classes to have a better distinction between the prediction, I think it would make most sense if you switch to a sentiment regression instead of a classification, where you predict a value in a continuous range. This comes somewhat natural to the fine-tuning of language models as in a normal classification you would take a continuous output from the model and map it to categorical classes using something like softmax, so for your needs you can just skip that last step and directly use the model output. Many python ML frameworks for fine-tuning or using language models have their own classes for regression tasks, check out <a href=""https://github.com/EliasK93/transformer-model-comparison-for-review-sentiment-regression"" rel=""nofollow noreferrer"">this repository</a> as an example.</p>
"
What is &quot;language modeling head&quot; in BertForMaskedLM,"<p>I have recently read about BERT and want to use BertForMaskedLM for fill_mask task. I know about BERT architecture. Also, as far as I know, BertForMaskedLM is built from BERT with a language modeling head on top, but I have no idea about what <em>language modeling head</em> means here. Can anyone give me a brief explanation.</p>
","nlp, bert-language-model, huggingface-transformers, language-model","<p>The BertForMaskedLM, as you have understood correctly uses a Language Modeling(LM) head .</p>
<p>Generally, as well as in this case, LM head is a linear layer having input dimension of hidden state (for BERT-base it will be 768) and output dimension of vocabulary size. Thus, it maps to hidden state output of BERT model to a specific token in the vocabulary. The loss is calculated based on the scores obtained of a given token with respect to the target token.</p>
"
Issue with Vanna AI Handling Multiple Schemas in the Same Database,"<p>I am working on implementing the “Talk to Data” feature using Vanna AI for our organization. As part of our setup, we are using a multi-tenant database architecture where each tenant’s data is stored in a separate schema within the same PostgreSQL database.</p>
<p>To accommodate this structure, I planned to create separate Vanna instances for each schema within our PostgreSQL database. Each instance is trained with a distinct training dataset specific to its corresponding schema. The intention behind this approach is to ensure that Vanna queries are schema-specific and do not cross-reference or combine data from different schemas or tenants.</p>
<p>However, I have encountered an issue where Vanna AI appears to be mixing up training datasets across different schemas. For example, when querying the instance that should only access Schema ‘X’, I noticed that Vanna is incorporating elements from the training data of Schema ‘Y’. This is happening even though I have explicitly trained each Vanna instance separately, ensuring that each has access only to its respective schema and training dataset.</p>
<p>Here’s a high-level illustration of what I’m seeing:</p>
<ul>
<li>Schema X: Intended to have its own Vanna instance trained with only Schema X data.</li>
<li>Schema Y: Intended to have a separate Vanna instance trained with only Schema Y data.</li>
</ul>
<p>But during operation, queries targeting Schema X sometimes incorrectly use patterns or insights from the Schema Y dataset and vice versa. For example:</p>
<ul>
<li>When querying customer information from Schema X, Vanna unexpectedly references fields or entities that exist only in Schema Y.</li>
<li>Similarly, queries that should strictly adhere to Schema X’s context are influenced by data relationships unique to Schema Y, leading to incorrect or mixed query results.</li>
</ul>
<p>Why is this happening? It seems that Vanna AI is not isolating the training datasets as expected, leading to cross-schema contamination in its queries.</p>
<p>suggest a solution to ensure that each Vanna instance operates within its defined schema boundaries without cross-referencing other schemas’ training data.</p>
<p>I can show you a demo code of what I did</p>
<pre><code>from vanna_ai import VannaManager

# Simulated function to initialize Vanna instances for each schema
def initialize_vanna_instances(schemas):
    vanna_instances = {}
    for schema in schemas:
        # Initialize a Vanna instance for each schema
        vanna_instance = VannaManager(schema_name=schema)
        # Train each instance with the schema-specific dataset
        vanna_instance.train(training_data_path=f&quot;/path/to/{schema}_training_data.csv&quot;)
        vanna_instances[schema] = vanna_instance
    return vanna_instances

# Simulated function to query a specific Vanna instance
def query_vanna_instance(vanna_instance, query):
    # Generate SQL using the Vanna instance
    sql_query = vanna_instance.generate_sql(question=query)
    print(f&quot;Generated SQL for schema {vanna_instance.schema_name}: {sql_query}&quot;)
    # Run SQL query against the schema-specific database
    result = vanna_instance.run_sql(sql=sql_query)
    return result

# Initialize Vanna instances for two different schemas
schemas = ['schema_x', 'schema_y']
vanna_instances = initialize_vanna_instances(schemas)

# Example queries for different schemas
query_x = &quot;SELECT * FROM customers WHERE purchase_date &gt; '2024-01-01';&quot;
query_y = &quot;SELECT * FROM orders WHERE order_amount &gt; 100;&quot;

# Query the instance for schema_x
result_x = query_vanna_instance(vanna_instances['schema_x'], query_x)
print(f&quot;Result for Schema X: {result_x}&quot;)

# Query the instance for schema_y
result_y = query_vanna_instance(vanna_instances['schema_y'], query_y)
print(f&quot;Result for Schema Y: {result_y}&quot;)
</code></pre>
<p>Explanation of the Code</p>
<ol>
<li><p>Initialize Vanna Instances: The initialize_vanna_instances function initializes a Vanna AI instance for each schema provided in the schemas list. Each instance is trained separately with its own schema-specific dataset.</p>
</li>
<li><p>Query Vanna Instance: The query_vanna_instance function demonstrates how a query is executed using the Vanna instance tied to a specific schema. It generates an SQL query using Vanna AI’s capabilities and runs it against the schema-specific database.</p>
</li>
<li><p>Potential Issue Illustration:</p>
<p>•   Schema X and Schema Y have their own Vanna instances and training datasets.</p>
<p>•   When querying Schema X with query_x, we expect it to only use data and context from Schema X.</p>
<p>•   Similarly, querying Schema Y with query_y should only reference Schema Y’s data.</p>
<p>•   Issue: Despite separate instances and datasets, you notice that queries are mixing data or context from different schemas.</p>
</li>
</ol>
","python, nlp, artificial-intelligence, large-language-model, gpt-4",
Clustering for SBERT embedding,"<p>I have a set of sentences which I have transformed into vectors using SBERT embedding. I would like to cluster these vectors.</p>
<p>When looking for informations online, I keep seeing post telling to do some classical clustering, either using KMeans (which then use the Euclidean distance), or using (1-cosine) as the distance (eg nltk.cluster.cosine_distance() ) which is definitely not a distance.
I don't understand the validity of these methods: in the first case, the distance used is not a distance that is meaningful for similarity, and in the second case, it is not a distance.</p>
<p>Are these methods valid? If yes, why? And if not, what would be a good approach to cluster?</p>
","nlp, cluster-analysis, bert-language-model",
Efficient implementation of BPE using priority queue,"<p>I think that it is not strictly BPE (<a href=""https://en.wikipedia.org/wiki/Byte_pair_encoding"" rel=""nofollow noreferrer"">byte pair encoding</a>), but there is a similar idea applied to strings.</p>

<p>Suppose there are three Chinese words in the dictionary (I will use a huge dictionary like <a href=""https://cc-cedict.org/editor/editor.php?handler=Download"" rel=""nofollow noreferrer"">CEDICT</a> for practical use.)</p>

<ul>
<li>我</li>
<li>喜欢</li>
<li>水果</li>
</ul>

<p>Then take an input like this below.</p>

<p>我喜欢水果 (I like fruit)</p>

<p>Since Chinese texts are not splitted by white spaces, it's difficult to process.</p>

<p>We can decompose the input string into multiple single characters.</p>

<p>我 喜 欢 水 果</p>

<p>Then lookup new symbol pair at [left, right] and combine them. If the combined word is in the dictionary, we can replace the combined word with a new symbol.</p>

<ul>
<li>我喜</li>
<li>喜欢 &lt;- in the dic</li>
<li>欢水</li>
<li>水果 &lt;- in the dic</li>
</ul>

<p>We found two new symbols, so the input text becomes </p>

<p>我 喜欢 水果</p>

<p>We should iterate until we cannot find any combined word in the dictionary. In this case, we cannot find a new symbol in the dictionary.</p>

<ul>
<li>我喜欢 水果</li>
<li>喜欢水果</li>
</ul>

<p>It's not difficult to implement this naively but we need to scan adjoining two words many times. Some said we can implement BPE efficiently with a priority queue. I'm not familiar with compression algorithms. I would be grateful if someone could tell me the implementation or useful documentations.</p>

<p>In this method, out of vocabulary words are decomposed into single characters, so we can avoid unknown words problems.</p>

<p>Best regards,</p>

<p>Reference: <a href=""https://arxiv.org/abs/1508.07909"" rel=""nofollow noreferrer"">Neural Machine Translation of Rare Words with Subword Units</a> He had to start with pre-tokenized words because of computational complexity.</p>
","string, algorithm, nlp, compression",
Why I cannot add new word into WordNet using library &quot;wn_editor&quot; (Python)?,"<p>(Python) I am now stucking at editing WordNet lexicons by using &quot;wn_editor&quot;. It keeps error like this in every single lexicon (except &quot;odenet&quot;). There are very few documents for me to fix this.</p>
<p><a href=""https://i.sstatic.net/5UcdWUHO.png"" rel=""nofollow noreferrer"">Here is the error image:</a></p>
<p>To initiate wn and wn_editor</p>
<pre><code>import wn
import wn_editor
</code></pre>
<p>Calling the LexiconEditor to edit the chosen lexicon</p>
<pre><code># own-en is an English WordNet lexicon
editor = LexiconEditor('own-en')

# Create new Synset to add a word
# ERROR here
new_ss = editor.create_synset()

# Add a new word
# ERROR here
new_ss.add_word('newWord')
</code></pre>
<p>And this step keeps error (except I use odenet lexicon)</p>
<pre><code># This is not error
editor = LexiconEditor('odenet')

# Create new Synset to add a word
# Not ERROR anymore
new_ss = editor.create_synset()

# Add a new word
# Not ERROR anymore
new_ss.add_word('newWord')
</code></pre>
<p>I currently runs Jupyter Notebook file and on VSCode:</p>
<ul>
<li>Python = 3.10.11</li>
<li>wn = 0.9.5</li>
<li>wn_editor = 0.5.4</li>
</ul>
","python, nlp, typeerror, wordnet",
"Layer expects 2 input(s), but it received 1 input tensors","<p>I am trying to build model to predict posts likes, the model takes text and content type which is one hot encoded column.</p>
<p>I have made a TensorFlow dataset but when trying to fit the model I got this error:</p>
<pre><code>Layer &quot;functional_13&quot; expects 2 input(s), but it received 1 input tensors. 
Inputs received: [&lt;tf.Tensor 'data:0' shape=(None, 1000) dtype=int64&gt;]
</code></pre>
<p>Here is some snippets of my code:</p>
<pre><code>dataset = tf.data.Dataset.from_tensor_slices((vectorized_text,
                                             content,
                                             df['likes_rate']))

dataset= dataset.cache()
dataset= dataset.shuffle(160000)
dataset= dataset.batch(16)
dataset= dataset.prefetch(8)
</code></pre>
<p>This is my model</p>
<pre><code>from tensorflow.keras.layers import Input, Embedding, Concatenate,LSTM,Bidirectional
text_input= Input(shape=(1000,))
content_input=Input(shape=(3,))

text_embeddings = tf.keras.layers.Embedding(Max_Features+1, 32)(text_input)  # Adjust embedding dim
lstm= Bidirectional(LSTM(32,activation='tanh'))(text_embeddings)
# Concatenate text embeddings and content features
combined_features = tf.keras.layers.Concatenate()([lstm, content_input])

# Hidden layers (adjust number/activation functions)
x = tf.keras.layers.Dense(256, activation='relu')(combined_features)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dropout(0.1)(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
# Output layer for likes prediction
output = tf.keras.layers.Dense(1, activation='linear')(x)
</code></pre>
<p>I want to make an embedding layer to for the text then pass it to LSTM then combine the output of the LSTM and the contnent to Dense layers.</p>
<p>When trying to fit the model I am getting the problem above.</p>
<pre><code>model = tf.keras.models.Model(inputs=[text_input, content_input], outputs=output)
model.compile(loss='mse',optimizer='Adam')
model.fit(dataset,epochs=10)
</code></pre>
<p>If I iterate over the dataset. The code works correctly. But <code>.fit</code> every time make a random weights so the model make no progress.</p>
<pre><code>for text_batch, content_batch, y_batch in dataset:
    # Train model on the current batch
    model.fit(x=[text_batch, content_batch], y=y_batch)
</code></pre>
","python, tensorflow, machine-learning, deep-learning, nlp",
What is the best way to remove rare words from a large text?,"<p>The dataset contains 2.14M words
The following is my code.</p>
<pre><code>uni = get_unique(ds) #to get all unique words

c = Counter(uni) #using counter from Collections to create a dictionary
v = list(c.values()) #dict value
ky = list(c.keys()) #dicks keys


junk = [] #indexes of rare words (words that appear less than 20 times)
num = 0 #the number words that appear more than 20 times 
for i in range(len(v)):
    if(v[i] &gt;= 20):
        num += 1
    else:
        junk.append(i)

rare_words = []

for i in junk:
    rare_words.append(ky[i]) #selecting rare words from the keys
</code></pre>
<p>A function to remove the rare words</p>
<pre><code>def remove_jnk(dataset,rare_words):
    ds = []
    for i in dataset:
        repl_wrd = &quot; &quot;
        res = &quot; &quot;.join([repl_wrd if idx in rare_words else idx for idx in i[0].split()])
        ds.append([res]) 
    return ds

ds = remove_jnk(ds, rare_words)
</code></pre>
<p><strong>this is too slow and it's taking hours to run</strong></p>
","python-3.x, nlp",
is it possible to train NER model on en_core_web_lg without static_vectors?,"<p>I am trying to train an NER model with custom tokenization. it works fine with the en_core_web_sm model, but I am trying to increase accuracy so I am now trying with en_core_web_lg. no matter what I seem to do to disable static vectors, it always ends up complaining:
<code>RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False</code></p>
<p>I have decided against using static vectors as I must use custom tokenization, and as I am new I decided that simply not using it would be better. is this wrong?</p>
<p>this is the code I am using for the large model:</p>
<pre><code>def train_model(train_data, output_dir, n_iter=50):
    # Enable GPU
    spacy.require_gpu()
    
    # Load the pre-trained model including tok2vec, but excluding vectors
    nlp = spacy.load(&quot;en_core_web_lg&quot;, exclude=[&quot;vectors&quot;])
    
    # Set custom tokenizer
    nlp.tokenizer = custom_tokenizer(nlp)
    
    # Ensure no component is using static vectors
    for component in nlp.pipe_names:
        if hasattr(nlp.get_pipe(component), &quot;include_static_vectors&quot;):
            nlp.get_pipe(component).include_static_vectors = False
    
    # Check if the NER pipe exists, if not add it
    if &quot;ner&quot; not in nlp.pipe_names:
        ner = nlp.add_pipe(&quot;ner&quot;, last=True)
    else:
        ner = nlp.get_pipe(&quot;ner&quot;)
    
    # Add labels to the NER model
    for example in train_data:
        for ent in example.reference.ents:
            ner.add_label(ent.label_)
    
    # Create an optimizer with the correct components
    optimizer = nlp.create_optimizer()

    for i in range(n_iter):
        random.shuffle(train_data)
        losses = {}
        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            nlp.update(batch, drop=0.5, losses=losses, sgd=optimizer)
        print(f&quot;Iteration {i + 1}/{n_iter}: Losses {losses}&quot;)
    
    # Save the model to the output directory
    if output_dir is not None:
        nlp.to_disk(output_dir)
        print(f&quot;Saved model to {output_dir}&quot;)
</code></pre>
<p>it results in the previously mentioned runtime error</p>
","python, machine-learning, nlp, spacy, named-entity-recognition",
My LSTM model returns empty with no output and no parameters captured,"<p>I am building an LSTM model for sentiment analysis using a hotel review dataset. However, the model always returns empty with empty output and parameters each time I run the code.</p>
<p>I have followed many processes from cleaning to tokenization, vectorization, encoding, and padding.</p>
<p><a href=""https://i.sstatic.net/oVNsQBA4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oVNsQBA4.png"" alt=""enter image description here"" /></a></p>
<p>See attached image.</p>
<p>I have followed the normal steps from cleaning, stopwords, tokenization, lemmatization, vectorization, padding.</p>
<p>I used the below code for the final model design:</p>
<pre><code># Design the model
model = Sequential()
model.add(Embedding(input_dim=len(vocab_int), output_dim=embedding_vector_length, input_length=max_sentence_length))
model.add(SimpleRNN(256, return_sequences=True, dropout=dropout, recurrent_dropout=dropout))
model.add(SimpleRNN(256, dropout=dropout, recurrent_dropout=dropout))
model.add(Dense(2, activation='softmax'))

model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

print(model.summary())
</code></pre>
<p>I was expecting the built model with output layers and parameters but got nothing.</p>
","python, nlp, vectorization, lstm, sequencing",
How to Process Data on GPU Instead of RAM for This Python Code?,"<p>I'm currently using the following code to process audio data, but it runs on the RAM. I want to offload the processing to the GPU to improve performance.
my code :</p>
<pre><code>def prepare_dataset(batch):
    audio = batch[&quot;audio&quot;]
    batch[&quot;input_features&quot;] = feature_extractor(
        audio[&quot;array&quot;], 
        sampling_rate=audio[&quot;sampling_rate&quot;]
    ).input_features[0]
    batch[&quot;labels&quot;] = tokenizer(batch[&quot;sentence&quot;]).input_ids
    return batch

common_voice = common_voice.map(
    prepare_dataset, 
    remove_columns=common_voice.column_names[&quot;train&quot;], 
    num_proc=1
)
</code></pre>
<p>How can I modify this code to utilize the GPU for processing instead of the RAM? Any guidance or specific changes are much appreciated!</p>
","nlp, gpu, torch, openai-whisper","<p>you can using the following code to process audio data on GPU</p>
<pre><code>import torch
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
print(device)

def prepare_dataset(batch):
    audio = batch[&quot;audio&quot;]

    input_features = feature_extractor(audio[&quot;array&quot;], sampling_rate=audio[&quot;sampling_rate&quot;]).input_features[0]
    batch[&quot;input_features&quot;] = torch.tensor(input_features).to(device)

    labels = tokenizer(batch[&quot;sentence&quot;]).input_ids
    batch[&quot;labels&quot;] = torch.tensor(labels).to(device)
    return batch

common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[&quot;train&quot;])
</code></pre>
"
TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: &#39;recursive_guard&#39;,"<p>Pls help. I need help setting up spacy inside jupyter enviornment</p>
<p>I am trying to use spacy to summarize youtube transcripts but finding lots of problems with spacy and python 3.12.4/3.12.3.<br />
I started with 3.12.4, then installed python 3.12.3 onto conda environment <code>py3.12.3</code>.</p>
<p>I install spacy 3.7.6 into env with <code>pip install stacy</code></p>
<pre><code>Here are the `conda env list: 
base                     /opt/anaconda3
py3.11                *  /opt/anaconda3/envs/py3.11
py3.12.3                 /opt/anaconda3/envs/py3.12.3
</code></pre>
<p>I added python kernels via cmd below</p>
<pre><code>jupyter kernelspec install py3.12.3
python -m ipykernel install --user --name=py3.12.3
</code></pre>
<p>On command line, import spacy is ok:</p>
<pre><code>(py3.12.3) UID ~ % python           
Python 3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 14:43:12) [Clang 14.0.6 ] on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import spacy
&gt;&gt;&gt; quit()
(py3.12.3) UID ~ % pip freeze | grep spacy
spacy==3.7.6
spacy-legacy==3.0.12
spacy-loggers==1.0.5

</code></pre>
<p>Inside jupyter (run from base, restarted after installing py3.12.3), <code>import spacy</code> gives error below.  Any help is welcomed???</p>
<pre><code>1. (base) UID ~ % python -m ipykernel install --user --name=py3.12.3
0.00s - Debugger warning: It seems that frozen modules are being used, which may
0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off
0.00s - to python to disable frozen modules.
0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.
Installed kernelspec py3.12.3 in /Users/UID/Library/Jupyter/kernels/py3.12.3


---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[2], line 1
----&gt; 1 import spacy # module will be used  to build NLP model
      2 from spacy.lang.en.stop_words import STOP_WORDS # module will be used  to build NLP model
      3 from string import punctuation

File /opt/anaconda3/lib/python3.12/site-packages/spacy/__init__.py:13
     10 # These are imported as part of the API
     11 from thinc.api import Config, prefer_gpu, require_cpu, require_gpu  # noqa: F401
---&gt; 13 from . import pipeline  # noqa: F401
     14 from . import util
     15 from .about import __version__  # noqa: F401

File /opt/anaconda3/lib/python3.12/site-packages/spacy/pipeline/__init__.py:1
----&gt; 1 from .attributeruler import AttributeRuler
      2 from .dep_parser import DependencyParser
      3 from .edit_tree_lemmatizer import EditTreeLemmatizer

File /opt/anaconda3/lib/python3.12/site-packages/spacy/pipeline/attributeruler.py:8
      6 from .. import util
      7 from ..errors import Errors
----&gt; 8 from ..language import Language
      9 from ..matcher import Matcher
     10 from ..scorer import Scorer

File /opt/anaconda3/lib/python3.12/site-packages/spacy/language.py:43
     41 from .lang.tokenizer_exceptions import BASE_EXCEPTIONS, URL_MATCH
     42 from .lookups import load_lookups
---&gt; 43 from .pipe_analysis import analyze_pipes, print_pipe_analysis, validate_attrs
     44 from .schemas import (
     45     ConfigSchema,
     46     ConfigSchemaInit,
   (...)
     49     validate_init_settings,
     50 )
     51 from .scorer import Scorer

File /opt/anaconda3/lib/python3.12/site-packages/spacy/pipe_analysis.py:6
      3 from wasabi import msg
      5 from .errors import Errors
----&gt; 6 from .tokens import Doc, Span, Token
      7 from .util import dot_to_dict
      9 if TYPE_CHECKING:
     10     # This lets us add type hints for mypy etc. without causing circular imports

File /opt/anaconda3/lib/python3.12/site-packages/spacy/tokens/__init__.py:1
----&gt; 1 from ._serialize import DocBin
      2 from .doc import Doc
      3 from .morphanalysis import MorphAnalysis

File /opt/anaconda3/lib/python3.12/site-packages/spacy/tokens/_serialize.py:14
     12 from ..errors import Errors
     13 from ..util import SimpleFrozenList, ensure_path
---&gt; 14 from ..vocab import Vocab
     15 from ._dict_proxies import SpanGroups
     16 from .doc import DOCBIN_ALL_ATTRS as ALL_ATTRS

File /opt/anaconda3/lib/python3.12/site-packages/spacy/vocab.pyx:1, in init spacy.vocab()

File /opt/anaconda3/lib/python3.12/site-packages/spacy/tokens/doc.pyx:49, in init spacy.tokens.doc()

File /opt/anaconda3/lib/python3.12/site-packages/spacy/schemas.py:195
    191         obj = converted
    192     return validate(TokenPatternSchema, {&quot;pattern&quot;: obj})
--&gt; 195 class TokenPatternString(BaseModel):
    196     REGEX: Optional[Union[StrictStr, &quot;TokenPatternString&quot;]] = Field(None, alias=&quot;regex&quot;)
    197     IN: Optional[List[StrictStr]] = Field(None, alias=&quot;in&quot;)

File /opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/main.py:286, in ModelMetaclass.__new__(mcs, name, bases, namespace, **kwargs)
    284 cls.__signature__ = ClassAttribute('__signature__', generate_model_signature(cls.__init__, fields, config))
    285 if resolve_forward_refs:
--&gt; 286     cls.__try_update_forward_refs__()
    288 # preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487
    289 # for attributes not in `new_namespace` (e.g. private attributes)
    290 for name, obj in namespace.items():

File /opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/main.py:808, in BaseModel.__try_update_forward_refs__(cls, **localns)
    802 @classmethod
    803 def __try_update_forward_refs__(cls, **localns: Any) -&gt; None:
    804     &quot;&quot;&quot;
    805     Same as update_forward_refs but will not raise exception
    806     when forward references are not defined.
    807     &quot;&quot;&quot;
--&gt; 808     update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (NameError,))

File /opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py:554, in update_model_forward_refs(model, fields, json_encoders, localns, exc_to_suppress)
    552 for f in fields:
    553     try:
--&gt; 554         update_field_forward_refs(f, globalns=globalns, localns=localns)
    555     except exc_to_suppress:
    556         pass

File /opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py:529, in update_field_forward_refs(field, globalns, localns)
    527 if field.sub_fields:
    528     for sub_f in field.sub_fields:
--&gt; 529         update_field_forward_refs(sub_f, globalns=globalns, localns=localns)
    531 if field.discriminator_key is not None:
    532     field.prepare_discriminated_union_sub_fields()

File /opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py:520, in update_field_forward_refs(field, globalns, localns)
    518 if field.type_.__class__ == ForwardRef:
    519     prepare = True
--&gt; 520     field.type_ = evaluate_forwardref(field.type_, globalns, localns or None)
    521 if field.outer_type_.__class__ == ForwardRef:
    522     prepare = True

File /opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py:66, in evaluate_forwardref(type_, globalns, localns)
     63 def evaluate_forwardref(type_: ForwardRef, globalns: Any, localns: Any) -&gt; Any:
     64     # Even though it is the right signature for python 3.9, mypy complains with
     65     # `error: Too many arguments for &quot;_evaluate&quot; of &quot;ForwardRef&quot;` hence the cast...
---&gt; 66     return cast(Any, type_)._evaluate(globalns, localns, set())

TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'
</code></pre>
","python, python-3.x, nlp, spacy, sentence",
optimum.onnxruntime Yields UnboundLocalError: local variable ‘all_files’ referenced before assignment,"<p>Code is below. I have tried numerous popular model names and all give the same error. I used <code>pip install optimum[exporters,onnxruntime]</code>. Both of the below give the same error.</p>
<pre><code>from optimum.exporters.tasks import TasksManager

all_files, _ = TasksManager.get_model_files(‘distilbert-base-uncased’)
</code></pre>
<p>and</p>
<pre><code>from optimum.onnxruntime import ORTModelForSequenceClassification

model_id = ‘distilbert-base-uncased’
onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id, export=True)
</code></pre>
","pytorch, nlp, huggingface-transformers, onnx, sentence-transformers",
Spacy/Torch fbgemm dependency,"<p>I'm trying to work with spacy to do some nlp in Python. Just for some background, I'm running on a windows PC with an Intel UHD Graphics card (So no cuda).</p>
<p>I tried installing the cpu form of pytorch with
<code>pip3 install torch torchvision torchaudio</code> as per the website, and I tried <code>pip install spacy</code> and <code>pip install spacy==3.7</code> (So I've tried both versions). I'm running with Python version <code>3.12.5</code>.</p>
<p>Whenever I try <code>import spacy</code> (or importing any nlp package for that matter), I keep getting the error:</p>
<pre><code>OSError: [WinError 126] The specified module could not be found. Error loading &quot;C:\Users\User\AppData\Local
\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfrs8p0\LocalCache
\local-packages\Python312\site-packages\torch\lib\fbgemm.dll&quot; or one of its dependencies.
</code></pre>
<p>I've googled it and these packages should work together? Why am I missing this <code>.dll</code> file? I've also tried this on a different windows PC that does have an nvidia graphics card and cuda, same issue. Why is this file not being downloaded correctly?</p>
<p>Even manually downloading the file, I still get <code>OSError: [WinError 126] The specified module could not be found. Error loading ... shm.dll or one of its dependencies.</code> What's going wrong here? Thanks.</p>
","python, pytorch, nlp, spacy",
"Input 0 of layer &quot;lstm_1&quot; is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 256)","<p>I am trying to build a generative LSTM model using tensorflow2. I am new to using LSTM layer in tensorflow. the code is given below::</p>
<pre><code>vec = layers.TextVectorization(output_sequence_length=maxlen,
                               max_tokens=379)
vec.adapt(use_for_vocab(train)) 
# here i just convert the train dataset into a form suitable to use for vectorization 

voc = vec.get_vocabulary()
voc_size = len(voc)

embed = layers.Embedding(input_dim=voc_size,
                         output_dim=256,
                         mask_zero=True)

inp_word = layers.Input(shape=(maxlen+2,), # maxlen is the maximum length of the sentence in the text
                   name=&quot;word_input&quot;)      # 2 is added to accommodate start_token and end_token
x_word = embed(inp_word)
x_word = layers.Dropout(0.5)(x_word)
x_word = layers.LSTM(256, return_sequences=True)(x_word)
ops_word = layers.GlobalAveragePooling1D(name=&quot;word_gap&quot;)(x_word)

</code></pre>
<p>Here is the model summary:</p>
<p><strong>Model: &quot;word_model&quot;</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Layer (type)</th>
<th>Output Shape</th>
<th>Param #</th>
</tr>
</thead>
<tbody>
<tr>
<td>word_input (InputLayer)</td>
<td>[(None, 35)]</td>
<td>0</td>
</tr>
<tr>
<td>embedding_1 (Embedding)</td>
<td>(None, 35, 128)</td>
<td>45184</td>
</tr>
<tr>
<td>dropout_6 (Dropout)</td>
<td>(None, 35, 128)</td>
<td>0</td>
</tr>
<tr>
<td>lstm_5 (LSTM)</td>
<td>(None, 35, 256)</td>
<td>394240</td>
</tr>
<tr>
<td>word_gap (GlobalAveragePooling1D)</td>
<td>(None, 256)</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>Total params: 439,424
Trainable params: 439,424
Non-trainable params: 0</p>
<hr />
<p>I have created a prefetched dataset for input. This is how I have built it using the function below:</p>
<pre><code>from tensorflow.data import Dataset, AUTOTUNE

def format_dataset(x, y):
  y = Dataset.from_tensor_slices(y)
  data = Dataset.zip((x, y))
  return data.batch(32).prefetch(AUTOTUNE)
</code></pre>
<p>I am able to compile the model without any error. But, while fitting the model, it is giving the following error:</p>
<pre><code> File &quot;/usr/local/lib/python3.9/dist-packages/keras/engine/training.py&quot;, line 1284, in train_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.9/dist-packages/keras/engine/training.py&quot;, line 1268, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.9/dist-packages/keras/engine/training.py&quot;, line 1249, in run_step  **
        outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.9/dist-packages/keras/engine/training.py&quot;, line 1050, in train_step
        y_pred = self(x, training=True)
    File &quot;/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py&quot;, line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File &quot;/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py&quot;, line 235, in assert_input_compatibility
        raise ValueError(

    ValueError: Exception encountered when calling layer 'caption_model' (type Functional).
    
    Input 0 of layer &quot;lstm_5&quot; is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)

</code></pre>
<p>Cannot understand why input sequence still is two-dimensional in spite of making return_sequences true. Any help will be appreciated.</p>
","keras, nlp, lstm, tensorflow2.0",
How to Visualize Cross-Attention Matrices in MarianMTModel During Output Generation,"<p>I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library. Specifically, I want to visualize the cross-attention matrices during the model's translation process. However, I encountered some difficulties in achieving this.</p>
<p><strong>What I’ve Tried:</strong></p>
<ul>
<li><p><strong>Initial Attempt:</strong> I noticed that the cross-attention matrices are not directly returned when the model generates a translation. The only example I found involved feeding both the source text and the translation to the model. However, my goal is to access the cross-attention matrices while the model generates the output, not for a translation given by me.</p>
</li>
<li><p><strong>Using Forward Hooks:</strong> To achieve this, I implemented forward hooks on both the key and query projections of the attention mechanism, while disabling the key-value caching (use_cache=False) to capture the full matrices at the last step. Here’s my implementation:</p>
</li>
</ul>
<pre class=""lang-py prettyprint-override""><code># VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)
from transformers import MarianMTModel, MarianTokenizer
import torch
import matplotlib.pyplot as plt
from torch.nn import functional as F

model_name = &quot;Helsinki-NLP/opus-mt-en-de&quot;
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)
model.eval()

keys = {}
queries = {}

def get_key(layer):
    def hook(module, input, output):
        key, = input
        keys[layer] = key
    return hook

def get_query(layer):
    def hook(module, input, output):
        query, = input
        queries[layer] = query
    return hook

def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()

hooks = []
for i, layer in enumerate(model.model.decoder.layers):
    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))
    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))

input_text = &quot;Please translate this to German.&quot;
inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;)

translated_tokens = model.generate(**inputs, use_cache=False)

translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)

input_tokens = tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0])
output_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])

attentions = []
for layer in range(len(keys)):
    K, Q = keys[layer], queries[layer]
    M = Q @ K.transpose(-2, -1)
    attentions.append(F.softmax(M, dim=-1))

attentions = torch.stack(attentions, dim=0)

print(&quot;layers, heads, output tokens, input tokens&quot;)
print(attentions.shape)
plt.figure(figsize=(10, 8))
plt.imshow(attentions[0, 0], cmap='viridis')
plt.colorbar()

plt.xticks(range(len(input_tokens)), input_tokens, rotation=90)
plt.yticks(range(len(output_tokens)), output_tokens)

plt.xlabel(&quot;Input Tokens&quot;)
plt.ylabel(&quot;Output Tokens&quot;)
plt.title(&quot;Cross-Attention Matrix&quot;)
plt.show()
</code></pre>
<p>This approach seemed to work in capturing the cross-attention matrices. However, I observed that the matrices only have 4 attention heads instead of the expected 8. This makes me question the correctness of my implementation.</p>
<p><strong>My Question</strong></p>
<p>Given the issues I’ve encountered, is there a more reliable method to extract and visualize the cross-attention matrices during the translation process? Additionally, if my current approach is fundamentally okay, how can I resolve the issue of capturing only 4 attention heads instead of 8?</p>
<p>I suspect that the issue might be related to that I'm currently not reshaping the key (K) and query (Q) tensors to the head dimension before multiplication, but I wanted to ask for advice in case there’s an easier or more effective way to do this.</p>
","python, pytorch, nlp, huggingface-transformers","<p>Huggingface has built in methods to return attention weights</p>
<pre class=""lang-py prettyprint-override""><code>translated_tokens = model.generate(**inputs, 
                                   output_attentions=True,
                                   return_dict_in_generate=True
                                  )

print(translated_tokens.keys())
&gt; odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])
</code></pre>
<p>With <code>return_dict_in_generate=True</code>, <code>model.generate</code> returns a dict-like object. With <code>output_attentions=True</code>, the output dict will contain all attention weights.</p>
<p>For this model, it will include encoder attentions, decoder attentions and cross attentions.</p>
"
Using text chunks in tensors,"<p>Got the following error:</p>
<p>File &quot;/home/user/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/data/data_collator.py&quot;, line 434, in 
examples = [torch.tensor(e, dtype=torch.long) for e in examples]
TypeError: only integer tensors of a single element can be converted to an index</p>
<pre><code>
setproctitle.setproctitle(&quot;HyoungJoonLee&quot;)

if torch.cuda.is_available():
    device = torch.device('cuda:2')
</code></pre>
<pre><code>if torch.cuda.is_available():
    device = torch.device('cuda:2')
    print(f'CUDA device detected: {torch.cuda.get_device_name(device)}')
else:
    device = torch.device('cpu')
    print(&quot;CUDA device is not available, using CPU.&quot;)

model_name = 'microsoft/phi-2'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
</code></pre>
<pre><code>training_args = TrainingArguments(
    output_dir=&quot;./output&quot;,
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=2,  # Adjust based on your GPU memory
    save_steps=10_000,
    save_total_limit=2,
    logging_dir=&quot;./logs&quot;,
    logging_steps=200,
    prediction_loss_only=True,
    report_to=&quot;none&quot;,  # Use 'wandb' if logging with Weights &amp; Biases
    learning_rate=5e-5,
    warmup_steps=500,
    resume_from_checkpoint=True,  # Allow resuming from a checkpoint
)

# Ensure the tokenizer and model are properly set up
tokenizer.pad_token = tokenizer.eos_token
model.resize_token_embeddings(len(tokenizer))

# Create the dataset for training
def create_text_dataset(chunks, tokenizer, block_size=512):
    encodings = tokenizer(chunks, return_tensors=&quot;pt&quot;, max_length=block_size, truncation=True, padding=&quot;max_length&quot;)
    return torch.utils.data.TensorDataset(encodings['input_ids'])

# Prepare the dataset
def prepare_data_for_training(text_file_path, tokenizer):
    with open(text_file_path, 'r') as file:
        text = file.read().split('#'*20)

    chunks = text
    return create_text_dataset(chunks, tokenizer)

# Path to your text file containing the chunks
text_file_path = '/home/user/data/agri_llm/training_data/pretraining_chunks.txt'

# Prepare the dataset
train_dataset = prepare_data_for_training(text_file_path, tokenizer)

# Define data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
)

# Start continual training
trainer.train()

# Save the final model and tokenizer
trainer.save_model(&quot;./final_model&quot;)
tokenizer.save_pretrained(&quot;./final_model&quot;)
</code></pre>
<p>I haven't figured out a way to fix this error. I will appreciate if someone could help with this matter.</p>
","python, nlp, large-language-model",
implement a search engine chain using tavily in langchain,"<p>I want to implement a search engine chain using tavily in langchain. This chain gives user's query as an input and returns up to 5 related documents. Each retrieved document must have the content of the document as page_content and the url of the corresponding site as metadata under the definition of LangChain Documents. I must use langchain_core.documents.base.Document class to define documents. So this chain will have two main parts:</p>
<ol>
<li>Tavily search platform</li>
<li>Parser with the aim of converting search output data into standard LangChai documents.</li>
</ol>
<p>I wrote this code but I don't know how to change tavily output format into standard form of document:</p>
<pre><code>from langchain_core.documents.base import Document
from langchain_community.tools.tavily_search import TavilySearchResults

search = TavilySearchResults(max_results=5)

class ParsedDocument(BaseModel):
    content: str = Field(description=&quot;This refers to the content of the search.&quot;)
    url: str = Field(description=&quot;This refers to the url of the search.&quot;)

search_parser = PydanticOutputParser(pydantic_object=ParsedDocument)
search_engine_chain = search | search_parser

</code></pre>
<p>I would be grateful if you could help me how to change this code.</p>
","python, nlp, search-engine, langchain, chain","<p>I finally found the answer:</p>
<pre><code>class ParsedDocument(BaseModel):
    content: str = Field(description=&quot;This refers to the content of the search.&quot;)
    url: str = Field(description=&quot;This refers to the url of the search.&quot;)

# Define a custom parser
def custom_parser(search_results):
    parsed_documents = []
    for result in search_results:  # Adjust this line based on the actual structure of search_results
        parsed_document = ParsedDocument(content=result['content'], url=result['url'])
        document = Document(page_content=parsed_document.content, metadata={'url': parsed_document.url})
        parsed_documents.append(document)
    return parsed_documents

search_engine_chain = search | custom_parser
</code></pre>
"
Facing Error while using langchain&#39;s document loader,"<p>I am making a project on using RAG and am making use of langchain for it.
Firstly, I am loading the data using Langchain's Document Loader.
And then creating a data-base using ChromaDB.</p>
<p>I have tried many workarounds but none of them are working:</p>
<ol>
<li>Using pip install python-magic-bin==0.4.14</li>
<li>Installing Microsoft C++ 14.0.27</li>
</ol>
<p>Here is the below code snippet:</p>
<pre><code>
    from langchain_community.document_loaders import DirectoryLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.schema import Document
    from langchain_openai import OpenAIEmbeddings
    from langchain_community.vectorstores import Chroma
    import openai
    from dotenv import load_dotenv
    import os
    import shutil

    load_dotenv()
    openai.api_key = os.environ['OPENAI_API_KEY']
    
    CHROMA_PATH = &quot;chroma&quot;
    DATA_PATH = &quot;data/books&quot;
    
    
    def main():
        generate_data_store()
    
    
    def generate_data_store():
        documents = load_documents()
        chunks = split_text(documents)
        save_to_chroma(chunks)
    
    
    def load_documents():
        loader = DirectoryLoader(DATA_PATH, glob=&quot;*.md&quot;)
        documents = loader.load()
        return documents
    
    
    def split_text(documents: list[Document]):
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,
            chunk_overlap=100,
            length_function=len,
            add_start_index=True,
        )
        chunks = text_splitter.split_documents(documents)
        print(f&quot;Split {len(documents)} documents into {len(chunks)} chunks.&quot;)
    
        document = chunks[10]
        print(document.page_content)
        print(document.metadata)
    
        return chunks
    
    
    def save_to_chroma(chunks: list[Document]):
        # Clear out the database first.
        if os.path.exists(CHROMA_PATH):
            shutil.rmtree(CHROMA_PATH)
    
        # Create a new DB from the documents.
        db = Chroma.from_documents(
            chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH
        )
        db.persist()
        print(f&quot;Saved {len(chunks)} chunks to {CHROMA_PATH}.&quot;)
    
    
    if __name__ == &quot;__main__&quot;:
        main()
</code></pre>
<p>Error snippet attached below:
<a href=""https://i.sstatic.net/cF5LZsgY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cF5LZsgY.png"" alt=""Error Image"" /></a></p>
<p>Any Help is Appreciated!</p>
","python, nlp, langchain",
How to load a percentage of data from huggingface load_dataset,"<p>I am trying to download the &quot;librispeech_asr&quot; dataset which totals 29GB, but due to limited space in google colab, I'm not able to download/load the dataset i.e. the notebook crashes.</p>
<p>So I did some research and found the <code>split</code> argument that we can pass in the <code>load_dataset</code> function to download a part of dataset, but it is still downloading the whole 30GB dataset on notebook. The argument <code>split</code> is not working...</p>
<pre><code>from datasets import load_dataset

dataset = load_dataset(&quot;librispeech_asr&quot;, 'clean', split=['train.360[:50%]', 'validation'])
</code></pre>
<p>I was trying to load only 50% of 'train.360' data but I'm unable to do so.</p>
<p>What is the correct method and what I am doing wrong?</p>
","python, nlp, speech-recognition, huggingface-transformers, huggingface-datasets",
Why doesn&#39;t permuting positional encodings in BERT affect the output as expected?,"<p>I am working on a Jupyter notebook about Transformers. In the section on positional encodings, I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence. I previously learned from another <a href=""https://stackoverflow.com/questions/78902301/why-doesnt-permuting-positional-encodings-in-gpt-2-affect-the-output-as-expecte/78903454#78903454"">question</a> I posted that this concept only applies to models that don't use masked attention, like GPT-2. However, when I attempted the same approach with a BERT model (which uses cross-attention) to predict a [MASK] token, I encountered unexpected results.</p>
<p><strong>What I expected to happen:</strong></p>
<ul>
<li>No permutation should cause the model to predict a different token, i.e., distribution A should be consistent over the vocabulary.</li>
<li>Permuting only the input IDs should return distribution B.</li>
<li>Permuting only the positional embeddings should return distribution B.</li>
<li>Permuting both the input IDs and positional embeddings should return distribution A.</li>
</ul>
<p><strong>What actually happens:</strong>
Sometimes the results align with my expectations, but other times, permuting one aspect (either the input IDs or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result.</p>
<p><strong>My question is:</strong> Is there something else in Hugging Face's BERT model that might be influenced by position, beyond just the positional encoding?</p>
<p>For completeness, I have included the full code from this part of the notebook below, so it can be tried out directly. The Important part happens in <code>masked_prediction</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import ipywidgets as widgets
from IPython.display import display
from transformers import BertForMaskedLM, AutoTokenizer
import matplotlib.pyplot as plt
import torch.nn.functional as F

# surpress renaming warnings
logging.getLogger(&quot;transformers.modeling_utils&quot;).setLevel(logging.ERROR)
warnings.simplefilter(&quot;ignore&quot;, FutureWarning)

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)

input_ids = torch.Tensor([[]])
tokens = []
permutation = []

output = widgets.Output()

def permute_columns(matrix, permutation=None):
    n = len(permutation)
    first_n_columns = matrix[:, :n]
    permuted_columns = first_n_columns[:, permutation]
    remaining_columns = matrix[:, n:]
    new_matrix = torch.hstack((permuted_columns, remaining_columns))
    return new_matrix

def update_permutation(ordered_tags):
    global permutation
    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]
    
    permutation = [tokens.index(tag) for tag in fixed_tokens]
    

def tokenize(text):
    global input_ids, tokens
    input_ids = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids
    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]
    
    if len(tokens) &gt; 2:
        reorderable_tokens = tokens[1:-1]
    else:
        reorderable_tokens = []
    
    with output:
        output.clear_output(wait=True)
        tags_input.allowed_tags = reorderable_tokens
        tags_input.value = reorderable_tokens
        update_permutation(tags_input.value)

def on_tags_change(change):
    if len(change['new']) != len(tags_input.allowed_tags):
        tags_input.value = tags_input.allowed_tags  # Restore original value


def masked_prediction(input_ids, permutation, permute_input, permute_encoding):
    
    with output:
        output.clear_output(wait=True)  # Clear previous outputs
        
        if input_ids.numel() == 0:
            print(&quot;You can't use an empty sequence for prediction&quot;)
            return
        
        model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)
        
        if permute_encoding:
            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T
        if permute_input:
            input_ids = permute_columns(input_ids, permutation)
            
        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)
            
        with torch.no_grad():
            outputs = model(input_ids)
            
        logits = outputs.logits

        top_k = 5

        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]
        print(decoded_text, mask_token_indices, permutation)
        num_masks = len(mask_token_indices)
        if num_masks == 0:
            print(&quot;You need to include a [MASK] token for prediction&quot;)
            return

        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))
        
        if num_masks == 1:
            axs = [axs]

        for i, idx in enumerate(mask_token_indices):
            mask_token_logits = logits[0, idx, :]

            softmax_probs = F.softmax(mask_token_logits, dim=0)

            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)

            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]
            predicted_confidences = top_token_probs.tolist()

            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')
            axs[i].set_xlabel('Predicted Tokens')
            axs[i].set_ylabel('Confidence')
            axs[i].set_title(f'Masked Token at Position {idx.item()}')
            axs[i].set_ylim(0, 1)

        plt.show()

def on_predict_button_click(b):
    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)

text_input = widgets.Text(placeholder='Write text here to encode.', description='Input:')
text_input.observe(lambda change: tokenize(change['new']), names='value')
tags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False)

# Observe changes in tags order to update the permutation and prevent deletion
tags_input.observe(on_tags_change, names='value')
tags_input.observe(lambda change: update_permutation(change['new']), names='value')

# Create checkboxes for permute_input and permute_encoding
permute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs')
permute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings')

# Create a button to trigger the prediction
predict_button = widgets.Button(description=&quot;Run Prediction&quot;)
predict_button.on_click(on_predict_button_click)

# Display the widgets
display(text_input)
display(tags_input)
display(permute_input_checkbox)
display(permute_encoding_checkbox)
display(predict_button)
display(output)
</code></pre>
","python, pytorch, nlp, huggingface-transformers","<p>The model inputs have token ids and position ids. There are four scenarios to consider:</p>
<ol>
<li>Baseline. Correct order for tokens and positions</li>
<li>Permute position ids only</li>
<li>Permute token ids only</li>
<li>Permute position ids and token ids</li>
</ol>
<p>You are correct that scenario 1 and 4 should produce the same results. However you are incorrect in assuming that permuting tokens or positions separately should give the same result. Consider:</p>
<pre class=""lang-py prettyprint-override""><code># Given:
tokens = [0, 1, 2]
positions = [0, 1, 2]
permutation = [2, 0, 1]

# Ex1: Permute tokens but not positions
[2, 0, 1] # permuted tokens
[0, 1, 2] # standard positions

# Ex2: Permute positions but not tokens
[0, 1, 2] # standard tokens
[2, 0, 1] # permuted positions
</code></pre>
<p>In <code>Ex1</code>, the model is told that token <code>2</code> occurs at position <code>0</code>. In <code>Ex2</code>, the model is told that token <code>2</code> occurs at position <code>1</code>. Even though we used the same permutation, the mapping of tokens to positions is different. This results in different model outputs.</p>
<p>The reason you sometimes see these results line up is because you can (through random chance) sample a permutation that results in token/position embeddings lining up the same way (or mostly the same way) when permuting just one of them. This is luck - the average case produces different results.</p>
<p>It is simple to test this. Huggingface models take a <code>position_ids</code> input parameter. We can use this to test permutations of the input ids without messing with the weight matrices.</p>
<p>To test this, we'll create input data, permute as needed, compute logits and compare logits.</p>
<p>When comparing logits, we will permute or depermute as needed to compare on a token to token basis. For example if token <code>i</code> in scenario 1 is permuted to token <code>j</code> in scenario 3, we want to compare logits <code>i</code> from scenario 1 to logits <code>j</code> in scenario 3.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import BertForMaskedLM, AutoTokenizer

def get_logits(inputs):
    with torch.no_grad():
        outputs = model(**inputs)  
        logits = outputs.logits
    return logits

def permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True):
    outputs = {}
    for k,v in inputs.items():
        if k=='position_ids' and permute_positions:
            outputs[k] = v[permutation]
        elif k!='position_ids' and permute_ids:
            outputs[k] = v[:,permutation]
        else:
            outputs[k] = v
            
    return outputs

# load tokenizer/model
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)
model.eval() # remember to set model to eval

# create input ids and position ids
inputs = tokenizer('input text test sequence', return_tensors='pt')

inputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))

# create permutation tensor
permutation = torch.randperm(inputs['input_ids'].shape[1])

# compute scenario data
data = {
    's1' : { # scenario 1 - baseline
        'inputs' : inputs,
        'permuted_ids' : False
    },
    's2' : { # scenario 2 - permute positions only
        'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True),
        'permuted_ids' : False
    },
    's3' : { # scenario 3 - permute token ids only
        'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False),
        'permuted_ids' : True
    },
    's4' : { # scenario 4 - permute tokens and positions
        'inputs' : permute_inputs(inputs, permutation),
        'permuted_ids' : True
    }
}

# compute logits
for k,v in data.items():
    v['logits'] = get_logits(v['inputs'])

comparisons = [
    ['s1', 's2'],
    ['s1', 's3'],
    ['s1', 's4'],
    ['s2', 's3'],
    ['s2', 's4'],
    ['s3', 's4'],
]

# compare scenarios 
for sa, sb in comparisons:
    data_a = data[sa]
    data_b = data[sb]
    
    logits_a = data_a['logits']
    logits_b = data_b['logits']
    
    if data_a['permuted_ids'] == data_b['permuted_ids']:
        # either both logits are permuted or both logits are unpermuted
        # so we can compare directly
        val = (logits_a - logits_b).abs().mean()
    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):
        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up
        val = (logits_a - logits_b[:,permutation]).abs().mean()
    else:
        # otherwise we permute `b` to make tokens line up
        val = (logits_a[:,permutation] - logits_b).abs().mean()
        
    print(f&quot;Comparison {sa}, {sb}: {val.item():.6f}&quot;)
</code></pre>
<p>The code should produce an output like:</p>
<pre><code>Comparison s1, s2: 1.407895
Comparison s1, s3: 1.583560
Comparison s1, s4: 0.000003
Comparison s2, s3: 1.750883
Comparison s2, s4: 1.407894
Comparison s3, s4: 1.583560
</code></pre>
<p>Run the code a bunch of times. You will find that the <code>S1, S4</code> comparison <em>always</em> has a small deviation. This is because permuting tokens and positions together always produces the same result, ignoring small deviations caused by numeric issues.</p>
<p>You will find the <code>S2, S3</code> comparison generally has a large deviation, but <em>sometimes</em> has a small deviation. As discussed, this is due to getting a lucky permutation where positions and ids mostly line up.</p>
"
Finetune LlaMA 7B model using Pytorch Lightning Framework,"<p>Need Expert help to solve this issue. LLaMA 7B model for sentiment classification with instructional Finetuning.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import LlamaTokenizer, LlamaForCausalLM, AdamW
from pytorch_lightning import LightningModule, Trainer, seed_everything
from datasets import load_dataset
import pandas as pd
import json

seed_everything(42)

DEVICE = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

class SentimentDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        prompt = f&quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501
        ### Instruction:
        {item[&quot;instruction&quot;]}
        ### Input:
        {item[&quot;input&quot;]}
        ### Response:
        {item[&quot;output&quot;]}&quot;&quot;&quot;
        return prompt

class SentimentClassifier(LightningModule):
    def __init__(self, base_model, learning_rate=2e-5):
        super().__init__()
        self.base_model = base_model
        self.tokenizer = LlamaTokenizer.from_pretrained(base_model)
        self.classifier = nn.Linear(self.base_model.config.hidden_size, 3)
        self.learning_rate = learning_rate

    def forward(self, input_ids, attention_mask):
        outputs = self.base_model(input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs.last_hidden_state
        logits = self.classifier(last_hidden_state[:, 0, :])
        return logits

    def training_step(self, batch, batch_idx):
        input_ids = batch[&quot;input_ids&quot;].to(self.device)
        attention_mask = batch[&quot;attention_mask&quot;].to(self.device)
        labels = batch[&quot;labels&quot;].to(self.device)
        logits = self(input_ids, attention_mask)
        loss = nn.CrossEntropyLoss()(logits, labels)
        self.log(&quot;train_loss&quot;, loss)
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids = batch[&quot;input_ids&quot;].to(self.device)
        attention_mask = batch[&quot;attention_mask&quot;].to(self.device)
        labels = batch[&quot;labels&quot;].to(self.device)
        logits = self(input_ids, attention_mask)
        loss = nn.CrossEntropyLoss()(logits, labels)
        self.log(&quot;val_loss&quot;, loss)

    def configure_optimizers(self):
        optimizer = AdamW(self.parameters(), lr=self.learning_rate)
        return optimizer

    def collate_fn(self, batch):
        encoding = self.tokenizer.batch_encode_plus(
            batch,
            padding=&quot;longest&quot;,
            truncation=True,
            return_tensors=&quot;pt&quot;
        )
        return {
            &quot;input_ids&quot;: encoding[&quot;input_ids&quot;].squeeze(),
            &quot;attention_mask&quot;: encoding[&quot;attention_mask&quot;].squeeze(),
            &quot;labels&quot;: encoding[&quot;input_ids&quot;].squeeze()
        }

train = load_dataset(&quot;json&quot;, data_files=&quot;alpaca-bitcoin-sentiment-dataset_train.json&quot;)
validation = load_dataset(&quot;json&quot;, data_files=&quot;alpaca-bitcoin-sentiment-dataset_test.json&quot;)

train_dataset = SentimentDataset(train)
val_dataset = SentimentDataset(validation)

LEARNING_RATE = 2e-5
BATCH_SIZE = 8

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

from peft import (
    LoraConfig,
    get_peft_model,
    get_peft_model_state_dict,
    prepare_model_for_int8_training,
)

BASE_MODEL = &quot;decapoda-research/llama-7b-hf&quot;
 
model = LlamaForCausalLM.from_pretrained(
    BASE_MODEL,
    load_in_8bit=True,
    torch_dtype=torch.float16,
    device_map=0,
)
 
tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)
 
tokenizer.pad_token_id = (
    0  # unk. we want this to be different from the eos token
)
tokenizer.padding_side = &quot;left&quot;

LORA_R = 8
LORA_ALPHA = 16
LORA_DROPOUT= 0.05
LORA_TARGET_MODULES = [
    &quot;q_proj&quot;,
    &quot;v_proj&quot;,
]
 
model = prepare_model_for_int8_training(model)
config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=LORA_TARGET_MODULES,
    lora_dropout=LORA_DROPOUT,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;,
)
model = get_peft_model(model, config)
model.print_trainable_parameters()
</code></pre>
<p>Output:</p>
<pre><code>trainable params: 4194304 || all params: 6742609920 || trainable%: 0.06220594176090199
</code></pre>
<pre><code>model = SentimentClassifier(model, learning_rate=LEARNING_RATE)
</code></pre>
<pre><code>HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' 
cannot start or end the name, max length is 96: 'PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096, padding_idx=31999)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): Linear8bitLt(
                in_features=4096, out_features=4096, bias=False
</code></pre>
<p>I was trying to finetune the LLaMA 7B model using pytorch Lightning framework. But unable to do so.</p>
","pytorch, nlp, pytorch-lightning, llama",
How to Improve Apache OpenNLP Output Using Custom Trained Models in Spring Boot?,"<p>I'm currently using Apache OpenNLP with a Spring Boot REST API, and I'm facing some issues with the output of the model. Here's what I'm experiencing:</p>
<ul>
<li><p><strong>Input text:</strong> <code>&quot;I'm Arun Vinc. What is your name?&quot;</code></p>
</li>
<li><p><strong>Output:</strong> <code>'m ? arun i is name vinc what your</code></p>
</li>
</ul>
<p>The output isn't coherent, and I suspect that the pre-trained models I'm using might not be well-suited for my application. After researching online, I found that I might need to train my own <code>.bin</code> models using appropriate corpora datasets to get better results.</p>
<p><strong>Spring Boot REST API:</strong> This is the code I’m using to load the models and perform the NLP tasks:</p>
<pre><code>@Service
public class OpenNLPService {

    private SentenceDetectorME sentenceDetector;
    private TokenizerME tokenizer;
    private POSTaggerME posTagger;

    public OpenNLPService() throws Exception {
        try (InputStream sentenceModelIn = getClass().getClassLoader().getResourceAsStream(&quot;models/en-sent.bin&quot;);
             InputStream tokenModelIn = getClass().getClassLoader().getResourceAsStream(&quot;models/en-token.bin&quot;);
             InputStream posModelIn = getClass().getClassLoader().getResourceAsStream(&quot;models/en-pos-maxent.bin&quot;)) {

            SentenceModel sentenceModel = new SentenceModel(sentenceModelIn);
            TokenizerModel tokenModel = new TokenizerModel(tokenModelIn);
            POSModel posModel = new POSModel(posModelIn);

            this.sentenceDetector = new SentenceDetectorME(sentenceModel);
            this.tokenizer = new TokenizerME(tokenModel);
            this.posTagger = new POSTaggerME(posModel);
        }
    }

    public String processText(String text) {
        String[] sentences = sentenceDetector.sentDetect(text);
        StringBuilder result = new StringBuilder();

        for (String sentence : sentences) {
            String[] tokens = tokenizer.tokenize(sentence);
            String[] posTags = posTagger.tag(tokens);
            result.append(String.join(&quot; &quot;, tokens)).append(&quot;\n&quot;);
        }

        return result.toString();
    }
}
</code></pre>
<h3>What I’ve Tried</h3>
<ol>
<li><p><strong>Pre-Trained Models:</strong> I’m currently using the default pre-trained models (<code>en-sent.bin</code>, <code>en-token.bin</code>, <code>en-pos-maxent.bin</code>) from Apache OpenNLP.</p>
</li>
<li><p><strong>Research:</strong> I’ve read that the accuracy and relevance of these models can be improved by training them with a more suitable corpus. However, I’m not sure which corpora would be best for general English text processing, nor how to properly train these models.</p>
</li>
</ol>
<h3>Error Messages and Debugging</h3>
<ul>
<li><strong>No specific errors</strong>, but the output is not meaningful, which suggests that the model may not be well-suited for the input data.</li>
</ul>
","spring-boot, rest, nlp, opennlp",
How does OpenAIEmbeddings() work? Is it creating a single vector of size 1536 for whole text corpus?,"<p>I'm working with the <code>OpenAIEmbeddings()</code> class from <code>OpenAI</code>, which uses the <code>text-embedding-3-small</code> model. According to the <a href=""https://platform.openai.com/docs/guides/embeddings/what-are-embeddings"" rel=""nofollow noreferrer"">documentation</a>, it generates a 1536-dimensional vector for any input text.</p>
<p>However, I'm a bit confused about how this works:</p>
<ul>
<li>Is the 1536-dimensional vector generated for the entire input text?</li>
<li>If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?</li>
</ul>
<p><strong>I was expecting this:</strong></p>
<p>If there are 100 words in my input text, i expected that OpenAIEmbeddings() would output 100 vectors, each having size 1536.</p>
<p>But the output is a single vector of size 1536 for the whole input text.</p>
<p>Why I expected this?</p>
<p>Because in my learning, i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus. How does this differ from the approach taken by OpenAIEmbeddings?</p>
<p>I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input.</p>
<p>Any insights or examples would be greatly appreciated!</p>
","deep-learning, nlp, openai-api, openaiembeddings","<p>Everything you described is 100% expected.</p>
<h3>Q: Is the 1536-dimensional vector generated for the entire input text?</h3>
<p>A: Yes.</p>
<h3>Q: If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?</h3>
<p>A: First, the OpenAI Embeddings model doesn't handle a single word any different than a long text. For the model, it's an input. The input can be even a single character (e.g., &quot;a&quot;), but it doesn't make sense to calculate an embedding vector out of it since &quot;a&quot; doesn't semantically mean anything to us humans.</p>
<p>Second, what you probably meant with this question is what happens when you do a similarity search with these embeddings. In other words, what happens when you <em>use</em> them? What happens if you use embeddings of words, sentences, paragraphs, or the whole text? Does it matter? Yes!</p>
<p>This is called chunking. The decision about how to chunk your text depends on the use case. The best thing is probably to simply try and see. If you get meaningful results after doing a similarity search, then this means that chunking is appropriate (even if this means chunking the whole text). If you don't get meaningful results after doing a similarity search, then this means that chunking isn't appropriate (e.g., instead of chunking by paragraph, try chunking by sentences).</p>
<p>There's an excellent Stack Overflow <a href=""https://stackoverflow.blog/2024/06/06/breaking-up-is-hard-to-do-chunking-in-rag-applications/"">blog post</a> about this topic you should read (pay attention to the bolded text because this is the best explanation):</p>
<blockquote>
<p>With RAG, you create text embeddings of the pieces of data that you
want to draw from and retrieve. That allows you to place a piece of
the source text within the semantic space that LLMs use to create
responses.</p>
<p>/.../</p>
<p>When it comes to RAG systems, you’ll need to pay special attention to
how big the individual pieces of data are. How you divide your data up
is called chunking, and it’s more complex than embedding whole
documents.</p>
<p>/.../</p>
<p>The size of the chunked data is going to make a huge difference in
what information comes up in a search. When you embed a piece of data,
the whole thing is converted into a vector. <strong>Include too much in a
chunk and the vector loses the ability to be specific to anything it
discusses. Include too little and you lose the context of the data.</strong></p>
</blockquote>
"
UDPipe 2 Models in R,"<p>I am not entirely happy with udpipe 2.5. Latin models available through the library <code>udpipe</code>, and looking for a way to use newer (2.12 or 2.14) models listed <a href=""https://ufal.mff.cuni.cz/udpipe/2/models#universal_dependencies_212_models"" rel=""nofollow noreferrer"">here</a>.</p>
<p>However, it seems that they are only available in Python:</p>
<blockquote>
<p>Compared to UDPipe 1, it is Python-only, it does not perform tokenization, and the models require more computation power (<a href=""https://ufal.mff.cuni.cz/udpipe/2"" rel=""nofollow noreferrer"">here</a>).</p>
</blockquote>
<p>I am not a Python user at all, is there a way to use these models in R?</p>
<p>So far, I have been able to download a huge archive with the newer conllu-files (from <a href=""https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-5502"" rel=""nofollow noreferrer"">here</a>) and trying to train a model as described <a href=""https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-train.html"" rel=""nofollow noreferrer"">here</a>. However, it does not lead to a significantly better result.</p>
","r, nlp, udpipe",
spacy doc.char_span raises error whenever there is any number in string,"<p>I was trying to train a model from spacy. I have strings and their token offsets saved into the JSON file.</p>
<p>I have read that file using <code>utf-8</code> encoding and there is no special character in it. But it raises <code>TypeError: object of type 'NoneType' has no len()</code></p>
<pre class=""lang-py prettyprint-override""><code># code for reading file
with open(&quot;data/results.json&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as file:
    training_data = json.loads(file.read())
</code></pre>
<p>I have also tried changing <code>alignment_type</code> from <code>strict</code> to <code>contract</code> &amp; <code>expand</code>. The <code>expand</code> works but shows incorrect spans.</p>
<pre class=""lang-py prettyprint-override""><code>span = doc.char_span(start, end, label, alignment_mode=&quot;contract&quot;)
</code></pre>
<p>The code that I'm using</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.tokens import DocBin

nlp = spacy.blank(&quot;en&quot;)
db = DocBin()
training_dataset = [[
        &quot;Department of Chemistry,Central University of Las Villas,Santa Clara,Villa Clara,54830,Cuba.&quot;,
        [
            [
                57,
                68,
                &quot;city_name&quot;
            ],
            [
                87,
                91,
                &quot;country_name&quot;
            ]
        ]
    ]]
for text, annotations in training_dataset:
    doc = nlp(text)
    ents = []
    for start, end, label in annotations:
        span = doc.char_span(start, end, label)
        ents.append(span)
    doc.ents = ents
    db.add(doc)
</code></pre>
<p>I have pasted the JSON object that is read from the file, directly into the program for debugging purposes.</p>
<p>When I tried after removing the <code>54830,</code> part, the program runs successfully.</p>
<p>I have also referred to this <a href=""https://stackoverflow.com/questions/69976538/spacy-preparing-training-data-doc-char-span-returning-none"">Issue</a>, but that issue has a special character. But this string doesn't have any special character.</p>
<p>Can anyone know why this is happening with all strings that contain a number in them?</p>
","python, json, nlp, spacy, spacy-3","<p>The error <code>TypeError: object of type 'NoneType' has no len()</code> occurs in line <code>doc.ents = ents</code> when one of the entries in <code>ents</code> is <code>None</code>.</p>
<p>The reason for having a <code>None</code> in the list is that <code>doc.char_span(start, end, label)</code> returns <code>None</code> when the <code>start</code> and <code>end</code> provided don't align with token boundaries.</p>
<p>The tokenizer of the model (<code>spacy.blank(&quot;en&quot;)</code>) doesn't behave as needed for this use case. It seems that it doesn't produce an end of token after a comma  that follows a number without space after the comma.</p>
<p>Examples:</p>
<p>Tokenizing a number with decimals:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; nlp = spacy.blank(&quot;en&quot;)
&gt;&gt;&gt; nlp.tokenizer.explain(&quot;5,1&quot;)
[('TOKEN', '5,1')]
</code></pre>
<p><strong>One</strong> single token.</p>
<p>Tokenizing a number + comma + letter:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; nlp.tokenizer.explain(&quot;5,a&quot;)
[('TOKEN', '5,a')]
</code></pre>
<p><strong>One</strong> single token.</p>
<p>Tokenizing a letter + comma + letter:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; nlp.tokenizer.explain(&quot;a,a&quot;)
[('TOKEN', 'a'), ('INFIX', ','), ('TOKEN', 'a')]
</code></pre>
<p><strong>Three</strong> tokens.</p>
<p>Tokenizing a number + comma + space + letter:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; nlp.tokenizer.explain(&quot;5, a&quot;)
[('TOKEN', '5'), ('SUFFIX', ','), ('TOKEN', 'a')]
</code></pre>
<p><strong>Three</strong> tokens.</p>
<p>Tokenizing a number + comma + space + number:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; nlp.tokenizer.explain(&quot;5, 1&quot;)
[('TOKEN', '5'), ('SUFFIX', ','), ('TOKEN', '1')]
</code></pre>
<p><strong>Three</strong> tokens.</p>
<p>Therefore, with the default tokenizer, a space is needed after a comma following a number so the comma is used to create the token boundaries.</p>
<p>Workarounds:</p>
<ul>
<li>Preprocess your text to add a space after the commas you desire to split tokens by. This would also require to update the <code>start</code> and <code>end</code> values of the annotations.</li>
<li>Create your custom tokenizer as described in Spacy documentation: <a href=""https://spacy.io/usage/linguistic-features#native-tokenizers"" rel=""noreferrer"">https://spacy.io/usage/linguistic-features#native-tokenizers</a></li>
</ul>
"
"RuntimeError: Expected 3-dimensional tensor, but got 2-dimensional tensor for argument","<p>I have two tensors names: <code>wy</code> and <code>x</code>, both of them with size 8:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

wy = torch.tensor([[7.2, -2.9, 5.2, -8.4, -3.8, -6.9, 7.4, -8.1]])

x = torch.tensor([[70., 77., 101., 75., 40., 83., 48., 73.]])
</code></pre>
<p>Now, I want to do bmm to multiply <code>x * wy</code> as follow:</p>
<pre class=""lang-py prettyprint-override""><code>xWy = x.bmm(wy.unsqueeze(2)).squeeze(3)
</code></pre>
<p>I got an error:</p>
<blockquote>
<p>RuntimeError: Expected 3-dimensional tensor, but got 2-dimensional tensor for argument #1 'batch1' (while checking arguments for bmm)</p>
</blockquote>
<p><code>8*8</code> should be possible. but I don't know why I got this error every time.</p>
<p>any help, please!</p>
","python, pytorch, nlp, matrix-multiplication, allennlp","<p><a href=""https://pytorch.org/docs/stable/generated/torch.bmm.html"" rel=""nofollow noreferrer""><code>bmm</code></a> stands for <strong>batch</strong> matrix-matrix product. So it expects <strong>both</strong> tensors with a batch dimension (i.e., 3D as the error says).</p>
<p>For single tensors, you want to use <a href=""https://pytorch.org/docs/stable/generated/torch.mm.html"" rel=""nofollow noreferrer""><code>mm</code></a> instead. Note that <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.mm.html"" rel=""nofollow noreferrer""><code>Tensor.mm()</code></a> also exists with the same behaviour.</p>
<pre><code>x.mm(wy.transpose(0, 1))
</code></pre>
<pre><code>tensor([[-5051.9199]])
</code></pre>
<p>Or better, for two 1D tensor you can use <a href=""https://pytorch.org/docs/stable/generated/torch.dot.html"" rel=""nofollow noreferrer""><code>dot</code></a> for dot product.</p>
<pre><code># Or simply do not initialise them with an additional dimension. Not needed.
x.squeeze().dot(wy.squeeze())
</code></pre>
<pre><code>tensor(-5051.9199)
</code></pre>
"
How to Disable Safety Settings in Gemini Vision Pro Model Using API?,"<p>Question:</p>
<p>I'm currently working with the Gemini Vision Pro model and I'm encountering issues with certain images while using the model through its API. Here's a simplified version of the code I'm using to access the model:</p>
<p>python:-</p>
<pre><code>import google.generativeai as genai
import streamlit as st

def Gemini_vision(prompt, image):
    st.markdown(&quot;&lt;p style='text-align:center;'&gt;Image will be used from here, Delete image when you are done ✅&lt;/p&gt;&quot;, unsafe_allow_html=True)
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-pro-vision')
    safe = [
        {
            &quot;category&quot;: &quot;HARM_CATEGORY_DANGEROUS&quot;,
            &quot;threshold&quot;: &quot;BLOCK_NONE&quot;,
        },
        {
            &quot;category&quot;: &quot;HARM_CATEGORY_HARASSMENT&quot;,
            &quot;threshold&quot;: &quot;BLOCK_NONE&quot;,
        },
        {
            &quot;category&quot;: &quot;HARM_CATEGORY_HATE_SPEECH&quot;,
            &quot;threshold&quot;: &quot;BLOCK_NONE&quot;,
        },
        {
            &quot;category&quot;: &quot;HARM_CATEGORY_SEXUALLY_EXPLICIT&quot;,
            &quot;threshold&quot;: &quot;BLOCK_NONE&quot;,
        },
        {
            &quot;category&quot;: &quot;HARM_CATEGORY_DANGEROUS_CONTENT&quot;,
            &quot;threshold&quot;: &quot;BLOCK_NONE&quot;,
        },
    ]
    response = model.generate_content([image[0], prompt], safety_settings=safe)
    return response.text
</code></pre>
<p>The issue arises when I try to generate content using certain images. The model throws an error, but strangely, it works fine with other images. I suspect that the safety settings in the model might be causing this discrepancy.</p>
<p>I got this error while uploading my selfie and asked about something:-</p>
<pre><code>google.api_core.exceptions.InvalidArgument: This app has encountered an error. The original error message is redacted to prevent data leaks. Full error details have been recorded in the logs (if you're on Streamlit Cloud, click on 'Manage app' in the lower right of your app).
</code></pre>
<p>My Question:</p>
<p>How can I disable or turn off all safety settings in the Gemini Vision Pro model using the API?
Are there any additional parameters or configurations I need to consider while making this adjustment?
I've tried passing an empty list for the safety_settings parameter, but it doesn't seem to have any effect. Any insights or suggestions on how to handle this situation would be greatly appreciated.</p>
<p>Thank you in advance!</p>
<p>Feel free to customize the question further or let me know if you need any modifications!</p>
","python, nlp, google-gemini, google-generativeai",
Poor lemmatization in Apple&#39;s Natural Language framework,"<p>I did a quick experiment to check the accuracy of lemmatization in Apple's Natural Language framework and the results are quite poor.</p>

<p>I wonder if I am doing something wrong or if the framework is really that bad.</p>

<p>For the experiment, I used code straight from Apple's documentation (which also gets repeated in the few online examples I could find).</p>

<pre><code>let tagger = NLTagger(tagSchemes: [.lemma])
tagger.string = text
let options: NLTagger.Options = [.omitPunctuation, .omitWhitespace]
tagger.enumerateTags(in: text.startIndex..&lt;text.endIndex, unit: .word, scheme: .lemma, options: options) { tag, tokenRange in
    print(""\(text[tokenRange]): \(tag?.rawValue ?? ""NO LEMMA"")"")
    return true
}
</code></pre>

<p>To test the output, I took a paragraph from a Euronews article, which is available in multiple languages.</p>

<p>The English version seems accurate, but in English, most words coincide with their lemma, so it's not a great benchmark.</p>

<p>I'm running the code in an Xcode Playground, on macOS 10.14.6. I tried both <em>macOS</em> and <em>iOS</em> as the platform for the playground, which makes no difference.</p>

<pre><code>let text = ""For the possible necessity of short-time work I want to make sure to build an incentive with connecting it to training. And I want Germany to be able to implement short-time work faster in case of a fast recession of the economic situation because of global economic risks.""

// Output

For: for
the: the
possible: possible
necessity: necessity
of: of
short: short
time: time
work: work
I: I
want: want
to: to
make: make
sure: sure
to: to
build: build
an: an
incentive: incentive
with: with
connecting: connect
it: it
to: to
training: training
And: and
I: I
want: want
Germany: Germany
to: to
be: be
able: able
to: to
implement: implement
short: short
time: time
work: work
faster: fast
in: in
case: case
of: of
a: a
fast: fast
recession: recession
of: of
the: the
economic: economic
situation: situation
because: because
of: of
global: global
economic: economic
risks: risk
</code></pre>

<p>I then tried Italian, which is my native language, so I can verify it easily. Here I started to see some problems.</p>

<pre><code>let text = ""Voglio assicurare che siano creati degli incentivi nel settore del lavoro a orario ridotto, collegati con periodi di training. E voglio che la Germania sia in grado di incrementare questo tipo di offerta lavorativa in modo veloce, in caso di recessione dell'economia per non farsi travolgere dai rischi che a livello globale subiremmo.""

// Output

Voglio: volersi
assicurare: assicurare
che: che
siano: essersi
creati: crearsi
degli: degli
incentivi: incentivo
nel: nel
settore: settore
del: del
lavoro: lavoro
a: a
orario: orario
ridotto: ridotto
collegati: collegarsi
con: con
periodi: periodo
di: di
training: training
E: e
voglio: volersi
che: che
la: la
Germania: Germania
sia: essersi
in: in
grado: grado
di: di
incrementare: incrementare
questo: questo
tipo: tipo
di: di
offerta: offerta
lavorativa: lavorativo
in: in
modo: modo
veloce: veloce
in: in
caso: caso
di: di
recessione: recessione
dell'economia: economia
per: per
non: non
farsi: farsi
travolgere: travolgere
dai: dai
rischi: rischio
che: che
a: a
livello: livello
globale: globale
subiremmo: subire
</code></pre>

<p>Here some verbs get strange lemmas: ""volersi"",  ""essersi"", ""crearsi"" are not the correct infinitive version of these verbs. For some reason, they are in a reflexive form. The problem though is that in many parts of the sample sentence they are not used reflexively.</p>

<p>But it's when I try with Russian (which I speak at an intermediate level) that things really fall apart.</p>

<pre><code>let text = ""Чтобы быть готовыми к потенциальному появлению необходимости в краткосрочной работе, я хочу создать возможности для обучения. Я хочу, чтобы Германия могла быстро выполнять работу в самые сжатые сроки в случае быстрого спада экономики из-за нависших над ней глобальных рисков.""

// Output

Чтобы: чтобы
быть: быть
готовыми: NO LEMMA
к: к
потенциальному: NO LEMMA
появлению: NO LEMMA
необходимости: NO LEMMA
в: в
краткосрочной: NO LEMMA
работе: NO LEMMA
я: я
хочу: NO LEMMA
создать: NO LEMMA
возможности: NO LEMMA
для: для
обучения: NO LEMMA
Я: я
хочу: NO LEMMA
чтобы: чтобы
Германия: Германия
могла: мочь
быстро: NO LEMMA
выполнять: NO LEMMA
работу: NO LEMMA
в: в
самые: самый
сжатые: NO LEMMA
сроки: NO LEMMA
в: в
случае: случай
быстрого: NO LEMMA
спада: спад
экономики: NO LEMMA
из: из
за: за
нависших: NO LEMMA
над: над
ней: ней
глобальных: NO LEMMA
рисков: риск
</code></pre>

<p>Here most words produce no lemma. These are not rare words, and in my opinion, they should not be hard to lemmatize either (but I am no NLP expert).</p>

<p>For example, the word ""быстро"" (fast, quickly) is among the top 300 most common Russian words and is an adverb of the adjective ""быстрый"". That's definitely a word I would expect a lemmatizer to recognize. Like the word ""хочу"" which is ""I want"".</p>

<p>The Italian output already leaves me perplexed, but the Russian one is definitely unusable.</p>

<p>Am I doing something wrong, or is Apple's framework really that bad? </p>
","ios, macos, nlp, lemmatization",
How to use both gpus in kaggle for training in pytorch?,"<p>I was training a model in kaggle gpu.
But as I can see only one GPU is working.
<a href=""https://i.sstatic.net/8E0H3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8E0H3.png"" alt=""enter image description here"" /></a>
I use the ordinary method for training like</p>
<pre class=""lang-py prettyprint-override""><code>device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = model.to(device)
</code></pre>
<p>How can I use both the gpus?</p>
","machine-learning, deep-learning, nlp, gpu, kaggle","<p>Using multiple GPUs is specific to machine learning libraries. I stumbled upon the same problem while doing image segmentation in Pytorch. The solution is to use the module <a href=""https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html"" rel=""noreferrer"">torch.nn.DataParallel()</a> with the model. The given code can be changed as follows:</p>
<pre><code>device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)
</code></pre>
<p>here, the <code>device_ids</code> is the index of GPUs. Suppose if you have 4 GPUs then it would be <code>device_ids = [0,1,2,3]</code> or whatever the index it maybe.</p>
<p>And the result of
<a href=""https://i.sstatic.net/pc8GW.png"" rel=""noreferrer"">using both GPUs</a> is here!.</p>
<p>PS: This is my first post in the prestigious stack overflow, please do share your comments and views.</p>
"
Does Padding in a Batch of Sequences Affect Performance? How Effective is the Attention Mask?,"<p>In Transformer models, sequences of variable lengths are typically padded to the maximum length in a batch. However, if my sequence lengths vary significantly, the batch may contain a substantial amount of padding (potentially over 50%).</p>
<p>I am curious about the following:</p>
<p>When PyTorch computes the Transformer, do padding tokens impact calculation speed negatively?
Does the presence of the attention mask allow the model to effectively skip over padding tokens, resulting in only a minimal performance impact?</p>
<p>Overall, how effective is the attention mask? If I have a sparse attention mask with only 10% non-zero values, does the computation effectively reduce to approximately 10%?</p>
<p>Thank you for your insights!</p>
","pytorch, nlp, huggingface-transformers, transformer-model","<p>Attention is computed on a tensor of shape <code>(batch_size, sequence_length, embedding_dimension)</code>. The compute and memory requirements scale with the size of those dimensions.</p>
<p>For an input of fixed size, the percent padding does not impact performance. There is some minor overhead from applying a padding mask at all (ie not having a padding mask saves you one mask fill operation), but between x% padding and y% padding you're not going to see a difference. The overall compute requirements are set by the tensor size.</p>
<p>With respect to batching sequences, there can be added inefficiencies for batching together sequences of wildly different length. Say you have 10 sequences of length <code>8</code> and 10 sequences of length <code>128</code>. Now pad and batch those sequences into two batches. If you mix lengths evenly, you get two batches with a sequence length of <code>128</code>. If you sort by length before batching, you get one batch with sequence length of <code>8</code> and another with length <code>128</code>. The first case (two batches of sequence length 128) requires overall more compute compared to the second case (one batch of 8, one of 128).</p>
<p>That said, for a fixed input size, you aren't going to see a performance change from the percent padding. There is no way for the attention operation to &quot;skip over&quot; padding tokens. The conditional control flow required for that sort of approach doesn't work well with the way GPUs execute operations in parallel. The only effect of the padding mask is it assigns 0 attention weight to padding tokens.</p>
"
llamaindex python program execution endless,"<p>I hope someone can help me with llamaindex. I try to play with it, and I am at the beginning. I have created a program for indexing a PDF document (741 pages). This program is in main.py.</p>
<p>And then I try to query the document PDF, which is indexed in a JSON file. The program is query1.py.</p>
<p>main.py created successfully index in JSON files.
Once I execute query1.py, and the CPU is 350% and execution is endless.</p>
<p>Someone, could tell me if something is wrong in my program or if optimizations should be done? I am novice so surely I don't use correctly llamaindex.</p>
<p><strong>Screenshots :</strong>
<a href=""https://i.sstatic.net/jyeGImRF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jyeGImRF.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/VZVj0Kth.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VZVj0Kth.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/3G9Hox3l.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3G9Hox3l.png"" alt=""enter image description here"" /></a></p>
<p><strong>main.py</strong></p>
<pre><code>from llama_index.core import SimpleDirectoryReader
from llama_index.core import GPTVectorStoreIndex
from sentence_transformers import SentenceTransformer
from llama_index.core.base.embeddings.base import BaseEmbedding
from llama_index.core import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

class LocalEmbeddingModel(BaseEmbedding):
  def __init__(self):
    self.model = SentenceTransformer('all-MiniLM-L6-v2')

  def embed(self, texts):
    return self.model.encode(texts)

# Initialiser le modèle d'embeddings local
#embed_model = LocalEmbeddingModel()

# Configurer LlamaIndex avec les paramètres appropriés
#settings = Settings(embed_model=embed_model)
Settings.embed_model = HuggingFaceEmbedding(model_name=&quot;BAAI/bge-small-en-v1.5&quot;)

#service_context = ServiceContext.from_defaults(embed_model=None)

# Charger les documents à partir d'un répertoire
documents = SimpleDirectoryReader('./mesdoc').load_data()

# Créer un index à partir des documents
index = GPTVectorStoreIndex(documents)

# Sauvegarder l'index pour une utilisation future
#index.save_to_disk('index.json')
index.storage_context.persist(persist_dir=&quot;./mesdoc&quot;)
</code></pre>
<p><strong>query1.py</strong></p>
<pre><code>from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import ServiceContext

# Configurer LlamaIndex avec les paramètres appropriés
Settings.embed_model = HuggingFaceEmbedding(model_name=&quot;BAAI/bge-small-en-v1.5&quot;)


# Fonction pour poser des questions
def ask_question(question):
  # Recharger l'index depuis le répertoire de persistance
  # Load documents and build index
  documents = SimpleDirectoryReader(&quot;./mesdoc&quot;).load_data()
  index = VectorStoreIndex.from_documents(documents)
  #index = GPTVectorStoreIndex.load_from_storage(persist_dir=&quot;./&quot;,)

  # Poser la question et obtenir une réponse
  query_engine = index.as_query_engine()
  response = query_engine.query(question)
  return response

# Exemple de question
question = &quot;What is tasklist ?&quot;
answer = ask_question(question)
print(f&quot;Réponse : {answer}&quot;)
</code></pre>
","python, nlp, llama-index",
Spacy hanging for badly formatted texts,"<p>I am using spacy (<a href=""https://spacy.io/usage/spacy-101"" rel=""nofollow noreferrer"">https://spacy.io/usage/spacy-101</a>) with the &quot;en_core_web_sm&quot; model to chunk lots of documents using sentence detection (sentor component). The documents have no structure and no specific formatting, and a few of them have extremely bad formatting.</p>
<p>Most of the time, our code is working well. However, there are rare cases where spacy encounters a badly formatted document, and hangs indefinitely: For example,</p>
<pre><code>import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(&quot;document containing lots of '''''''&quot;) # hangs here
</code></pre>
<p>The problem is the sentence chunking is happening inside a loop, and if one hangs, the rest will not be processed.</p>
<p>Is there a way for spacy to throw an error if it encounters a text that it can't handle? Or what would be the ideal approach?</p>
","python-3.x, nlp, spacy",
How to evaluate SciSpaCy&#39;s entity linking,"<p>I'm using <a href=""https://github.com/allenai/scispacy#entitylinker"" rel=""nofollow noreferrer"">SciSpaCy's Entity Linker</a> with a custom knowledge base. As I'm updating some components of my application (e.g. the underlying language model, sentence tokenization pipeline, the knowledge base itself, etc), I'm noticing that (1) the number of entities that the application picks up changes and (2) the linked concepts themselves change (not the detected entities but the concepts that are linked to these entities). With this in mind, I'd like to be able to evaluate my entity-linking application.</p>
<p>Unfortunately, I cannot seem to find any resources for that. I was hoping to find either an evaluation library of some sort (assuming we are not just interested in a confusion matrix) or a &quot;gold standard&quot; dataset with entities in various forms (e.g. abbreviated, inflected, etc) and the expected linked concept.</p>
<p>I'm afraid I'm a novice in this field which is why I'm reaching out here, hoping that anyone might be able to point me to a set of useful resources or share some tips with me.</p>
<p>Many thanks in advance.</p>
","nlp, spacy, entity-linking",
How to optimize this function and improve running time?,"<p>I have function aimed at creating a data-frame with three columns; bigram-phrase, count (of the bigram-phrase), and PMI score (for the bigram-phrase). Since I want to run this on a large dataset with over a million phrases, the compute time is incredibly long. I recognize that the nested for-loops and matching conditions are contributing to the computation difficulties. Is there an alternative way to do the same thing and cut down run-time?</p>
<p>Here's my code:</p>
<pre><code>def pmi_count_phrase_create(pmi_tups,freq_list):

    import pandas as pd

    &quot;&quot;&quot;pmi_tups is result of running pmi_tups = [i for i in finder.score_ngrams(bigram_measures.pmi)]  
       freq_list is a result of running freq_list= finder.ngram_fd.items() 
       
       -&gt; df made up of columns for  pmi list, count list, phrase list&quot;&quot;&quot;
    pmi3_list =[]
    count3_list =[]
    phrase3_list =[]
    for phrase, pmi in pmi_tups: #pmi_tups is list of tuples of form:[((phrase),pmi),..]
        for item in freq_list:  
            quadgram,count = item
            if quadgram == phrase:
                pmi3_list.append(pmi)
                count3_list.append(count)
                phrase3_list.append(phrase)

                # create dataframe
    df = pd.DataFrame({'Phrase':phrase3_list,'PMI':pmi3_list,'Count':count3_list})
    return df 
</code></pre>
<p>Running this code on my pmi_tups and freq_list, it is still running and it's been over 1000 minutes. I'm open to also using a different library to evaluate the bi-gram phrases, pmi's and frequencies.</p>
","python, performance, optimization, nlp, nltk","<p>Ended up changing my function to convert freq_list to a dictionary and list comprehensions instead of for loops and this code instantly returned a data-frame:</p>
<pre><code>def quicker_func(pmi_tups, freq_list):
    import pandas as pd
    freq_dict = dict(freq_list)  # Create a dictionary for faster lookups 

    pmi_list = [pmi for phrase, pmi in pmi_tups if phrase in freq_dict]
    count_list = [freq_dict[phrase] for phrase, pmi in pmi_tups if phrase in freq_dict]
    phrase_list = [phrase for phrase, pmi in pmi_tups if phrase in freq_dict]

    df = pd.DataFrame({'Phrase': phrase_list, 'PMI': pmi_list, 'Count': count_list})
    return df
</code></pre>
"
Why does importing NLTK in python gives error,"<p>I upgraded NLTK to latest version and while import nltk, I get the following error</p>

<pre><code>import nltk
</code></pre>

<p>File ""C:\ProgramData\Anaconda2\lib\site-packages\nltk\tag\sequential.py"", line 210
    print(""[Trained Unigram tagger:"", end="" "")
                                         ^
SyntaxError: invalid syntax</p>

<p>I commented this line but then another error. python version used is 2.7</p>
","nlp, nltk, token",
How to parse multiple chunks in nltk?,"<p>is it possible to parse multiple chunks in a single nltk.regexp parser?</p>
<p>can grammar have multiple chunks define like this?</p>
<pre><code>def parser(s):
    grammar = &quot;&quot;&quot;
    NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;+}
    VN: {&lt;VB.?&gt;&lt;DT&gt;?&lt;NN&gt;}
    &quot;&quot;&quot;
    words = nltk.word_tokenize(s)
    tagged_words = nltk.pos_tag(words)
    cp = nltk.RegexpParser(grammar)
    chunked = cp.parse(tagged_words)
    print(chunked)
    for _ in chunked:
        if isinstance(_, nltk.tree.Tree):
            if _.label() == 'NP':
                print(_.leaves(), 'Noun Phrase')
            if _.label() == 'VN':
                print(_.leaves(),' a verbed noun')
</code></pre>
<p>The result do not include VN chunks</p>
<pre><code>(S
  i/NNS
  need/VBP
  (NP a/DT project/NN manager/NN)
  who/WP
  knows/VBZ
  (NP c++/NN))
[('a', 'DT'), ('project', 'NN'), ('manager', 'NN')] Noun Phrase
[('c++', 'NN')] Noun Phrase
</code></pre>
","python, regex, nlp, nltk",
Excluding the Header and Footer Contents of a page of a PDF file while extracting text?,"<p>Is it possible to exclude the <code>contents of footers and headers of a page</code> from a pdf file during extracting the text from it. As these contents are least important and almost redundant.</p>

<p>Note: For extracting the text from the .pdf file, I am using the PyPDF2 package on python version = 3.7. </p>

<p>How to exclude the contents of the footers and headers in PyPDF2. Any help is appreciated.</p>

<p>The code snippet is as follows:</p>

<pre><code>import PyPDF2

def Read(startPage, endPage):
    global text
    text = []
    cleanText = "" ""
    pdfFileObj = open('C:\\Users\\Rocky\\Desktop\\req\\req\\0000 - gamma j.pdf', 'rb')
    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)
    num_pages = pdfReader.numPages
    print(num_pages)
    while (startPage &lt;= endPage):
        pageObj = pdfReader.getPage(startPage)
        text += pageObj.extractText()
        startPage += 1
    pdfFileObj.close()
    for myWord in text:
        if myWord != '\n':
           cleanText += myWord
    text = cleanText.strip().split()
    print(text)

Read(1, 1)
</code></pre>
","python-3.x, pdf, text, nlp, pypdf",
High VRAM usage after loading peft weights with the base model,"<p>I have fine-tuned llama-3 using qlora technique and wanted to do inference.
I use the below code for inference.</p>
<pre><code>bnb_config = BitsAndBytesConfig(
    load_in_4bit= True,
    bnb_4bit_quant_type= &quot;nf4&quot;,
    bnb_4bit_compute_dtype= torch.bfloat16,
    bnb_4bit_use_double_quant= False)

access_token = ''

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,    quantization_config = bnb_config,
    device_map=&quot;auto&quot;,    #use_auth_token=True/
    token = access_token
    )

peft_model = PeftModel.from_pretrained(base_model, path)
</code></pre>
<p>When I load the base model in 4-bit precision, the VRAM usage is around 6 GB. However, when I load the PEFT model using the same function, the VRAM usage spikes to 19 GB. During fine-tuning, only 11% of the parameters are trainable.</p>
<p>I'm trying to understand why there's such a large increase in VRAM usage after loading the adapter weights. Could it be because the QLoRA adapter weights are being loaded in 16-bit precision? Is there a way to load these adapter weights in 8-bit?</p>
<p>Any insights or suggestions would be greatly appreciated!</p>
","nlp, huggingface-transformers, peft, llama3",
how to extract data from website including all of its internal links in Python,"<p>I want to extract all the information from a website including from its internal links for the purpose of ceating chatbot for this website . how can i do this ?</p>
<p>I have scrap the main site like <a href=""https://www.uetmardan.edu.pk/uetm/"" rel=""nofollow noreferrer"">https://www.uetmardan.edu.pk/uetm/</a> but i want to scrap also all of its internal links data</p>
<pre><code>from bs4 import BeautifulSoup
import requests

url =  https://www.uetmardan.edu.pk/uetm/
response.raise_for_status()
soup = BeautifulSoup(response.content, 'html.parser')
data = soup.get_text(separator='\n')
</code></pre>
","web-scraping, nlp, chatbot, langchain, rag",
Python - RegEx for splitting text into sentences (sentence-tokenizing),"<p>I want to make a list of sentences from a string and then print them out. I don't want to use NLTK to do this.  So it needs to split on a period at the end of the sentence and not at decimals or abbreviations or title of a name or if the sentence has a .com   This is attempt at regex that doesn't work.</p>

<pre><code>import re

text = """"""\
Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.
""""""
sentences = re.split(r' *[\.\?!][\'""\)\]]* *', text)

for stuff in sentences:
        print(stuff)    
</code></pre>

<p>Example output of what it should look like</p>

<pre><code>Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. 
Did he mind?
Adam Jones Jr. thinks he didn't.
In any case, this isn't true...
Well, with a probability of .9 it isn't.
</code></pre>
","python, regex, nlp, tokenize","<pre><code>(?&lt;!\w\.\w.)(?&lt;![A-Z][a-z]\.)(?&lt;=\.|\?)\s
</code></pre>
<p>Try this. split your string this.You can also check demo.</p>
<p><a href=""http://regex101.com/r/nG1gU7/27"" rel=""nofollow noreferrer"">http://regex101.com/r/nG1gU7/27</a></p>
<p>Another variant.</p>
<pre><code>(?&lt;!\w\.\w.)(?&lt;!\b[A-Z][a-z]\.)(?&lt;![A-Z]\.)(?&lt;=\.|\?)\s|\\n
</code></pre>
<p><a href=""https://regex101.com/r/EVUIUT/1"" rel=""nofollow noreferrer"">https://regex101.com/r/EVUIUT/1</a></p>
"
`mlflow.transformers.log_model()` does not finish,"<h3>Problem</h3>
<p>I want to use <code>mlflow.transformers.log_model()</code> to log a finetuned huggingface model.</p>
<p><strong>However, when the <code>mlflow.transformers.log_model</code> method is running, it simply does not finish - runs forever - throws no errors.</strong></p>
<p>I suspect my configuration is not right, the model is too big?
The output says <code>Skipping saving pretrained model weights to disk</code> so that should not be the problem.</p>
<p>Any ideas how to do this properly?</p>
<h3>Example</h3>
<p>This is more or less how my setup looks like, you cannot run this, it includes some pseudocode...</p>
<p>I am on python 3.11.9 with <code>transformers = &quot;^4.41.2&quot;</code> &amp; <code>mlflow = &quot;^2.15.1&quot;</code>.</p>
<pre><code>import mlflow
import torch
from peft import LoraConfig
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from trl import SFTTrainer, setup_chat_format

train_dataset = ...
eval_dataset = ...

model_id = &quot;LeoLM/leo-hessianai-7b-chat-bilingual&quot;

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=&quot;auto&quot;,
    torch_dtype=torch.bfloat16,
    quantization_config=bnb_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True)
model, tokenizer = setup_chat_format(model, tokenizer)
peft_config = LoraConfig(...)
args = TrainingArguments(...)

# Define Trainer
trainer = SFTTrainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=peft_config,
    tokenizer=tokenizer,
    packing=True,
)

# mlflow
mlflow.set_experiment(&quot;my_experiment&quot;)
with mlflow.start_run() as run:
    mlflow.transformers.autolog()
    trainer.train()
    
     components = {
         &quot;model&quot;: trainer.model,
         &quot;tokenizer&quot;: tokenizer_no_pad,
     }
     # !!! This function all does not finish... !!!
     mlflow.transformers.log_model(
         transformers_model=components,
         artifact_path=&quot;model&quot;,
    )
</code></pre>
<p>The last output I get in the console is:</p>
<pre><code>INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.
Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}
/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead.
/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.
  warnings.warn(&quot;Setuptools is replacing distutils.&quot;)
</code></pre>
","python, nlp, huggingface-transformers, mlflow, mlops","<p>Before defining the trainer, the model has be turned into a Peft model object via <code>get_peft_model</code>, then the <code>mlflow.transformers.log_model</code> works:</p>
<pre><code>from peft import LoraConfig, get_peft_model

model = ...
peft_config = LoraConfig(...)
args = TrainingArguments(...)

peft_model = get_peft_model(model, peft_config)

trainer = SFTTrainer(
    model=peft_model,
    args=args,
    ...
)


# mlflow
mlflow.set_experiment(&quot;my_experiment&quot;)
with mlflow.start_run() as run:
    mlflow.transformers.autolog()
    trainer.train()
    
     components = {
         &quot;model&quot;: trainer.model,
         &quot;tokenizer&quot;: tokenizer_no_pad,
     }
     # !!! Now the logginig of the model works, we can find it in the artifacts !!!
     mlflow.transformers.log_model(
         transformers_model=components,
         artifact_path=&quot;model&quot;,
    )
</code></pre>
"
Azure AI Search Scoring Profiles are not modifying the score retrival,"<p>I have been using Azure Ai search and scoring profiles to boost the documents of my index that come form the 'reviewed' source that means I want to send to the very TOP documents that have the string 'reviewed' on the field source, so I configured this scoring profile:</p>
<pre><code>  &quot;scoringProfiles&quot;: [
    {
      &quot;name&quot;: &quot;pubs_peer_house&quot;,
      &quot;functionAggregation&quot;: &quot;sum&quot;,
      &quot;text&quot;: {
        &quot;weights&quot;: {
          &quot;title&quot;: 3,
          &quot;content&quot;: 10
        }
      },
      &quot;functions&quot;: [
        {
          &quot;fieldName&quot;: &quot;source&quot;,
          &quot;interpolation&quot;: &quot;linear&quot;,
          &quot;type&quot;: &quot;tag&quot;,
          &quot;boost&quot;: 100,
          &quot;freshness&quot;: null,
          &quot;magnitude&quot;: null,
          &quot;distance&quot;: null,
          &quot;tag&quot;: {
            &quot;tagsParameter&quot;: &quot;reviewed&quot;
          }
        }
      ]
    }
  ],
</code></pre>
<p>I have this code in Python:</p>
<pre><code>from azure.core.exceptions import HttpResponseError

# Define the search query
search_query = &quot;cybersecurity in Arizona from tech today&quot;

# Function to get the embedding of the search query
search_vector = get_embedding(search_query)

# Define the scoring parameters
scoring_parameters = [&quot;reviewed-100&quot;]  

try:
    # Perform the search using the Azure Cognitive Search client
    response = search_client.search(
        search_query, 
        top=5,  # Return the top 5 results
        vector_queries=[
            VectorizedQuery(
                vector=search_vector,  # The vector representation of the search query
                k_nearest_neighbors=5,  # Number of nearest neighbors to find
                fields=&quot;vector_space_search&quot;  # Field in the index to search the vector space
            )
        ],
        query_type=&quot;semantic&quot;,  # Specify the query type as semantic
        semantic_configuration_name=semantic_profile,  # The name of the semantic configuration
        scoring_profile='pubs_peer_house',  # The scoring profile to use
        scoring_parameters=scoring_parameters  # The scoring parameters
    )

    # Iterate over the search results
    for idx, doc in enumerate(response, start=1):
        # Default values if fields are not found
        found_content = &quot;Not found&quot;
        
        # Extract fields from the search result
        date = doc.get('date', 'N/A')  # Date of the document
        source = doc.get('source', 'N/A')  # Source of the document
        title = doc.get('title', 'N/A')  # Title of the document
        content = doc.get('content', 'N/A')  # Content of the document
        
        # Print the results
        print(f&quot;{idx}&quot;)
        print(f&quot;Score: {doc['@search.score']:.5f}&quot;)
        print(f&quot;Source: {source}&quot;)
        print(f&quot;Title: {title}&quot;)
        print(f&quot;Content: {content}\n\n&quot;)
except HttpResponseError as e:
    # Handle HTTP response errors
    print(f&quot;HTTP Response Error: {e.message}&quot;)
    print(f&quot;Details: {e.response}&quot;)
except Exception as ex:
    # Handle other exceptions
    print(f&quot;An error occurred: {ex}&quot;)
</code></pre>
<p>Nonetheless when I ask for anything and I implement my profiles scoring along with the semantic ranker in a hybrid search it doesn't matter the value of the booster. I always get the same results:</p>
<p>Look:</p>
<blockquote>
<p>1 Score: 0.03667 Source: americas Subtitle: What level of
cybersecurity do you have? Content: We comply with industry standards
for cybersecurity and recommend that you...</p>
<p>2 Score: 0.02639 Source: reviewed Subtitle: What do I need to operate
securely? Content: The key or security signature. It is an 8-digit
alphanumeric code ML te....</p>
<p>3 Score: 0.01562 Source: europe Subtitle: Passkey password and pin
still better than faceID Content: careful whose face do you trust....</p>
</blockquote>
<p>even with params like : <code>scoring_parameters = [&quot;reviewed-2500000&quot;]</code>    I still get:</p>
<blockquote>
<p>1 Score: 0.03667
Source: americas
Subtitle: What level of
cybersecurity do you have? Content: We comply with industry standards
for cybersecurity and recommend that you...</p>
<p>2 Score: 0.02639 Source: reviewed Subtitle: What do I need to operate
securely? Content: The key or security signature. It is an 8-digit
alphanumeric code ML te....</p>
<p>3 Score: 0.01562 Source: europe Subtitle: Passkey password and pin
still better than faceID Content: careful whose face do you trust....</p>
</blockquote>
<p>Am I doing something wrong? I can't seem to find a tutorial on this in Python online.</p>
<p>This is my Index Config:</p>
<pre><code>    &quot;name&quot;: &quot;idx_americas_europe_pubs_houses&quot;,
  &quot;defaultScoringProfile&quot;: null,
  &quot;fields&quot;: [
    {
      &quot;name&quot;: &quot;content&quot;,
      &quot;type&quot;: &quot;Edm.String&quot;,
      &quot;searchable&quot;: true,
      &quot;filterable&quot;: true,
      &quot;retrievable&quot;: true,
      &quot;stored&quot;: true,
      &quot;sortable&quot;: true,
      &quot;facetable&quot;: false,
      &quot;key&quot;: false,
      &quot;indexAnalyzer&quot;: null,
      &quot;searchAnalyzer&quot;: null,
      &quot;analyzer&quot;: null,
      &quot;normalizer&quot;: null,
      &quot;dimensions&quot;: null,
      &quot;vectorSearchProfile&quot;: null,
      &quot;vectorEncoding&quot;: null,
      &quot;synonymMaps&quot;: []
    },
    {
      &quot;name&quot;: &quot;title&quot;,
      &quot;type&quot;: &quot;Edm.String&quot;,
      &quot;searchable&quot;: true,
      &quot;filterable&quot;: true,
      &quot;retrievable&quot;: true,
      &quot;stored&quot;: true,
      &quot;sortable&quot;: true,
      &quot;facetable&quot;: false,
      &quot;key&quot;: false,
      &quot;indexAnalyzer&quot;: null,
      &quot;searchAnalyzer&quot;: null,
      &quot;analyzer&quot;: null,
      &quot;normalizer&quot;: null,
      &quot;dimensions&quot;: null,
      &quot;vectorSearchProfile&quot;: null,
      &quot;vectorEncoding&quot;: null,
      &quot;synonymMaps&quot;: []
    },
    {
      &quot;name&quot;: &quot;source&quot;,
      &quot;type&quot;: &quot;Edm.String&quot;,
      &quot;searchable&quot;: true,
      &quot;filterable&quot;: true,
      &quot;retrievable&quot;: true,
      &quot;stored&quot;: true,
      &quot;sortable&quot;: true,
      &quot;facetable&quot;: false,
      &quot;key&quot;: false,
      &quot;indexAnalyzer&quot;: null,
      &quot;searchAnalyzer&quot;: null,
      &quot;analyzer&quot;: null,
      &quot;normalizer&quot;: null,
      &quot;dimensions&quot;: null,
      &quot;vectorSearchProfile&quot;: null,
      &quot;vectorEncoding&quot;: null,
      &quot;synonymMaps&quot;: []
    },
    {
      &quot;name&quot;: &quot;pub_date&quot;,
      &quot;type&quot;: &quot;Edm.String&quot;,
      &quot;searchable&quot;: true,
      &quot;filterable&quot;: true,
      &quot;retrievable&quot;: true,
      &quot;stored&quot;: true,
      &quot;sortable&quot;: true,
      &quot;facetable&quot;: false,
      &quot;key&quot;: false,
      &quot;indexAnalyzer&quot;: null,
      &quot;searchAnalyzer&quot;: null,
      &quot;analyzer&quot;: null,
      &quot;normalizer&quot;: null,
      &quot;dimensions&quot;: null,
      &quot;vectorSearchProfile&quot;: null,
      &quot;vectorEncoding&quot;: null,
      &quot;synonymMaps&quot;: []
    },
    {
      &quot;name&quot;: &quot;vector_space_search&quot;,
      &quot;type&quot;: &quot;Collection(Edm.Single)&quot;,
      &quot;searchable&quot;: true,
      &quot;filterable&quot;: false,
      &quot;retrievable&quot;: true,
      &quot;stored&quot;: true,
      &quot;sortable&quot;: false,
      &quot;facetable&quot;: false,
      &quot;key&quot;: false,
      &quot;indexAnalyzer&quot;: null,
      &quot;searchAnalyzer&quot;: null,
      &quot;analyzer&quot;: null,
      &quot;normalizer&quot;: null,
      &quot;dimensions&quot;: 1536,
      &quot;vectorSearchProfile&quot;: &quot;embedding_profile&quot;,
      &quot;vectorEncoding&quot;: null,
      &quot;synonymMaps&quot;: []
    },
    {
      &quot;name&quot;: &quot;metadata_storage_path&quot;,
      &quot;type&quot;: &quot;Edm.String&quot;,
      &quot;searchable&quot;: false,
      &quot;filterable&quot;: false,
      &quot;retrievable&quot;: true,
      &quot;stored&quot;: true,
      &quot;sortable&quot;: false,
      &quot;facetable&quot;: false,
      &quot;key&quot;: true,
      &quot;indexAnalyzer&quot;: null,
      &quot;searchAnalyzer&quot;: null,
      &quot;analyzer&quot;: null,
      &quot;normalizer&quot;: null,
      &quot;dimensions&quot;: null,
      &quot;vectorSearchProfile&quot;: null,
      &quot;vectorEncoding&quot;: null,
      &quot;synonymMaps&quot;: []
    }
  ],
   &quot;scoringProfiles&quot;: [
    {
      &quot;name&quot;: &quot;pubs_peer_house&quot;,
      &quot;functionAggregation&quot;: &quot;sum&quot;,
      &quot;text&quot;: {
        &quot;weights&quot;: {
          &quot;title&quot;: 3,
          &quot;content&quot;: 10
        }
      },
      &quot;functions&quot;: [
        {
          &quot;fieldName&quot;: &quot;source&quot;,
          &quot;interpolation&quot;: &quot;linear&quot;,
          &quot;type&quot;: &quot;tag&quot;,
          &quot;boost&quot;: 100,
          &quot;freshness&quot;: null,
          &quot;magnitude&quot;: null,
          &quot;distance&quot;: null,
          &quot;tag&quot;: {
            &quot;tagsParameter&quot;: &quot;reviewed&quot;
          }
        }
      ]
  ]
    }
  ],
  &quot;corsOptions&quot;: null,
  &quot;suggesters&quot;: [],
  &quot;analyzers&quot;: [],
  &quot;normalizers&quot;: [],
  &quot;tokenizers&quot;: [],
  &quot;tokenFilters&quot;: [],
  &quot;charFilters&quot;: [],
  &quot;encryptionKey&quot;: null,
  &quot;similarity&quot;: {
    &quot;@odata.type&quot;: &quot;#Microsoft.Azure.Search.BM25Similarity&quot;,
    &quot;k1&quot;: null,
    &quot;b&quot;: null
  },
  &quot;semantic&quot;: {
    &quot;defaultConfiguration&quot;: null,
    &quot;configurations&quot;: [
      {
        &quot;name&quot;: &quot;ranker_profile_pubs&quot;,
        &quot;prioritizedFields&quot;: {
          &quot;titleField&quot;: {
            &quot;fieldName&quot;: &quot;title&quot;
          },
          &quot;prioritizedContentFields&quot;: [
            {
              &quot;fieldName&quot;: &quot;content&quot;
            }
          ],
          &quot;prioritizedKeywordsFields&quot;: []
        }
      }
    ]
  },
  &quot;vectorSearch&quot;: {
    &quot;algorithms&quot;: [
      {
        &quot;name&quot;: &quot;hnsw_config&quot;,
        &quot;kind&quot;: &quot;hnsw&quot;,
        &quot;hnswParameters&quot;: {
          &quot;metric&quot;: &quot;cosine&quot;,
          &quot;m&quot;: 3,
          &quot;efConstruction&quot;: 300,
          &quot;efSearch&quot;: 250
        },
        &quot;exhaustiveKnnParameters&quot;: null
      }
    ],
    &quot;profiles&quot;: [
      {
        &quot;name&quot;: &quot;embedding_profile&quot;,
        &quot;algorithm&quot;: &quot;hnsw_config&quot;,
        &quot;vectorizer&quot;: null,
        &quot;compression&quot;: null
      }
    ],
    &quot;vectorizers&quot;: [],
    &quot;compressions&quot;: []
  }
}
</code></pre>
","python, azure, nlp, azure-ai-search",
How to Sample Multiple Completions (n) Directly from Claude API without a for loop?,"<p>I'm using the Anthropic Claude API and I'm trying to generate multiple completions (n completions) for a given prompt in a single API call. OpenAI's API provides an n parameter in their sampling settings to achieve this, but I can't find an equivalent option in the Claude API.</p>
<h3>My Current Approach:</h3>
<p>I'm currently using a retry mechanism to handle potential errors during API calls, which looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>from tenacity import retry, stop_after_attempt, wait_exponential

def before_sleep(retry_state):
    print(f&quot;(Tenacity) Retry, error that caused it: {retry_state.outcome.exception()}&quot;)

def retry_error_callback(retry_state):
    exception = retry_state.outcome.exception()
    exception_str = str(exception)
    if &quot;prompt is too long&quot; in exception_str and &quot;400&quot; in exception_str:
        raise exception
    return 'No error that requires us to exit early.'

@retry(stop=stop_after_attempt(20), wait=wait_exponential(multiplier=2, max=256), 
       before_sleep=before_sleep, retry_error_callback=retry_error_callback)
def call_to_anthropic_client_api_with_retry(gen: AnthropicGenerator, prompt: str) -&gt; dict:
    response = gen.llm.messages.create(
        model=gen.model,
        max_tokens=gen.sampling_params.max_tokens,
        system=gen.system_prompt,
        messages=[
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: prompt}]}
        ],
        temperature=gen.sampling_params.temperature,
        top_p=gen.sampling_params.top_p,
        n=gen.sampling_params.n,  # Intended to generate multiple completions
        stop_sequences=gen.sampling_params.stop[:3],
    )
    return response
</code></pre>
<p>Problem:</p>
<p>I can't find an n parameter in the Anthropic API documentation that allows generating multiple completions in one request.</p>
<p>Questions:</p>
<ol>
<li>Does the Claude API support generating multiple completions (n completions) directly within a single API call?</li>
<li>If not, is there a recommended workaround or best practice to achieve this without resorting to looping multiple requests?</li>
</ol>
<p>cross discord: <a href=""https://discord.com/channels/1072196207201501266/1213976011998498816/threads/1273440866861846549"" rel=""nofollow noreferrer"">https://discord.com/channels/1072196207201501266/1213976011998498816/threads/1273440866861846549</a>
cross: <a href=""https://dev.to/brando90/how-to-sample-multiple-completions-n-directly-from-claude-api-without-a-for-loop-2m1e"" rel=""nofollow noreferrer"">https://dev.to/brando90/how-to-sample-multiple-completions-n-directly-from-claude-api-without-a-for-loop-2m1e</a></p>
<hr />
<p>For now doing this:</p>
<pre class=""lang-py prettyprint-override""><code>@retry(stop=stop_after_attempt(20), wait=wait_exponential(multiplier=2, max=256), 
       before_sleep=before_sleep, retry_error_callback=retry_error_callback)
def call_to_anthropic_client_api_with_retry(gen: AnthropicGenerator, prompt: str) -&gt; dict:
    # max_tokens=8192,  # max_tokens for Claude 3.5 https://docs.anthropic.com/en/docs/about-claude/models#model-comparison
    # client = anthropic.Anthropic(api_key=gen.api_key)
    # response = client.messages.create(
    # response_text: str = gen.llm.messages.create(
    #     model=gen.sampling_params.model,
    #     max_tokens=gen.sampling_params.max_tokens,
    #     # temperature=temperature,  # note the prompt generator doesn't give this as an input
    #     system=gen.sampling_params.system,
    #     messages=[
    #         {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: prompt}]}
    #     ],
    #     temperature=gen.sampling_params.temperature,
    #     top_p=gen.sampling_params.top_p,
    #     n=gen.sampling_params.n,
    #     stop=gen.sampling_params.stop[:3],
    # ).content[0].text
    if not hasattr(gen.sampling_params, 'n'):
        gen.sampling_params.n = 1
    content: list[dict] = [] 
    for _ in range(gen.sampling_params.n):
        response = gen.llm.messages.create(
            model=gen.model,
            max_tokens=gen.sampling_params.max_tokens,
            system=gen.system_prompt,
            messages=[
                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: prompt}]}
            ],
            temperature=gen.sampling_params.temperature,
            top_p=gen.sampling_params.top_p,
            n=gen.sampling_params.n,
            stop_sequences=gen.sampling_params.stop[:3],
        )
        content.append(response)
    response = dict(content=content)
    # message example: https://docs.anthropic.com/en/api/messages-examples
    return response
</code></pre>
","machine-learning, nlp, large-language-model, anthropic",
langchain&#39;s chroma vectordb.similarity_search_with_score() and vectordb.similarity_search_with_relevancy_scores() returning the same output,"<p>I have been working with langchain's chroma vectordb. It has two methods for running similarity search with scores.</p>
<ol>
<li><code>vectordb.similarity_search_with_score()</code></li>
<li><code>vectordb.similarity_search_with_relevance_scores()</code></li>
</ol>
<p>According to the documentation, the first one should return a cosine distance in <code>float</code>.
<a href=""https://i.sstatic.net/mpvnZ.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/mpvnZ.png"" alt=""enter image description here"" /></a></p>
<p>Smaller the better.</p>
<p>And the second one should return a score from 0 to 1, 0 means dissimilar and 1 means similar.
<a href=""https://i.sstatic.net/Vomd3.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/Vomd3.png"" alt=""enter image description here"" /></a></p>
<p>But when I tried the same it is giving me exactly same results with same scores which overflows the upperlimit 1, which should not be the case for the second function.</p>
<p>What's going on here?</p>
","nlp, langchain",
HuggingFace LLM Evaluate: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long,"<p><strong>Context</strong>:
I tried to create an evaluation pipeline for a text summary task using <a href=""https://github.com/huggingface/evaluate"" rel=""nofollow noreferrer"">HuggingFace evaluate</a> packages. I got the issue of receiving dtype Long for the tensor, but I did not feed any long type and the two columns specified for the evaluate pipeline are text only. Further investigation looks like the issue is rooted from torch and my version of Mac (M1). I'm not sure how to proceed with this.</p>
<p>Here is what I did:</p>
<p><strong>My code</strong>:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
from evaluate import evaluator
from datasets import load_dataset

# Load data:
booksum = load_dataset(&quot;kmfoda/booksum&quot;, split=&quot;validation[:1000]&quot;)

# Load pipeline
pipe = pipeline(
    task=&quot;summarization&quot;,
    model=&quot;pszemraj/led-base-book-summary&quot;,
    device=&quot;mps&quot;
)

# Setup Evaluate task using Rouge
task_evaluator = evaluator(&quot;summarization&quot;)

# The code that yield issue:
eval_results = task_evaluator.compute(
    model_or_pipeline=pipe,
    data=booksum,
    metric=&quot;rouge&quot;,
    input_column=&quot;chapter&quot;,
    label_column=&quot;summary_text&quot;
)
</code></pre>
<p>This gives me the value error below:</p>
<p><strong>Short Error message:</strong></p>
<pre><code>File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:157, in MinLengthLogitsProcessor.__call__(self, input_ids, scores)
    154 @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)
    155 def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -&gt; torch.FloatTensor:
    156     vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)
--&gt; 157     eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id)
    158     scores_processed = scores.clone()
    159     if input_ids.shape[-1] &lt; self.min_length:

RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long
</code></pre>
<p><strong>Full Error message:</strong></p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[10], line 1
----&gt; 1 eval_results = task_evaluator.compute(
      2     model_or_pipeline=pipe,
      3     data=booksum,
      4     metric=&quot;rouge&quot;,
      6     input_column=&quot;chapter&quot;,
      7     label_column=&quot;summary_text&quot;
      8 )

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/text2text_generation.py:191, in SummarizationEvaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, generation_kwargs)
    166 @add_start_docstrings(
    167     EVALUTOR_COMPUTE_START_DOCSTRING,
    168     TASK_DOCUMENTATION_KWARGS,
   (...)
    189     generation_kwargs: dict = None,
    190 ) -&gt; Tuple[Dict[str, float], Any]:
--&gt; 191     result = super().compute(
    192         model_or_pipeline=model_or_pipeline,
    193         data=data,
    194         subset=subset,
    195         split=split,
    196         metric=metric,
    197         tokenizer=tokenizer,
    198         strategy=strategy,
    199         confidence_level=confidence_level,
    200         n_resamples=n_resamples,
    201         device=device,
    202         random_state=random_state,
    203         input_column=input_column,
    204         label_column=label_column,
    205         generation_kwargs=generation_kwargs,
    206     )
    208     return result

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/text2text_generation.py:133, in Text2TextGenerationEvaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, generation_kwargs)
    130 if generation_kwargs is not None:
    131     self.PIPELINE_KWARGS.update(generation_kwargs)
--&gt; 133 result = super().compute(
    134     model_or_pipeline=model_or_pipeline,
    135     data=data,
    136     subset=subset,
    137     split=split,
    138     metric=metric,
    139     tokenizer=tokenizer,
    140     strategy=strategy,
    141     confidence_level=confidence_level,
    142     n_resamples=n_resamples,
    143     device=device,
    144     random_state=random_state,
    145     input_column=input_column,
    146     label_column=label_column,
    147 )
    149 return result

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/base.py:255, in Evaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping)
    252 metric = self.prepare_metric(metric)
    254 # Compute predictions
--&gt; 255 predictions, perf_results = self.call_pipeline(pipe, pipe_inputs)
    256 predictions = self.predictions_processor(predictions, label_mapping)
    258 metric_inputs.update(predictions)

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/base.py:513, in Evaluator.call_pipeline(self, pipe, *args, **kwargs)
    511 def call_pipeline(self, pipe, *args, **kwargs):
    512     start_time = perf_counter()
--&gt; 513     pipe_output = pipe(*args, **kwargs, **self.PIPELINE_KWARGS)
    514     end_time = perf_counter()
    515     return pipe_output, self._compute_time_perf(start_time, end_time, len(pipe_output))

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:269, in SummarizationPipeline.__call__(self, *args, **kwargs)
    245 def __call__(self, *args, **kwargs):
    246     r&quot;&quot;&quot;
    247     Summarize the text(s) given as inputs.
    248 
   (...)
    267           ids of the summary.
    268     &quot;&quot;&quot;
--&gt; 269     return super().__call__(*args, **kwargs)

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:167, in Text2TextGenerationPipeline.__call__(self, *args, **kwargs)
    138 def __call__(self, *args, **kwargs):
    139     r&quot;&quot;&quot;
    140     Generate the output text(s) using text(s) given as inputs.
    141 
   (...)
    164           ids of the generated text.
    165     &quot;&quot;&quot;
--&gt; 167     result = super().__call__(*args, **kwargs)
    168     if (
    169         isinstance(args[0], list)
    170         and all(isinstance(el, str) for el in args[0])
    171         and all(len(res) == 1 for res in result)
    172     ):
    173         return [res[0] for res in result]

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/base.py:1235, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1231 if can_use_iterator:
   1232     final_iterator = self.get_iterator(
   1233         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params
   1234     )
-&gt; 1235     outputs = list(final_iterator)
   1236     return outputs
   1237 else:

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124, in PipelineIterator.__next__(self)
    121     return self.loader_batch_item()
    123 # We're out of items within a batch
--&gt; 124 item = next(self.iterator)
    125 processed = self.infer(item, **self.params)
    126 # We now have a batch of &quot;inferred things&quot;.

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125, in PipelineIterator.__next__(self)
    123 # We're out of items within a batch
    124 item = next(self.iterator)
--&gt; 125 processed = self.infer(item, **self.params)
    126 # We now have a batch of &quot;inferred things&quot;.
    127 if self.loader_batch_size is not None:
    128     # Try to infer the size of the batch

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/base.py:1161, in Pipeline.forward(self, model_inputs, **forward_params)
   1159     with inference_context():
   1160         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
-&gt; 1161         model_outputs = self._forward(model_inputs, **forward_params)
   1162         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(&quot;cpu&quot;))
   1163 else:

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:191, in Text2TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)
    184     in_b, input_length = tf.shape(model_inputs[&quot;input_ids&quot;]).numpy()
    186 self.check_inputs(
    187     input_length,
    188     generate_kwargs.get(&quot;min_length&quot;, self.model.config.min_length),
    189     generate_kwargs.get(&quot;max_length&quot;, self.model.config.max_length),
    190 )
--&gt; 191 output_ids = self.model.generate(**model_inputs, **generate_kwargs)
    192 out_b = output_ids.shape[0]
    193 if self.framework == &quot;pt&quot;:

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/torch/utils/_contextlib.py:116, in context_decorator.&lt;locals&gt;.decorate_context(*args, **kwargs)
    113 @functools.wraps(func)
    114 def decorate_context(*args, **kwargs):
    115     with ctx_factory():
--&gt; 116         return func(*args, **kwargs)

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/utils.py:2028, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)
   2020     input_ids, model_kwargs = self._expand_inputs_for_generation(
   2021         input_ids=input_ids,
   2022         expand_size=generation_config.num_beams,
   2023         is_encoder_decoder=self.config.is_encoder_decoder,
   2024         **model_kwargs,
   2025     )
   2027     # 14. run beam sample
-&gt; 2028     result = self._beam_search(
   2029         input_ids,
   2030         beam_scorer,
   2031         logits_processor=prepared_logits_processor,
   2032         logits_warper=prepared_logits_warper,
   2033         stopping_criteria=prepared_stopping_criteria,
   2034         generation_config=generation_config,
   2035         synced_gpus=synced_gpus,
   2036         **model_kwargs,
   2037     )
   2039 elif generation_mode == GenerationMode.GROUP_BEAM_SEARCH:
   2040     # 11. prepare beam search scorer
   2041     beam_scorer = BeamSearchScorer(
   2042         batch_size=batch_size,
   2043         num_beams=generation_config.num_beams,
   (...)
   2049         max_length=generation_config.max_length,
   2050     )

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/utils.py:3200, in GenerationMixin._beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)
   3195 next_token_logits = outputs.logits[:, -1, :].clone()
   3196 next_token_scores = nn.functional.log_softmax(
   3197     next_token_logits, dim=-1
   3198 )  # (batch_size * num_beams, vocab_size)
-&gt; 3200 next_token_scores_processed = logits_processor(input_ids, next_token_scores)
   3201 if do_sample:
   3202     next_token_scores_processed = logits_warper(input_ids, next_token_scores_processed)

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:98, in LogitsProcessorList.__call__(self, input_ids, scores, **kwargs)
     96         scores = processor(input_ids, scores, **kwargs)
     97     else:
---&gt; 98         scores = processor(input_ids, scores)
    100 return scores

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:157, in MinLengthLogitsProcessor.__call__(self, input_ids, scores)
    154 @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)
    155 def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -&gt; torch.FloatTensor:
    156     vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)
--&gt; 157     eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id)
    158     scores_processed = scores.clone()
    159     if input_ids.shape[-1] &lt; self.min_length:

RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long
</code></pre>
<p><strong>Notes</strong>: I did try to add <code>device=&quot;mps&quot;</code> to the <code>task_evaluator.compute</code> but it gave me another error of <code>ValueError: This pipeline was instantiated on device None but device=mps was passed to 'compute'.</code></p>
","pytorch, nlp, apple-m1, huggingface, huggingface-evaluate","<p>I ran into a similar issue trying to run Facebook's <a href=""https://github.com/facebookresearch/nougat"" rel=""nofollow noreferrer"">nougat</a> OCR tool.</p>
<p>The error message mentions macOS 14 (Sonoma) and I was on macOS 13 (Ventura). Upgrading to macOS 14 fixed the issue for me.</p>
"
How to do NLP fill-mask with restricted possible inputs,"<p>I want to use NLP to fill in a masked word in a text, but instead of choosing from all possible words I want to find which is the more likely of two candidate words. For example, imagine I have a sentence &quot;The [MASK] was stuck in the tree&quot; and I want to evaluate whether &quot;kite&quot; or &quot;bike&quot; is the more likely word.</p>
<p>I know how to find the globally most probable words using hugging face's fill-mask pipeline</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM

# Define the input sentence with a masked word
input_text = &quot;The [MASK] was stuck in the tree&quot;

# Load the pre-trained model and tokenizer
model_name = &quot;bert-base-cased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForMaskedLM.from_pretrained(model_name)

# Tokenize the input sentence
tokenized_text = tokenizer.tokenize(input_text)

# Use the pipeline to generate a list of predicted words and probabilities
mlm = pipeline(&quot;fill-mask&quot;, model=model, tokenizer=tokenizer)
results = mlm(input_text)

# Print outputs
for result in results:
    token = result[&quot;token_str&quot;]
    print(f&quot;{token:&lt;15} {result['score']}&quot;)
</code></pre>
<p>However if &quot;bike&quot; and &quot;kite&quot; arent in the first few most probable words this doesnt help.</p>
<p>How can I use fill-mask to find the probability of specific masks?</p>
<p>P.S. I'm not sure if overflow is the best place to post this question, there doesnt seem to be a place for nlp specific questions.</p>
","nlp, mask, huggingface","<p>Since <a href=""https://github.com/huggingface/transformers/releases/tag/v3.1.0"" rel=""nofollow noreferrer"">version 3.1.0</a> the fill-mask pipeline also supports the <code>targets</code> argument to do this directly in the pipeline:</p>
<pre><code>from transformers import pipeline

pipe = pipeline(&quot;fill-mask&quot;, model=&quot;bert-base-cased&quot;)
results = pipe(&quot;The [MASK] was stuck in the tree&quot;, targets=[&quot;bike&quot;, &quot;kite&quot;])

for result in results:
    print(f'{result[&quot;token_str&quot;]}: {result[&quot;score&quot;]}')
</code></pre>
<p>Note that in case any of the words are not in the vocabulary, it will instead return the score for its first subword token.</p>
"
How to update an already Fine-tuned GPT2 model?,"<p>I have trained my model using GPT-2 for my dataset. It has been trained and is giving the correct output. Now, I want to train my model on more data while retaining the previously trained model. I do not want to create a new model; I only want to use the previous one with the updated data for training.</p>
<p>I tried doing <code>overwrite=False</code> in the training, but I don't know whether it will work or not.</p>
","nlp, dataset, huggingface-transformers, gpt-2",
Get contextual entropy of each word in a sentence,"<p>I am trying to get the contextual entropy of each word within sentences of a dataset.
I am following the definition of Contextual Entropy provided in <a href=""https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00612/118718"" rel=""nofollow noreferrer"">this</a> paper, so for each position within a sentence, I compute the probability of each word of the vocabulary given the sentence (left) context, and then I sum them. This process is being very time-consuming though, so I need advice on alternative ways of computing the CE.</p>
<p>So far my script looks roughly like this:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;google-bert/bert-base-chinese&quot;)
model = AutoModelForMaskedLM.from_pretrained(&quot;google-bert/bert-base-chinese&quot;)

pipe = pipeline(&quot;fill-mask&quot;, model=&quot;google-bert/bert-base-chinese&quot;)

BERTvocab = tokenizer.get_vocab()

for sentence in all_sentences:
    for i,word in enumerate(sentence):
        # for each word the context is the left context + [MASK]
        context = sentence[:i+1]
        context = context+'[MASK]'

        for vw in BERTvocab:
            # get the probability distribution of each word in the [MASK] position
            teres = pipe(context, targets= [vw])
            softtyent = teres[0][&quot;score&quot;]
            
            # compute the p(w|context)*log(p(w|context))
            temp_tyent = softtyent*math.log(softtyent)
            singles.append(temp_tyent)
        
         entropy = -sum(singles)

</code></pre>
<p>My questions are:</p>
<ol>
<li>Is my understanding of the Contextual Entropy right?</li>
<li>Considering that the dataset has 400 sentences (long 15 words on average), is there a faster and computationally &quot;more elegant&quot; way to do this? It's taking me ages to run the script...</li>
</ol>
<p>Thanks!</p>
","nlp, entropy",
SpaCy Matcher with optional suffix in pattern reports multiple matches on same text,"<p>Using the following Matcher rule:</p>
<pre><code>{'label': 'R-1',
 'pattern': [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}],
 'greedy': 'LONGEST', }
</code></pre>
<p>on the text: 'MyLabel: Some Value'</p>
<p>I get <strong>two</strong> matches: 'MyLabel' and 'MyLabel:'</p>
<p>For me, that was quite surprising - I was expecting a single match on 'MyLabel:'.
Adding the new greedy flag didn't make any difference.</p>
<ul>
<li>Is this the intended behavior or is it a bug?</li>
<li>How should I determine that the second match really is just a subset of the first match?</li>
<li>Will the shorter match always be reported before the longer match?</li>
</ul>
<p>SpaCy version 3.7.5</p>
","nlp, spacy, matcher","<p>i will say that the behavior you're observing with the SpaCy <code>Matcher</code> is expected, and it is not a bug. When you use the <code>{'TEXT': ':', 'OP': '?'}</code> pattern, the <code>OP: '?'</code> operator means that the colon is optional, so the matcher will generate both the shorter and the longer match, as you've seen.</p>
<h3>Explanation:</h3>
<ul>
<li><strong>Pattern</strong>: <code>{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}</code>.</li>
<li><strong>Text</strong>: <code>'MyLabel: Some Value'</code>.</li>
</ul>
<p>So for this pattern, SpaCy  will try to match:</p>
<ol>
<li><code>'MyLabel'</code> alone (because the colon is optional).</li>
<li><code>'MyLabel:'</code> (because the colon can be included).</li>
</ol>
<p>Therefore, you will get two matches: <code>'MyLabel'</code> and <code>'MyLabel:'</code>.</p>
<h3>Now to  Answer Your Questions:</h3>
<ol>
<li><p><strong>Is this the intended behavior or is it a bug?</strong></p>
<ul>
<li>This is intended behavior. The <code>OP: '?'</code> operator allows the colon to be optionally matched, leading to multiple matches.</li>
</ul>
</li>
<li><p><strong>How should I determine that the second match really is just a subset of the first match?</strong></p>
<ul>
<li>To determine if one match is a subset of another, you can compare the start and end indices of the matches. The longer match will have the same start index but a different end index. Now i wrote a code below even using spacy version 3.7.5, see details below</li>
</ul>
</li>
</ol>
<pre><code>pip show spacy
Name: spacy
Version: 3.7.5
Summary: Industrial-strength Natural Language Processing (NLP) in Python
Home-page: https://spacy.io
Author: Explosion
Author-email: contact@explosion.ai
License: MIT
Location: /home/adesoji/Downloads/visis-backend-assessment-Adesoji/visisenv/lib/python3.11/site-packages
Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel
Required-by: en-core-web-sm
</code></pre>
<p>Now Example in code:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.matcher import Matcher

nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(&quot;MyLabel: Some Value&quot;)

matcher = Matcher(nlp.vocab)
pattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?'}]
matcher.add(&quot;R-1&quot;, [pattern])

matches = matcher(doc)
for match_id, start, end in matches:
    span = doc[start:end]
    print(f&quot;Match: {span.text}, Start: {start}, End: {end}&quot;)

# Now, we Determine if one match is a subset of another
matches.sort(key=lambda x: (x[1], -x[2]))  # Sort by start index, then by end index descending
filtered_matches = []
last_end = -1
for match_id, start, end in matches:
    if start &gt;= last_end:  # This is for Avoiding adding subsets
        filtered_matches.append((match_id, start, end))
        last_end = end

for match_id, start, end in filtered_matches:
    span = doc[start:end]
    print(f&quot;Filtered Match: {span.text}&quot;)
</code></pre>
<p>Now, This code will filter out the shorter match and your output will be</p>
<pre><code>Match: MyLabel, Start: 0, End: 1
Match: MyLabel:, Start: 0, End: 2
Filtered Match: MyLabel:   , you can see MYLabel: with the colon symbol there

</code></pre>
<ol start=""3"">
<li><strong>Now Will the shorter match always be reported before the longer match?</strong>
<ul>
<li>I don't think the matches are not guaranteed to be reported in a specific order. so to handle this, you can sort the matches by their start and end indices as shown in the code example above.Now, After sorting, you can now filter out matches that are subsets of longer matches.</li>
</ul>
</li>
</ol>
<h3>Another Alternative Solution:</h3>
<p>If you want to ensure that only the longest match is returned, you can change the way you define the pattern:</p>
<pre class=""lang-py prettyprint-override""><code>pattern = [{'TEXT': 'MyLabel'}, {'TEXT': ':', 'OP': '?', 'greedy': 'LONGEST'}]
</code></pre>
<p>note that the <code>greedy</code> flag doesn't change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings.</p>
<h3>Now back to the Summary of what i explained:</h3>
<ul>
<li>The behavior you're seeing is by design, due to the optional <code>OP: '?'</code> operator.</li>
<li>in addition, you can filter out the shorter match by comparing start and end indices of the matches.</li>
<li>furthermore, Sorting the matches by start and end indices allows you to keep only the longest, non-overlapping matches.</li>
</ul>
"
"Is it necessary for torch_dtype when loading a model and the precision for trainable weights to be different? If so, why?","<p>According to <a href=""https://github.com/huggingface/peft/issues/341#issuecomment-1884911753"" rel=""nofollow noreferrer"">this comment</a> in the huggingface/peft package, if a model is loaded in fp16, the trainable weights must be cast to fp32. From this comment, I understand that generally, the <code>torch_dtype</code> used when loading a model and the precision used for training must be different. Why is it necessary to change the precision? Also, does this principle apply to both fine-tuning and continual pretraining?</p>
<p>As a minimal working example, I'm attempting to perform a continual pretraining on <a href=""https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/"" rel=""nofollow noreferrer"">microsoft/Phi-3-mini-128k-instruct</a>, whose default <code>torch_dtype</code> is <a href=""https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/blob/bb5bf1e4001277a606e11debca0ef80323e5f824/config.json#L135"" rel=""nofollow noreferrer"">bfloat16</a>. When loading the model with <code>torch_dtype=torch.float16</code>, training commenced when the precision for trainable weights was set to <code>TrainingArguments(fp16=False, bf16=True)</code> (i.e. different precision for model loading and trainable weights). However, when the precision for trainable weights was set to <code>TrainingArguments(fp16=True, bf16=False)</code> (i.e. same precision for model loading and trainable weights), an error <code>raise ValueError(&quot;Attempting to unscale FP16 gradients.&quot;)</code> occurred, preventing the start of training. The execution environment was an NVIDIA RTX3060 with only 12GB of vRAM. For continual pretraining of the Phi-3 model, how should the <code>torch_dtype</code> be set when loading the model and for trainable weights to minimize vRAM usage? For instance, should the model be loaded with <code>torch_dtype=fp32</code> and the precision for trainable weights set to <code>TrainingArguments(fp16=True, bf16=False)</code>, or should the model be loaded with <code>load_in_8bit</code> and the precision for trainable weights set to <code>TrainingArguments(fp16=True, bf16=False)</code>? I would like to know effective and feasible combinations.</p>
<h1>MWE</h1>
<h2>train_deepspeed.py</h2>
<pre class=""lang-py prettyprint-override""><code>import argparse
import os
import warnings
from typing import Dict, List
import deepspeed
import torch
from datasets import load_dataset
from omegaconf import OmegaConf
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    PreTrainedTokenizer,
    Trainer,
    TrainingArguments,
)
import gc
from utils import seed_everything

warnings.filterwarnings(&quot;ignore&quot;)

os.environ[&quot;TOKENIZERS_PARALLELISM&quot;] = &quot;false&quot;

def preprocess_function(
    examples: Dict[str, List[str]],
    tokenizer: PreTrainedTokenizer,
    max_length: int,
) -&gt; Dict[str, List[int]]:
    inputs = tokenizer(
        examples[&quot;text&quot;],
        truncation=True,
        padding=&quot;max_length&quot;,
        max_length=max_length,
    )
    inputs[&quot;labels&quot;] = inputs.input_ids.copy()
    return inputs


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        &quot;--train_config&quot;,
        &quot;-p&quot;,
        type=str,
        default=&quot;./configs/train_configs/train_base.yaml&quot;,
    )
    parser.add_argument(
        &quot;--local_rank&quot;,
        &quot;-l&quot;,
        type=int,
        default=0,
    )
    args = parser.parse_args()

    config = OmegaConf.load(args.train_config)

    # distributed learning
    deepspeed.init_distributed()

    # set seed
    seed_everything(config.seed)

    # load model
    model = AutoModelForCausalLM.from_pretrained(
        config.model.model,
        torch_dtype=torch.float16,
        use_cache=config.model.use_cache,
        device_map={&quot;&quot;: 0},
        attn_implementation=&quot;flash_attention_2&quot;,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        config.model.tokenizer,
        add_eos_token=True,
    )

    # load dataset
    dataset = load_dataset(
        path=config.dataset.path,
        name=config.dataset.subset,
        split=config.dataset.split,
        cache_dir=config.dataset.cache_dir,
    )

    # transform dataset
    dataset = dataset.map(
        lambda examples: preprocess_function(
            examples, tokenizer, config.model.max_length
        ),
        batched=True,
        remove_columns=dataset.column_names,
        num_proc=32,
    )

    dataset = dataset.train_test_split(test_size=0.2)

    # initiate training
    training_args = TrainingArguments(**config.train)
    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset[&quot;train&quot;],
        eval_dataset=dataset[&quot;test&quot;],
        args=training_args,
        # data_collator=data_collator,
    )

    with torch.autocast(&quot;cuda&quot;):
        trainer.train()

    del dataset
    del trainer

    gc.collect()
    deepspeed.runtime.utils.empty_cache()
    torch.cuda.empty_cache()


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<h2>train_base.yaml</h2>
<pre class=""lang-yaml prettyprint-override""><code>model:
  model: microsoft/Phi-3-mini-128k-instruct
  tokenizer: microsoft/Phi-3-mini-128k-instruct
  use_cache: False
  max_length: 512


train:
  output_dir: ./outputs
  evaluation_strategy: steps
  logging_strategy: steps
  save_strategy: steps
  learning_rate: 1e-6
  num_train_epochs: 3
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 256 # per_device_train_bath_size*gradient_accumulation_steps=256
  gradient_checkpointing: True
  weight_decay: 0.01
  warmup_ratio: 0.1
  optim: adamw_bnb_8bit # adamw_torch
  fp16: False
  bf16: True
  dataloader_num_workers: 1
  eval_steps: 50
  save_steps: 100
  logging_steps: 5
  run_name: test
  save_total_limit: 2
  save_on_each_node: False
  neftune_noise_alpha: 5 # NEFTTune
  # deepspeed: ./configs/deepspeed/ds_config_zero2.json
  report_to: wandb
  torch_compile: True
  logging_dir: ./outputs/log
  
seed: 42

dataset:
  path: hotchpotch/wikipedia-ja-20231030
  subset: chunked #!!null
  split: train
  cache_dir: /mnt/d/huggingface/datasets
</code></pre>
<h2>pyproject.toml</h2>
<pre class=""lang-ini prettyprint-override""><code>[tool.poetry]
name = &quot;continual-pretrain&quot;
version = &quot;0.1.0&quot;
description = &quot;&quot;
authors = [&quot;Carlos Luis Rivera&quot;]
license = &quot;MIT&quot;
readme = &quot;README.md&quot;

[tool.poetry.dependencies]
python = &quot;^3.11&quot;
fsspec = &quot;2024.3.1&quot;
datasets = &quot;^2.19.2&quot;
accelerate = &quot;^0.31.0&quot;
aiohttp = &quot;^3.9.5&quot;
aiosignal = &quot;^1.3.1&quot;
annotated-types = &quot;^0.7.0&quot;
appdirs = &quot;^1.4.4&quot;
async-timeout = &quot;^4.0.3&quot;
attrs = &quot;^23.2.0&quot;
bitsandbytes = &quot;^0.43.1&quot;
certifi = &quot;^2024.6.2&quot;
charset-normalizer = &quot;^3.3.2&quot;
click = &quot;^8.1.7&quot;
deepspeed = &quot;^0.14.2&quot;
dill = &quot;^0.3.8&quot;
docker-pycreds = &quot;^0.4.0&quot;
docstring-parser = &quot;^0.16&quot;
filelock = &quot;^3.14.0&quot;
frozenlist = &quot;^1.4.1&quot;
gitdb = &quot;^4.0.11&quot;
gitpython = &quot;^3.1.43&quot;
hjson = &quot;^3.1.0&quot;
huggingface-hub = &quot;^0.23.3&quot;
idna = &quot;^3.7&quot;
jinja2 = &quot;^3.1.4&quot;
markdown-it-py = &quot;^3.0.0&quot;
markupsafe = &quot;^2.1.5&quot;
mdurl = &quot;^0.1.2&quot;
mpmath = &quot;^1.3.0&quot;
multidict = &quot;^6.0.5&quot;
multiprocess = &quot;^0.70.16&quot;
networkx = &quot;^3.3&quot;
ninja = &quot;^1.11.1.1&quot;
numpy = &quot;^1.26.4&quot;
nvidia-ml-py = &quot;^12.555.43&quot;
packaging = &quot;^24.0&quot;
pandas = &quot;^2.2.2&quot;
peft = &quot;0.6.0&quot;
protobuf = &quot;&lt;5.0.0&quot;
psutil = &quot;^5.9.8&quot;
py-cpuinfo = &quot;^9.0.0&quot;
pyarrow = &quot;^16.1.0&quot;
pyarrow-hotfix = &quot;^0.6&quot;
pydantic = &quot;^2.7.3&quot;
pydantic-core = &quot;^2.18.4&quot;
pygments = &quot;^2.18.0&quot;
pynvml = &quot;^11.5.0&quot;
python-dateutil = &quot;^2.9.0.post0&quot;
pytz = &quot;^2024.1&quot;
pyyaml = &quot;^6.0.1&quot;
regex = &quot;^2024.5.15&quot;
requests = &quot;^2.32.3&quot;
rich = &quot;^13.7.1&quot;
safetensors = &quot;^0.4.3&quot;
scipy = &quot;^1.13.1&quot;
sentencepiece = &quot;^0.2.0&quot;
sentry-sdk = &quot;^2.5.1&quot;
setproctitle = &quot;^1.3.3&quot;
shtab = &quot;^1.7.1&quot;
six = &quot;^1.16.0&quot;
smmap = &quot;^5.0.1&quot;
sympy = &quot;^1.12.1&quot;
tokenizers = &quot;^0.19.1&quot;
tqdm = &quot;^4.66.4&quot;
transformers = &quot;^4.41.2&quot;
trl = &quot;^0.9.4&quot;
typing-extensions = &quot;^4.12.2&quot;
tyro = &quot;^0.8.4&quot;
tzdata = &quot;^2024.1&quot;
urllib3 = &quot;^2.2.1&quot;
wandb = &quot;^0.17.1&quot;
xxhash = &quot;^3.4.1&quot;
yarl = &quot;^1.9.4&quot;
omegaconf = &quot;^2.3.0&quot;
llama-cpp-python = { version = &quot;^0.2.77&quot;, source = &quot;llama_cpp_python_cu121&quot; }
torch = { version = &quot;^2.3.1+cu121&quot;, source = &quot;torch_cu121&quot; }
nvidia-cublas-cu12 = { version = &quot;^12.1.3.1&quot;, source = &quot;torch_cu121&quot; }
nvidia-cuda-cupti-cu12 = { version = &quot;^12.1.105&quot;, source = &quot;torch_cu121&quot; }
nvidia-cuda-nvrtc-cu12 = { version = &quot;^12.1.105&quot;, source = &quot;torch_cu121&quot; }
nvidia-cuda-runtime-cu12 = { version = &quot;^12.1.105&quot;, source = &quot;torch_cu121&quot; }
nvidia-cudnn-cu12 = { version = &quot;^8.9.2.26&quot;, source = &quot;torch_cu121&quot; }
nvidia-cufft-cu12 = { version = &quot;^11.0.2.54&quot;, source = &quot;torch_cu121&quot; }
nvidia-curand-cu12 = { version = &quot;^10.3.2.106&quot;, source = &quot;torch_cu121&quot; }
nvidia-cusolver-cu12 = { version = &quot;^11.4.5.107&quot;, source = &quot;torch_cu121&quot; }
nvidia-cusparse-cu12 = { version = &quot;^12.1.0.106&quot;, source = &quot;torch_cu121&quot; }
nvidia-nccl-cu12 = { version = &quot;^2.20.5&quot;, source = &quot;torch_cu121&quot; }
nvidia-nvtx-cu12 = { version = &quot;^12.1.105&quot;, source = &quot;torch_cu121&quot; }
optimum = &quot;^1.20.0&quot;
tensorboard = &quot;^2.17.0&quot;
wheel = &quot;^0.43.0&quot;
pytorch-triton = { version = &quot;^2.3.0&quot;, source = &quot;torch_cu121&quot; }

[tool.poetry.group.dev.dependencies]
black = &quot;^24.4.2&quot;
flake8 = &quot;^7.0.0&quot;
ipykernel = &quot;^6.29.4&quot;
ipywidgets = &quot;^8.1.3&quot;
seedir = &quot;^0.4.2&quot;
emoji = &quot;^2.12.1&quot;
nbformat = &quot;^5.10.4&quot;
nbclient = &quot;^0.10.0&quot;
nbconvert = &quot;^7.16.4&quot;


[[tool.poetry.source]]
name = &quot;torch_cu121&quot;
url = &quot;https://download.pytorch.org/whl/cu121&quot;
priority = &quot;explicit&quot;


[[tool.poetry.source]]
name = &quot;llama_cpp_python_cu121&quot;
url = &quot;https://abetlen.github.io/llama-cpp-python/whl/cu121&quot;
priority = &quot;explicit&quot;


[[tool.poetry.source]]
name = &quot;torch_nightly_cu121&quot;
url = &quot;https://download.pytorch.org/whl/nightly/cu121/&quot;
priority = &quot;explicit&quot;

[build-system]
requires = [&quot;poetry-core&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;
</code></pre>
","python, pytorch, nlp, huggingface-transformers, large-language-model",
How to use Spacy-PyTextRank in newer versions?,"<p>I'm trying to do key phrase extraction with TextRank, I have installed 3.3.0 version, nltk and the en_core_web_trf are on 3.7.3, i dont know if this is the problem.
This is the error that I got</p>
<pre><code>import pytextrank
nlp=spacy.load('en_core_web_trf')
nlp.add_pipe(&quot;textrank&quot;)

  [E002] Can't find factory for 'textrank' for language English (en).
</code></pre>
<p>I'm using spacy for cuda on windows wsl ubuntu in a notebook</p>
<p>I tried to downgrade the versions, reinstalling all, calling directly the pipe(but says its deprecated) and using the efficiency core</p>
","python, nlp, artificial-intelligence, spacy, textrank",
Extracting titles from PDFs based on formatting via classification,"<p>I have about 20000 pdfs with probably 100 different layouts. Unfortunately, not all PDFs contain clean metadata. People are often lazy and did not always provide the title with it or sometimes a very short one or even worse just the file name as title.</p>
<p>Because of the number of documents, I started with PyMuPDF before diving into deep learning (e.g layout-parser, layout_lm etc.).</p>
<p>A naive implementation would be to filter for the text with the largest font size at the beginning of the document. But as can be seen here that does not work well for different layouts:
<a href=""https://www.ema.europa.eu/en/documents/overview/vectormune-nd-epar-summary-public_en.pdf"" rel=""nofollow noreferrer"">title and subtitle</a>,
<a href=""https://www.ema.europa.eu/en/documents/scientific-guideline/ich-q-7-good-manufacturing-practice-active-pharmaceutical-ingredients-step-5_en.pdf"" rel=""nofollow noreferrer"">old format</a>,
<a href=""https://www.ema.europa.eu/system/files/documents/scientific-guideline/wc500191492_en.pdf"" rel=""nofollow noreferrer"">simple example for which a naive solution works well</a></p>
<p>I thought about training a classifier based on a feature matrix created like:</p>
<pre><code>def create_feature_matrix(blocks):
    &quot;&quot;&quot;
    blocks created using pymupdf like:
    # Open the PDF file
    doc = fitz.open(pdf_path)

    # Extract data from the first page
    page = doc[0]
    text_instances = page.get_text(&quot;dict&quot;)[&quot;blocks&quot;]
    &quot;&quot;&quot;
    text_instances = blocks

    # Initialize the feature matrix
    feature_matrix = []

    # Iterate through text instances and extract features
    for instance in text_instances:
        if &quot;lines&quot; in instance:
            for line in instance[&quot;lines&quot;]:
                for span in line[&quot;spans&quot;]:
                    # Extract text, color, and positional information
                    text = span[&quot;text&quot;]
                    color = span[&quot;color&quot;]
                    size = span[&quot;size&quot;]
                    font = span[&quot;font&quot;]
                    bbox = span[&quot;bbox&quot;]  # bbox = (x0, y0, x1, y1)
                    feature_matrix.append({
                        &quot;text&quot;: text,
                        &quot;color&quot;: color,
                        &quot;size&quot;: size,
                        &quot;font&quot;: font,
                        &quot;x0&quot;: bbox[0],
                        &quot;y0&quot;: bbox[1],
                        &quot;x1&quot;: bbox[2],
                        &quot;y1&quot;: bbox[3]
                    })

    return feature_matrix
</code></pre>
<p>this code could be used together with PyMuPDF like so:</p>
<pre><code>import pandas as pd
import fitz
pdf_path = some_path
doc = fitz.open(pdf_path)
# Extract data from the first page
page = doc[0]
blocks = page.get_text(&quot;dict&quot;)[&quot;blocks&quot;]
FM_for_one_page = pd.DataFrame(create_feature_matrix(blocks))
</code></pre>
<p>My plan is to manually label the rows based on the text like: text is title: 1, not a title: 0.</p>
<p>This solution might be an improvement to my naive rule based classifier. However, I doubt my approach is very robust and it opens more questions:</p>
<ul>
<li>Would it be ok to just concatenate the feature matrices for all first pages? I do loose the information about the page boundaries. But I have no idea what else I could do.</li>
<li>The features of the titles depend on the surrounding structure and sequence. What model could capture that?</li>
<li>Do you think I am on a right path? If not, how would you tackle such a challenge?</li>
</ul>
","python, pdf, nlp",
Fairseq Installation Fails: Missing version.txt Error During Build Pyhton 3.11 how to fix?,"<p>I am trying to install the fairseq library using pip in a Conda environment, but the installation fails with the following error</p>
<pre class=""lang-py prettyprint-override""><code>error: subprocess-exited-with-error

× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─&gt; [19 lines of output]
    Traceback (most recent call last):
      File &quot;/opt/conda/envs/beyond_scale_div_coeff/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;, line 353, in &lt;module&gt;
        main()
      File &quot;/opt/conda/envs/beyond_scale_div_coeff/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;, line 335, in main
        json_out['return_val'] = hook(**hook_input['kwargs'])
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File &quot;/opt/conda/envs/beyond_scale_div_coeff/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;, line 118, in get_requires_for_build_wheel
        return hook(config_settings)
               ^^^^^^^^^^^^^^^^^^^^^
      File &quot;/tmp/pip-build-env-nrzuksph/overlay/lib/python3.11/site-packages/setuptools/build_meta.py&quot;, line 327, in get_requires_for_build_wheel
        return self._get_build_requires(config_settings, requirements=[])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File &quot;/tmp/pip-build-env-nrzuksph/overlay/lib/python3.11/site-packages/setuptools/build_meta.py&quot;, line 297, in _get_build_requires
        self.run_setup()
      File &quot;/tmp/pip-build-env-nrzuksph/overlay/lib/python3.11/site-packages/setuptools/build_meta.py&quot;, line 313, in run_setup
        exec(code, locals())
      File &quot;&lt;string&gt;&quot;, line 27, in &lt;module&gt;
      File &quot;&lt;string&gt;&quot;, line 18, in write_version_py
    FileNotFoundError: [Errno 2] No such file or directory: 'fairseq/version.txt'
    [end of output]

note: This error originates from a subprocess, and is likely not a problem with pip.

</code></pre>
<p>What I’ve Tried:</p>
<ul>
<li>updating pip</li>
<li>pip install fairseq</li>
</ul>
<p>Environment Details:</p>
<pre><code>Python Version: 3.11
Operating System: Ubuntu 22.04
Pip Version: 23.1.2
Conda Environment: Yes
</code></pre>
<p>How can I successfully install fairseq without encountering this error? Is there a workaround or a specific setup process I need to follow?</p>
","nlp, huggingface-transformers, fairseq",
Error when using inputs_embeds with generate method,"<p>I'm encountering a problem when trying to use inputs_embeds to pass the embedding to my model:</p>
<pre><code>ValueError: You passed `inputs_embeds` to `.generate()`, but the model class LlamaForCausalLM doesn't have its forwarding implemented. See the GPT2 implementation for an example (https://github.com/huggingface/transformers/pull/21405), and feel free to open a PR with it!
</code></pre>
<p>Can someone help me understand what is going on and how to fix this issue?</p>
<p>I have an embedding with the shape <code>torch.Size([1, 46, 4096])</code> and I want to pass it to my model. Here is my code:</p>
<pre><code>if True:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = &quot;/content/drive/My Drive/finetuneunslothllama&quot;, 
        max_seq_length = max_seq_length,`your text`
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Activer native 2x faster inference

outputs = model.generate(inputs_embeds=embeddings, max_new_tokens=64, use_cache=True)

generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)
print(generated_text)
</code></pre>
","nlp, huggingface-transformers, large-language-model, embedding, word-embedding",
NLLB Fine-Tuning Error: Missing data_prefix Configuration (English-German Translation),"<p>I'm attempting to fine-tune the NLLB model <code>&quot;facebook/nllb-200-distilled-600M&quot;</code> for a scientific translation task from English (eng_Latn) to German (deu_Latn). I followed the official guidelines for fine-tuning by authors of nllb.</p>
<p>Documentation: <a href=""https://github.com/facebookresearch/fairseq/tree/nllb?tab=readme-ov-file"" rel=""nofollow noreferrer"">link</a></p>
<p>This is the code block which is giving error:</p>
<pre><code>DATA_CONFIG = &quot;/content/sample_data/data_config.json&quot;
OUTPUT_DIR = &quot;/content/outputs&quot;
MODEL_FOLDER = &quot;/content/drive/MyDrive/Thesis/nllb-checkpoints&quot;
DROP = 0.1
SRC = &quot;eng_Latn&quot;
TGT = &quot;deu_Latn&quot;
!python /content/fairseq/examples/nllb/modeling/train/train_script.py \
    cfg=nllb200_dense3.3B_finetune_on_fbseed \
    cfg/dataset=default \
    cfg.dataset.lang_pairs=&quot;$SRC-$TGT&quot; \
    cfg.fairseq_root=$(pwd) \
    cfg.output_dir=$OUTPUT_DIR \
    cfg.dropout=$DROP \
    cfg.warmup=10 \
    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt
</code></pre>
<p>This is the error:</p>
<pre><code>/content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path=&quot;conf&quot;, config_name=&quot;base_config&quot;)
/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
TRAINING DIR:  /content/outputs
Error executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt']
Traceback (most recent call last):
  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 289, in main
    train_module = TrainModule(config)
  File &quot;/content/fairseq/examples/nllb/modeling/train/train_script.py&quot;, line 122, in __init__
    assert cluster_name in cfg.dataset.data_prefix
omegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct
    full_key: cfg.dataset.data_prefix
    object_type=dict

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
</code></pre>
<p>So far, I understand there is a <code>Missing data_prefix configuration</code>. I created a demo custom data_config.json. Which looks like this:</p>
<pre><code>{
    &quot;data_prefix&quot;: &quot;/content/sample_data&quot;,
    &quot;train_data&quot;: &quot;train_demo.json&quot;,
    &quot;test_data&quot;: &quot;test_demo.json&quot;,
    &quot;lang_pairs&quot;: &quot;eng_Latn-deu_Latn&quot;
}
</code></pre>
<p>While the official documentation provides some information, I'm encountering difficulties in applying it to my specific use case. Can someone share a detailed guide or point me to helpful resources on fine-tuning NLLB?</p>
","python, nlp, machine-translation, fine-tuning, fairseq","<p>While I can't help you with the concrete error message you are getting (my guess would be issues with structure of the provided JSON files), my personal recommendation would be to fine-tune NLLB in the <code>transformers</code> library, specifically using the <code>Seq2SeqTrainer</code>.</p>
<p>I did this before for multiple models, including NLLB, check out this repository: <a href=""https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/"" rel=""nofollow noreferrer"">https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/</a></p>
<p>This way the fine-tuning and inference process for the NLLB model is the same as any bilingual model (you can find guides for those more easiely), with the only exception that you load the tokenizer like so:</p>
<pre><code>tokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=&quot;eng_Latn&quot;, tgt_lang=&quot;deu_Latn&quot;)
</code></pre>
<p>and generate translations like this:</p>
<pre><code>model.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(&quot;deu_Latn&quot;)[1], max_length=512)
</code></pre>
"
"what are from_messages, from_template, format, format_messages in language? Can someone simplify?","<p>Can you give me the context on when to use and why of the above mentioned functions?</p>
<p>I tried reading the docs but couldn't understand much. Any referecnces to blogs or links for this would be helpful as well.</p>
<p>Here are the docs I referred to
PromptTemplate -
<a href=""https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html"" rel=""nofollow noreferrer"">https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html</a></p>
<p>ChatPromptTemplate - <a href=""https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html"" rel=""nofollow noreferrer"">https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html</a></p>
<p>Thanks in advance.</p>
","python, nlp, prompt, langchain",
Where to find spacy.py file to rename,"<p>I am attempting to install <a href=""https://spacy.io/usage/"" rel=""nofollow noreferrer"">spacy</a> and have tried a number of methods using <code>pip</code>, <code>conda</code>, and installing directing from git. However, I am running into the same error:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[26], line 3
      1 import spacy
----&gt; 3 nlp = spacy.load(&quot;en_core_web_sm&quot;)

AttributeError: module 'spacy' has no attribute 'load'
</code></pre>
<p>From reading various articles online like <a href=""https://stackoverflow.com/questions/67769003/attributeerror-module-spacy-has-no-attribute-load"">this one</a>, I see that my error is likely due to a file called &quot;spacy.py&quot; that is causing a shadowing error. However, I can't find this file.</p>
<p>Using <code>which python</code> in my dir shows me:</p>
<pre><code>/Users/my_name/anaconda3/bin/python
</code></pre>
<p>Looking into my anaconda3/bin/python dir, I do see a Unix Executable file named &quot;spacy&quot; but renaming it has not fixed my error.</p>
","python, nlp, spacy","<p>Make sure you're also installing your preferred package:</p>
<p>For pip:</p>
<pre><code>python -m spacy download en_core_web_sm
</code></pre>
<p>Also, if you're using a virtual environment in your IDE, you should also run the following commands BEFORE installing spacy and your preferred package:</p>
<p>For pip:</p>
<pre><code>python -m venv .env
source .env/bin/activate
</code></pre>
<p>For conda:</p>
<pre><code>conda create -n venv
conda activate venv
</code></pre>
<p>There could also be a <a href=""https://github.com/explosion/spaCy/issues/2306"" rel=""nofollow noreferrer"">mismatch</a> between your environments. If you're using Anaconda, try installing spacy in the Anaconda Powershell. Otherwise, try installing spacy using conda and/or pip in your IDE's terminal.</p>
"
Closest match for job titles,"<p>I have master list of known job titles and looking for ways to extract the same from the searched term. For example:</p>

<p>Searched job title: <strong>Senior Digital Marketing Specialist</strong><br>
Extracted to: <strong>Senior Digital Marketing</strong></p>

<p>Searched job title: <strong>Retail In-Store Sales Assistant; Full Time</strong><br>
Extracted to: <strong>Retail Sales Assistant</strong></p>

<p>So I tried to extract parameters that would be helpful for cleaning up the searched query. <br><br>
1) The occurrence of the 2 tokens in the db. (To get mathematical evaluation of how much are the terms related with each other)
Example: </p>

<pre><code> t01-&gt;t0 or t1        Senior || java---&gt;226374 
 t02-&gt;t0 or t2        Senior || software---&gt;2566450 
 t03-&gt;t0 or t3        Senior || engineer---&gt;7220787 
 t12-&gt;t1 or t2        java || software---&gt;315397
 t13-&gt;t1 or t3        java || engineer---&gt;407682
 t23-&gt;t2 or t3        software || engineer---&gt;11533495

 total =t01+t02+t03+t12+t13+t23
</code></pre>

<p><img src=""https://i.sstatic.net/o39hO.png"" alt=""enter image description here""></p>

<p>2) The occurrence of the token taken 1 at time in the entire db.
Example: </p>

<pre><code>t0-&gt;    Senior-----&gt;55042636  
t1-&gt;    java-----&gt;1655805
t2-&gt;    software-----&gt;26136204
t3-&gt;    engineer-----&gt;81574912
</code></pre>

<p>3) I took the sum of the related tokens and put a minimum threshold of 5% and that give me the following output, i.e (txy*100)/total > 5</p>

<p>My Output : <strong>Senior software engineer</strong><br>
Anyone have any experience with similar projects or ideas for further improvement ?</p>
","java, nlp, extract, data-analysis, summarization",
What is the easiest way of converting json to vector database,"<p>I need to convert a json file to vector database. I use chromadb.</p>
<pre><code>collection.add(
    documents=[&quot;This is a document&quot;, &quot;This is another document&quot;],
    metadatas=[{&quot;source&quot;: &quot;my_source&quot;}, {&quot;source&quot;: &quot;my_source&quot;}],
    ids=[&quot;id1&quot;, &quot;id2&quot;]
)
</code></pre>
<p>This code is used to add a new data to the collection. How can I give a one big json file with complex fields(for example some fields has array values) to save into the collection. Or is there a better way to keep vector database rather than chromadb</p>
","nlp, large-language-model, vector-database",
Removing bi-grams after tokenization for TfidfVectorizer,"<p>I'm attempting to remove bi-grams that are created by <code>TfidfVectorizer</code>.  I'm using <code>text.TfidfVectorizer</code> so that I can use my own preprocessor function.</p>
<p>Test strings and preprocessor function:</p>
<pre><code>doc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', 
        'another that has aa aa and start date and hurricane hitting south carolina']

def remove_bigrams(doc):
    gram_2 = ['past performance', 'start date', 'aa aa']
    res = []
    for record in doc:
        the_string = record
        for phrase in gram_2:
            the_string = the_string.replace(phrase, &quot;&quot;)
        res.append(the_string)
    return res

remove_bigrams(doc2)
</code></pre>
<p>My <code>TfidfVectorizer</code> instantiation and <code>fit_transform</code>:</p>
<pre><code>from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import text

custom_stop_words = [i for i in stop_words]

vec = text.TfidfVectorizer(stop_words=custom_stop_words,
                           analyzer='word',
                           ngram_range=(2, 2),
                           preprocessor=remove_bigrams,
                          )

features = vec.fit_transform(doc2)
</code></pre>
<p>Here is my error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [49], in &lt;cell line: 5&gt;()
      3 #t3_cv = CountVectorizer(t2, stop_words = stop_words)
      4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)
----&gt; 5 features = vec.fit_transform(doc2)

File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)
   2072 self._check_params()
   2073 self._tfidf = TfidfTransformer(
   2074     norm=self.norm,
   2075     use_idf=self.use_idf,
   2076     smooth_idf=self.smooth_idf,
   2077     sublinear_tf=self.sublinear_tf,
   2078 )
-&gt; 2079 X = super().fit_transform(raw_documents)
   2080 self._tfidf.fit(X)
   2081 # X is already a transformed view of raw_documents so
   2082 # we set copy to False

File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)
   1330             warnings.warn(
   1331                 &quot;Upper case characters found in&quot;
   1332                 &quot; vocabulary while 'lowercase'&quot;
   1333                 &quot; is True. These entries will not&quot;
   1334                 &quot; be matched with any documents&quot;
   1335             )
   1336             break
-&gt; 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)
   1340 if self.binary:
   1341     X.data.fill(1)

File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)
   1207 for doc in raw_documents:
   1208     feature_counter = {}
-&gt; 1209     for feature in analyze(doc):
   1210         try:
   1211             feature_idx = vocabulary[feature]

File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)
    111     doc = preprocessor(doc)
    112 if tokenizer is not None:
--&gt; 113     doc = tokenizer(doc)
    114 if ngrams is not None:
    115     if stop_words is not None:

TypeError: expected string or bytes-like object
</code></pre>
<p>How to resolve it?</p>
","python, scikit-learn, nlp, preprocessor, tfidfvectorizer","<p>The preprocessor should handle documents, not the whole corpus. (The clues are the &quot;expected string&quot; in the error, and the fact that <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">the <code>TfidfVectorizer</code> docs</a> refer to &quot;the preprocessing (string transformation) stage&quot;. The docs could definitely be clearer.)</p>
<p>This should fix it:</p>
<pre><code>def remove_bigrams(doc: str) -&gt; str:
    &quot;&quot;&quot;Remove certain bi-grams from a document.&quot;&quot;&quot;
    gram_2 = ['past performance', 'start date', 'aa aa']
    for phrase in gram_2:
        doc = doc.replace(phrase, &quot;&quot;)
    return doc
</code></pre>
"
Spacy Transformer Model Not Able to Unload From GPU When Run in Loop,"<p>Why is the SpaCy model not able to unload from GPU when run in loop? I want to unload the model at different interval of time. Also their is CPU memory leak issue with this model. How to resolve this issue.</p>
<pre><code>import spacy
import torch
import time
spacy.require_gpu()

for _ in range(10):
    print(&quot;The GPU occupied before model load: &quot;, torch.cuda.memory_allocated())
    ner_model = spacy.load(&quot;en_core_web_trf&quot;, disable=[&quot;tagger&quot;, &quot;parser&quot;, &quot;attribute_ruler&quot;, &quot;lemmatizer&quot;, &quot;morphologizer&quot;, &quot;senter&quot;, &quot;textcat&quot;])
    print(&quot;The GPU occupied after model load: &quot;, torch.cuda.memory_allocated())
    time.sleep(10)
    del ner_model
    time.sleep(10)
</code></pre>
<p>I deleted the model object but still not able to unload the model. Why?</p>
<p>I want to unload the model periodically. I want the model should unload whenever i need.</p>
","nlp, gpu, transformer-model, spacy-3, spacy-transformers",
Why Is My Skip-Gram Implementation Producing Incorrect Results?,"<p>I'm implementing a Skip-Gram model for Word2Vec using Python. However, my model doesn't seem to be working correctly, as indicated by the resulting embeddings and their visualization. Here is an example of the 3D plot of the embeddings, which shows words clustered together and overlapping, making it difficult to distinguish between them:</p>
<p>I suspect that the issue lies in my implementation rather than the plotting function.</p>
<pre><code>import numpy as np
from nltk.corpus import stopwords
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import re

np.random.seed(10)

def softmax(x):
    '''
                 (xi - max{x1,x2,...,xv})
                e
    xi =    --------------
                  (xj - max{x1,x2,...,xv})
             ∑j  e
    '''
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

class SkipGram:
    def __init__(self,ws=2,dim=8) -&gt; None:
        self.X = []
        self.N = dim
        self.Y = []
        self.window_size = ws
        self.alpha = 0.1
        self.vocab = {}
        self.vocab_size = 0
        
    def __create_vocabulary(self,corpus):
        stop_words = set(stopwords.words(&quot;english&quot;))
        filtered_corpus = []
        self.vocab_size = 0
        for i,sentence in enumerate(corpus):
            if isinstance(sentence,str) :
                corpus[i] = sentence.split()
            filtered_corpus.append([])
            j = 0
            for word in corpus[i]:
                w = re.sub(r'[^a-z]+','',word.lower())
                if w != '' and w not in stop_words:
                    corpus[i][j] = w
                    filtered_corpus[i].append(w)
                else:
                    continue
                if corpus[i][j].lower() not in self.vocab:
                    self.vocab[corpus[i][j].lower()] = self.vocab_size
                    self.vocab_size += 1
                j += 1
        return filtered_corpus
        
    def __create_context_and_center_words(self,processed_corpus):
        for sentence in processed_corpus:
            for i,word in enumerate(sentence):
                center_word = np.zeros((self.vocab_size,1))
                center_word[self.vocab[word]][0] = 1
                context = np.zeros((self.vocab_size,1))
                
                for j in range(i-self.window_size,i + self.window_size + 1):
                    if j != i and j &gt;= 0 and j &lt; len(sentence):
                        context[self.vocab[sentence[j]]][0] += 1
                self.X.append(center_word)
                self.Y.append(context)
        self.X = np.array(self.X)
        self.Y = np.array(self.Y)
                
        
    def initialize(self,corpus):
        corpus = self.__create_vocabulary(corpus)
        self.__create_context_and_center_words(corpus)
        self.W1 = np.random.rand(self.vocab_size,self.N)
        self.W2 = np.random.rand(self.N,self.vocab_size)
    
    def feed_forward(self,x):
        h = np.dot(self.W1.T,x) # N V . V 1 -&gt; N 1
        u = np.dot(self.W2.T,h) # V N . N 1 -&gt; V 1
        y = softmax(u)
        return h,u,y
    
    def backpropagate(self,x,y_actual,y_result,h):
        e = y_result - y_actual # V 1
        dw2 = np.dot(h,e.T) # N 1 . 1 V -&gt;  N V
        eh = np.dot(self.W2,e) # N x V . V x 1 -&gt;  N x 1
        dw1 = np.dot(x,eh.T) # V x 1 . 1 x N -&gt; V x N
        return dw1,dw2
        
    def train(self,epochs):
        for i in range(epochs):
            loss = 0
            dw1,dw2 = np.zeros_like(self.W1),np.zeros_like(self.W2)
            for j in range(len(self.X)):
                h,_,y = self.feed_forward(self.X[j])
                a,b = self.backpropagate(self.X[j],self.Y[j],y,h)
                dw1 += a
                dw2 += b
                loss -=  np.sum(self.Y[j] * np.log(y+1e-08))
            loss /= len(self.X)
            [dw1,dw2] = [dw1/len(self.X), dw2/len(self.X)]
            self.W1 -= self.alpha * dw1
            self.W2 -= self.alpha * dw2
            print(f'Epoch : {i+1}, Loss = {loss}')
            
    def get_similar_words(self,word,n):
        if word in self.vocab:
            x = np.zeros((self.vocab_size,))
            x[self.vocab[word]] = 1
            _,_,y = self.feed_forward(x)
            output = {}
            for i in range(self.vocab_size):
                output[y[i]] = i
            words = {i:word for i,word in enumerate(self.vocab.keys())}
            context = []
            for k in sorted(output,reverse=True):
                context.append(words[output[k]])
                if len(context) == n:
                    break
            return context
        else:
            print(&quot;Given Word not found&quot;)
            
    def get_vector(self,word):
        return self.W1[self.vocab[word]]
            
    def plot(self):
        tsne = TSNE(n_components=3,random_state=0,perplexity=self.vocab_size-1)
        vectors_3d = tsne.fit_transform(self.W1)
        fig = plt.figure(figsize=(12,8))
        ax = fig.add_subplot(111,projection='3d')
        ax.scatter(vectors_3d[:,0],vectors_3d[:,1],vectors_3d[:,2],marker='o',edgecolors='k')
        for word,i in self.vocab.items():
            ax.text(vectors_3d[i,0],vectors_3d[i,1],vectors_3d[i,2],word)
        ax.set_title('Word2Vec Word Embeddings')
        ax.set_xlabel('Dimension 1')
        ax.set_ylabel('Dimension 2')
        ax.set_zlabel('Dimension 3')
        plt.show()
        
</code></pre>
<pre><code>#main.py
from nltk.corpus import gutenberg

corpus = gutenberg.sents()[:40]
w2v = SkipGram(3,20)
w2v.initialize(corpus)
w2v.train(200)
w2v.plot()
</code></pre>
<p><img src=""https://i.sstatic.net/TCoZiNJj.png"" alt=""Output of my model"" /></p>
<p>I have tried adjusting the learning rate and initializing weights with different values, but the issue persists.</p>
<p>What might be going wrong with my implementation?</p>
<p>Reviewed the code for generating vocabulary and context words.
Checked the weight initialization and learning rate settings.</p>
","python, machine-learning, nlp, word2vec",
Hugging Face Transformers embedding layer position and token_type embeddings,"<p>I am trying to evaluate different Integrated Gradients methods on my RoBERTa based model, and I came to a paper introducing &quot;Sequential Integrated Gradients&quot; with this github repo:
<a href=""https://github.com/josephenguehard/time_interpret/blob/main/experiments/nlp/utils.py"" rel=""nofollow noreferrer"">text</a></p>
<p>I understand to get the IGs we need to pass input_embeddings not ids to the model so it's able to compute gradients but in the given code the author is producing the embeddings like:</p>
<p><code>getattr(model, model_name).embeddings.word_embeddings(input_ids)</code></p>
<p>and later generate positional and type embeddings (also attention mask) manually and finally summing them and applying the LayerNorm and dropout layers to pass to the model using this wrapper:</p>
<pre><code>class ForwardModel(nn.Module):
    def __init__(self, model, model_name):
        super().__init__()
        self.model = model
        self.model_name = model_name

    def forward(
        self,
        input_embed,
        attention_mask=None,
        position_embed=None,
        type_embed=None,
        return_all_logits=False,
    ):
        embeds = input_embed + position_embed
        if type_embed is not None:
            embeds += type_embed

        # Get predictions
        embeds = getattr(self.model, self.model_name).embeddings.dropout(
            getattr(self.model, self.model_name).embeddings.LayerNorm(embeds)
        )
        pred = self.model(
            inputs_embeds=embeds,
            attention_mask=attention_mask,
        )[0]

        # Return all logits or just maximum class
        if return_all_logits:
            return pred
        else:
            return pred.max(1).values
</code></pre>
<p>however when I pass the model just &quot;embeds&quot; itself using argument &quot;inputs_embeds&quot; I get the same output as passing &quot;input_ids&quot; using &quot;input_ids&quot; argument, it means the model handles the position and type embeddings itself. So adding them manually to input_embeddings brings a different output. It seems they did it maybe because the model bypasses those embeddings if the input_embeddings instead of ids are passed. Can someone clarify?</p>
","nlp, huggingface-transformers",
Is there any way to flexibly change between VLM and LLM?,"<p>I want to train a VLM from LLM but when I inference, does the input have to contain image embedding and if I train with PEFT, will the other tasks of LLM be weakened?</p>
<p>My goal is that I want to keep the strengths of other tasks on LLM while it learns more about vision and when inferring the user can flexibly change between adding and not adding to the image. So is there a way?</p>
<p>Can I build something similar to BLIP2 but with an LLM of my own choosing?</p>
<p>I tried building a QFomer based layer with 2 images but the loss hardly decreases after each training, and I don't know where I'm going wrong.</p>
<p>This is my model:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoImageProcessor, AutoModel
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel, PretrainedConfig


class SelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super(SelfAttention, self).__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads).to(torch.bfloat16)
    
    def forward(self, x):
        x = x.to(torch.bfloat16)
        attn_output, _ = self.self_attn(x, x, x)
        return attn_output

class CrossAttention(nn.Module):
    def __init__(self, query_dim, key_dim, hidden_dim, num_heads):
        super(CrossAttention, self).__init__()
        self.query_proj = nn.Linear(query_dim, hidden_dim).to(torch.bfloat16)
        self.key_proj = nn.Linear(key_dim, hidden_dim).to(torch.bfloat16)
        self.value_proj = nn.Linear(key_dim, hidden_dim).to(torch.bfloat16)
        self.multihead_attn = nn.MultiheadAttention(hidden_dim, num_heads).to(torch.bfloat16)
        self.output_proj = nn.Linear(hidden_dim, 1024).to(torch.bfloat16)
        
    def forward(self, query, key):
        query = query.to(torch.bfloat16)
        key = key.to(torch.bfloat16)
        value = self.value_proj(key)
        query = self.query_proj(query)
        key = self.key_proj(key)
        attn_output, _ = self.multihead_attn(query, key, value)
        output = self.output_proj(attn_output)
        return output
    
class QFormerLayer(nn.Module):
    def __init__(self, img_dim, hidden_dim, num_heads, feedforward_dim, text_dim):
        super(QFormerLayer, self).__init__()
        self.self_attn1 = SelfAttention(img_dim, num_heads)
        self.self_attn2 = SelfAttention(img_dim, num_heads)
        self.cross_attn_imgs = CrossAttention(img_dim, img_dim, hidden_dim, num_heads)
        self.cross_attn_text = CrossAttention(text_dim, img_dim, hidden_dim, num_heads)
        self.feedforward = nn.Sequential(
            nn.Linear(img_dim, feedforward_dim).to(torch.bfloat16),
            nn.ReLU(),
            nn.Linear(feedforward_dim, img_dim).to(torch.bfloat16)
        )
        
    def forward(self, text, img1, img2):
        img1_attn = self.self_attn1(img1)
        img2_attn = self.self_attn2(img2)
        
        img_combined = self.cross_attn_imgs(img1_attn, img2_attn)
        cross_attn_output = self.cross_attn_text(text, img_combined)
        output = self.feedforward(cross_attn_output)
        return output, img1, img2

class QFormer(nn.Module):
    def __init__(self, img_dim, hidden_dim, num_heads, feedforward_dim, text_dim, num_layers):
        super(QFormer, self).__init__()
        self.layers = nn.ModuleList([QFormerLayer(img_dim, hidden_dim, num_heads, feedforward_dim, text_dim) for _ in range(num_layers)])

    def forward(self, text, img1, img2):
        for layer in self.layers:
            output, img1, img2 = layer(text, img1, img2)
        return output

class PreTrainedModelForQFormer(PreTrainedModel):
    config_class = PretrainedConfig

    def __init__(self, config):
        super().__init__(config)
        self.text_dim = config.text_dim
        self.img_dim = config.img_dim
        self.hidden_dim = config.hidden_dim
        self.num_heads = config.num_heads
        self.feedforward_dim = config.feedforward_dim
        self.num_layers = config.num_layers
        self.qformer = QFormer(self.img_dim, self.hidden_dim, self.num_heads, self.feedforward_dim, self.text_dim, self.num_layers)
        self.output_proj = nn.Linear(self.img_dim, 2048).to(torch.bfloat16)
        self.vs = AutoModel.from_pretrained(config.vision_model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).to('cuda')
        self.vs1 = AutoModel.from_pretrained(config.vision_model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).to('cuda')
        self.lm = AutoModelForCausalLM.from_pretrained(config.lm_model_name).to(torch.bfloat16).to('cuda')
        self.tokenizer = AutoTokenizer.from_pretrained(config.lm_model_name)
        for param in self.lm.parameters():
            param.requires_grad = False
        for param in self.vs.parameters():
            param.requires_grad = False
        for param in self.vs1.parameters():
            param.requires_grad = True

    def forward(self, text, img1, img2, label):
        img1 = img1.to(torch.bfloat16)
        img2 = img2.to(torch.bfloat16)
        text, labels, attn_mask = collate_fn(self.tokenizer, text, label, device=&quot;cuda&quot;)
        img1 = self.vs(img1).pooler_output
        img2 = self.vs1(img2).pooler_output
        qformer_output = self.qformer(text, img1, img2)
        qformer_output = self.output_proj(qformer_output)
        text_embeds = self.lm.get_input_embeddings()(text)
        inputs_embeds = torch.cat([qformer_output.unsqueeze(1), text_embeds], dim=1)
        labels = torch.cat([torch.full((labels.size(0), 1), -100, dtype=labels.dtype, device=labels.device), labels], dim=1)
        attn_mask = torch.cat([torch.ones((attn_mask.size(0), 1), dtype=attn_mask.dtype, device=attn_mask.device), attn_mask], dim=1)
        return self.lm(inputs_embeds=inputs_embeds, labels=labels, attention_mask=attn_mask)
    
    def save_pretrained(self, save_directory):
        torch.save(self.qformer.state_dict(), f&quot;{save_directory}/qformer_weights.pth&quot;)
        torch.save(self.output_proj.state_dict(), f&quot;{save_directory}/output_proj_weights.pth&quot;)

    @classmethod
    def from_pretrained(cls, config, load_directory):
        model = cls(config)
        model.qformer.load_state_dict(torch.load(f&quot;{load_directory}/qformer_weights.pth&quot;, map_location=torch.device('cuda')))
        model.output_proj.load_state_dict(torch.load(f&quot;{load_directory}/output_proj_weights.pth&quot;, map_location=torch.device('cuda')))
        return model

class QFormerConfig(PretrainedConfig):
    def __init__(self, text_dim=512, img_dim=2048, hidden_dim=512, num_heads=8, feedforward_dim=1024, num_layers=6, lm_model_name=&quot;google/gemma-1.1-2b-it&quot;, vs_model_name='OpenGVLab/InternViT-300M-448px', **kwargs):
        super().__init__(**kwargs)
        self.text_dim = text_dim
        self.img_dim = img_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.feedforward_dim = feedforward_dim
        self.num_layers = num_layers
        self.lm_model_name = lm_model_name
        self.vision_model_name = vs_model_name

text_dim = 512
img_dim = 1024
hidden_dim = 1024
num_heads = 8
feedforward_dim = 2048
num_layers = 1
lm_model_name = &quot;google/gemma-1.1-2b-it&quot;
</code></pre>
","pytorch, nlp, large-language-model",
word-embedding: Convert supervised model into unsupervised model,"<p>I want to load an pre-trained embedding to initialize my own unsupervise FastText model and retrain with my dataset.</p>
<p>The trained embedding file I have loads fine with <code>gensim.models.KeyedVectors.load_word2vec_format('model.txt')</code>. But when I try:
<code>FastText.load_fasttext_format('model.txt')</code> I get: <code>NotImplementedError: Supervised fastText models are not supported</code>.</p>
<p>Is there any way to convert supervised KeyedVectors to unsupervised FastText? And if possible, is it a bad idea?</p>
<p>I know that has an great difference between supervised and unsupervised models. But I really wanna try use/convert this and retrain it. I'm not finding a trained unsupervised model to load for my case (it's a portuguese dataset), and the best model I find <a href=""http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc"" rel=""nofollow noreferrer"">is that</a></p>
","python, nlp, gensim, unsupervised-learning, fasttext","<p>If your <code>model.txt</code> file loads OK with <code>KeyedVectors.load_word2vec_format('model.txt')</code>, then that's just a simple set of word-vectors. (That is, not a 'supervised' model.)</p>
<p>However, Gensim's <code>FastText</code> doesn't support preloading a simple set of vectors for further training - for continued training, it needs a full <code>FastText</code> model, either from Facebook's binary format, or a prior Gensim <code>FastText</code> model <code>.save()</code>.</p>
<p>(That trying to load a plain-vectors file generates that error suggests the <code>load_fasttext_format()</code> method is momentarily mis-interpreting it as some other kind of binary FastText model it doesn't support.)</p>
<p><em>Update after comment below:</em></p>
<p>Of course you <em>can</em> mutate a model however you like, including ways not officially supported by Gensim. Whether that's helpful is another matter.</p>
<p>You can create an FT model with a compatible/overlapping vocabulary, load old word-vectors separately, then copy each prior vector over to replace the corresponding (randomly-initialized) vectors in the new model. (Note that the property to affect further training is actually <code>ftModel.wv.vectors_vocab</code> trained-up full-word vectors, <em>not</em> the <code>.vectors</code> which is composited from full-words &amp; ngrams,)</p>
<p>But the tradeoffs of such an ad-hoc strategy are many. The ngrams would still start random. Taking some prior model's just-word vectors isn't quite the same as a FastText model's full-words-to-be-later-mixed-with-ngrams.</p>
<p>You'd want to make sure your new model's sense of word-frequencies is meaningful, as those affect further training - but that data <em>isn't</em> usually available with a plain-text prior word-vector set. (You could plausibly synthesize a good-enough set of frequencies by assuming a Zipf distribution.)</p>
<p>Your further training might get a &quot;running start&quot; from such initialization - but that wouldn't necessarily mean the end-vectors remain comparable to the starting ones. (All positions may be arbitrarily changed by the volume of newer training, progressively diluting away most of the prior influence.)</p>
<p>So: you'd be in an improvised/experimental setup, somewhat far from usual FastText practices and thus where you'd want to re-verify lots of assumptions, and rigorously evaluate if those extra steps/approximations are actually improving things.</p>
"
How to find a text name of Tool classes in LangChain?,"<p>The LangChain documentation states that it offers integrations with various services and APIs to let agents interact with the world. These agents are mostly available under <code>langchain_community</code> or <code>langchain_core</code> packages.</p>
<p>Code examples show that tools can be loaded through their class names like <code>ArxivQueryRun</code> or using the <code>load_tools</code> function like below:</p>
<pre class=""lang-py prettyprint-override""><code>from langchain_community.agent_toolkits.load_tools import load_tools

tools = load_tools(['arxiv'])

print(tools[0].invoke(&quot;StackOverflow&quot;))
</code></pre>
<p>To me, <code>load_tools</code> function seems much more straightforward as it provides a single way to retrieve all LangChain tools. The catch, however, is that you are required to know text names of tool classes like the example of <code>'arxiv'</code> and <code>ArxivQueryRun</code>.</p>
<p>Is there a way to get the text name of any tool class so that we can pass it to the <code>load_tools</code> function?</p>
","nlp, langchain",
Difference between CBOW and Skipgram gradients in word2vec?,"<p>Why are <code>f</code> values that are greater than or lower than <code>MAX_EXP</code> taken into account during the updates in CBOW, but ignored in Skipgram? </p>

<p>I'm specifically looking at the Google implementation of word2vec, but the same functionality has been replicated throughout many other projects, one of which is <a href=""https://github.com/dav/word2vec/blob/master/src/word2vec.c"" rel=""nofollow noreferrer"">here</a>, for larger context.</p>

<pre><code>// CBOW negative sampling gradient calculations  
f = 0;
l2 = target * layer1_size;
for (c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1neg[c + l2];
// ** here, we still update, but essentially round the value to 1 or 0
if (f &gt; MAX_EXP) g = (label - 1) * alpha;
else if (f &lt; -MAX_EXP) g = (label - 0) * alpha;
else g = (label - expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;

// ---------------------------

// Skipgram hierarchical softmax gradient calculations
f = 0;
l2 = vocab[word].point[d] * layer1_size;
for (c = 0; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1[c + l2];
// ** here, we don't update if f is outside the range given by MAX_EXP **
if (f &lt;= -MAX_EXP) continue;
else if (f &gt;= MAX_EXP) continue;
else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];
g = (1 - vocab[word].code[d] - f) * alpha;
</code></pre>
","c++, nlp, word2vec",
Polars/Spark/SQL Standardize similar company names in table column,"<p>I have a table with a column of company names. The same company can appear with a variety of names (e.g. <code>'Ciao', 'Ciao Inc', 'Ciao Inc User'</code>).
I want to provide the same company under different names with a unique identifier, as per the following example (assume the arrays are columns):
<code>['Ciao', 'Ciao Inc', 'HB', 'Ciao Inc User', 'HB lmtd'] -&gt; [1, 1, 2, 1, 2]</code></p>
<p>I would like to achieve this in either Polars, Spark, or plain SQL.</p>
<p>Do you know of any simple way to achieve this?
Thanks a lot</p>
","python, sql, pyspark, nlp, python-polars",
Issue with PyTorch&#39;s transformer Model repeating last token during inference,"<p>I’ve been trying to implement PyTorch’s nn.TransformerEncoder and nn.TransformerDecoder solutions into a simple model, but I’m running into an issue that I’m unable to resolve where during inference the model only produces the last token fed into it.</p>
<p>For example lets say I have a tensor [1,2,3,4,5] the model will continue the sequence with [1,2,3,4,5,5,5,5,5,5,…] or if I had [5,2,8,3] it would continue to produce [5,2,8,3,3,3,3,3,3,3,…] even when using training data as input although when using a new randomly initialized model it will produce diverse output although since not trained is useless.</p>
<p>Although it produces the above results, the loss continues to decrease as I train it further indicating that its managing to learn the dataset. Due to this I initially thought this was just a problem with the dataset where the target was the same as the input which would cause it to produce the same tokens, but after further testing I’m sure that the targets are definitely the next token in the sequence, for example the input would be [1,2,3,4] and the target would be [2,3,4,5].</p>
<p>This lead me to my current standing theory that there is something wrong with the seq2seq implementation but after much research and trying different implementations of the common components such as positional encoding, adjusting hyper-parameters and removing / adding masks to the encoder and decoder, but regardless still weeks later and I’m still zero progress towards identifying the issue.</p>
<p>For reference here is the model and training step I’m using:</p>
<pre><code>class TextEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embed_dim: int, padding_index: int):
        super(TextEmbedding, self).__init__()
        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=padding_index)

    def forward(self, x):
        return self.embedding(x)

class TextTransformer(nn.Module):
    def __init__(self, vocab_size, embed_dim = 512, nhead = 8, num_encoder_layers = 6, num_decoder_layers = 6, max_length = 5000, padding_index = 0):
        super(TextTransformer, self).__init__()
        self.vocab_size = vocab_size
        self.max_length = max_length

        self.text_embedding = TextEmbedding(vocab_size, embed_dim, padding_index)
        self.positional_encoding = nn.Parameter(torch.zeros(1, max_length, embed_dim))

        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=2048)
        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_encoder_layers)

        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=2048)
        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=num_decoder_layers)

        self.fc = nn.Sequential(
            nn.Linear(embed_dim, vocab_size)
        )

    def forward(self, src, tgt, src_mask, tgt_mask):
        #Embedding + Positional Encoding
        src_embedding = self.text_embedding(src) + self.positional_encoding[:, :src.size(1), :]
        tgt_embedding = self.text_embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]

        tgt_square_mask = create_square_mask(tgt.size(1)).to(src.device)

        #Encoder
        memory = self.encoder(src_embedding.permute(1, 0, 2), src_key_padding_mask=src_mask)

        #Decoder
        decoder_out = self.decoder(tgt_embedding.permute(1, 0, 2), memory, tgt_mask=tgt_square_mask, tgt_key_padding_mask=tgt_mask)
        decoder_out = decoder_out.permute(1, 0, 2)

        #FC output
        output = self.fc(decoder_out)

        return output

    def seq2seq(self, src, src_mask, stop_token, max_length = 500):
        src_embedding = self.text_embedding(src) + self.positional_encoding[:, :src.size(1), :]

        memory = self.encoder(src_embedding.permute(1, 0, 2), src_key_padding_mask=src_mask)
        sequence = src
        stop = False

        while sequence.shape[1] &lt; min(self.max_length, max_length) and not stop:
            tgt_embedding = self.text_embedding(sequence) + self.positional_encoding[:, :sequence.size(1), :]

            tgt_square_mask = create_square_mask(sequence.size(1)).to(src.device)
            dec_output = self.decoder(tgt_embedding.permute(1, 0, 2), memory, tgt_mask=tgt_square_mask)
            dec_output = dec_output.permute(1, 0, 2)

            out = self.fc(dec_output)[:, -1, :]
            predicted = out.argmax(dim=1)
            
            if predicted.item() == stop_token:
                stop = True

            sequence = torch.cat((sequence, predicted.unsqueeze(dim=0)),dim=1)

        return sequence

    def create_square_mask(size):
        mask = torch.triu(torch.ones(size, size), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))
        return mask
</code></pre>
<pre><code>def train_step(model, dataloader, criterion, optimizer, device):
    avg_loss = 0
    model.train()
    for batch, (text_data, text_pad_mask) in enumerate(dataloader):
        text_data, text_pad_mask = text_data.to(device), text_pad_mask.to(device)

        #shift data so that the in_text is the initial tokens and that tgt_text is the next predicted token in the sequence
        in_text = text_data[:, :-1]
        in_mask = text_pad_mask[:, :-1]
        tgt_text = text_data[:, 1:]
        tgt_mask = text_pad_mask[:, 1:]


        out = model(in_text, tgt_text, in_mask, tgt_mask)

        outputs = out[:, :].reshape(-1, model.vocab_size)# Reshape to [batch_size * steps, vocab_size]
        targets = tgt_text[:, :].reshape(-1)# Reshape to [batch_size * steps]

        loss = criterion(outputs, targets)
        avg_loss += loss.item()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    return avg_loss / len(dataloader)
</code></pre>
<p>The loss function is CrossEntropyLoss, the optimizer is AdamW and the dataloader returns tokenized texts in the shape of (batch, sequence). I think this is all that is necessary to try diagnose the issue as I’m 100% sure the tokenizer and data loader is working perfectly as I’ve done a lot of testing on them and don’t want to flood this post with too much code but I can provide the code for them upon request if it helps at all.</p>
","python, pytorch, nlp, autoencoder, transformer-model",
How to get the Information Content (IC) of a sense in WordNet?,"<p>I want to get the Lin similarity between two senses using the formula and check the result using NLTK's WordNet APIs.</p>
<p>According to the <a href=""https://www.nltk.org/howto/wordnet.html"" rel=""nofollow noreferrer"">NLTK WordNet Documentation</a></p>
<blockquote>
<p>synset1.lin_similarity(synset2, ic): Lin Similarity: Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node) and that of the two input Synsets. The relationship is given by the equation 2 * IC(lcs) / (IC(s1) + IC(s2)).</p>
</blockquote>
<p>So, I got the Lin similarity using the following NLTK code:</p>
<pre><code>from nltk.corpus import wordnet as wn, wordnet_ic

brown_ic = wordnet_ic.ic('ic-brown.dat')
shoe = wn.synsets(&quot;shoe&quot;)[0]
cap = wn.synsets(&quot;cap&quot;)[0]

&gt;&gt;&gt; shoe.lin_similarity(cap, brown_ic)
0.5185480689211218
</code></pre>
<p>Now I want to get the Lin similarity using the equation: 2 * IC(lcs) / (IC(s1) + IC(s2)), but I need to get the Information Content (IC) of a sense in the Brown corpus.</p>
<p>I tried to get IC in different ways.</p>
<p>Based on the definition of Information Content from [<a href=""https://www.researchgate.net/publication/232645326_Sentence_Similarity_Based_on_Semantic_Nets_and_Corpus_Statistics"" rel=""nofollow noreferrer"">Li et al 2006</a>], which is 1-(log(n+1)/log(N+1)) where N is the total number of words in the corpus and n is the frequency of the word:</p>
<pre><code>from nltk.corpus import brown
import math

words = brown.words()
total = len(words)
def get_ic(synset):
    lemma_names = synset.lemma_names()
    freq = sum(words.count(word) for word in lemma_names)
    return 1 - (math.log(freq+1)/math.log(tot+1))

shoe_ic = get_ic(shoe)
cap_ic = get_ic(cap)

&gt;&gt;&gt; shoe_ic
0.8060824738989061
&gt;&gt;&gt; cap_ic
0.7930268277244025
</code></pre>
<p>Then I found the lowest common subsumer (LCS) and got the Lin similarity but it is wrong:</p>
<pre><code>&gt;&gt;&gt; cap.lowest_common_hypernyms(shoe)
[Synset('covering.n.02')]

covering = wordnet.synset('covering.n.02')

covering_ic = get_ic(covering)

&gt;&gt;&gt; covering_ic
0.7518262499115436
&gt;&gt;&gt; 2 * covering_ic / (shoe_ic + cap_ic)
0.9350243019743509
</code></pre>
<p>I also tried using brown_ic but the IC values don't seems to be the IC of the sense (which should be between 0 and 1) and differ from the sense occurrences counting:</p>
<pre><code>&gt;&gt;&gt; brown_ic['n'][shoe.offset()]
239.0
&gt;&gt;&gt; sum(words.count(word) for word in shoe.lemma_names())
14
</code></pre>
<p>How can I get the IC of a sense used in lin_similarity function?</p>
","nlp, nltk, similarity, wordnet, sentence-similarity",
Dep extraction rules using NLPPort,"<p>When I'm trying to extract to deps from chunk and entities (even from depchunks) from sentence extract &quot;A Lua e o Sol estão muito distantes.&quot; I got some entries like this:</p>
<pre><code>(...)
A: Triple [subject=o Sol, predicate=ser, object=A Lua]
[A: o &lt;START:thing&gt; Sol &lt;END&gt; ser A &lt;START:thing&gt; Lua &lt;END&gt;]
A: Triple [subject=A Lua, predicate=ser, object=o Sol]
[A: A &lt;START:thing&gt; Lua &lt;END&gt; ser o &lt;START:thing&gt; Sol &lt;END&gt;]

B: Triple [subject=A Lua e o Sol, predicate=estar, object=muito distantes]
[B: A &lt;START:thing&gt; Lua &lt;END&gt; e o &lt;START:thing&gt; Sol &lt;END&gt; estar muito distantes]



--&gt; [Attention!]  E: Triple [subject=o Sol, predicate=ser, object=A Lua]
[E: o &lt;START:thing&gt; Sol &lt;END&gt; ser A &lt;START:thing&gt; Lua &lt;END&gt;]
E: Triple [subject=estão, predicate=ser, object=muito distantes]
[E: estão ser muito distantes]
--&gt; [Attention!] E: Triple [subject=A Lua, predicate=ser, object=o Sol]
[E: A &lt;START:thing&gt; Lua &lt;END&gt; ser o &lt;START:thing&gt; Sol &lt;END&gt;]

F: Triple [subject=A Lua e o Sol, predicate=estar, object=muito distantes]
[F: A &lt;START:thing&gt; Lua &lt;END&gt; e o &lt;START:thing&gt; Sol &lt;END&gt; estar muito distantes]
(...)
</code></pre>
<p>How can I adjust the pipeline from NLPPORT to skip (or improve) this rule that tell us that Lua is a subject and &quot;o Sol&quot; an object when we have connections between two entities (&quot;e&quot;, cc)  + (&quot;Lua&quot;, conj) ?</p>
<p>Is this right ?</p>
<p>If I use deps to create a KG to KBQA from a question &quot;O sol é a lua ?&quot; I will have entries like this:</p>
<pre><code>E: Triple [subject=é, predicate=ser, object=a lua]
[E: é ser a lua]
F: Triple [subject=O sol, predicate=ser, object=a lua]
[F: O sol ser a lua]
</code></pre>
<p>So, I think will be a problem. I think that correct would be &quot;O Sol e a Lua&quot; are two subjects connected  by ('e' cc) being one only entity (or two?) suffering the action of same verb.</p>
","python, java, parsing, nlp, named-entity-recognition",
Text summarization with deep learning,"<p>I'm finetuning the Mt5  model on the Arabic part of the Xl-sum data set
For ten epochs and the resulted manipulation model was stored in hugging face library, there were good results on the training and the validation loss as well as the Rouge measure was around 60, but when doing the testing  process, I surprised that the  results is a mixture of Arabic symbols and garbage, and it is not understandable.</p>
<p>i want to know , what is the error</p>
","python, pandas, dataframe, nlp, model",
Guidance on Extracting Compliance Items from PDF documents by fine-tuning a LLM,"<p>Need some guidance on extracting large compliance items from raw PDF documents. I have csv with these compliance items and I want to fine-tune a LLM such that if it reads any new PDF documents it can firstly identify the compliance items and extract them.</p>
<p>I have seen LLMs used for NER but this kinds of falls into Named Phrase Recognition(NPR - don't know if an acronym like this exists).</p>
<p>The PDFs are servicing guides.
Examples of them can be seen from the following site: <a href=""https://servicing-guide.fanniemae.com/"" rel=""nofollow noreferrer"">https://servicing-guide.fanniemae.com/</a></p>
<p>These are documents with lots of pages. They have compliance items inside them. I have a training document with these compliance documents.</p>
<p><strong>Compliance Items</strong>:</p>
<p>Compliance items are varied in nature. They can range from</p>
<ol>
<li>&quot;The servicer must document in the mortgage loan servicing file the date that the COVID - related hardship began and the date of the insured loss event.&quot;</li>
<li>An incentive fee payment for an eligible short sale is disbursed as outlined in the following table  2. Fannie Mae reviews eligibility for the short sale incentive fee and makes the determination based on information provided by the servicer through Fannie Maeâ€™s servicing solutions system.</li>
</ol>
<p>So, regex or anything is not suitable since there are various types of these items.</p>
<p>My goal is to train a LLM model on these compliance documents so that if I provide a new PDF it can predict/extract compliance items from it.</p>
<p>Can anyone guide me in which model will be better suited for this task, what tokenizers etc.</p>
","nlp, large-language-model, text-extraction",
&quot;torch.cuda.OutOfMemoryError: CUDA out of memory&quot; when loading llama model with Flan v2 dataset,"<p>I am trying to fine tune the llama model <code>meta-llama/Meta-Llama-3-8B-Instruct</code> on the Flan v2 dataset <a href=""https://github.com/google-research/FLAN/tree/main/flan/v2"" rel=""nofollow noreferrer"">FlanV2</a>. I have an experiment function that is the first thing that will be run, where I try to load my model</p>
<pre><code>import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig 

import flan.v2.mixtures

def experiment():
    model_id = &quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;
    access_token = &quot;...&quot;
    
    print(torch.cuda.memory_summary(device=None, abbreviated=False))

    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type=&quot;nf4&quot;,
        bnb_4bit_compute_dtype=getattr(torch, &quot;float16&quot;),
        bnb_4bit_use_double_quant=False,
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        token=access_token,
        quantization_config=quant_config
    )

    print(torch.cuda.memory_summary(device=None, abbreviated=False))
</code></pre>
<p>However I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/cluster/home/username/afsl/examples/bug.py&quot;, line 34, in &lt;module&gt;
    main(args)
  File &quot;/cluster/home/username/afsl/examples/bug.py&quot;, line 29, in main
    experiment()
  File &quot;/cluster/home/username/afsl/examples/bug.py&quot;, line 20, in experiment
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/transformers/models/auto/auto_factory.py&quot;, line 563, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/transformers/modeling_utils.py&quot;, line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/transformers/modeling_utils.py&quot;, line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/transformers/modeling_utils.py&quot;, line 888, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py&quot;, line 219, in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/bitsandbytes/nn/modules.py&quot;, line 191, in to
    return self.cuda(device)
           ^^^^^^^^^^^^^^^^^
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/bitsandbytes/nn/modules.py&quot;, line 168, in cuda
    w = self.data.contiguous().half().cuda(device)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 42.50 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 1002.01 MiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management
</code></pre>
<p>I am running on a cluster where I have access to NVIDIA GeForce RTX 3090 and 4090 with 24GiB of memory per GPU. I intend to use the <code>cot_zsopt</code> dataset as shown in this example file <a href=""https://github.com/google-research/FLAN/blob/main/flan/v2/run_example.py"" rel=""nofollow noreferrer"">run_example.py</a></p>
<p>I have tried to monitor my memory consumption to find the source of the problem and here is what I found</p>
<pre><code>W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

`low_cpu_mem_usage` was None, now set to True since model is quantized.

Loading checkpoint shards:   0%|          | 0/4 [00:00&lt;?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:01&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;/cluster/home/username/afsl/examples/bug.py&quot;, line 34, in &lt;module&gt;
    main(args)
  File &quot;/cluster/home/username/afsl/examples/bug.py&quot;, line 29, in main
    experiment()
  File &quot;/cluster/home/username/afsl/examples/bug.py&quot;, line 20, in experiment
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/transformers/models/auto/auto_factory.py&quot;, line 563, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/transformers/modeling_utils.py&quot;, line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/transformers/modeling_utils.py&quot;, line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/transformers/modeling_utils.py&quot;, line 888, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py&quot;, line 219, in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/bitsandbytes/nn/modules.py&quot;, line 191, in to
    return self.cuda(device)
           ^^^^^^^^^^^^^^^^^
  File &quot;/cluster/home/username/llama/lib64/python3.11/site-packages/bitsandbytes/nn/modules.py&quot;, line 168, in cuda
    w = self.data.contiguous().half().cuda(device)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 42.50 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 1002.01 MiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management
</code></pre>
<p>Now if I remove the line <code>import flan.v2.mixtures</code>, it works as intended and I get the following output</p>
<pre><code>|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

`low_cpu_mem_usage` was None, now set to True since model is quantized.

Loading checkpoint shards:   0%|          | 0/4 [00:00&lt;?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:07&lt;00:21,  7.18s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:12&lt;00:12,  6.25s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18&lt;00:05,  5.86s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:19&lt;00:00,  4.06s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:19&lt;00:00,  4.87s/it]
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   5908 MiB |   5908 MiB |  19221 MiB |  13313 MiB |
|       from large pool |   5827 MiB |   5827 MiB |  19141 MiB |  13313 MiB |
|       from small pool |     80 MiB |     80 MiB |     80 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   5908 MiB |   5908 MiB |  19221 MiB |  13313 MiB |
|       from large pool |   5827 MiB |   5827 MiB |  19141 MiB |  13313 MiB |
|       from small pool |     80 MiB |     80 MiB |     80 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   5876 MiB |   5876 MiB |  19188 MiB |  13312 MiB |
|       from large pool |   5796 MiB |   5796 MiB |  19108 MiB |  13312 MiB |
|       from small pool |     80 MiB |     80 MiB |     80 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   5934 MiB |   5986 MiB |  10414 MiB |   4480 MiB |
|       from large pool |   5852 MiB |   5904 MiB |  10332 MiB |   4480 MiB |
|       from small pool |     82 MiB |     82 MiB |     82 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26488 KiB | 131745 KiB |   1947 MiB |   1921 MiB |
|       from large pool |  25088 KiB | 130048 KiB |   1899 MiB |   1875 MiB |
|       from small pool |   1400 KiB |   2040 KiB |     47 MiB |     46 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     835    |     835    |    1059    |     224    |
|       from large pool |     386    |     386    |     610    |     224    |
|       from small pool |     449    |     449    |     449    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     835    |     835    |    1059    |     224    |
|       from large pool |     386    |     386    |     610    |     224    |
|       from small pool |     449    |     449    |     449    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     189    |     189    |     249    |      60    |
|       from large pool |     148    |     148    |     208    |      60    |
|       from small pool |      41    |      41    |      41    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      15    |      17    |     151    |     136    |
|       from large pool |      12    |      14    |     110    |      98    |
|       from small pool |       3    |       4    |      41    |      38    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
</code></pre>
<p>I am confused, because it seems that the memory should be empty in both cases, but in one the model has enough space on the GPU and in the other case it doesn't? What is the source for this error?</p>
","python, pytorch, nlp, huggingface-transformers, llama",
"How can I convert user query to sql for a search engine to get exact matching documents from db, the query is in japanese","<p>I am using tfidf as a search algorithm but tfidf doesn't give exact matches so could you suggest a different algo  that can match exactly with the database .With tfidf when we search for show me domestic stocks domestic bonds also come up in the search and I want only domestic stocks in answers.</p>
<p>We have tried tfidf also llm for our use case but llm also gets confused and gives extra filters in answers</p>
","search, nlp, large-language-model",
Determine if text is in English?,"<p>I am using both <a href=""http://www.nltk.org/"" rel=""noreferrer"">Nltk</a> and <a href=""http://scikit-learn.org/stable/"" rel=""noreferrer"">Scikit Learn</a> to do some text processing. However, within my list of documents I have some documents that are not in English. For example, the following could be true:</p>

<pre><code>[ ""this is some text written in English"", 
  ""this is some more text written in English"", 
  ""Ce n'est pas en anglais"" ] 
</code></pre>

<p>For the purposes of my analysis, I want all sentences that are not in English to be removed as part of pre-processing. However, is there a good way to do this? I have been Googling, but cannot find anything specific that will let me recognize if strings are in English or not. Is this something that is not offered as functionality in either <code>Nltk</code> or <code>Scikit learn</code>? <b>EDIT</b> I've seen questions both like <a href=""https://stackoverflow.com/questions/29099621/how-to-find-out-wether-a-word-exists-in-english-using-nltk"">this</a> and <a href=""https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python"">this</a> but both are for individual words... Not a ""document"". Would I have to loop through every word in a sentence to check if the whole sentence is in English?</p>

<p>I'm using Python, so libraries that are in Python would be preferable, but I can switch languages if needed, just thought that Python would be the best for this.</p>
","python, scikit-learn, nlp, nltk",
Training LLM to perform text classification,"<p>I am trying to perform text classification using GPTNeo, using the tweet_eval dataset from huggingface. I am following this example <a href=""https://huggingface.co/docs/transformers/tasks/sequence_classification"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/tasks/sequence_classification</a>, but there is some error. I am a beginner at LLMs and it will be very helpful if someone can help me solve the issue. Thanks in advance. This is my code:</p>
<pre><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
import datasets
import torch as t
from transformers import DataCollatorWithPadding
import evaluate
import numpy as np

dataset = datasets.load_dataset(&quot;tweet_eval&quot;,&quot;emotion&quot;)

x_train = dataset[&quot;train&quot;][&quot;text&quot;]
y_train = dataset[&quot;train&quot;][&quot;label&quot;]

x_test = dataset[&quot;test&quot;][&quot;text&quot;]
y_test = dataset[&quot;test&quot;][&quot;label&quot;]

def load_LLM(llm, device):
    num_labels = 4
    id2label = {0: &quot;Anger&quot;, 1: &quot;Joy&quot;, 2: &quot;Optimism&quot;, 3: &quot;Sadness&quot;}
    label2id = {&quot;Anger&quot;: 0, &quot;Joy&quot;: 1, &quot;Optimism&quot;: 2, &quot;Sadness&quot;:3}
    model = AutoModelForSequenceClassification.from_pretrained(llm,num_labels=num_labels,id2label=id2label, label2id=label2id)
    model.to(device)
    tokenizer = AutoTokenizer.from_pretrained(llm)
    return model, tokenizer

llm = &quot;EleutherAI/gpt-neo-2.7B&quot;
device = t.device('cuda' if t.cuda.is_available() else 'cpu')
model,tokenizer = load_LLM(llm,device)

tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenizer.pad_token = '[PAD]'
train_inputs = tokenizer(x_train, truncation=True, padding=True)
test_inputs = tokenizer(x_test, truncation=True, padding=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

accuracy = evaluate.load(&quot;accuracy&quot;)
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

training_args = TrainingArguments(
    output_dir=&quot;my_awesome_model&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    load_best_model_at_end=True,
    push_to_hub=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_inputs,
    eval_dataset=test_inputs,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

trainer.train()
</code></pre>
<p>I am getting this error:</p>
<pre><code>type here---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[18], line 1
----&gt; 1 trainer.train()

File ~\anaconda3\envs\pt\lib\site-packages\transformers\trainer.py:1664, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1659     self.model_wrapped = self.model
   1661 inner_training_loop = find_executable_batch_size(
   1662     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1663 )
-&gt; 1664 return inner_training_loop(
   1665     args=args,
   1666     resume_from_checkpoint=resume_from_checkpoint,
   1667     trial=trial,
   1668     ignore_keys_for_eval=ignore_keys_for_eval,
   1669 )

File ~\anaconda3\envs\pt\lib\site-packages\transformers\trainer.py:1909, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1906     rng_to_sync = True
   1908 step = -1
-&gt; 1909 for step, inputs in enumerate(epoch_iterator):
   1910     total_batched_samples += 1
   1911     if rng_to_sync:

File ~\anaconda3\envs\pt\lib\site-packages\torch\utils\data\dataloader.py:633, in _BaseDataLoaderIter.__next__(self)
    630 if self._sampler_iter is None:
    631     # TODO(https://github.com/pytorch/pytorch/issues/76750)
    632     self._reset()  # type: ignore[call-arg]
--&gt; 633 data = self._next_data()
    634 self._num_yielded += 1
    635 if self._dataset_kind == _DatasetKind.Iterable and \
    636         self._IterableDataset_len_called is not None and \
    637         self._num_yielded &gt; self._IterableDataset_len_called:

File ~\anaconda3\envs\pt\lib\site-packages\torch\utils\data\dataloader.py:677, in _SingleProcessDataLoaderIter._next_data(self)
    675 def _next_data(self):
    676     index = self._next_index()  # may raise StopIteration
--&gt; 677     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    678     if self._pin_memory:
    679         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)

File ~\anaconda3\envs\pt\lib\site-packages\torch\utils\data\_utils\fetch.py:54, in _MapDatasetFetcher.fetch(self, possibly_batched_index)
     52 else:
     53     data = self.dataset[possibly_batched_index]
---&gt; 54 return self.collate_fn(data)

File ~\anaconda3\envs\pt\lib\site-packages\transformers\trainer_utils.py:704, in RemoveColumnsCollator.__call__(self, features)
    702 def __call__(self, features: List[dict]):
    703     features = [self._remove_columns(feature) for feature in features]
--&gt; 704     return self.data_collator(features)

File ~\anaconda3\envs\pt\lib\site-packages\transformers\data\data_collator.py:249, in DataCollatorWithPadding.__call__(self, features)
    248 def __call__(self, features: List[Dict[str, Any]]) -&gt; Dict[str, Any]:
--&gt; 249     batch = self.tokenizer.pad(
    250         features,
    251         padding=self.padding,
    252         max_length=self.max_length,
    253         pad_to_multiple_of=self.pad_to_multiple_of,
    254         return_tensors=self.return_tensors,
    255     )
    256     if &quot;label&quot; in batch:
    257         batch[&quot;labels&quot;] = batch[&quot;label&quot;]

File ~\anaconda3\envs\pt\lib\site-packages\transformers\tokenization_utils_base.py:2966, in PreTrainedTokenizerBase.pad(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)
   2962 # The model's main input name, usually `input_ids`, has be passed for padding
   2963 if self.model_input_names[0] not in encoded_inputs:
   2964     raise ValueError(
   2965         &quot;You should supply an encoding or a list of encodings to this method &quot;
-&gt; 2966         f&quot;that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}&quot;
   2967     )
   2969 required_input = encoded_inputs[self.model_input_names[0]]
   2971 if required_input is None or (isinstance(required_input, Sized) and len(required_input) == 0):

AttributeError: 'list' object has no attribute 'keys'
</code></pre>
<p>I was trying to perform text classification and wanted to fine tune the model before using it to make predictions.</p>
","nlp, huggingface, large-language-model",
ImportError: cannot import name &#39;pre_init&#39; from &#39;langchain_core.utils&#39;,"<p>When I want to install langchain libraries from requirements.txt  I'm getting</p>
<pre><code>ImportError: cannot import name 'pre_init' from 'langchain_core.utils'
</code></pre>
<p>I've tried to install libraries from terminal using these commands :</p>
<pre><code>pip install gigachain
pip install gigachat
pip install -U langchain-community
</code></pre>
<p>and it's working, so I used</p>
<pre><code>pip freeze
</code></pre>
<p>And pasted all libraries from the terminal to requirements.txt and it's doesn't work. It would be nice if someone could help</p>
","python, pip, nlp, langchain, large-language-model","<p>Your import error suggests that the <a href=""https://pypi.org/project/langchain-core/"" rel=""nofollow noreferrer""><code>langchain-core</code></a> module has not been installed. You can confirm whether this is the case by checking the output from the following command:</p>
<pre><code>pip show langchain-core
</code></pre>
<p>If it hasn't been installed, ensure it is installed via the following command:</p>
<pre><code>pip install langchain-core
</code></pre>
<p>As for using a <code>requirements.txt</code> file, simply creating it doesn't automatically install the packages listed within. You still have to run the following command in your terminal to install the listed packages:</p>
<pre><code>python -m pip install -r /path/to/requirements.txt
</code></pre>
<p>Finally, you don't have to manually create the <code>requirements.txt</code> file by coping and pasting the output from <code>pip freeze</code>. You can simply run:</p>
<pre><code>python -m pip freeze &gt; /path/to/requirements.txt
</code></pre>
<p>You can read more about requirement files under the <a href=""https://pip.pypa.io/en/latest/user_guide/#requirements-files"" rel=""nofollow noreferrer"">requirements files</a> section of the <code>pip</code> user guide.</p>
"
How to process a very long document in spaCy?,"<p>I'm trying to perform a NLP analysis of a text in Spanish. So, to do lemmatization I'm using Spacy, because NLTK has not a Spanish version for lemma. The problem with Spacy is that I a have restriction in the numbers of words that I can pass through Lemmatizer:</p>
<blockquote>
<p><strong>ValueError:</strong> [E088] Text of length 6095095 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may
cause memory allocation errors. If you're not using the parser or NER,
it's probably safe to increase the <code>nlp.max_length</code> limit. The limit
is in number of characters, so you can check whether your inputs are
too long by checking <code>len(text)</code>.</p>
</blockquote>
<p>I tried with <code>nlp.max_length= 6095095</code> but the session crashed after using all available RAM.</p>
<p>Any suggestion?</p>
","nlp, spacy, lemmatization",
Classifying texts using word embeddings,"<p>I'm trying to train a model that is able to classify short texts (200-600 words per text). I got a training set with their corresponding labels, and a text might have one or more labels.</p>
<p>My first approach was to use TF-IDF with Naive Bayes but it wasn't performing very well. But after replacing Naive Bayes with a logistic regeression, the results improved a bit.</p>
<p>However, what are some of the more state-of-the-art methods that I could try? For example, can a text embedding model from OpenAI be of use? Compared to TF-IDF, embeddings would be able to handle synonyms, etc. But I'm struggling to put it all together, how should the vector (or vectors) for each text be calculated? It feels like embedding are better suited for sentence length classification or comparisons.</p>
","text, nlp, classification, text-classification, openaiembeddings",
Huggingface Mistral-Nemo-Instruct-2407 python script for text generation just hanging on Mac M3?,"<p>I installed pytorch, transformers, and python-dotenv to run this script:</p>
<pre><code>from transformers import pipeline
from dotenv import load_dotenv
import torch

import os
import json
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

load_dotenv()

device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;

# device = torch.device(&quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;)

def summarize_definitions(definitions):
    # Use a pipeline as a high-level helper

    pipe = pipeline(&quot;text-generation&quot;, model=&quot;mistralai/Mistral-Nemo-Instruct-2407&quot;, device=device)


    cleaned_definitions = {}

    for i, (term, defs) in enumerate(definitions.items()):
        combined_defs = f&quot;Please summarize these definitions into a JSON array of simple ideally 1-3 word definitions: {json.dumps(defs, indent=2)}&quot;

        # Summarize the combined definitions

        messages = [
          {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: combined_defs},
        ]
        summary = pipe(messages, min_length=1, max_new_tokens=1000)

        print(summary)

    return cleaned_definitions

def main():
    with open('import/language/tibetan/definitions.out.json', 'r', encoding='utf-8') as f:
        definitions = json.load(f)

    cleaned_definitions = summarize_definitions(definitions)
    print(cleaned_definitions)

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>When I run it, I just see this (it already previously installed 5 model chunks, each about 4-5GB):</p>
<pre><code>$ python3 transform.py
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:37&lt;00:00,  7.48s/it]
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
</code></pre>
<p>Then it just sits there.</p>
<p>I tried commenting back in this line:</p>
<pre><code>device = torch.device(&quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;)
</code></pre>
<p>To see if I could use the GPU to speed things up? But it just said this and exited without any other content/messaging:</p>
<pre><code>$ python3 transform.py
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:39&lt;00:00,  7.88s/it]
zsh: killed     python3 transform.py
(venv) $ /opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
</code></pre>
<p>Any ideas what I'm doing wrong? Is it supposed to take a long time to get the first output? Or what is supposed to happen? I am new to this AI stuff, though have been programming for a while.</p>
<p>When I <code>CTRL+C</code>, it errors at line 31, which is right when I call <code>pipe(messages)</code>:</p>
<pre><code>^CTraceback (most recent call last):
  File &quot;./transform.py&quot;, line 60, in &lt;module&gt;
    if __name__ == &quot;__main__&quot;:
        ^^^^^^
  File &quot;./transform.py&quot;, line 56, in main

  File &quot;./transform.py&quot;, line 31, in summarize_definitions
    ]

  File &quot;./venv/lib/python3.12/site-packages/transformers/pipelines/text_generation.py&quot;, line 257, in __call__
    return super().__call__(Chat(text_inputs), **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
</code></pre>
","python, pytorch, nlp, huggingface-transformers, text-generation",
Retrieval-augmented generation without OpenAIEmbeddings,"<p>I'm playing with HuggingFace and some of the models on there. I'm trying to achieve something along the lines of <a href=""https://python.langchain.com/docs/use_cases/question_answering/"" rel=""nofollow noreferrer"">RAG</a>. Seems like a pretty clear guide with all the needed ingredients and recipe. But the cooking sequence is what I need help.</p>
<p>What I want to do:</p>
<ul>
<li>Have a question and answer chat app</li>
<li>But the chat app will depend on custom information given the model to answer some specialized questions.</li>
</ul>
<p>Thus</p>
<ul>
<li>the model will 100% reside on HuggingFace and &quot;inferred&quot; (I hope I'm using the term right) remotely from a VPS using HuggingFace client</li>
<li>and the VPS will locally have the custom documents used for the &quot;indexing&quot; and &quot;fine tuning&quot;</li>
</ul>
<p><strong>What I've done</strong></p>
<p>With that in mind, here's what I've been able to do so far in the past couple of days (see python script below)</p>
<p>But the problem I keep encountering is, EVERY article/tutorial/documentation I've come across so far CANNOT write 3 sentences without mentioning something along the lines of OpenAI* <em>sigh</em></p>
<p>I do not want to use ANYTHING OpenAI (is that even possible?)</p>
<p>Below is my python script.</p>
<pre><code>import os
from langchain.chains import LLMChain
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from huggingface_hub import InferenceClient

# &lt;rant&gt;
# Where to import what from seems to be a whack-a-mole sport with this 
# langchain project. They can't seem to keep a module at a location
# even for a few version upgrades straight. Woow!

os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'hf_xx'
# os.environ[&quot;OPENAI_API_KEY&quot;] = 'sk-xx' # I don't wanna use anything OpenAI*

import gradio as gr

# Load openchat model
openchat = InferenceClient(model=&quot;openchat/openchat_3.5&quot;, token='hf_xx')
documents = [&quot;trainingData/pdfs.pdf&quot;]

# Define embedding function
def extract_embedding(document):
    # Extract embedding using OpenAIEmbeddings
    embedding = OpenAIEmbeddings.embed_query(text=document)
    return embedding

# Load document embeddings
document_embeddings = [extract_embedding(document) for document in documents]

# Load vectorstore index
index = FAISS.from_documents(documents=documents, embeddings=document_embeddings)

# Define LLM chain
llm_chain = LLMChain(llm=openchat)

# Create RagChain manually
def predict(docs, input):
    # Retrieve relevant documents from index
    retrieved_docs = index.query(input)

    # Run LLM chain on retrieved documents
    outputs = []
    for doc in retrieved_docs:
        output = llm_chain.predict(input=doc)
        outputs.append(output)

    return outputs

# Launch Gradio app
iface = gr.Interface(fn=predict, inputs=&quot;text&quot;, outputs=&quot;text&quot;).launch()
</code></pre>
<p>How far off am I from the mark? And how do I get on track?</p>
<p>Currently, when I run the above script, I get the error:</p>
<pre><code>TypeError: OpenAIEmbeddings.embed_query() missing 1 required positional argument: 'self'
</code></pre>
<p>Like mentioned earlier, I'd prefer not to use OpenAI anywhere in the script above, so any alternatives are welcomed.</p>
<p>Will appreciate any insights</p>
","python, nlp, artificial-intelligence, langchain",
Spacy Custom Name Entity Recognition (NER) &#39;catastrophic forgetting&#39; issue,"<p>The model is unable to remember the previous labels on which it was trained
i know that its 'catastrophic forgetting', but no example or blog seems to help this issue.
the most common response for this is this blog is this <a href=""https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting"" rel=""nofollow noreferrer"">https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting</a> but this is pretty old now and is not helping</p>
<p>Here is my code:</p>
<pre><code>from __future__ import unicode_literals, print_function
import json
labeled_data = []
with open(r&quot;/content/emails_labeled.jsonl&quot;, &quot;r&quot;) as read_file:
    for line in read_file:
        data = json.loads(line)
        labeled_data.append(data)

TRAIN_DATA = []
for entry in labeled_data:
    entities = []
    for e in entry['labels']:
        entities.append((e[0], e[1],e[2]))
    spacy_entry = (entry['text'], {&quot;entities&quot;: entities})
    TRAIN_DATA.append(spacy_entry)       
import plac
import random
import warnings
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding


# new entity label
LABEL = &quot;OIL&quot;

# training data
# Note: If you're using an existing model, make sure to mix in examples of
# other entity types that spaCy correctly recognized before. Otherwise, your
# model might learn the new type, but &quot;forget&quot; what it previously knew.
# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting
'''
TRAIN_DATA = [
    (
        &quot;Horses are too tall and they pretend to care about your feelings&quot;,
        {&quot;entities&quot;: [(0, 6, LABEL)]},
    ),
    (&quot;Do they bite?&quot;, {&quot;entities&quot;: []}),
    (
        &quot;horses are too tall and they pretend to care about your feelings&quot;,
        {&quot;entities&quot;: [(0, 6, LABEL)]},
    ),
    (&quot;horses pretend to care about your feelings&quot;, {&quot;entities&quot;: [(0, 6, LABEL)]}),
    (
        &quot;they pretend to care about your feelings, those horses&quot;,
        {&quot;entities&quot;: [(48, 54, LABEL)]},
    ),
    (&quot;horses?&quot;, {&quot;entities&quot;: [(0, 6, LABEL)]}),
]
'''

@plac.annotations(
    model=(&quot;Model name. Defaults to blank 'en' model.&quot;, &quot;option&quot;, &quot;m&quot;, str),
    new_model_name=(&quot;New model name for model meta.&quot;, &quot;option&quot;, &quot;nm&quot;, str),
    output_dir=(&quot;Optional output directory&quot;, &quot;option&quot;, &quot;o&quot;, Path),
    n_iter=(&quot;Number of training iterations&quot;, &quot;option&quot;, &quot;n&quot;, int),
)
def main(model='/content/LinkModelOutput', new_model_name=&quot;Oil21&quot;, output_dir='/content/Last', n_iter=30):
    &quot;&quot;&quot;Set up the pipeline and entity recognizer, and train the new entity.&quot;&quot;&quot;
    random.seed(0)
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(&quot;Loaded model '%s'&quot; % model)
    else:
        nlp = spacy.blank(&quot;en&quot;)  # create blank Language class
        print(&quot;Created blank 'en' model&quot;)
    # Add entity recognizer to model if it's not in the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if &quot;ner&quot; not in nlp.pipe_names:
        ner = nlp.create_pipe(&quot;ner&quot;)
        nlp.add_pipe(ner)
    # otherwise, get it, so we can add labels to it
    else:
        ner = nlp.get_pipe(&quot;ner&quot;)

    ner.add_label(LABEL)  # add new entity label to entity recognizer
    # Adding extraneous labels shouldn't mess anything up
    #ner.add_label(&quot;VEGETABLE&quot;)
    if model is None:
        optimizer = nlp.begin_training()
    else:
        optimizer = nlp.resume_training()
    move_names = list(ner.move_names)
    # get names of other pipes to disable them during training
    pipe_exceptions = [&quot;ner&quot;, &quot;trf_wordpiecer&quot;, &quot;trf_tok2vec&quot;]
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
    # only train NER
    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():
        # show warnings for misaligned entity spans once
        warnings.filterwarnings(&quot;once&quot;, category=UserWarning, module='spacy')

        sizes = compounding(1.0, 4.0, 1.001)
        # batch up the examples using spaCy's minibatch
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            batches = minibatch(TRAIN_DATA, size=sizes)
            losses = {}
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.entity.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)
            print(&quot;Losses&quot;, losses)

    # test the trained model
    test_text = &quot;Here is Hindustan petroleum's oil reserves coup in Australia. Details can be found at https://www.textfixer.com/tools/remove-line-breaks.php?&quot;
    doc = nlp(test_text)
    print(&quot;Entities in '%s'&quot; % test_text)
    for ent in doc.ents:
        print(ent.label_, ent.text)

    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.meta[&quot;name&quot;] = new_model_name  # rename model
        nlp.to_disk(output_dir)
        print(&quot;Saved model to&quot;, output_dir)

        # test the saved model
        print(&quot;Loading from&quot;, output_dir)
        nlp2 = spacy.load(output_dir)
        # Check the classes have loaded back consistently
        assert nlp2.get_pipe(&quot;ner&quot;).move_names == move_names
        doc2 = nlp2(test_text)
        for ent in doc2.ents:
            print(ent.label_, ent.text)


if __name__ == &quot;__main__&quot;:
    plac.call(main)
</code></pre>
<p>and the data annotation was done on 'Daccano'.
Here is a look at the data:</p>
<pre><code>{&quot;id&quot;: 174, &quot;text&quot;: &quot;service\tmarathon petroleum reduces service postings marathon petroleum co said it reduced the contract price it will pay for all grades of service oil one dlr a barrel effective today the decrease brings marathon s posted price for both west texas intermediate and west texas sour to dlrs a bbl the south louisiana sweet grade of service was reduced to dlrs a bbl the company last changed its service postings on jan reuter&quot;, &quot;meta&quot;: {}, &quot;annotation_approver&quot;: null, &quot;labels&quot;: [[61, 70, &quot;OIL&quot;], [147, 150, &quot;OIL&quot;]]}
{&quot;id&quot;: 175, &quot;text&quot;: &quot;mutual funds\tmunsingwear inc mun th qtr jan loss shr loss cts vs loss seven cts net loss vs loss revs mln vs mln year shr profit cts vs profit cts net profit vs profit revs mln vs mln avg shrs vs note per shr adjusted for for stock split july and for split may reuter&quot;, &quot;meta&quot;: {}, &quot;annotation_approver&quot;: null, &quot;labels&quot;: []}
</code></pre>
","python, nlp, spacy, named-entity-recognition, doccano","<p>I am not spacy expert, but I had the same problem. There are some points which are necessary: annotation tool, amount of train data, mixing of correct predicted entities.
First make sure, that your training data is correctly labeled by tool of your choice (you don't get userwarnings). For a good prediction your model needs a lot of data. It means at least 200 examples for each entity you want to train. I personally label as much data as possible. And spacy's maker reccomend to mix the entities which your model corretly predicted.</p>
"
How to host NLP model as API?,"<p>I have trained a chatbot model and want to integrate it into an Android app. My goal is to enable chatbot functionality in the app by hosting the model as an API so users can interact with it. I'm looking for free solutions to host the model. Can you suggest some free hosting options and provide guidance on how to host the model as an API? So far, I have trained the model and saved it, but I'm not sure how to proceed with hosting and API integration.</p>
","android, nlp",
CUDA error: device-side assert triggered Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions,"<p>I am trying to convert my text into its embeddings using a bert model , when i apply this to my dataset it works fine for some of my inputs then stops and gives that error</p>
<p>I have set TORCH_USE_CUDA_DSA and CUDA_LAUNCH_BLOCKING to 1 and the inputs are not exceeding the number of token limit also and this is my embedding code i have tried to free the memory of my gpu and still it didn't work.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import numpy as np

def embeddings(text):
    if len(text) &gt; 4000:
        flag = text.split(&quot;.&quot;)
        t1 = flag[:len(flag) // 2]
        t2 = flag[len(flag) // 2:]
        t1 = &quot;.&quot;.join(t1)
        t2 = &quot;.&quot;.join(t2)
        emb_avg = np.mean([embeddings(t1), embeddings(t2)], axis=0)
        return emb_avg
    else:
        with torch.no_grad(): 
            encoded_input = tokenizer(text, return_tensors='pt')
            encoded_input.to(device)

            output = model(**encoded_input)
            emb = output.encoder_last_hidden_state
            emb_np = emb.cpu().numpy()  
           
            del emb
            del output
            del encoded_input
            gc.collect()
            
            torch.cuda.empty_cache()  
        emb_avg = np.mean(emb_np, axis=1)
        emb_avg = emb_avg.flatten()
        torch.cuda.empty_cache()
        return emb_avg
</code></pre>
<p>and I'm applying to my data set</p>
<pre><code>from tqdm.auto import tqdm
tqdm.pandas()  
df['emb'] = df['abstract'].progress_apply(embeddings)
</code></pre>
","pytorch, nlp, bert-language-model",
CPU Memory Leak While Inference Models in Infinite Loop,"<p>I'm experiencing a CPU memory leak while running a Python script that processes text using various NLP models in an infinite loop. The script includes language translation, sentiment analysis, and topic classification. Here's a simplified version of the problematic code:</p>
<pre><code>import ctranslate2 
import torch
import torch.nn.functional as F
from transformers import  AutoTokenizer, AutoModelForSequenceClassification , DistilBertForSequenceClassification, DistilBertTokenizer
import spacy
spacy.require_gpu()
ner_model = spacy.load('ner_model_path', disable=[&quot;tagger&quot;, &quot;parser&quot;, &quot;attribute_ruler&quot;, &quot;lemmatizer&quot;]) 

OTHER_LANG_DICT = {'hi': 'nllb_hi'}
LANGUAGE_MODEL = ctranslate2.Translator('nllb-200-3.3B-int8',  device=&quot;cuda&quot;)

def convertToEng(input_data):
    try:
        for k,v in input_data.items():
            text = v['text']
            lang = v['lang']
        if lang and lang == 'en':
                translated_text = text
        if lang and lang in OTHER_LANG_DICT:
            tokenizer = AutoTokenizer.from_pretrained(LANGUAGE_MODEL, src_lang=OTHER_LANG_DICT[lang])
            tokens = tokenizer.encode(text, return_tensors=&quot;pt&quot;)
            tokens_list = tokenizer.convert_ids_to_tokens(tokens[0])
            results = LANGUAGE_MODEL.translate_batch([tokens_list], target_prefix=[[&quot;eng_Latn&quot;]])
            target = results[0].hypotheses[0][1:]
            translated_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(target), skip_special_tokens=True)
        return translated_text
    except Exception as e:
         print(str(e))

MC_TOKENIZER = AutoTokenizer.from_pretrained('bert_model_path')
MC_MODEL = AutoModelForSequenceClassification.from_pretrained('bert_model_path').to(&quot;cuda&quot;)
MC_MODEL.eval()
THRESHOLD = 0.3

MODEL_3_MODEL = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased_model_path').to(torch.device(&quot;cuda&quot;))
MODEL_3_TOKENIZER = DistilBertTokenizer.from_pretrained('distilbert-base-uncased_model_path')


def model2_prediction(input_text):
    try:
        tokens = MC_TOKENIZER(input_text, add_special_tokens=True, return_tensors=&quot;pt&quot;, padding=True)
        tokens = {key: value.to('cuda') for key, value in tokens.items()}
        with torch.no_grad():
            logits = MC_MODEL(**tokens)[0].to('cuda')
        pred = F.softmax(logits, dim=1)
        filtered_classes = [[i for i, class_prob in enumerate(prob) if class_prob &gt;= THRESHOLD] for prob in pred]
        return filtered_classes
    except Exception as e:
        print(str(e))

def model2_labelling(input_data: str):
    try:
        classes = []
        selected_classes = model2_prediction(input_data)
        for class_id in selected_classes[0]:
            class_name = MC_MODEL.config.id2label[class_id]
            classes.append(class_name)
        return classes
    except Exception as e:
        print(str(e))


def Model3(input_data: dict):
    try:
        id_val = input_data[&quot;id&quot;]
        text_val = str(input_data[&quot;text&quot;]).lower()
        tokens = MODEL_3_TOKENIZER(text_val, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
        tokens = {k: v.to(torch.device(&quot;cuda&quot;)) for k, v in tokens.items()}
        with torch.no_grad():
            outputs = MODEL_3_MODEL(**tokens)
            pred = F.softmax(outputs.logits, dim=1).tolist()[0]
            pred.insert(1, pred.pop(2))
            result = {&quot;id&quot;: id_val, &quot;sentiment&quot;: pred}
            return result
    except Exception as e:
        print(str(e))

def text_preprocessing(text):
    pass

def ner_pred(text):
    text = text_preprocessing(text)
    doc = ner_model(text)
    entity = []
    for ent in doc.ents:
        if ent.label_ == &quot;PERSON&quot;:
            entity.append(ent.text)
    return entity

def ner_result(text):
    pass # post processing of the result

def get_result(queue_name):
    try:
        data = queue_name
            
        data = convertToEng(input_data = data)
        data_text = data.get('text')
        
        if data_text:
            id = data['id']

            # sentiment = SentimentAnalysis(input_data = {&quot;id&quot;: id, &quot;text&quot;: ' '.join(data_text.split()[:20])})
            sentiment = Model3(input_data = {&quot;id&quot;: id, &quot;text&quot;: ' '.join(data_text.split()[:20])}) 
            topics = model2_labelling(input_data = data_text)
            final_result = [id,{'sentiment':sentiment['sentiment'],'topic':topics}]
        return final_result
    except Exception as e:
        print(e)


data = {&quot;doc1&quot;:{'text': 'ram is a good boy.', 'lang': 'en'}}

if __name__ == &quot;__main__&quot;:
    while True:
        status = get_result(queue_name=data)
</code></pre>
<p>I an using following library along with python version 3.10 and i have NVIDIA GeForce RTX 3070, Driver Version: 535.183.01, CUDA Version: 12.2:</p>
<ul>
<li><p>nvidia-cublas-cu12         12.1.3.1</p>
</li>
<li><p>nvidia-cuda-cupti-cu12     12.1.105</p>
</li>
<li><p>nvidia-cuda-nvrtc-cu12     12.1.105</p>
</li>
<li><p>nvidia-cuda-runtime-cu12   12.1.105</p>
</li>
<li><p>nvidia-cudnn-cu12          8.9.2.26</p>
</li>
<li><p>nvidia-cufft-cu12          11.0.2.54</p>
</li>
<li><p>nvidia-curand-cu12         10.3.2.106</p>
</li>
<li><p>nvidia-cusolver-cu12       11.4.5.107</p>
</li>
<li><p>nvidia-cusparse-cu12       12.1.0.106</p>
</li>
<li><p>nvidia-nccl-cu12           2.18.1</p>
</li>
<li><p>nvidia-nvjitlink-cu12      12.5.40</p>
</li>
<li><p>nvidia-nvtx-cu12           12.1.105</p>
</li>
<li><p>torch                      2.1.0</p>
</li>
<li><p>spacy                      3.7.4</p>
</li>
<li><p>spacy-alignments           0.9.1</p>
</li>
<li><p>spacy-curated-transformers 0.2.2</p>
</li>
<li><p>spacy-legacy               3.0.12</p>
</li>
<li><p>spacy-loggers              1.0.5</p>
</li>
<li><p>spacy-transformers         1.3.5</p>
</li>
<li><p>accelerate                 0.29.3</p>
</li>
<li><p>transformers               4.36.2</p>
</li>
</ul>
<p>I have tried to clear the cache using <code>gc</code> and also i have set the environment to <code>os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,garbage_collection_threshold:0.8'</code> and
<code>os.environ['ONEDNN_PRIMITIVE_CACHE_CAPACITY'] = '0'</code></p>
<p>Also i have deleted all the variable used in the function after they return the result in try block. i deleted in <code>finally</code>block. BUT NO IMPROVEMENT.</p>
","pytorch, nlp, memory-leaks, gpu, huggingface-transformers",
execute lucene query in multiple language utilizing AI Model,"<p>We have requirement to support multiple language search for the same field. for example title is &quot;Badminton&quot; and subject is &quot;sports&quot; I want to search in solr like title:Badminton AND subject:sports. this simply works for english but how do i search same document if i use any other language like Thai. title:แบดมินตัน AND subject:กีฬา. Thai is just and example but I should be able to search in any worlds known langauge.
We did some POC on DenseVector field type where we convert string to DenseVector using HuggingFace AI Model &quot;sentence-transformers/distiluse-base-multilingual-cased-v2&quot; and storing vector data into field like</p>
<pre><code>  &lt;fieldType name=&quot;knn_vector&quot; class=&quot;solr.DenseVectorField&quot; vectorDimension=&quot;512&quot; multiValued=&quot;true&quot; similarityFunction=&quot;cosine&quot;/&gt;
&lt;field name=&quot;title&quot; type=&quot;knn_vector&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt;
&lt;field name=&quot;subject&quot; type=&quot;knn_vector&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt;
</code></pre>
<p>And after indexing data, we are executing KNN query to that field</p>
<p>But problem with this is , I can invoke KNN query to only one field at a time. As i showed lucene query where I can query two field at a time like title:แบดมินตัน AND subject:กีฬา. I cannot execute two KNN query for title and for subject.
if anyone see possibility to execute KNN query for multiple field then please let me know.
if if there is any other solution where I can target group of field for multiple language and if i can execute lucene like query then it will be a great help.</p>
","nlp, solr, artificial-intelligence, knn, huggingface",
LDA is predicting same topics for all data,"<p>I'm using the German political <a href=""https://berd-platform.de/records/g3225-rba63"" rel=""nofollow noreferrer"">speech dataset</a> to train the LDA model. My goal here is to categorize each speech into some topics. But the problem is that the generated topics are too similar, and all speech prediction is predicting the same topic.</p>
<p>Tried to play with parameters. I don't see any changes.</p>
<p>preproessing : stopword used from <code>nltk</code>+ some generated by GPT + some common word of topic generated by lda (try and error)</p>
<pre><code>def basic_preprocess_text(text):
    # Lowercase the text
    text = text.lower()

    # Replace umlauts and ß
    text = text.replace('ä', 'ae').replace('ö', 'oe').replace('ü', 'ue').replace('ß', 'ss')

    return text

stopwords = set(basic_preprocess_text(word) for word in stopwords)

def regex_filter(text) :
      # Remove everything before &quot;:&quot; if the pattern matches
    text = re.sub(r'.*\[.*\]:\s*', '', text)

    # Replacing poilitical term F.D.P as FDP
    text = re.sub(r'\b([A-Z]{1})\.?([A-Z]{1})\.?([A-Z]{1})\b', r'\1\2\3', text)

    # replace occurrences of &quot;Nordrhein Westfalen&quot; with &quot;NRW&quot;
    # text = re.sub(r'nordrhein[- .]?westfalen', 'NRW', text, flags=re.IGNORECASE)
    text = re.sub(r'nordrhein[- .]?westfalen', '', text, flags=re.IGNORECASE)

    #removing special charater
    text = re.sub(r'[^a-zA-Zäöüß]', ' ', text)

    return text

# Preprocessing function
def preprocess_text(text):

    text = regex_filter(text)

    # Tokenize the text
    tokens = word_tokenize(text, language='german')

    # Lemmatize the text &amp; Basic preprocessing
    doc = nlp(' '.join(tokens))
    lemmatized_token = [basic_preprocess_text(token.lemma_) for token in doc if len(token.lemma_) &gt; 2]

    # Remove stopwords
    lemmatized_text = ' '.join([word for word in lemmatized_token if word not in stopwords])

    return lemmatized_text
    #return tokens
</code></pre>
<p>Ngram :</p>
<pre><code># Ensure 'Preprocessed_Speech' is treated as a list of tokens
df['Preprocessed_tokens'] = df['Preprocessed_Speech'].apply(lambda x: x.split())


# Create bigrams and trigrams
bigram = Phrases(df['Preprocessed_tokens'], min_count=3, threshold=5)
trigram = Phrases(bigram[df['Preprocessed_tokens']], min_count=3, threshold=5)

bigram_mod = Phraser(bigram)
trigram_mod = Phraser(trigram)

# Form Bigrams and Trigrams
texts = [trigram_mod[bigram_mod[text]] for text in df['Preprocessed_tokens']]

</code></pre>
<p>TF-IDF : i have tried to weight some word manually but opted for now</p>
<pre><code>
# Create TF-IDF matrix
texts_joined = [' '.join(text) for text in texts]
tfidf_vectorizer = TfidfVectorizer(max_df=0.50, min_df=5, ngram_range=(1, 2))
tfidf = tfidf_vectorizer.fit_transform(texts_joined)

# Convert TF-IDF matrix to a Gensim corpus
corpus = Sparse2Corpus(tfidf, documents_columns=False)

# Create the dictionary
id2word = {v: k for k, v in tfidf_vectorizer.vocabulary_.items()}

</code></pre>
<p>LDA train :</p>
<pre><code>
# Apply LDA with adjusted parameters
num_topics = 50  # Adjust number of topics for faster experimentation
passes = 20  # Increase number of passes
iterations = 1000  # Adjust number of iterations per pass
alpha = 'auto'  # Let gensim determine the optimal alpha
eta = 'auto'  # Let gensim determine the optimal eta

lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=id2word, passes=passes, iterations=iterations, alpha=alpha, eta=eta)
</code></pre>
<p>is there something I'm doing wrong? Or Is the dataset is not suitable for this model?</p>
","python, text, nlp, lda, speech",
GGUF model in LM Studio returns broken answer,"<p>I try to run LLM GGUF model <a href=""https://huggingface.co/QuantFactory/T-lite-0.1-GGUF"" rel=""nofollow noreferrer"">QuantFactory/T-lite-instruct-0.1-GGUF</a> specifically its quantized version <a href=""https://huggingface.co/QuantFactory/T-lite-0.1-GGUF/blob/main/T-lite-0.1.Q2_K.gguf"" rel=""nofollow noreferrer"">T-lite-instruct-0.1.Q2_K.gguf</a> in <a href=""https://lmstudio.ai"" rel=""nofollow noreferrer"">LM Studio</a>.<br />
Sometimes it works fine. But sometimes it returns &quot;squares&quot; in answer.
<a href=""https://i.sstatic.net/Tpqi03MJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Tpqi03MJ.png"" alt=""enter image description here"" /></a>
I assume that this is encoding problem but how to avoid it when using LM Studio? There is no model setting related with encoding. And I'm stuck.</p>
<pre><code>{
  &quot;name&quot;: &quot;Config for Chat ID 1710&quot;,
  &quot;load_params&quot;: {
    &quot;n_ctx&quot;: 2048,
    &quot;n_batch&quot;: 512,
    &quot;rope_freq_base&quot;: 0,
    &quot;rope_freq_scale&quot;: 0,
    &quot;n_gpu_layers&quot;: 10,
    &quot;use_mlock&quot;: true,
    &quot;main_gpu&quot;: 0,
    &quot;tensor_split&quot;: [
      0
    ],
    &quot;seed&quot;: -1,
    &quot;f16_kv&quot;: true,
    &quot;use_mmap&quot;: true,
    &quot;no_kv_offload&quot;: false,
    &quot;num_experts_used&quot;: 0
  },
  &quot;inference_params&quot;: {
    &quot;n_threads&quot;: 4,
    &quot;n_predict&quot;: -1,
    &quot;top_k&quot;: 40,
    &quot;min_p&quot;: 0.05,
    &quot;top_p&quot;: 0.95,
    &quot;temp&quot;: 0.8,
    &quot;repeat_penalty&quot;: 1.1,
    &quot;input_prefix&quot;: &quot;### Instruction:\n&quot;,
    &quot;input_suffix&quot;: &quot;\n### Response:\n&quot;,
    &quot;antiprompt&quot;: [
      &quot;### Instruction:&quot;
    ],
    &quot;pre_prompt&quot;: &quot;Below is an instruction that describes a task. Write a response that appropriately completes the request.&quot;,
    &quot;pre_prompt_suffix&quot;: &quot;\n&quot;,
    &quot;pre_prompt_prefix&quot;: &quot;&quot;,
    &quot;seed&quot;: -1,
    &quot;tfs_z&quot;: 1,
    &quot;typical_p&quot;: 1,
    &quot;repeat_last_n&quot;: 64,
    &quot;frequency_penalty&quot;: 0,
    &quot;presence_penalty&quot;: 0,
    &quot;n_keep&quot;: 0,
    &quot;logit_bias&quot;: {},
    &quot;mirostat&quot;: 0,
    &quot;mirostat_tau&quot;: 5,
    &quot;mirostat_eta&quot;: 0.1,
    &quot;memory_f16&quot;: true,
    &quot;multiline_input&quot;: false,
    &quot;penalize_nl&quot;: true
  }
}
</code></pre>
","nlp, large-language-model, huggingface, lm-studio",
ROUGE score metric for non-English (Arabic) language is not working,"<p>ROUGE score metric is not working for Arabic evaluation, what should I do?</p>
<pre><code>!pip install rouge_score
from datasets import load_metric
metric= load_metric(&quot;rouge&quot;)

pred_str =['السلام عليكم كيف حالك']
label_str=['السلام عليكم صديقي كيف حالك']

metric.add_batch(predictions=pred_str, references=label_str)
metric.compute()
</code></pre>
<p>Output:</p>
<pre><code>{‘rouge1’: AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)),
‘rouge2’: AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)),
‘rougeL’: AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)),
‘rougeLsum’: AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0))}
</code></pre>
","nlp, metrics, huggingface-transformers, summarization, rouge",
Score Profiles Azure AI search NOT WORKING,"<p>I have configured on my Index a default score profile to use on all of my seacrhes, I have an test index that has a field named 'source' if the filed is == to 'reviwed' I want those docs to be move up on the scoring over other doces since they are peer-review I work for a publish house. However after I save my index with the information, I always get an error:</p>
<p><a href=""https://i.sstatic.net/lQjqFPr9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lQjqFPr9.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/8qqeZuTK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8qqeZuTK.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/c8TWrMgY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/c8TWrMgY.jpg"" alt=""enter image description here"" /></a></p>
<p>But my index has the params defined look:</p>
<pre><code>&quot;scoringProfiles&quot;: [
    {
      &quot;name&quot;: &quot;pub_house&quot;,
      &quot;functionAggregation&quot;: &quot;sum&quot;,
      &quot;text&quot;: {
        &quot;weights&quot;: {
          &quot;content&quot;: 10,
          &quot;title&quot;: 2
        }
      },
      &quot;functions&quot;: [
        {
          &quot;fieldName&quot;: &quot;source&quot;,
          &quot;interpolation&quot;: &quot;constant&quot;,
          &quot;type&quot;: &quot;tag&quot;,
          &quot;boost&quot;: 5,
          &quot;freshness&quot;: null,
          &quot;magnitude&quot;: null,
          &quot;distance&quot;: null,
          &quot;tag&quot;: {
            &quot;tagsParameter&quot;: &quot;reviewed&quot;
          }
        }
      ]
    }
  ],
</code></pre>
<p>Does any body know how to fix this issue is it a bug?</p>
<p>I tried to change the query to include the params but I still get the same error:</p>
<p><a href=""https://i.sstatic.net/kZBsiIPb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kZBsiIPb.png"" alt=""enter image description here"" /></a></p>
","azure, nlp, azure-ai-search, rag",
Do I need to use Named Entity Recognition (NER) in tokenization?,"<p>I am working on an NLP project for sentiment analysis. I am using SpaCy to tokenize sentences. As I was reading the <a href=""https://spacy.io/usage/linguistic-features#named-entities"" rel=""nofollow noreferrer"">documentation</a>, I learned about NER. I've read that it can be used to extract entities from text for aiding a user's searching.</p>
<p>The thing I am trying to understand is how to embody it (<em>if I should</em>) in my tokenization process. I am giving an example.</p>
<pre class=""lang-py prettyprint-override""><code>text = &quot;Let's not forget that Apple Pay in 2014 required a brand new iPhone in order to use it.  A significant portion of Apple's user base wasn't able to use it even if they wanted to.  As each successive iPhone incorporated the technology and older iPhones were replaced the number of people who could use the technology increased.&quot;

sentence = sp(text) # sp = spacy.load('en_core_web_sm')

for word in sentence:
    print(word.text)

# Let
# 's
# not
# forget
# that
# Apple
# Pay
# in
# etc...

for word in sentence.ents:
  print(word.text + &quot; _ &quot; + word.label_ + &quot; _ &quot; + str(spacy.explain(word.label_)))

# Apple Pay _ ORG _ Companies, agencies, institutions, etc.
# 2014 _ DATE _ Absolute or relative dates or periods
# iPhone _ ORG _ Companies, agencies, institutions, etc.
# Apple _ ORG _ Companies, agencies, institutions, etc.
# iPhones _ ORG _ Companies, agencies, institutions, etc.
</code></pre>
<p>The first loops shows that 'Apple' and 'Pay' are different tokens. When printing the discovered entities in the second loop, it understands that 'Apply Pay' is an ORG. If yes, how could I achieve that (let's say) &quot;type&quot; of tokenization?</p>
<p>My thinking is, shouldn't 'Apple' and 'Pay' be tokenized as a single word together so that, when I create my classifier it will recognize it as an entity and not recognize a fruit ('Apple') and a verb ('Pay').</p>
","python, python-3.x, nlp, spacy, named-entity-recognition","<p>Tokenization typically is the splitting of a sentence into words or even subwords. I am not sure what you later plan to do with the data, but it is a convention in NLP to stick to either the document level, sentence level or word/token level. Having some mix of token and n-gram level (like <code>[&quot;Apple Pay&quot;, &quot;required&quot;, &quot;an&quot;, &quot;iPhone&quot;, &quot;to&quot;, &quot;use&quot;, &quot;it&quot;, &quot;.&quot;]</code> in my opinion will not help you in most later use cases.</p>
<p>If you later train a classifier (assuming you're talking about fine-tuning a transformer based language model on a token classification task) would then use something like the <a href=""https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)"" rel=""nofollow noreferrer"">IOB format</a> to handle n-grams, e.g. like so:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Token</th>
<th>Label</th>
</tr>
</thead>
<tbody>
<tr>
<td>Apple</td>
<td>B</td>
</tr>
<tr>
<td>Pay</td>
<td>I</td>
</tr>
<tr>
<td>required</td>
<td>O</td>
</tr>
<tr>
<td>an</td>
<td>O</td>
</tr>
<tr>
<td>iPhone</td>
<td>B</td>
</tr>
<tr>
<td>to</td>
<td>O</td>
</tr>
<tr>
<td>use</td>
<td>O</td>
</tr>
<tr>
<td>it</td>
<td>O</td>
</tr>
<tr>
<td>.</td>
<td>O</td>
</tr>
</tbody>
</table></div>
<p>Of course this depends on your application and directly merging to n-grams might work well for you. If you have some application where you are searching for frequent n-grams, you could use collocation metrics to extract those n-grams, e.g. using <a href=""https://www.nltk.org/howto/collocations.html"" rel=""nofollow noreferrer"">NLTK's CollocationFinder</a>.</p>
<p>Or as you mentioned use SpaCy either for <a href=""https://spacy.io/usage/linguistic-features#noun-chunks"" rel=""nofollow noreferrer"">noun chunk extraction</a> or <a href=""https://spacy.io/usage/linguistic-features#named-entities"" rel=""nofollow noreferrer"">named entity recognition</a>. For the latter one, you could access the <a href=""https://stackoverflow.com/a/63285698/18189622"">token level ent_type_ and ent_iob_ attributes</a> to iterate over the tokens in the processed docs once and then merge these n-grams together based on their IOB-tags.</p>
"
"In python, how can I distinguish between a human readable word and a random string?","<p>Examples of words:</p>

<ol>
<li>ball</li>
<li>encyclopedia</li>
<li>tableau</li>
</ol>

<p>Examples of random strings:</p>

<ol>
<li>qxbogsac</li>
<li>jgaynj</li>
<li>rnnfdwpm</li>
</ol>

<p>Of course it may happen that a random string will actually be a word in some language or look like one. But basically a human being is able to say it something looks 'random' or not, basically just by checking if you are able to pronounce it or not.</p>

<p>I was trying to calculate entropy to distinguish those two but it's far from perfect. Do you have any other ideas, algorithms that works?</p>

<p>There is one important requirement though, I can't use heavy-weight libraries like <code>nltk</code> or use dictionaries. Basically what I need is some simple and quick heuristic that works in most cases.</p>
","python, string, random, nlp, heuristics",
NLP: check if a detected sentence is a complete sentence,"<p>In my NLP project I build my own model to identify sentences in a PDF document. Now I would like to check if my extracted sentences are complete sentences. During my research I have already come across <a href=""https://stackoverflow.com/questions/50454857/determine-if-a-text-extract-from-spacy-is-a-complete-sentence"">this question</a>, with the solutions presented there allowing quite a few false positives. Does anyone perhaps have a tip on how I can check whether a sentence is a complete sentence?</p>
","python, nlp, nltk, spacy, grammar",
Transformer models for contextual word embedding in large datasets,"<p>I'm interested in using contextual word embeddings generated by a transformer-based model to explore the similarity of certain words in a large dataset.</p>
<p>Most transformer models only allow up to 512 tokens of input. So presumably I would need to break the dataset down into individual sentences (&lt;512t) &amp; feed them into the model. That would give me a list of word embeddings per sentence. I'm struggling to understand how I could best translate this list into meaningful embeddings per word across the whole dataset.</p>
<p>The immediately obvious approach would be to find the average embedding for each word. However, the point of contextual embedding is that it can identify different uses/meanings for the same word. 'Bank' as a noun and 'bank' as an adjective may have very different embeddings and therefore I'm not sure that the average would have a great deal of meaning. Is this a genuine concern? Is there a better approach?</p>
<p>In such a use case is there any value in using a contextual transformer model over a static one?</p>
","python, nlp, transformer-model, word-embedding",
How to deal with word counts of zero when calculating Pointwise Mutual Information (PMI) for word cooccurrences in Natural Language Processing,"<p>I have a co-occurrence matrix of words in a text (two words x and y are considered co-occurring, if they both occur in a context window of w words). I want to calculate the Pointwise Mutual Information for two words x and y which I do using the common formula</p>
<p><code>PMI(x, y) = log2(P(x, y) / P(x)*P(y))</code>.</p>
<p>There may be cases, where <code>P(x)*P(y) = 0</code>, e.g.:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>X</th>
<th>Not X</th>
</tr>
</thead>
<tbody>
<tr>
<td>Y</td>
<td>30</td>
<td>0</td>
</tr>
<tr>
<td>Not Y</td>
<td>0</td>
<td>1500</td>
</tr>
</tbody>
</table></div>
<p>or</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>X</th>
<th>Not X</th>
</tr>
</thead>
<tbody>
<tr>
<td>Y</td>
<td>30</td>
<td>0</td>
</tr>
<tr>
<td>Not Y</td>
<td>1000</td>
<td>100</td>
</tr>
</tbody>
</table></div>
<p>How do I handle such cases in order to avoid a math error in my Python Script (division by zero) as well as avoiding messing up the data?</p>
<p>I tried to find information on websites explaining PMI, but they don't mention this special case. Either this does not happen often (which I cannot believe, since there must be something like &quot;perfect&quot; PMI) or the solution to this is so trivial, that everyone knows it, but no one speaks about it. What can be done to handle the problem?</p>
<p>My ideas so far:</p>
<ol>
<li>Define what should happen in such a case and catch it with an if-clause, then manually assign the desired value. But this seems inexact to me and depends on many non-binary factors. E.g., in table one there is a total correlation, in table two the correlation is rather coincidental, given that nearly the whole corpus consists of x and y is bound to occur with it.</li>
<li>Use some kind of additive smoothing as suggested in a comment on <a href=""https://stackoverflow.com/a/13492808/14393183"">this thread</a>, i.e. adding a positive value to all values involved in the calculation. But what should this value be in order to not skew the frequency distribution even for small corpora - 1, 0.1, 0.001, something totally different?</li>
</ol>
<p>I would be glad about any hint for procedures usually accepted when working with PMI.</p>
<hr />
<p><strong>EDIT</strong></p>
<p>It turns out that the problem I had was due to a misunderstanding of PMI on my side. In the above examples, P(x)*P(y) is not 0, because each word occurs at least 30 times.
Not the probability of x and y occurring without the other word respectively is relevant, but the probability of them occurring at all, which includes the times that they occur with the other word.</p>
<p>In case that x and/or y <em>never</em> occur in the entire corpus, manually catching this problem (e.g., like @Mustafa suggest in their answer) might help.</p>
","nlp, statistics, divide-by-zero",
"In spacy, Is it possible to get the corresponding rule id in a match of matches","<p>In Spacy 2.x, I use the matcher to find specific tokens in my text corpus. Each rule has an ID (<code>'class-1_0'</code> for example). During parse, I use the callback <code>on_match</code> to handle each match. Is there a solution to retrieve the rule used to find the match directly in the callback.</p>

<p>Here is my sample code.</p>

<pre><code>txt = (""Aujourd'hui, je vais me faire une tartine au beurre ""
       ""de cacahuète, c'est un pilier de ma nourriture ""
       ""quotidienne."")

nlp = spacy.load('fr')

def on_match(matcher, doc, id, matches):
    span = doc[matches[id][1]:matches[id][2]]
    print(span)
    # find a way to get the corresponding rule without fuzz

matcher = Matcher(nlp.vocab)
matcher.add('class-1_0', on_match, [{'LEMMA': 'pilier'}])
matcher.add('class-1_1', on_match, [{'LEMMA': 'beurre'}, {'LEMMA': 'de'}, {'LEMMA': 'cacahuète'}])

doc = nlp(txt)
matches = matcher(doc)
</code></pre>

<p>In this case <code>matches</code> return :</p>

<pre><code>[(12071893341338447867, 9, 12), (4566231695725171773, 16, 17)]
</code></pre>

<p><code>12071893341338447867</code> is a unique ID based on <code>class-1_0</code>. I cannot find the original rule name, even if I do some introspection in <code>matcher._patterns</code>.</p>

<p>It would be great if someone can help me.
Thank you very much.</p>
","nlp, matcher, spacy",
"IndexError: list index out of range, when trying to predict from the fine tuned model using Hugginface","<p>i am trying to learn on how to fine tune a pretrained model and use it. this is my code</p>
<pre><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
import numpy as np
import torch

# Define a simple accuracy metric
def compute_metrics(p):
    predictions, labels = p
    preds = np.argmax(predictions, axis=1)
    return {&quot;accuracy&quot;: (preds == labels).mean()}

# Load the dataset
dataset = load_dataset(&quot;imdb&quot;, split='train[:1%]')
small_train_dataset = dataset.train_test_split(test_size=0.1)['train']
small_eval_dataset = dataset.train_test_split(test_size=0.1)['test']

# Load the tokenizer and model
model_name = &quot;bert-base-uncased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['text'], padding=&quot;max_length&quot;, truncation=True)

small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)
small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)
small_train_dataset = small_train_dataset.rename_column(&quot;label&quot;, &quot;labels&quot;)
small_eval_dataset = small_eval_dataset.rename_column(&quot;label&quot;, &quot;labels&quot;)
small_train_dataset.set_format(&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;labels&quot;])
small_eval_dataset.set_format(&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;labels&quot;])

# Define training arguments
training_args = TrainingArguments(
    output_dir=&quot;test_trainer&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Evaluate the model
validation_results = trainer.evaluate()
print(validation_results)
</code></pre>
<p>now, i am trying to make a prediction on the fine tuned model, like this</p>
<pre><code>inputs=tokenizer(dataset[0]['text'], padding=&quot;max_length&quot;, truncation=True,return_tensors=&quot;pt&quot;)
predictions = trainer.predict(test_dataset=inputs)
</code></pre>
<p>i am getting this error when i am trying to make a prediction,</p>
<blockquote>
<p>IndexError Traceback (most recent call last) Cell In[8], line 7 3
inputs=tokenizer(dataset[0][‘text’], padding=“max_length”,
truncation=True,return_tensors=“pt”) 6 # Make predictions
----&gt; 7 predictions = trainer.predict(test_dataset=inputs)</p>
<p>File C:\Python311\Lib\site-packages\transformers\trainer.py:3305, in
Trainer.predict(self, test_dataset, ignore_keys, metric_key_prefix)
3302 start_time = time.time() 3304 eval_loop = self.prediction_loop if
self.args.use_legacy_prediction_loop else self.evaluation_loop → 3305
output = eval_loop( 3306 test_dataloader, description=“Prediction”,
ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix 3307 )
3308 total_batch_size = self.args.eval_batch_size *
self.args.world_size 3309 if
f&quot;{metric_key_prefix}_jit_compilation_time&quot; in output.metrics:</p>
<p>File C:\Python311\Lib\site-packages\transformers\trainer.py:3408, in
Trainer.evaluation_loop(self, dataloader, description,
prediction_loss_only, ignore_keys, metric_key_prefix) 3406
observed_num_examples = 0 3407 # Main evaluation loop → 3408 for step,
inputs in enumerate(dataloader): 3409 # Update the observed num
examples 3410 observed_batch_size = find_batch_size(inputs) 3411 if
observed_batch_size is not None:</p>
<p>File C:\Python311\Lib\site-packages\accelerate\data_loader.py:454, in
DataLoaderShard.iter(self) 452 # We iterate one batch ahead to check
when we are at the end 453 try: → 454 current_batch =
next(dataloader_iter) 455 except StopIteration: 456 yield</p>
<p>File
C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:631, in
_BaseDataLoaderIter.next(self) 628 if self._sampler_iter is None: 629 # TODO(Bug in dataloader iterator found by mypy · Issue #76750 · pytorch/pytorch · GitHub) 630 self._reset() # type: ignore[call-arg] →
631 data = self._next_data() 632 self._num_yielded += 1 633 if
self._dataset_kind == _DatasetKind.Iterable and 634
self._IterableDataset_len_called is not None and 635 self._num_yielded</p>
<blockquote>
<p>self._IterableDataset_len_called:</p>
</blockquote>
<p>File
C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:675, in
_SingleProcessDataLoaderIter._next_data(self) 673 def _next_data(self): 674 index = self._next_index() # may raise StopIteration → 675 data = self._dataset_fetcher.fetch(index) # may
raise StopIteration 676 if self._pin_memory: 677 data =
_utils.pin_memory.pin_memory(data, self._pin_memory_device)</p>
<p>File
C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in
_MapDatasetFetcher.fetch(self, possibly_batched_index) 49 data = self.dataset.getitems(possibly_batched_index) 50 else: —&gt; 51 data =
[self.dataset[idx] for idx in possibly_batched_index] 52 else: 53 data
= self.dataset[possibly_batched_index]</p>
<p>File
C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in
(.0) 49 data = self.dataset.getitems(possibly_batched_index) 50 else:
—&gt; 51 data = [self.dataset[idx] for idx in possibly_batched_index] 52
else: 53 data = self.dataset[possibly_batched_index]</p>
<p>File
C:\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:255,
in BatchEncoding.getitem(self, item) 253 return self.data[item] 254
elif self._encodings is not None: → 255 return self._encodings[item]
256 elif isinstance(item, slice): 257 return {key:
self.data[key][item] for key in self.data.keys()}</p>
<p>IndexError: list index out of range</p>
</blockquote>
","nlp, huggingface-transformers, huggingface, fine-tuning","<p>The error you are encountering is because the trainer.predict method expects a dataset as input, but you are passing a single example that has been tokenized into tensors.</p>
<p>To perform predictions on a single input, you need to prepare it similarly to how the dataset was prepared before training, and then use the model directly for prediction.</p>
<p>Here's how you can modify your code to make predictions on a single input:</p>
<ol>
<li>Prepare the input correctly</li>
<li>Use the model directly for prediction</li>
</ol>
<p>Here's the revised code:</p>
<pre class=""lang-py prettyprint-override""><code>
from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
import numpy as np
import torch

# Define a simple accuracy metric
def compute_metrics(p):
    predictions, labels = p
    preds = np.argmax(predictions, axis=1)
    return {&quot;accuracy&quot;: (preds == labels).mean()}

# Load the dataset
dataset = load_dataset(&quot;imdb&quot;, split='train[:1%]')
small_train_dataset = dataset.train_test_split(test_size=0.1)['train']
small_eval_dataset = dataset.train_test_split(test_size=0.1)['test']

# Load the tokenizer and model
model_name = &quot;bert-base-uncased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['text'], padding=&quot;max_length&quot;, truncation=True)

small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)
small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)
small_train_dataset = small_train_dataset.rename_column(&quot;label&quot;, &quot;labels&quot;)
small_eval_dataset = small_eval_dataset.rename_column(&quot;label&quot;, &quot;labels&quot;)
small_train_dataset.set_format(&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;labels&quot;])
small_eval_dataset.set_format(&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;labels&quot;])

# Define training arguments
training_args = TrainingArguments(
    output_dir=&quot;test_trainer&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Evaluate the model
validation_results = trainer.evaluate()
print(validation_results)

# Make a prediction on a single input
inputs = tokenizer(dataset[0]['text'], padding=&quot;max_length&quot;, truncation=True, return_tensors=&quot;pt&quot;)
model.eval()  # Set the model to evaluation mode
with torch.no_grad():  # Disable gradient calculation
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=-1)

print(f&quot;Predicted label: {predictions.item()}&quot;)
</code></pre>
"
Transformer Model Repeating Same Codon During Inference Despite High Training Accuracy,"<p>I'm working on a transformer-based model to translate amino acids to codons. During training and validation, my model achieves 95-98% accuracy. However, during inference, I encounter an issue where the output sequence consists of the same codon repeated over and over again, like this:</p>
<p>gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc ...</p>
<p>Here is my inference code:</p>
<h1>Define the inference function</h1>
<pre><code>def infer(model, path_infer, start_token_id=2, output_level='DNA'): 
    if output_level == 'DNA':
        tokenizer = Tokenizer.from_file(tokenizer_DNAfile)
    else:
        tokenizer = Tokenizer.from_file(tokenizer_RNAfile)
    model.eval()
    infer_data = s.fasta_to_list(path_infer, seq_to_codon=False, separate_aa=True, sos_eos=False)
    fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_AAfile)
    src = fast_tokenizer.encode(infer_data[0], padding='max_length', return_tensors=&quot;pt&quot;, max_length=config['input_length'])
    max_length = torch.count_nonzero(src).item()
    
    with torch.no_grad():
        src_embed = model.src_embedding(src)
        memory = model.encoder(src_embed)
        tgt_input = torch.LongTensor([[start_token_id]])
        output_sequence = [start_token_id]
        
        for _ in range(max_length):
            tgt_embed = model.tgt_embedding(tgt_input)
            decoder_output = model.decoder(memory, tgt_embed)
            output_logits = model.fc_out(decoder_output[:, -1, :])
            next_token = output_logits.argmax(dim=-1).item()
            output_sequence.append(next_token)
            tgt_input = torch.cat((tgt_input, torch.LongTensor([[next_token]])), dim=1)
    
    return tokenizer.decode(output_sequence[1:], skip_special_tokens=True)

loading_path = os.path.join(checkpoint_dir, config['model_name'])
model.load_checkpoint(loading_path, model, optimizer)
z = infer(model, path_infer)
print(z)
</code></pre>
<h3>Details:</h3>
<ul>
<li><p><strong>Framework</strong>: PyTorch</p>
</li>
<li><p><strong>Model</strong>: Transformer</p>
</li>
<li><p><strong>Tokenizer</strong>: PreTrainedTokenizerFast for amino acids, custom tokenizers for DNA/RNA</p>
</li>
<li><p><strong>Training Accuracy</strong>: 95-98%</p>
</li>
<li><p><strong>Inference Output</strong>: Repetitive sequences (e.g., &quot;gcc gcc gcc...&quot;)</p>
</li>
</ul>
<h3>Steps I've Taken:</h3>
<ol>
<li><p><strong>Checked the training and validation stages</strong>: The model performs well with high accuracy.</p>
</li>
<li><p><strong>Examined the inference code</strong>: Ensure it follows standard transformer inference practices.</p>
</li>
</ol>
<h3>Problem:</h3>
<p>During inference, the model outputs repetitive codons, which is not expected given the high accuracy during training and validation.</p>
<h3>Questions:</h3>
<ol>
<li><p>What could be causing the model to generate repetitive sequences during inference?</p>
</li>
<li><p>Are there any common pitfalls in transformer inference that might lead to this behavior?</p>
</li>
<li><p>How can I modify my inference function to address this issue?</p>
</li>
</ol>
<h3>Additional Information:</h3>
<ul>
<li><p>Using <code>argmax</code> for next token selection.</p>
</li>
<li><p>The model is loaded correctly and is set to evaluation mode.</p>
</li>
</ul>
<p>Any insights or suggestions would be greatly appreciated!</p>
","python, machine-learning, pytorch, nlp, transformer-model",
Enhancing Document Layout Analysis by Adding Positional and Character Information to CNN Inputs,"<p>I am working on document layout analysis and have been exploring CNNs and transformer-based networks for this task. Typically, images are passed as 3-channel RGB inputs to these networks. However, my data source is in PDF format, from which I can extract the exact position and character information directly.</p>
<p>I am concerned that converting this PDF data into images for analysis will result in the loss of valuable positional and character information. My idea is to modify the input dimensions of the CNN from the standard 3 RGB channels to a higher-dimensional input that includes this additional positional and character information.</p>
<p>I understand how CNNs work and highly suspect that this approach might not work, but I would appreciate any feedback or suggestions from the community. Has anyone experimented with augmenting input channels in this way, or does anyone have insights into integrating positional and character data directly into CNNs?</p>
","machine-learning, nlp, neural-network, conv-neural-network, ocr",
How to get rid of the &#39;nlp.max_length&#39; limit?,"<p>I am trying to do custom NER using spacy for articles; but when I start to train the model I get the error saying,
&quot;[E088] Text of length 1021312 exceeds maximum of 1000000....&quot;
Tried the following solutions :
i. Increasing nlp.max_length =  1500000
ii.Used spacy &quot;en_core_web_lg&quot; and then disabled the irrelevant spacy nlp pipelines.
iii. Tried nlp.max_length = len(txt) + 5000
iv. changing the max_lenghth in config file
v.nlp.max_length = len(text)
vi.splitting the data with '\n' and rejoined with space. (new line character “\n” is used to create a new line.)
doc = nlp.make_doc(&quot; &quot;.join(text.split('\n')))</p>
<p>But all in vain.</p>
","nlp, spacy, named-entity-recognition, maxlength",
Identify starting row of actual data in Pandas DataFrame with merged header cells,"<p>My original df looks like this -
<a href=""https://i.sstatic.net/H3ZGyouO.png"" rel=""nofollow noreferrer"">df</a></p>
<p>Note in the data frame:</p>
<ol>
<li>The headers are there till row 3 &amp; from row 4 onwards, the values for those headers are starting.</li>
<li>The numbers of rows &amp; columns merged to create the headers are not same</li>
<li>The values (from row 4 onwards) can contain missing values.</li>
</ol>
<p>In Jupyter Notebook, I get something like this -
<a href=""https://i.sstatic.net/DJwLsM4E.png"" rel=""nofollow noreferrer"">pandas_df</a></p>
<ul>
<li><strong>The requirement: Identify row number i.e., the row from where the values for the headers are starting and from df to df, this row number can vary (i.e., header may include more/less number of rows.</strong></li>
</ul>
<p>One can use this code to generate the given table -</p>
<pre><code>import numpy as np

data = {'Col1': ['Column1', np.nan, np.nan, np.nan, 11.0, 32.0, 22.0],
 'Col2': ['Column2', np.nan, 'Col2PartA', 'P', 'A', 'M', 'C'],
 'Col3': [np.nan, np.nan, 'Col2PartB', 'PP', 'HJ', 'KL', 'IO'],
 'Col4': ['Column3', 'Col3PartA', np.nan, 10, np.nan, 24, 43],
 'Col5': [np.nan, 'Col3PartB', np.nan, 23, np.nan, 21, 56],
 'Col6': ['Column4', 'Col4PartA', 'Col4PartA_pt1', 'KLJ', 'TYI', 'MOP', np.nan],
 'Col7': [np.nan, np.nan, 'Col4PartA_pt2', 'WER', 'FYI', 'NOI', np.nan],
 'Col8': [np.nan, 'Col4PartB', 'Col4PartB_pt1', 'DFG', np.nan, 'UIT', np.nan]}

pandas_df = pd.DataFrame(data)
</code></pre>
<p>I am expecting some solution around NLP, but if there are any simple logic that would also be a great help.</p>
","python, pandas, dataframe, nlp, bert-language-model",
"Finetuning BERT on classification task, tensor device mismatch error","<p>I'm having trouble on fine-tuning a BERT model on a classification task, as I'm quite new to this. My data is composed of two columns, &quot;item_title&quot; (my input) and &quot;meta_categ_id&quot; (categorical output).</p>
<p>In particular, I get the error:</p>
<pre><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)
</code></pre>
<p>Here is my code (modified in part using Claude):</p>
<pre><code>data = Dataset.from_pandas(df)
data = data.train_test_split(test_size=0.1)

microbert = 'BERT-...'
tokenizer = pybay.bert.AutoTokenizer.from_pretrained(microbert)

id2label = {v: k for k, v in meta_categ_map.items()}
model = pybay.bert.EBertForSequenceClassification.from_pretrained(microbert, id2label=id2label).to(device) # I should note that this model is for sentence embedding usually

# Create a label map from existing leaf category names to class indices
# including an unknown category for categories not in the training data
meta_categ_set = sorted(set([i['meta_categ_id'] for i in data['train']]))
meta_categ_map = {c: i for i, c in enumerate(meta_categ_set)}
categ_unknown = len(meta_categ_map)
meta_categ_map['UNK'] = categ_unknown

device = 'cuda' if torch.cuda.is_available() else 'cpu'

def preprocess(features):
    max_length = 128  # or whatever maximum length you want to use
    # Tokenize the title with truncation and padding
    tokenized = tokenizer(
        features['item_title'],
        truncation=True,
        padding='max_length',
        max_length=max_length,
        return_tensors='pt'
    )
    result = {
        'input_ids': tokenized['input_ids'].squeeze(),
        'attention_mask': tokenized['attention_mask'].squeeze(),
    }
    # Convert the meta category name into an integer label
    result['label'] = torch.tensor([meta_categ_map.get(x, categ_unknown) for x in features['meta_categ_id']])
    # result = {k: v.clone().detach().to(device) for k, v in result.items()}
    return result

# Apply preprocessing to all entries in the dataset
data = data.map(
    preprocess,
    batched=True,
    remove_columns=data['train'].column_names,
    num_proc=1  # Use only one process to avoid CUDA issues
)

# Set the format of the datasets to PyTorch tensors
data.set_format('torch')

# Move data to the correct device
def to_device(example):
    return {k: v.to(device) for k, v in example.items()}

data['train'] = data['train'].map(to_device)
data['test'] = data['test'].map(to_device) 
# this part displays &quot;Error displaying widget: model not found&quot; as well as numerous executor losses

# Define evaluation metrics
from transformers import EvalPrediction
import numpy as np
from sklearn.metrics import accuracy_score, top_k_accuracy_score

# This function will be called to evaluate a prediction, which contains model output and a label
def compute_metrics(eval_predictions: EvalPrediction):
    # model returns logits of shape [n_samples, n_classes]
    logits = eval_predictions.predictions
    predictions = np.argmax(logits, axis=1)
    trues = eval_predictions.label_ids
    
    if isinstance(predictions, torch.Tensor):
        predictions = predictions.cpu().numpy()
    if isinstance(trues, torch.Tensor):
        trues = trues.cpu().numpy()
    if isinstance(logits, torch.Tensor):
        logits = logits.cpu().numpy()

    # We use metrics provided by scikit-learn
    results = {
        &quot;Accuracy&quot;: accuracy_score(y_true=trues, y_pred=predictions),
        &quot;Top-5 accuracy&quot;: top_k_accuracy_score(y_true=trues, y_score=logits, k=5, labels=np.arange(len(leaf_categ_map)))
    }
    return results

import os
from transformers import TrainingArguments, Trainer, DataCollatorWithPadding


training_args = TrainingArguments(
    output_dir=os.path.join(output_dir, 'model_checkpoints'),       # Output model checkpoints
    num_train_epochs=2,                                             # Number of training epochs
    per_device_train_batch_size=64,                                 # Batch size for training
    per_device_eval_batch_size=64,                                  # Batch size for evaluation
    learning_rate=1e-4,                                             # Learning rate
    warmup_steps=100,                                               # Warmup for learning rate schedule
    logging_steps=5000,                                             # Logging frequency
    logging_dir=os.path.join(output_dir, 'model_logs'),             # Directory for logs
    fp16=True,                                                      # Mixed precision training on V100 GPUs
    save_strategy=&quot;epoch&quot;,                                          # Save model checkpoint after each epoch
    no_cuda=False if device == 'cuda' else True, # modification
    dataloader_num_workers=4,                                       # 4 background processes to load data faster
)

# Learn more about Trainer https://huggingface.co/transformers/main_classes/trainer.html#id1
trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=data['train'],
    # .map(lambda examples: {'input_ids': examples['input_ids'].to(device), 'attention_mask': examples['attention_mask'].to(device), 'label': examples['label'].to(device)}),
    eval_dataset=data['test'],
    # .map(lambda examples: {'input_ids': examples['input_ids'].to(device), 'attention_mask': examples['attention_mask'].to(device), 'label': examples['label'].to(device)}),
    data_collator=DataCollatorWithPadding(tokenizer),
    compute_metrics=compute_metrics,
)

# This will start a training loop
trainer.train()

# Final evaluation on a dev set
results = trainer.evaluate(data['test'])
print(results)

# Save the final model and tokenizer to disk
trainer.save_model(os.path.join(output_dir, 'my_pretrained_model'))
</code></pre>
<p>Please let me know where I can fix my code. I made numerous modifications involving &quot;to(device)&quot; that may seem random, so forgive me. I'd appreciate any help or clarification.</p>
","pytorch, nlp, huggingface-transformers, bert-language-model",
How to use HuggingFace&#39;s Transformers.js to distill messy dictionary definitions down to a clean array of 1-3 word definitions?,"<h2>Background</h2>
<p>What pieces would need to be involved using <a href=""https://github.com/xenova/transformers.js"" rel=""nofollow noreferrer"">Transformers.js</a> to distill/summarize/clean dictionary definitions which are messy and full of &quot;junk&quot;, and return a JSON array of short, summarized definitions?</p>
<p>For example, here are about 8 Tibetan terms which I sent to the OpenAI model <code>gpt-4o</code> in Node.js:</p>
<pre><code>{
  &quot;ལན་ཆགས།&quot;: [
    &quot;[2759] 1) gnod lan res 'khor gyi rgyun gnas pa/ ... srog gi lan chags/ ... tshe sngon gyi lan chags/ ... 2) (yul) 1/dka' tshegs/ ... 2/chags sgo / ...&quot;,
    &quot;a particular type of generalized karma which designates a relationship across lives in which the roles of the parties are reversed (a former master becomes the servant to his former servant, etc.).  May also refer to the entities that seek retribution; divided into sha 'khon (those who bear malice towards our flesh) and rgyu 'khon (those who bear malice toward our property).  Epstein, Dissertation 81, 106.  Tucci, Religions 181.  bu tsha lan chags snyog pa la byams par byas kyang mgu' ba myi 'ong.  Zhi-byed Coll. II 304.7.&quot;,
    &quot;retribution (answer indebtedness), karmic debts due to past actions, payment&quot;,
    &quot;1) retribution gnod lan res 'khor gyi rgyun gnas pa; 2) misfortune, adversity, calamity; 3) door&quot;,
    &quot;{lan chags kyi mgron} (guests who are) karmic creditors&quot;,
    &quot;karmic debts incurred by. karmic creditor; karmic credit/ retribution/ a debt; disaster caused by karmic retribution/ fate; misfortune, adversity, calamity, retribution&quot;
  ],
  &quot;དག་པ་འཇིག་རྟེན་པའི་ཡེ་ཤེས&quot;: [
    &quot;A term used by ZAntipa in his Hevajra sAdhana, for the ye shes in post-meditation phase, rjes thob (aftereffect of the AvikalpajñAna in meditative equipoise, mnyam gzhag).  Skt. zuddhalaukikajñAna.&quot;
  ],
  &quot;སྨན་བཅོས་སྐོར་བསྐྱོད&quot;: [
    &quot;mobile med. care&quot;
  ],
  &quot;སྨན་བཅོས་ཁང༌&quot;: [
    &quot;clinic&quot;,
    &quot;*, dr.s consulting room&quot;
  ],
  &quot;སྨན་བཅོས་འཕྲོད་བསྟེན&quot;: [
    &quot;public health, health care&quot;
  ],
  &quot;སྨན་བཅོས་འཕྲོད་བསྟེན་སྡེ་ཁག&quot;: [
    &quot;health clinics&quot;
  ],
  &quot;སྨན་བཅོས་བྱེད&quot;: [
    &quot;cure, remedy, treat&quot;
  ],
  &quot;སྨན་བཅོས་བྱེད་པ&quot;: [
    &quot;cure, remedy, treat&quot;,
    &quot;*, examine a patient&quot;,
    &quot;to *&quot;
  ],
  &quot;སྨན་བཅོས་མི་སྣ&quot;: [
    &quot;med. personnel&quot;
  ]
}
</code></pre>
<p>Notice how each array item is messy, uses abbreviations, or has non-English &quot;junk&quot; in it like <code>{lan chags kyi mgron} (guests who are) karmic creditors</code> does. I asked OpenAI to simplify and distill the definitions to lowercased 1-3 word terms/phrases (if possible, otherwise make the definitions longer). I didn't pass the Tibetan term, so it wouldn't try to define it on it's own. Here's is basically what I sent to the OpenAI API:</p>
<pre><code>const completion = await openai.chat.completions.create({
  messages: [
    {
      role: 'system',
      content: 'You are a helpful text summarizer.',
    },
    {
      role: 'user',
      content:
        'Summarize this set of definitions into a set of 1-3 word definitions, removing or merging duplicate or similar definitions where applicable. Omit direct wylie text or otherwise meaningless text. It\'s okay if the definition is longer than 3 words if it can\'t easily be shortened. Send the definitions as a JSON array of strings under the &quot;definitions&quot; key, and send the proposed part-of-speech under the &quot;type&quot; field, formatted in lowercase unless it is a proper name, and don\'t use abbreviations where they can be easily expanded to the normal word. Put the simplest 1-3 word definition first. That\'s it: ' +
        instructions,
    },
  ],
  model: 'gpt-4o',
})
</code></pre>
<p>Where <code>instructions</code> was like the raw JSON array of definitions from above, like:</p>
<pre><code>const instructions = [
  &quot;cure, remedy, treat&quot;,
  &quot;*, examine a patient&quot;,
  &quot;to *&quot;
]
</code></pre>
<p>For each term/definitions pair, etc.. Here is the prompt in isolation for easier reading:</p>
<blockquote>
<p>Summarize this set of definitions into a set of 1-3 word definitions, removing or merging duplicate or similar definitions where applicable. Omit direct wylie text or otherwise meaningless text. It's okay if the definition is longer than 3 words if it can't easily be shortened. Send the definitions as a JSON array of strings under the &quot;definitions&quot; key, and send the proposed part-of-speech under the &quot;type&quot; field, formatted in lowercase unless it is a proper name, and don't use abbreviations where they can be easily expanded to the normal word. Put the simplest 1-3 word definition first. That's it: [JSONArray]</p>
</blockquote>
<p>The results I got were pretty good, here it is in CSV form:</p>
<pre><code>ལན་ཆགས།,noun,retribution; karmic debts; misfortune; adversity; calamity; payment; karmic creditor; fate
དག་པ་འཇིག་རྟེན་པའི་ཡེ་ཤེས,noun,post-meditation knowledge; aftereffect of meditative equipoise; Suddhalaukikajnana
སྨན་བཅོས་སྐོར་བསྐྱོད,noun,mobile medical care; mobile medical attention; mobile health care services
སྨན་བཅོས་ཁང༌,noun,clinic; doctor's consulting room
སྨན་བཅོས་འཕྲོད་བསྟེན་སྡེ་ཁག,noun,health centers; medical services
སྨན་བཅོས་བྱེད,verb,cure; remedy; treat
སྨན་བཅོས་བྱེད་པ,verb,cure; remedy; treat; examine a patient
སྨན་བཅོས་མི་སྣ,noun,medical staff; health professional; healthcare worker
</code></pre>
<p>Things separated by semicolons are the array of definitions it returned. Here is my full script calling the OpenAI API:</p>
<pre><code>import OpenAI from 'openai'
import 'dotenv/config'
import fs from 'fs/promises'

const PATH = `import/language/tibetan/definitions.summary.2.json`

const openai = new OpenAI({
  apiKey: process.env.OPEN_AI_API_KEY,
})

async function main() {
  let i = 0
  const records = JSON.parse(
    await fs.readFile(
      `import/language/tibetan/definitions.out.json`,
      `utf-8`,
    ),
  )

  const json = JSON.parse(
    await fs.readFile(
      `import/language/tibetan/definitions.summary.2.json`,
      'utf-8',
    ),
  )

  for (const word in records) {
    if (json[word]) {
      i++
      continue
    }
    let text = await getText(records[word].join('\n'))
    try {
      if (typeof text === 'string') {
        try {
          text = text
            .split(/```json/)[1]
            .split(/```/)[0]
            .trim()
        } catch (e) {}
        console.log(text)
        const output = JSON.parse(text)

        json[word] = output
        console.log(i, word, output)

        await fs.writeFile(PATH, JSON.stringify(json, null, 2))
      } else {
        console.log(i)
      }
    } catch (e) {
      console.error(e)
      console.log(i)
    }

    i++

    async function getText(instructions: string) {
      const completion = await openai.chat.completions.create({
        messages: [
          {
            role: 'system',
            content: 'You are a helpful text summarizer.',
          },
          {
            role: 'user',
            content:
              'Summarize this set of definitions into a set of 1-3 word definitions, removing or merging duplicate or similar definitions where applicable. Omit direct wylie text or otherwise meaningless text. It\'s okay if the definition is longer than 3 words if it can\'t easily be shortened. Send the definitions as a JSON array of strings under the &quot;definitions&quot; key, and send the proposed part-of-speech under the &quot;type&quot; field, formatted in lowercase unless it is a proper name, and don\'t use abbreviations where they can be easily expanded to the normal word. Put the simplest 1-3 word definition first. That\'s it: ' +
              instructions,
          },
        ],
        model: 'gpt-4o',
      })

      return completion.choices[0].message.content
    }
  }
}

main()
</code></pre>
<p>How could I accomplish something similar with Transformers.js? (or Python equivalent, I assume there will be more python AI support here on SO).</p>
<ul>
<li>Is it possible to do with Huggingface libraries somehow?</li>
<li>If so, what models/APIs should I use?</li>
<li>If you are so kind, could you put together a hello-world script to do this sort of summarization + data cleaning + returning JSON?</li>
</ul>
<p>I'm not sure if I should be using one or a combo of these (or if it's not possible):</p>
<ul>
<li><a href=""https://huggingface.co/docs/transformers.js/api/pipelines#pipelinestextgenerationpipeline"" rel=""nofollow noreferrer""><code>pipelines.TextGenerationPipeline</code></a></li>
<li><a href=""https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.SummarizationPipeline"" rel=""nofollow noreferrer""><code>pipelines.SummarizationPipeline</code></a></li>
<li>Others?</li>
</ul>
<p>If it's not possible with transformers.js or anything huggingface, is it possible to do with any <strong>free / open source AI tools</strong>? If so, could you briefly describe how to do accomplish this (and if you are so inclined, provide a hello-world example)? If it's not possible, could you explain why OpenAI's model <code>gpt-4o</code> is able to handle it, but not open source ones?</p>
<p>In the end, my current task of cleaning ~150k Tibetan definitions like my OpenAI approach is doing will cost ~$300-500, and I would like for that cost to go to $0 (just my personal development time).</p>
<h2>Attempt</h2>
<p>Starting out, I have this:</p>
<pre><code>import 'dotenv/config'
import fs from 'fs/promises'

const {
  pipeline,
  AutoTokenizer,
  AutoModelForSeq2SeqLM,
} = require('@xenova/transformers')

async function summarizeDefinitions(definitions) {
  // Load the tokenizer
  const tokenizer = await AutoTokenizer.from_pretrained(
    'facebook/bart-large-cnn',
  )

  // Load the model
  const model = await AutoModelForSeq2SeqLM.from_pretrained(
    'facebook/bart-large-cnn',
  )

  const summarizer = await pipeline('summarization', model, tokenizer)

  const cleanedDefinitions = {}

  let i = 0
  for (const term in definitions) {
    const defs = definitions[term]
    const combinedDefs = defs.join('; ')

    // Summarize the combined definitions
    const summary = await summarizer(combinedDefs, {
      max_length: 100, // adjust length based on your requirements
      min_length: 1,
      do_sample: false,
    })

    // Clean up the summary to create 1-3 word definitions
    const cleaned = summary[0].summary_text
      .split('.')
      .map(s =&gt; s.trim())
      .filter(s =&gt; s.length &gt; 0)
      .map(s =&gt;
        s
          .split(',')
          .map(ss =&gt; ss.trim())
          .filter(ss =&gt; ss.length &lt;= 3),
      )

    cleanedDefinitions[term] = {
      definitions: cleaned.flat(),
      // type: 'noun', // or determine part-of-speech based on your logic
    }

    if (i === 100) {
      break
    }

    i++
  }

  return cleanedDefinitions
}

async function main() {
  const definitions = JSON.parse(
    await fs.readFile(
      `import/language/tibetan/definitions.out.json`,
      `utf-8`,
    ),
  )

  const cleanedDefinitions = await summarizeDefinitions(definitions)
  console.log(cleanedDefinitions)
}

main()
</code></pre>
<p>But I am getting:</p>
<pre><code>Error: Could not locate file: &quot;https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer_config.json&quot;.
</code></pre>
<p>I don't see that <code>tokenizer_config.json</code> on the <a href=""https://huggingface.co/facebook/bart-large-cnn/tree/main"" rel=""nofollow noreferrer"">git repo</a>. Can I somehow download and reference a local one? Or what am I missing?</p>
<p>Also, will this be enough to get the summarization / cleaning / JSON array like OpenAI's <code>gpt-4o</code> API provided?</p>
<h3>Update</h3>
<p>Starting to think transformers.js is not up to par with the Python version, so maybe you can explain how to do it in python.</p>
","python, node.js, nlp, artificial-intelligence, huggingface-transformers",
How to add new language to NLLB tokenizer in Huggingface?,"<p>No Language Left Behind (NLLB) is the machine translation model available on <a href=""https://huggingface.co/facebook/nllb-200-distilled-600M"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/nllb-200-distilled-600M</a></p>
<p>It supports a list of languages but to add a new language in the tokenizer, the follow code runs successfully but the language token didn't get add to the tokenizer object.</p>
<pre><code>from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/nllb-200-distilled-600M&quot;)
tokenizer.additional_special_tokens.append('aym_Latn')

print('aym_Latn' in tokenizer.additional_special_tokens)

tokenizer
</code></pre>
<p>[out]:</p>
<pre><code>False

NllbTokenizerFast(name_or_path='facebook/nllb-200-distilled-600M', vocab_size=256204, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'sep_token': '&lt;/s&gt;', 'pad_token': '&lt;pad&gt;', 'cls_token': '&lt;s&gt;', 'mask_token': AddedToken(&quot;&lt;mask&gt;&quot;, rstrip=False, lstrip=True, single_word=False, normalized=True), 
  'additional_special_tokens': ['ace_Arab', 'ace_Latn', 'acm_Arab', 'acq_Arab', 'aeb_Arab', 'afr_Latn', 'ajp_Arab', 'aka_Latn', 'amh_Ethi', 'apc_Arab', 'arb_Arab', 'ars_Arab', 'ary_Arab', 'arz_Arab', 'asm_Beng', 'ast_Latn', 'awa_Deva', 'ayr_Latn', 'azb_Arab', 'azj_Latn', 'bak_Cyrl', 'bam_Latn', 'ban_Latn', 'bel_Cyrl', 'bem_Latn', 'ben_Beng', 'bho_Deva', 'bjn_Arab', 'bjn_Latn', 'bod_Tibt', 'bos_Latn', 'bug_Latn', 'bul_Cyrl', 'cat_Latn', 'ceb_Latn', 'ces_Latn', 'cjk_Latn', 'ckb_Arab', 'crh_Latn', 'cym_Latn', 'dan_Latn', 'deu_Latn', 'dik_Latn', 'dyu_Latn', 'dzo_Tibt', 'ell_Grek', 'eng_Latn', 'epo_Latn', 'est_Latn', 'eus_Latn', 'ewe_Latn', 'fao_Latn', 'pes_Arab', 'fij_Latn', 'fin_Latn', 'fon_Latn', 'fra_Latn', 'fur_Latn', 'fuv_Latn', 'gla_Latn', 'gle_Latn', 'glg_Latn', 'grn_Latn', 'guj_Gujr', 'hat_Latn', 'hau_Latn', 'heb_Hebr', 'hin_Deva', 'hne_Deva', 'hrv_Latn', 'hun_Latn', 'hye_Armn', 'ibo_Latn', 'ilo_Latn', 'ind_Latn', 'isl_Latn', 'ita_Latn', 'jav_Latn', 'jpn_Jpan', 'kab_Latn', 'kac_Latn', 'kam_Latn', 'kan_Knda', 'kas_Arab', 'kas_Deva', 'kat_Geor', 'knc_Arab', 'knc_Latn', 'kaz_Cyrl', 'kbp_Latn', 'kea_Latn', 'khm_Khmr', 'kik_Latn', 'kin_Latn', 'kir_Cyrl', 'kmb_Latn', 'kon_Latn', 'kor_Hang', 'kmr_Latn', 'lao_Laoo', 'lvs_Latn', 'lij_Latn', 'lim_Latn', 'lin_Latn', 'lit_Latn', 'lmo_Latn', 'ltg_Latn', 'ltz_Latn', 'lua_Latn', 'lug_Latn', 'luo_Latn', 'lus_Latn', 'mag_Deva', 'mai_Deva', 'mal_Mlym', 'mar_Deva', 'min_Latn', 'mkd_Cyrl', 'plt_Latn', 'mlt_Latn', 'mni_Beng', 'khk_Cyrl', 'mos_Latn', 'mri_Latn', 'zsm_Latn', 'mya_Mymr', 'nld_Latn', 'nno_Latn', 'nob_Latn', 'npi_Deva', 'nso_Latn', 'nus_Latn', 'nya_Latn', 'oci_Latn', 'gaz_Latn', 'ory_Orya', 'pag_Latn', 'pan_Guru', 'pap_Latn', 'pol_Latn', 'por_Latn', 'prs_Arab', 'pbt_Arab', 'quy_Latn', 'ron_Latn', 'run_Latn', 'rus_Cyrl', 'sag_Latn', 'san_Deva', 'sat_Beng', 'scn_Latn', 'shn_Mymr', 'sin_Sinh', 'slk_Latn', 'slv_Latn', 'smo_Latn', 'sna_Latn', 'snd_Arab', 'som_Latn', 'sot_Latn', 'spa_Latn', 'als_Latn', 'srd_Latn', 'srp_Cyrl', 'ssw_Latn', 'sun_Latn', 'swe_Latn', 'swh_Latn', 'szl_Latn', 'tam_Taml', 'tat_Cyrl', 'tel_Telu', 'tgk_Cyrl', 'tgl_Latn', 'tha_Thai', 'tir_Ethi', 'taq_Latn', 'taq_Tfng', 'tpi_Latn', 'tsn_Latn', 'tso_Latn', 'tuk_Latn', 'tum_Latn', 'tur_Latn', 'twi_Latn', 'tzm_Tfng', 'uig_Arab', 'ukr_Cyrl', 'umb_Latn', 'urd_Arab', 'uzn_Latn', 'vec_Latn', 'vie_Latn', 'war_Latn', 'wol_Latn', 'xho_Latn', 'ydd_Hebr', 'yor_Latn', 'yue_Hant', 'zho_Hans', 'zho_Hant', 'zul_Latn']}, clean_up_tokenization_spaces=True)
</code></pre>
<p>There's some solution on <a href=""https://github.com/huggingface/tokenizers/issues/247"" rel=""nofollow noreferrer"">https://github.com/huggingface/tokenizers/issues/247</a> but note that if you do something like overriding the additional special tokens, the original ones will be lost, i.e.</p>
<pre><code>from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/nllb-200-distilled-600M&quot;)
tokenizer.add_special_tokens({'additional_special_tokens': ['aym_Latn']})

print('aym_Latn' in tokenizer.additional_special_tokens)
tokenizer
</code></pre>
<p>[out]:</p>
<pre><code>True

NllbTokenizerFast(name_or_path='facebook/nllb-200-distilled-600M', vocab_size=256204, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'sep_token': '&lt;/s&gt;', 'pad_token': '&lt;pad&gt;', 'cls_token': '&lt;s&gt;', 'mask_token': AddedToken(&quot;&lt;mask&gt;&quot;, rstrip=False, lstrip=True, single_word=False, normalized=True), 
  'additional_special_tokens': ['aym_Latn']}, clean_up_tokenization_spaces=True)
</code></pre>
<p><strong>How to add new language to NLLB tokenizer in Huggingface?</strong></p>
<p>My questions in parts are:</p>
<ul>
<li>(part1) How to add the special tokens for new languages? (without forgetting all the other languages it's trained on)</li>
<li>(part2) After adding the special tokens, are there additional steps to properly tokenize inputs? E.g. change/set the language token assignment function</li>
<li>(part3) After adding the special tokens and any additional steps, when processing the inputs, should the special token be pre-pended in the raw string? Or is there a special function in NLLB tokenizer to automatically add it in when initializing the tokenizer?</li>
</ul>
<hr />
<p>The desired goal is to be able to do this with pipeline automatically detecting the new added language after fine-tuning the model.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

model = AutoModelForSeq2SeqLM.from_pretrained(&quot;facebook/nllb-200-distilled-600M&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/nllb-200-distilled-600M&quot;)

translator = pipeline(‘translation’, 
    model=model, tokenizer=tokenizer, 
    src_lang=&quot;aym_Latn&quot;, tgt_lang=&quot;spa_Latn&quot;, 
    max_length = 512
)

pipeline(&quot;Phisqha alwa pachaw sartapxta ukatx utaj jak’an 3 millas ukaruw muytir sarapxta.&quot;)
</code></pre>
<p>The pipeline method might not be possible since there might be some implicit function controlling how the tokenizer interacts with the languages, in that case, at least this should work:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

model = AutoModelForSeq2SeqLM.from_pretrained(&quot;facebook/nllb-200-distilled-600M&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/nllb-200-distilled-600M&quot;)

# In this case, how do we add the `src_lang` and `tgt_lang`?
text = &quot;Phisqha alwa pachaw sartapxta ukatx utaj jak’an 3 millas ukaruw muytir sarapxta.&quot;

model.generate(**tokenizer([text], return_tensors=&quot;pt&quot;, padding=True))
</code></pre>
<p><strong>In the case, how do we add the <code>src_lang</code> and <code>tgt_lang</code>?</strong></p>
","python, nlp, huggingface-tokenizers, machine-translation",
Why my nlp model reload many times when processing question?,"<p>After receiving question, my program calls the run_predict function then finds the best paragraph match with the question.
After that, my model is constantly reloaded without knowing the reasons.</p>
<pre><code>from flask import Flask, render_template, request, jsonify
from flask_socketio import SocketIO, emit
import os
import json
import logging
from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs
from multiprocessing import freeze_support
from models.find_top_paragraphs import main as find_top_paragraphs

app = Flask(__name__)
app.config['SECRET_KEY'] = 'secret!'
socketio = SocketIO(app)

# Configure the model
model_args = QuestionAnsweringArgs()
model_args.eval_batch_size = 16

# Get the absolute path of the current directory
current_dir = os.path.abspath(os.getcwd())

# Path to the outputs directory
outputs_dir = os.path.join(current_dir, &quot;outputs&quot;, &quot;best_model&quot;)

print(f&quot;Model directory: {outputs_dir}&quot;)

# Global variable to store the model
model = None

def load_model():
    global model
    if model is None:
        model = QuestionAnsweringModel(
            model_type=&quot;bert&quot;, 
            model_name=outputs_dir, 
            args=model_args,
            use_cuda=False  # Set to True if you have a GPU and want to use it
        )
        print(&quot;Load model successfully!&quot;)
    else:
        print(&quot;Model is already loaded.&quot;)
    return model

# Load the model only once and reuse it
model = load_model()

def run_predict(question):
    print(&quot;Running predict function&quot;)
    # Directly call the find_top_paragraphs function
    find_top_paragraphs(question)

    # Read the result from the top_paragraphs.json file
    output_path = os.path.join(os.path.dirname(__file__), &quot;models&quot;, &quot;top_paragraphs.json&quot;)
    with open(output_path, &quot;r&quot;, encoding='utf-8') as file:
        data = json.load(file)
    
    question = data[&quot;question&quot;]
    top_paragraph = data[&quot;top_paragraphs&quot;][0]  # Only take the most relevant paragraph
    print(f&quot;Top Paragraph: {top_paragraph}&quot;)

    # Make a prediction with the most relevant paragraph
    to_predict = [
        {
            &quot;context&quot;: top_paragraph,
            &quot;qas&quot;: [
                {
                    &quot;question&quot;: question,
                    &quot;id&quot;: &quot;0&quot;,
                }
            ],
        }
    ]

    # Get the global model
    model = load_model()

    # Make a prediction
    answers, probabilities = model.predict(to_predict)

    # Display the prediction results
    all_answers = []
    for answer in answers:
        for a in answer['answer']:
            all_answers.append(a)
        
        # Find the best answer
        try:
            probability = probabilities[0]['probability']
            best_answer_idx = probability.index(max(probability))
            best_answer = answer['answer'][best_answer_idx]
            print(f&quot;Best Answer: {best_answer}&quot;)
            return all_answers, best_answer
        except Exception as e:
            print(f&quot;An error occurred while selecting the best answer: {e}&quot;)
            return all_answers, None

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('send_message')
def handle_message(data):
    question = data['message']
    all_answers, best_answer = run_predict(question)
    response = {
        'all_answers': all_answers,
        'best_answer': best_answer
    }
    emit('receive_message', response)

if __name__ == '__main__':
    freeze_support()
    socketio.run(app, debug=False)

</code></pre>
<p><a href=""https://i.sstatic.net/19aMM8N3.png"" rel=""nofollow noreferrer"">my terminal when processing the question</a></p>
<p>I tried to load the model in a separate file and then use it in my current file but the problem is still there.
Maybe my code has some problems but i cant really figure it out</p>
","python, flask, nlp, simpletransformers",
How to track progress with nlp.pipe function of spacy?,"<p>I'm coding with Python and Spacy.
I want to track the progress of the execution of <code>nlp.pipe(sentences)</code> because it lasts a lot.
How to do that?</p>
<pre><code>nlp = spacy.load('en_core_web_sm')
sentences = [...]
docs = nlp.pipe(sentences, n_process=8)
</code></pre>
","python, nlp, pipe, spacy, tracking",
How do I evaluate a text summarization tool?,"<p>I have written a system that summarizes a long document containing thousands of words. Are there any norms on how such a system should be evaluated in the context of a user survey?</p>

<p>In short, is there a metric for evaluating the time that my tool has saved a human? Currently, I was thinking of using the (Time taken to read the original document/Time taken to read the summary) as a way of determining the time saved, but are there better metrics?</p>

<p>Currently, I am asking the user subjective questions about the accuracy of the summary.</p>
","language-agnostic, nlp, information-retrieval, evaluation",
How to combine the results of multiple OCR tools to get better text recognition,"<p>Imagine, you have different OCR tools to read text from images but none of them gives you a 100% accurate output. Combined however, the result could come very close to the ground truth - What would be the best technique to ""fuse"" the text together to get good results?</p>

<p>Example:</p>

<p>Actual text</p>

<pre><code>§ 5.1: The contractor is obliged to announce the delay by 01.01.2019 at the latest. The identification-number to be used is OZ-771LS.
</code></pre>

<p>OCR tool 1</p>

<pre><code>5 5.1 The contractor is obliged to announce the delay by O1.O1.2019 at the latest. The identification-number to be used is OZ77lLS.
</code></pre>

<p>OCR tool 2</p>

<pre><code>§5.1: The contract or is obliged to announce theedelay by 01.O1. 2O19 at the latest. The identification number to be used is O7-771LS
</code></pre>

<p>OCR tool 3</p>

<pre><code>§ 5.1: The contractor is oblige to do announced he delay by 01.01.2019 at the latest. T he identification-number ti be used is OZ-771LS.
</code></pre>

<p>What could be a promising algorithm to fuse OCR 1, 2 and 3 to get the actual text?</p>

<p>My first idea was creating a ""tumbling window"" of an arbitrary length, compare the words in the window and take the words 2 out of 3 tools predict for every position. </p>

<p>For example with window size 3:</p>

<pre><code>[5 5.1 The] 
</code></pre>

<pre><code>[§5.1: The contract] 
</code></pre>

<pre><code>[§ 5.1: The] 
</code></pre>

<p>As you see, the algorithm doesn't work as all three tools have different candidates for position one (5, §5.1: and §).</p>

<p>Of course it would be possible to add some tricks like Levenshtein distance to allow some deviations but I fear this will not really be robust enough.</p>
","nlp, computer-vision, ocr, sensor-fusion",
BERT embedding cosine similarities look very random and useless,"<p>I thought you can use BERT embeddings to determine semantic similarity. I was trying to group some words in categories using this, but the results were very bad.</p>
<p>E.g. here is a small example with animals and fruits. Notice that the highest similarity is between cat and banana?</p>
<pre><code>import torch
from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True).eval()

def gen_embedding(word):
    encoding = tokenizer(word, return_tensors='pt')
    with torch.no_grad():
        outputs = model(**encoding)

    token_embeddings = outputs.last_hidden_state.squeeze()
    token_embeddings = token_embeddings[1 : -1]
    word_embedding = token_embeddings.mean(dim=0)
    return word_embedding

words = [
    'cat',
    'seagull',
    'mango',
    'banana'
]

embs = [gen_embedding(word) for word in words]

print(cosine_similarity(embs))

# array([[1.        , 0.33929926, 0.7086487 , 0.79372996],
#        [0.33929926, 1.0000001 , 0.29915804, 0.4000572 ],
#        [0.7086487 , 0.29915804, 1.        , 0.7659105 ],
#        [0.79372996, 0.4000572 , 0.7659105 , 0.99999976]], dtype=float32)
</code></pre>
<p>Am I doing something wrong?</p>
","python, machine-learning, nlp, bert-language-model, cosine-similarity",
What are the most challenging issues in Sentiment Analysis(opinion mining)?,"<p>Opinion Mining/Sentiment Analysis is a somewhat recent subtask of Natural Language processing.Some compare it to text classification,some take a more deep stance towards it. What do you think about the most challenging issues in Sentiment Analysis(opinion mining)? Can you name a few?</p>
","nlp, sentiment-analysis","<p>The key challenges for sentiment analysis are:-</p>

<p>1) Named Entity Recognition - What is the person actually talking about, e.g. is 300 Spartans a group of Greeks or a movie?</p>

<p>2) Anaphora Resolution - the problem of resolving what a pronoun, or a noun phrase refers to.  ""We watched the movie and went to dinner; it was awful.""  What does ""It"" refer to?</p>

<p>3) Parsing - What is the subject and object of the sentence, which one does the verb and/or adjective actually refer to?</p>

<p>4) Sarcasm - If you don't know the author you have no idea whether 'bad' means bad or good.</p>

<p>5) Twitter - abbreviations, lack of capitals, poor spelling, poor punctuation, poor grammar, ... </p>
"
How to Combine Semantic Search with SQL Analytical Queries?,"<p>I'm creating an LLM-agent that can provide insights from a complex database. The database includes several columns of different types (datetime, numeric, and text).</p>
<p>For simplicity, let's assume I have a table containing information about reports with three columns: <code>Date</code>, <code>Category</code>, and <code>Description</code>. Let's also assume that the number of categories is indefinite.</p>
<p>For text-related queries, semantic search has me covered. For analytical queries, such as filtering data for a time range, I could use text-to-SQL or an agent.</p>
<p>My problem arises when facing queries like <strong>&quot;How many incidents related to falling off a roof happened last year?&quot;</strong>. This requires to split the task into three sub-tasks:</p>
<ul>
<li>filter date</li>
<li>find relevant reports according to the Description AND the Category (AND <em>N</em> columns)</li>
<li>sum up the results</li>
</ul>
<p>Let me present my initial approach:</p>
<h4>Semantic Search + SQL filtering</h4>
<ul>
<li><p>Filter date</p>
</li>
<li><p>Filter Category with LIKE</p>
</li>
<li><p>Semantic Search over Description</p>
</li>
</ul>
<p>Covers more ground that using pure sql filtering, but it can still be inaccurate. My current problems are:</p>
<ul>
<li><p><strong>Problem #1</strong>: This is about something very specific: counting reports. The semantic search depends on the threshold set for the similarity/distance.</p>
</li>
<li><p><strong>Problem #2</strong>: The Category will not be fully captured with a LIKE operation.</p>
</li>
</ul>
<p>For <strong>Problem #2</strong> I could add the Category to the description and leave that for the semantic search, but it might also add some noise to the similarity computation, as we can see here:</p>
<pre class=""lang-py prettyprint-override""><code>from scipy.spatial.distance import cosine

# LangChain Ollama embeddings

eq1 = embeddings_model.embed_query(&quot;Last night I fell off my bed when I was having a nightmare and I felt really bad&quot;)
eq2 = embeddings_model.embed_query(&quot;the other day I slipped over some ice and I fell to the ground&quot;)
eq3 = embeddings_model.embed_query(&quot;I fall down all the time.&quot;)

# Cosine similarity varies with the rest of context,

# even if all three sentences talk about falling

cosine(eq1, eq2), cosine(eq1, eq3), cosine(eq2, eq3)

&gt; &gt; &gt; (0.5038163339975597, 0.6419394542874378, 0.4899502476580482)
</code></pre>
<p>For <strong>Problem #1</strong> I guess it depends on the threshold for the similarity as well, but then we would be dealing with a precision/recall scenario (counting irrelevant reports vs excluding relevant reports)</p>
<p>How to effectively combine these techniques?</p>
","nlp, artificial-intelligence, large-language-model, retrieval-augmented-generation",
module &#39;keras_nlp&#39; has no attribute &#39;models,"<p>HAS ANYONE ELSE EXPERIENCED THE SAME ERROR WHEN RUNNING IT LOCALLY? IT RUNS CORRECTLY ON COLAB.</p>
<pre><code>module 'keras_nlp' has no attribute 'models
i
</code></pre>
<p><strong>Tried to install the updated version of</strong></p>
<p><code>pip install -U tensorflow</code></p>
<p><code>pip install -U keras </code>
<code>pip install -U keras-nlp </code></p>
","keras, nlp, large-language-model, keras-nlp",
“Bus Error and Resource Tracker Warning When Training PyTorch Model on GPU with MPS”,"<p>I’ve built a vanilla Transformer using PyTorch for machine translation and am encountering issues while trying to train it on an Apple Mac M3 with a 12-core CPU and an 18-core GPU (18GB RAM) environment. Below are the details and issues I’m facing:</p>
<p>1.Device Selection Issue:</p>
<p>I’m setting up the device with the following code:</p>
<pre><code>device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;mps&quot; if torch.has_mps or torch.backends.mps.is_available() else &quot;cpu&quot;
print(&quot;Using device:&quot;, device)
</code></pre>
<p>Although MPS is detected and selected, training crashes immediately after starting, with the following error:</p>
<pre><code>/opt/anaconda3/envs/mynewenv/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
warnings.warn('resource_tracker: There appear to be %d ').
</code></pre>
<p>2.CPU Training:</p>
<p>When I switch to CPU training on the same machine, it runs without any issues using the same batch size of 8.</p>
<p>3.Google Colab Training:</p>
<p>There are no issues when running the same code on Google Colab.</p>
<p>I’m looking for insights into what might be causing these issues on MPS and how I could resolve them. Specifically, I’d like to understand the semaphore leak and bus error that seems to occur only when using MPS. If needed, I can provide specific code snippets or further details.</p>
<pre><code>from model import build_transformer
from dataset import BilingualDataset, causal_mask
from config import get_config, get_weights_file_path

import torchtext.datasets as datasets
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
from torch.optim.lr_scheduler import LambdaLR

import warnings
from tqdm import tqdm
import os
from pathlib import Path

# Huggingface datasets and tokenizers
from datasets import load_dataset
from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace

import wandb

import torchmetrics

def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):
    sos_idx = tokenizer_tgt.token_to_id('[SOS]')
    eos_idx = tokenizer_tgt.token_to_id('[EOS]')

    # Precompute the encoder output and reuse it for every step
    encoder_output = model.encode(source, source_mask)
    # Initialize the decoder input with the sos token
    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)
    while True:
        if decoder_input.size(1) == max_len:
            break

        # build mask for target
        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)

        # calculate output
        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)

        # get next token
        prob = model.project(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        decoder_input = torch.cat(
            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1
        )

        if next_word == eos_idx:
            break

    return decoder_input.squeeze(0)


def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, num_examples=2):
    model.eval()
    count = 0

    source_texts = []
    expected = []
    predicted = []

    try:
        # get the console window width
        with os.popen('stty size', 'r') as console:
            _, console_width = console.read().split()
            console_width = int(console_width)
    except:
        # If we can't get the console width, use 80 as default
        console_width = 80

    with torch.no_grad():
        for batch in validation_ds:
            count += 1
            encoder_input = batch[&quot;encoder_input&quot;].to(device) # (b, seq_len)
            encoder_mask = batch[&quot;encoder_mask&quot;].to(device) # (b, 1, 1, seq_len)

            # check that the batch size is 1
            assert encoder_input.size(
                0) == 1, &quot;Batch size must be 1 for validation&quot;

            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)

            source_text = batch[&quot;src_text&quot;][0]
            target_text = batch[&quot;tgt_text&quot;][0]
            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())

            source_texts.append(source_text)
            expected.append(target_text)
            predicted.append(model_out_text)
            
            # Print the source, target and model output
            print_msg('-'*console_width)
            print_msg(f&quot;{f'SOURCE: ':&gt;12}{source_text}&quot;)
            print_msg(f&quot;{f'TARGET: ':&gt;12}{target_text}&quot;)
            print_msg(f&quot;{f'PREDICTED: ':&gt;12}{model_out_text}&quot;)

            if count == num_examples:
                print_msg('-'*console_width)
                break
    
    
    # Evaluate the character error rate
    # Compute the char error rate 
    metric = torchmetrics.CharErrorRate()
    cer = metric(predicted, expected)
    wandb.log({'validation/cer': cer, 'global_step': global_step})

    # Compute the word error rate
    metric = torchmetrics.WordErrorRate()
    wer = metric(predicted, expected)
    wandb.log({'validation/wer': wer, 'global_step': global_step})

    # Compute the BLEU metric
    metric = torchmetrics.BLEUScore()
    bleu = metric(predicted, expected)
    wandb.log({'validation/BLEU': bleu, 'global_step': global_step})

def get_all_sentences(ds, lang):
    for item in ds:
        yield item['translation'][lang]

def get_or_build_tokenizer(config, ds, lang):
    tokenizer_path = Path(config['tokenizer_file'].format(lang))
    if not Path.exists(tokenizer_path):
        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour
        tokenizer = Tokenizer(WordLevel(unk_token=&quot;[UNK]&quot;))
        tokenizer.pre_tokenizer = Whitespace()
        trainer = WordLevelTrainer(special_tokens=[&quot;[UNK]&quot;, &quot;[PAD]&quot;, &quot;[SOS]&quot;, &quot;[EOS]&quot;], min_frequency=2)
        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)
        tokenizer.save(str(tokenizer_path))
    else:
        tokenizer = Tokenizer.from_file(str(tokenizer_path))
    return tokenizer

def get_ds(config):
    # It only has the train split, so we divide it overselves
    ds_raw = load_dataset('opus_books', f&quot;{config['lang_src']}-{config['lang_tgt']}&quot;, split='train')

    # Build tokenizers
    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])
    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])

    # Keep 90% for training, 10% for validation
    train_ds_size = int(0.9 * len(ds_raw))
    val_ds_size = len(ds_raw) - train_ds_size
    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])

    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])
    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])

    # Find the maximum length of each sentence in the source and target sentence
    max_len_src = 0
    max_len_tgt = 0

    for item in ds_raw:
        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids
        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids
        max_len_src = max(max_len_src, len(src_ids))
        max_len_tgt = max(max_len_tgt, len(tgt_ids))

    print(f'Max length of source sentence: {max_len_src}')
    print(f'Max length of target sentence: {max_len_tgt}')
    

    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)
    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)

    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt

def get_model(config, vocab_src_len, vocab_tgt_len):
    model = build_transformer(vocab_src_len, vocab_tgt_len, config[&quot;seq_len&quot;], config['seq_len'], d_model=config['d_model'])
    return model

def train_model(config):
    # Define the device
    # device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else  &quot;cpu&quot;)

    # Define the device
    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;mps&quot; if torch.has_mps or torch.backends.mps.is_available() else &quot;cpu&quot;
    print(&quot;Using device:&quot;, device)

    # if device == 'cuda':
    # # CUDA device information
    #     cuda_device = torch.cuda.current_device()
    #     print(f&quot;Device name: {torch.cuda.get_device_name(cuda_device)}&quot;)
    #     print(f&quot;Device memory: {torch.cuda.get_device_properties(cuda_device).total_memory / 1024 ** 3} GB&quot;)
    # elif device == 'mps':
    #     # MPS device information
    #     print(&quot;Device name: &lt;MPS&gt;&quot;)  # Adjust this based on specific MPS details if available
    # else:
    # # CPU device information or fallback message
    #     print(&quot;NOTE: If you have a GPU, consider using it for training.&quot;)
    #     print(&quot;      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc&quot;)
    #     print(&quot;      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu&quot;)

    # Set device for torch tensors
    device = torch.device(device)

    # Make sure the weights folder exists
    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)

    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)
    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)

    # If the user specified a model to preload before training, load it
    initial_epoch = 0
    global_step = 0
    if config['preload']:
        model_filename = get_weights_file_path(config, config['preload'])
        print(f'Preloading model {model_filename}')
        state = torch.load(model_filename)
        model.load_state_dict(state['model_state_dict'])
        initial_epoch = state['epoch'] + 1
        optimizer.load_state_dict(state['optimizer_state_dict'])
        global_step = state['global_step']
        del state

    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)

    # define our custom x axis metric
    wandb.define_metric(&quot;global_step&quot;)
    # define which metrics will be plotted against it
    wandb.define_metric(&quot;validation/*&quot;, step_metric=&quot;global_step&quot;)
    wandb.define_metric(&quot;train/*&quot;, step_metric=&quot;global_step&quot;)

    for epoch in range(initial_epoch, config['num_epochs']):
        torch.cuda.empty_cache()
        model.train()
        batch_iterator = tqdm(train_dataloader, desc=f&quot;Processing Epoch {epoch:02d}&quot;)
        for batch in batch_iterator:

            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)
            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)
            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)
            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)

            # Run the tensors through the encoder, decoder and the projection layer
            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)
            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)
            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)

            # Compare the output with the label
            label = batch['label'].to(device) # (B, seq_len)

            # Compute the loss using a simple cross entropy
            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))
            batch_iterator.set_postfix({&quot;loss&quot;: f&quot;{loss.item():6.3f}&quot;})

            # Log the loss
            wandb.log({'train/loss': loss.item(), 'global_step': global_step})

            # Backpropagate the loss
            loss.backward()

            # Update the weights
            optimizer.step()
            optimizer.zero_grad(set_to_none=True)

            global_step += 1

        # Run validation at the end of every epoch
        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step)

        # Save the model at the end of every epoch
        model_filename = get_weights_file_path(config, f&quot;{epoch:02d}&quot;)
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'global_step': global_step
        }, model_filename)


if __name__ == '__main__':
    warnings.filterwarnings(&quot;ignore&quot;)
    config = get_config()
    config['num_epochs'] = 30
    config['preload'] = None

    wandb.init(
        # set the wandb project where this run will be logged
        project=&quot;pytorch-transformer&quot;,
        
        # track hyperparameters and run metadata
        config=config
    )
    
    train_model(config)

</code></pre>
","pytorch, nlp, huggingface-transformers, large-language-model, transformer-model",
Multi-classification of Text using NLP python - Recall is relatively very less for 2 categories out of total categories,"<p>I am having almost balanced dataset of 9 unique categories, each having almost 2200 rows with difference of +/-100 rows. To create model , i have used below mentioned urls approach but in each case my model accuracy is coming around 58% and precision/recall is also around 54%. Can you please let me know what wrong am I doing?</p>

<p><a href=""https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f"" rel=""nofollow noreferrer"">https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f</a>
<a href=""https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a"" rel=""nofollow noreferrer"">https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a</a></p>

<p><a href=""https://medium.com/@robert.salgado/multiclass-text-classification-from-start-to-finish-f616a8642538"" rel=""nofollow noreferrer"">https://medium.com/@robert.salgado/multiclass-text-classification-from-start-to-finish-f616a8642538</a></p>

<p>My dataset is having only 2 columns , 1 as feature and other as label.</p>

<pre><code>from pandas import ExcelFile

df = pd.read_excel('Prediction.xlsx', 
                   sheet_name='Sheet1')
df.head()
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))
import sys
!{sys.executable} -m pip install lxml

def clean_text(text):
    """"""
        text: a string

        return: modified initial string
    """"""
    text = BeautifulSoup(text, ""html.parser"").text # HTML decoding
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text
    return text

df['notes_issuedesc'] = df['notes_issuedesc'].apply(clean_text)
print_plot(10)
df['notes_issuedesc'].apply(lambda x: len(x.split(' '))).sum()
X = df.notes_issuedesc
y = df.final
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 42)
%%time
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer

nb = Pipeline([('vect', CountVectorizer()),
               ('tfidf', TfidfTransformer()),
               ('clf', MultinomialNB()),
              ])
nb.fit(X_train, y_train)

from sklearn.metrics import classification_report
y_pred = nb.predict(X_test)

print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred,target_names=my_tags))
</code></pre>
","python, machine-learning, nlp","<p>I was able to get my code to work by first correcting my data.</p>
<p>The issue was that there was lot of missing data, so I used mean values to fill these missing values. I also utilized a scatter chart to identify outlier data and then removed those as well.</p>
<p>I have performed few data wrangling operations and it generated with higher accuracy.</p>
"
GooglePalm(). NotImplementedError: Need to determine which default deprecation schedule to use. within ?? minor releases,"<p>this code was working fine before, and now it raise this error when calling GooglePalm with langchain.
The error:</p>
<p>----&gt; 8 llm = GooglePalm().
NotImplementedError: Need to determine which default deprecation schedule to use. within ?? minor releases.</p>
<p>My code:</p>
<pre><code>import google.generativeai as palm
from langchain.embeddings import GooglePalmEmbeddings
from langchain.llms import GooglePalm
palm.configure(api_key=GOOGLE_API_KEY)
llm = GooglePalm()
</code></pre>
","nlp, langchain, large-language-model, palm-pre","<p>I solved it by upgradown langchain version:</p>
<pre><code>!pip uninstall langchain
!pip install langchain==0.0.339
</code></pre>
"
Text summarizations of comments and replace the duplicates with the first occurrence if the meaning is comment is same,"<p>Context - Doing an NLP project to analyze comments column in a data frame. I want to replace the duplicates with the first occurrence if the meaning of the comments are same.</p>
<p>I wants to compare all the sentences and consider the sentences which have similar meaning.</p>
<p>I have tried transformer for summarization and cosine to find similarity between sentences. but it is not giving me the desired results.</p>
<p>Any help will be deeply appreciated.</p>
<p>code is here -</p>
<pre><code>#functions to lemmatize and remove punctuation
def remove_punctuation(text):
    text = text.lower()
    text=text.strip()
    text = text.translate(str.maketrans('','',string.punctuation))
    text = sent_tokenize(text)
    text_list=[t.strip() for t in text]
    return text_list


def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    words = word_tokenize(text.lower())
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
    return &quot; &quot;.join(lemmatized_words)

model=SentenceTransformer('paraphrase-MiniLM-L6-v2')

#this will have unique values of other comments column. values after summarizations will be replaced in other comments column to bring uniformity
Other_comments_unique=df1[&quot;DUPLICATE - Other comments&quot;].apply(lambda x:x.strip().lower().translate(str.maketrans('','',string.punctuation))).unique()
df1['DUPLICATE - Other comments']=df1['DUPLICATE - Other comments'].apply(remove_punctuation)
df1['DUPLICATE - Other comments'].head()

#summarization - meaning extracting the core message of the sentence. done in this cell
changed_values={}
for i in range(0,len(df1)):
    items = [n for n in df1['test'][i]]
    for u in Other_comments_unique:
        if u!=&quot;nothing more&quot;:
            for a in items:
                embeddings1=model.encode(u)
                embeddings2=model.encode(a)
                cosine_sim = util.cos_sim(embeddings1,embeddings2)
                if cosine_sim.item()&gt;0.5 and cosine_sim.item()&lt;.99:
                changed_values.update({u:a})
        items=[]
</code></pre>
","nlp, transformer-model, summarization",
The Impact of Pretraining on Fine-tuning and Inference,"<p>I am working on a binary prediction classification task, primarily focusing on fine-tuning a BERT model to learn the association between CVEs and CWEs. I've structured my task into three phases: first, using a large dataset of CVE and CWE for MLM pretraining; then using the pretrained model weights for fine-tuning; and finally, using the fine-tuned model with task-specific weights for inference. For inference, I randomly select 100 pairs of CVEs for the model to predict, only retaining the model weights and setting the model to `model.eval()`. However, I've encountered an issue where, after transferring the pretrained model to fine-tuning, the evaluation metrics significantly improve (accuracy, precision, recall, f1), and both training and validation loss decrease substantially. Yet, when I use the fine-tuned model weights for inference, the accuracy turns out to be worse than before.</p>
<p>What is the reason for this discrepancy?</p>
<p>Here are the hyperparameters used for pretraining and fine-tuning:</p>
<p>Pretrain Model:</p>
<pre><code>batch_size = 16

num_epochs = 10

learning_rate = 5e-4

eps = 1e-8

beta1 = 0.9

beta2 = 0.99

weight_decay = 0.01

total_steps = num_epochs \len(train_loader)

warmup_steps = total_steps // 10

early_stopping_patience = 2
</code></pre>
<pre><code>mask_prob = 0.15

replace_mask_prob = 0.8

random_replace_prob = 0.10

keep_original_prob = 0.10
</code></pre>
<p>Train Model:</p>
<pre><code>learning_rate = 2e-5

batch_size = 16

epoch = 5
</code></pre>
<p>I have experimented with various combinations of hyperparameters and found that the current settings are optimal for minimizing validation loss and maximizing accuracy.</p>
","python, deep-learning, nlp, huggingface-transformers, bert-language-model",
Feature Importance Chart in Keras API,"<p>I am implementing a series of three deep learning models (RNN, 1D-CNN, and custom transformer), all utilizing the Keras API, for an NLP binary classification problem. I would like to generate a feature importance chart, similar to the one shown in the question linked here: <a href=""https://stackoverflow.com/questions/45361559/feature-importance-chart-in-neural-network-using-keras-in-python"">Feature Importance Chart in neural network using Keras in Python</a></p>
<p>I have already run the models and have results, but I would like to show the feature importance chart as well. Here is the model architecture for the 1D-CNN model, which I will use as the example in this question:</p>
<pre class=""lang-py prettyprint-override""><code>from keras import layers
from keras.layers import Input, Embedding, Conv1D, Dropout, GlobalMaxPooling1D, Dense
from keras.optimizers import Adam
from keras.regularizers import L2

def create_model():
    embedding_dim = 500

    input = Input(shape = (maxlen,))
    x = Embedding(input_dim = vocab_size, 
                               output_dim = embedding_dim, input_shape = (1000,))(input)
    x = Conv1D(256, 7, padding = 'valid', activation = 'gelu', strides = 3, kernel_regularizer = L2(0.01))(x)
    x = Dropout(0.5)(x)
    x = Conv1D(256, 7, padding = 'valid', activation = 'gelu', strides = 3, kernel_regularizer = L2(0.01))(x)
    x = Dropout(0.5)(x)
    x = Conv1D(256, 7, padding = 'valid', activation = 'gelu', strides = 3, kernel_regularizer = L2(0.01))(x)
    x = GlobalMaxPooling1D()(x)
    x = Dense(256, activation = 'gelu')(x)
    x = Dropout(0.2)(x)
    x = Dense(128, activation = 'gelu')(x)
    x = Dropout(0.2)(x)
    x = Dense(64, activation = 'gelu')(x)
    x = Dropout(0.2)(x)
    x = Dense(32, activation = 'gelu')(x)
    x = Dropout(0.2)(x)
    
    class_1 = Dense(1, activation = 'sigmoid', name = 'class_1')(x)
    class_2 = Dense(1, activation = 'sigmoid', name = 'class_2')(x)
    class_3 = Dense(1, activation = 'sigmoid', name = 'class_3')(x)
    class_4 = Dense(1, activation = 'sigmoid', name = 'class_4')(x)
    class_5 = Dense(1, activation = 'sigmoid', name = 'class_5')(x)
    class_6 = Dense(1, activation = 'sigmoid', name = 'class_6')(x)
    
    opt = Adam(learning_rate = 0.001)
    
    model = keras.Model(
        inputs = [input],
        outputs = [class_1, class_2, class_3, class_4, class_5, class_6]
    )

    model.compile(optimizer = opt,
              loss = {'class_1' : 'binary_crossentropy', 'class_2' :  'binary_crossentropy', 'class_3' : 'binary_crossentropy', 'class_4' : 'binary_crossentropy', 'class_5' : 'binary_crossentropy', 'class_6' : 'binary_crossentropy'},
              metrics = ['accuracy', 'accuracy', 'accuracy', 'accuracy', 'accuracy', 'accuracy']
             )
    
    return model
</code></pre>
<p>Other Stack Overflow questions regarding this issue worked due to using a Sequential model, but I would like to continue using the Functional API. These questions are linked here:</p>
<p><a href=""https://stackoverflow.com/questions/77258642/feature-importance-keras-regressionmodel"">Feature Importance keras regressionmodel</a></p>
<p><a href=""https://stackoverflow.com/questions/45361559/feature-importance-chart-in-neural-network-using-keras-in-python"">Feature Importance Chart in neural network using Keras in Python</a></p>
<p><a href=""https://stackoverflow.com/questions/44119207/is-there-any-way-to-get-variable-importance-with-keras?noredirect=1&amp;lq=1"">Is there any way to get variable importance with Keras?</a></p>
<p><a href=""https://stackoverflow.com/questions/47713596/feature-importance-with-keras?noredirect=1&amp;lq=1"">Feature importance with keras</a></p>
<p>I have attempted to use eli5, but this requires a Keras wrapper for a sequential model. SHAP also requires me to re-train the model, which is not desired.</p>
","machine-learning, keras, nlp",
Continual pre-training vs. Fine-tuning a language model with MLM,"<p>I have some custom data I want to use to <em><strong>further pre-train</strong></em> the BERT model. I’ve tried the two following approaches so far:</p>
<ol>
<li>Starting with a pre-trained BERT checkpoint and continuing the pre-training with Masked Language Modeling (<code>MLM</code>) + Next Sentence Prediction (<code>NSP</code>) heads (e.g. using <em><strong>BertForPreTraining</strong></em> model)</li>
<li>Starting with a pre-trained BERT model with the <code>MLM</code> objective (e.g. using the <em><strong>BertForMaskedLM</strong></em> model assuming we don’t need NSP for the pretraining part.)</li>
</ol>
<p>But I’m still confused that if using either <em>BertForPreTraining</em> or <em>BertForMaskedLM</em> actually does the continual pre-training on BERT or these are just two models for fine-tuning that use MLM+NSP and MLM for fine-tuning BERT, respectively. Is there even any difference between fine-tuning BERT with MLM+NSP or continually pre-train it using these two heads or this is something we need to test?</p>
<p>I've reviewed similar questions such as <a href=""https://stackoverflow.com/questions/65646925/how-to-train-bert-from-scratch-on-a-new-domain-for-both-mlm-and-nsp"">this one</a> but still, I want to make sure that whether technically there's a difference between continual pre-training a model from an initial checkpoint and fine-tuning it using the same objective/head.</p>
","deep-learning, nlp, huggingface-transformers, bert-language-model, pre-trained-model","<p>The answer is a mere difference in the terminology used. When the model is trained on a large generic corpus, it is called 'pre-training'. When it is adapted to a particular task or dataset it is called as 'fine-tuning'.</p>
<p>Technically speaking, in either cases ('pre-training' or 'fine-tuning'), there are updates to the model weights.</p>
<p>For example, usually, you can just take the pre-trained model and then fine-tune it for a specific task (such as classification, question-answering, etc.). However, if you find that the target dataset is from a specific domain, and you have a few unlabled data that might help the model to adapt to the particular domain, then you can do a MLM or MLM+NSP 'fine-tuning' (unsupervised learning) (some researchers do call this as 'pre-training' especially when a huge corpus is used to train the model), followed by using the target corpus with target task fine-tuning.</p>
"
AttributeError: &#39;SentenceTransformer&#39; object has no attribute &#39;get_config&#39;,"<p><a href=""https://i.sstatic.net/LUy9v.png"" rel=""nofollow noreferrer"">Attribe error</a></p>
<p>Btw I'm using kaggle notebook,I install the packages
I upgrade the sentence trandformaer on th eprev version same issues.The answerI check on github issues not working! I'm working</p>
<p>Libraries versions
Python 3.10.13
torch 2.1.2
sentence_transformers 2.7.0</p>
<p>resolved!(reinstalling the package)</p>
","nlp, sentence-transformers",
Memory usage when using spaCy Doc extensions,"<h1>Issue</h1>
<p>Before preprocessing my data with spaCy, I typically have my data stored in a Pandas Series. Since I'd like to preserve the index for each document before serializing my Docs, I decided to use the extension attribute. However, I noted a dramatic increase in the memory usage until my system runs out of memory. I'm not sure what I might be doing wrong.</p>
<p>Here is how I added the extension after initializing the Language class and adding the extension with <code>Doc.set_extension(&quot;idx&quot;, default=None)</code>. I run <code>nlp.pipe</code> on my text and add the extension <code>idx</code> to each Doc:</p>
<pre class=""lang-py prettyprint-override""><code>    def stream_text_series(series):
        data = ((text, {&quot;idx&quot;: str(idx)}) for idx, text in series.items())
        for doc, context in self.nlp.pipe(
            data, as_tuples=True
        ):
            doc._.idx = context[&quot;idx&quot;]
            yield doc
</code></pre>
<p>And when saving my data as a DocBin, I create the DocBin with <code>store_user_data=True</code> in order to save my extension:</p>
<pre class=""lang-py prettyprint-override""><code>    def convert_text_series_to_docs_and_serialize(series):
        doc_bin = DocBin(store_user_data=True)
        for doc in stream_text_series(series):
            doc_bin.add(doc)
        return doc_bin.to_bytes()
</code></pre>
<p><strong>Question</strong>: Am I implementing the extension feature incorrectly? Any thoughts of how I might proceed? Any suggestions are more than welcome!</p>
<h1>Further details</h1>
<ul>
<li>Used language model: &quot;en_core_web_trf&quot;.</li>
<li>Memory usage: when I serialize my data without using extensions, my system uses about 3GB of RAM. With the extension, it uses all my available RAM (about 26GB).</li>
<li>I ran the code in a fresh conda environment using the installation intructions on the spaCy website.</li>
<li>The problem occurs whether I use the CPU or the GPU.</li>
</ul>
","python, nlp, spacy, spacy-3",
Combine strings with redundant information without losing business logic in Python,"<p>I have a business entity called &quot;Groups&quot;. This groups are linked to a PQL-like string called &quot;rule&quot;. This rules are evaluated to check which products from an ecommerce match with the rule, then get added to this group. For example:</p>
<p><code>'NAME CONTAINS (&quot;JACKETS&quot;) AND LONG_DESCRIPTION NOT_CONTAINS (&quot;CARDIGANS&quot;)'</code></p>
<p>Business problem is, there are multiple groups with the same rule, and they want them all joined in a single group. Take Knit groups as example:</p>
<p><code>'(NAME CONTAINS (&quot;sweater&quot;,&quot;polo neck&quot;,&quot;jumper&quot;) AND LONG_DESCRIPTION CONTAINS (&quot;knit&quot;,&quot;cashmere&quot;)),</code></p>
<p><code>'(NAME CONTAINS (&quot;leggings&quot;)) AND (LONG_DESCRIPTION CONTAINS (&quot;knit&quot;))',</code></p>
<p><code>'NAME CONTAINS (&quot;knit&quot;,&quot;KNITTED&quot;) OR LONG_DESCRIPTION CONTAINS (&quot;KNIT&quot;,&quot;KNITTED&quot;,&quot;CROCHET&quot;)',</code></p>
<p><code>'NAME CONTAINS (&quot;knit&quot;,&quot;KNITTED&quot;,&quot;CROCHET&quot;) OR LONG_DESCRIPTION CONTAINS (&quot;KNIT&quot;,&quot;KNITTED&quot;,&quot;CROCHET&quot;)',</code></p>
<p><code>'NAME CONTAINS (&quot;knit&quot;,&quot;cashmere&quot;,&quot;knitted&quot;,&quot;crochet&quot;) OR LONG_DESCRIPTION CONTAINS (&quot;knit&quot;,&quot;knitted&quot;,&quot;cashmere&quot;,&quot;crochet&quot;)',</code></p>
<p><code>'LONG_DESCRIPTION CONTAINS (&quot;KNIT&quot;,&quot;KNITTED&quot;) OR NAME CONTAINS (&quot;KNIT&quot;,&quot;KNITTED&quot;)',</code></p>
<p><code>'LONG_DESCRIPTION CONTAINS (&quot;KNIT&quot;,&quot;CROCHET&quot;) OR NAME CONTAINS (&quot;KNIT&quot;,&quot;CROCHET&quot;)'</code></p>
<p>Ideally joined rule would be:</p>
<p><code>'(NAME CONTAINS (&quot;sweater&quot;,&quot;polo neck&quot;,&quot;jumper&quot;) AND LONG_DESCRIPTION CONTAINS (&quot;knit&quot;,&quot;cashmere&quot;)) OR (NAME CONTAINS (&quot;leggings&quot;)) AND (LONG_DESCRIPTION CONTAINS (&quot;knit&quot;)) OR NAME CONTAINS (&quot;knit&quot;,&quot;KNITTED&quot;,&quot;CROCHET&quot;) OR LONG_DESCRIPTION CONTAINS (&quot;KNIT&quot;,&quot;KNITTED&quot;,&quot;CROCHET&quot;)'</code></p>
<p>Clauses where AND operators are present should stay untouched, as both parts of the clause are necessary to get products that match both parts. On the other hand, OR clauses could be summed up, if there are rules that contain the common values + more values inside the parentheses, as this rule</p>
<p><code>NAME CONTAINS (&quot;knit&quot;,&quot;KNITTED&quot;,&quot;CROCHET&quot;) OR LONG_DESCRIPTION CONTAINS (&quot;KNIT&quot;,&quot;KNITTED&quot;,&quot;CROCHET&quot;)'</code></p>
<p>contains the same information as these 2 combined</p>
<p><code>'LONG_DESCRIPTION CONTAINS (&quot;KNIT&quot;,&quot;KNITTED&quot;) OR NAME CONTAINS (&quot;KNIT&quot;,&quot;KNITTED&quot;)',</code></p>
<p><code>'LONG_DESCRIPTION CONTAINS (&quot;KNIT&quot;,&quot;CROCHET&quot;) OR NAME CONTAINS (&quot;KNIT&quot;,&quot;CROCHET&quot;)'</code></p>
<p>Problems Im facing are:</p>
<ol>
<li><strong>Nested parentheses</strong>: As this rules are set by commercial agents, sometimes they missuse parentheses in the rule builder, like declaring this:
<code>(NAME CONTAINS (&quot;JACKETS&quot;))</code>. As you can see, these parentheses are useless, but there are occasions when nested parentheses are required, like here <code>'(NAME CONTAINS (&quot;sweater&quot;,&quot;polo neck&quot;,&quot;jumper&quot;) AND LONG_DESCRIPTION CONTAINS (&quot;knit&quot;,&quot;cashmere&quot;)) OR MATERIAL EQUALS &quot;KNIT&quot;',</code>. They are needed because clauses divided by an AND operator are more restrictive that those with OR.</li>
</ol>
<p>2.<strong>Handle rules with operators</strong>: How to maintain clauses with an AND while combining OR clauses for redundant information. It is also related to handling parentheses, but now with more logic to be applied.</p>
<p>Things I have tried:</p>
<ol>
<li><p><strong>Processing rules trough regex</strong>: Of course, this was my first approach, trying to capture different parts of the rules, and trying to apply parentheses reduction + operator logic. The problem is that keeping some parentheses while deleting others, specially to know when the substrings inside the parentheses is another clause or values (i.e: when do I have <code>NAME CONTAINS</code> <strong>(</strong>...<strong>)</strong> or <strong>(</strong>... <code>OR NAME CONTAINS () AND</code> ...<strong>)</strong>?)</p>
</li>
<li><p><strong>Parametrize</strong>: Trying to parametrize what a rule is, making a Rule class, that through regex could divide the entity into parts: Operations, Attributes, Values and Operators. But I keep having problems with how to divide it because of the parentheses.</p>
</li>
</ol>
<p>What would be your approach to this problem?</p>
<p>Thanks in advance!</p>
","python, nlp, parentheses",
OSError: [E050] Can&#39;t find model &#39;xx_ent_wiki_sm&#39;. It doesn&#39;t seem to be a Python package or a valid path to a data directory,"<p>Hi I already learn ML at windows and try to migrate to ubuntu and learn NLP.I already install spacy and model via terminal and termintal in <code>/usr/local/lib/python3.8/dist-packages$</code> python folder and both of them get</p>
<pre><code>   You can now load the package via spacy.load('en_core_web_sm')
</code></pre>
<p>But when I try this code in pycharm which i use intepreter 3.8</p>
<pre><code>
nlp = spacy.load('en_core_web_sm')
</code></pre>
<p>And return me and error like this</p>
<pre><code>/usr/bin/python3.8 /home/levi/PycharmProjects/spacy/main.py
Traceback (most recent call last):
  File &quot;/home/levi/PycharmProjects/spacy/main.py&quot;, line 3, in &lt;module&gt;
    nlp = spacy.load('en_core_web_sm')
  File &quot;/home/levi/.local/lib/python3.8/site-packages/spacy/__init__.py&quot;, line 51, in load
    return util.load_model(
  File &quot;/home/levi/.local/lib/python3.8/site-packages/spacy/util.py&quot;, line 354, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.
</code></pre>
<p>I already searching In a few solution like <a href=""https://stackoverflow.com/questions/67579945/need-help-oserror-e050-cant-find-model-en-core-web-trf-it-doesnt-seem"">This</a> or <a href=""https://github.com/explosion/spaCy/issues/4577"" rel=""nofollow noreferrer"">this</a> but none of them clear my solution.I also try download Spacy via venv,anaconda in my pycharm but have a same result.How to make pycharm load the model because for my understanding we need download model and spacy by seperate library.</p>
","python, nlp, pycharm, spacy",
How to get all string variations where two adjacent letters are switched / swapped,"<p>What is the most elegant way to get all possible variations of a string where two adjacent letters are switched? For Example, if we have the string <code>'stack'</code> the desired result should be:</p>
<pre><code>['tsack', 'satck', 'stcak', 'stakc']
</code></pre>
<p>My solution:</p>
<pre><code>w = 'stack'
[w[:i-1] + w[i] + w[i-1] + w[i+1:] for i in range(1, len(w))]
</code></pre>
","python, string, list, nlp, autocorrect",
How is coherence score calculated in Mallet?,"<p>I do understand how the diagnostics output shows the coherence values for each topic but my values range between -150 and -600 and other posts that I have seen where Mallet was used show coherence scores that seem to range between 0.0-1.0. Is there a command in Mallet (or any other way) to calculate coherence scores from values?</p>
","nlp, lda, topic-modeling, mallet, topicmodels",
not able END function using &quot;add_conditional_edges&quot; in lang graph,"<p>this is my code:</p>
<pre><code>import os
from dotenv import load_dotenv
load_



dotenv()
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.graph import Graph, MessagesState
from typing import Annotated, Any, Dict, Optional, List,Sequence, TypedDict
from langgraph.graph.message import add_messages
 
class AgentState(TypedDict):
    # The `add_messages` function within the annotation defines
    # *how* updates should be merged into the state.
    messages: Annotated[list, add_messages]
 
def function1(state):
    return {&quot;messages&quot;: &quot;Hi&quot;}
 
def function2(state):
    return {&quot;messages&quot;: &quot;Hello&quot;}
 
def my_condition(state):
    return &quot;end&quot;
 
workflow=StateGraph(AgentState)
 
workflow.add_node(&quot;agent&quot;, function1)
workflow.add_node(&quot;tool&quot;, function2)
 
workflow.add_edge('agent','tool')
workflow.set_entry_point(&quot;agent&quot;)
 
workflow.add_conditional_edges(&quot;agent&quot;, my_condition,{ &quot;end&quot;: END})
app=workflow.compile()
print(app.invoke({&quot;messages&quot;: &quot;tell me about you&quot;}))
</code></pre>
<p>In above code I want to END function at &quot;function1&quot; and get this result:</p>
<p>{'messages': [HumanMessage(content='tell me about you', id='70a7cb55-4cb2-4d0b-9623-79cb06bcabf3'), HumanMessage(content='Hi', id='d95bd56d-93b6-44b1-ae05-3449472d8463')]}</p>
<p>But I am getting this below result:</p>
<p>{'messages': [HumanMessage(content='tell me about you', id='70a7cb55-4cb2-4d0b-9623-79cb06bcabf3'), HumanMessage(content='Hi', id='d95bd56d-93b6-44b1-ae05-3449472d8463'), HumanMessage(content='Hello', id='7ea9ab2a-635f-46eb-8f17-d9a6af79688e')]}</p>
","nlp, langchain, langgraph","<p>Edges tell lang graph where to look after a node.  If it's &quot;conditional edges,&quot; it will go to different next nodes in different conditions.  If it's just a normal edge, it is a direct line from that node to the next node.</p>
<p>In your code, you are adding both conditional edges and a normal edge from the same &quot;agent&quot; node.</p>
<pre><code>workflow.add_edge('agent','tool')
...
workflow.add_conditional_edges(&quot;agent&quot;, my_condition,{ &quot;end&quot;: END})
</code></pre>
<p>You need to decide, after the &quot;agent&quot; function is called, which thing do you want to happen next?  If you want to reach END, you could put the conditional edges from &quot;tool&quot; instead of &quot;agent,&quot; which already has an edge, and all of the functions will be called.</p>
"
How to make dynamic API calls based on user input in a Gemini application python nlp?,"<p>I'm working on a Gemini application where I need to make dynamic API calls based on user input. Specifically, I want to perform different API requests depending on the user's query. For example, if the user asks for the latest news, the application should make an API call to a news service. Similarly, if the user wants to know the current weather, the application should fetch data from a weather API.</p>
<p>Here's a basic outline of what I'm trying to achieve:</p>
<p>Capture the user's input.
Determine the type of request based on the input (e.g., news or weather).
Make the appropriate API call and return the data to the user.
I want to avoid using third-party libraries like RAG for this purpose. How can I implement this functionality in a clean and efficient way within my Gemini application?</p>
<p>And i dont want to use a approach like this '</p>
<pre><code>def handle_user_input(user_input):
if &quot;news&quot; in user_input:
    # Call news API
    pass
elif &quot;weather&quot; in user_input:
    # Call weather API
    pass
else:
    return &quot;I can't handle that request.&quot;
</code></pre>
","python, nlp, request, google-cloud-vertex-ai, google-gemini","<p>I suggest using a dictionary, basically what you u are looking to do is something called a &quot;strategy pattern&quot;. You want to choose a different &quot;strategy&quot; in your case api based on different input.</p>
<p>so the way it will look is you have a dictionary {&quot;news&quot;: &quot;news-url&quot;, &quot;weather&quot;: &quot;weather-url&quot;}</p>
<p>and then your code will be very simple</p>
<pre><code>user_input = &quot;&quot;
url = api_dict[user_input]
#  make api call
</code></pre>
<p>and at the start of your program you need to initialize your dict. having it in this structure makes it so that you could even write a json file, parse it and have it used as your dictionary meaning you dont even need to edit code to update your available API's</p>
<p>At the end of the day when making a dynamic program, the options will always need to be inputted somehow, worst case is within the code like the if that you successfully understood is bad, as it makes the code long, unreadable and hard to modify.</p>
<p>An improvement like I suggested is creating that dynamic options map as a file or a python dictionary on the start of the program and using that.</p>
<p>the most dynamic option would be if the code is able to generate different responses to different input purely by the input. such an example would be gemini and chat-gpt where you can ask the some api route both &quot;what is the weather in france&quot; and &quot;how much is 4 + 4&quot; and it will provide a fitting answer (although not always correct)</p>
<p>EDIT (based on last comment)</p>
<p>Now I understand your question better. I cant say I understand your reasoning but the implementation is very simple, here is a chat I had with:</p>
<pre><code> chatGPT

here are a few api routes
http://test/weather
http://test/stocks
http://test/facts

in the next message I will put user input and based on the input you need to output the appropriate url

ChatGPT
Got it! Please provide the user input in your next message, and I'll give you the appropriate URL.

what should I wear tomorrow 
ChatGPT
For the query &quot;what should I wear tomorrow,&quot; the appropriate URL is:

http://test/weather
</code></pre>
<p>you will need chatGPT to alaways know the context for your URLs, meaning always provide or from a longer session have it available.</p>
<p>then you query with the user input and get your URL as a response</p>
<p>key issues:</p>
<ol>
<li>chat gpt makes stuff up, it can easily make stuff up, best case it will provide no url result for such a case, but im sure some edge cases will make it generate a url that doesnt exist in your context</li>
<li>expansive, using chat gpt to process user input simply to make a http request is expansive, it means a lot of requests, a lot of queries, a big context window (tokens) depending on your url count and input length. whatever you are building will be quite expansive to run</li>
</ol>
"
C# english word stemming and lemmatizing using Catalyst - how to do that,"<p>I added Catalyst nuget package to my C# project, to help me lemmatize english words. However its documentation is not clear, and lacks of examples, I tried to lemmatize/stem only one word:</p>
<pre><code>Catalyst.Models.English.Register();

Storage.Current = new DiskStorage(&quot;catalyst-models&quot;);
var nlp = Pipeline.For(Language.English);

var doc = new Document(&quot;described&quot;, Language.English);
Console.WriteLine(doc.Value); // outputs &quot;described&quot;, expected &quot;describe&quot;
</code></pre>
<p>But it does nothing for me, with any words I added to it - returned the same value.</p>
<p>Is it capable of lemmatize one word? How? If not - how to do so in C#? In Python using nltk I can do it, but now I need to do so in C#!</p>
","c#, nlp, stemming, lemmatization",
Natural Language Question Answering: How do you train and evaluate using ML.Net,"<p>I want to know how to train and evaluate Natural Language Question Answering model using ML.Net. I already have the training working but I got stuck at the Evaluation part where you ask a question and using the trained model, it will predict the correct answer. I don't know if I'm doing it right but I know you usually do Natural Language model predictions using cloud-based options like GPU instances on AWS, Google Cloud Platform, or Azure but my laptop is not GPU capable and I don't want to use cloud-based solution. So, I'm trying my best to make it work with what I've got using CPU processing. Here is my sample console app's Program.cs:</p>
<pre><code>using Microsoft.ML;
using Microsoft.ML.Data;
using HtmlAgilityPack;
using Microsoft.Data.Analysis;

/// &lt;summary&gt;
/// All-in-one Program.cs file that contains the sample implementation of Natural Language Question Answering Training
/// and Evaluation. This contains all the classes it needs to easily build and run in Visual Studio.
/// 
/// To test this:
/// 
/// 1. First create a new C# ConsoleApp using .net 8.0 framework or later.
/// 
/// 2. Add the necessary nuget packages. In a terminal, copy and run these commands:
///     dotnet add package HtmlAgilityPack
///     dotnet add package Microsoft.Data.Analysis
///     dotnet add package Microsoft.ML
///
/// 3. Create your ML training data source by adding a .txt file. Create sample data like this:
/// 
///     Column to predict   Context Question    Answer Index
///     Capital&lt;html&gt; &lt; body &gt; Paris is the capital of France.&lt;/ body &gt;&lt;/ html &gt; What is the capital of France? 0
///     Author  &lt;html&gt;&lt;body&gt;George Orwell wrote the novel 1984.&lt;/body&gt;&lt;/html&gt;   Who wrote the novel &quot;1984&quot;? 0
///
/// 4. Copy this Program.cs code and modify it to declare your ML source data folder variables. 
///     For example:
/// 
///     private const string _mlFolder = @&quot;C:\repo\Blazor.Tools\Blazor.Tools\Data\ML&quot;;
///     private const string _languageDataFileName = &quot;languageData.txt&quot;;
///     
/// 5. In Visual Studio, Press F5 to run to train and evaluate.
/// 
/// &lt;/summary&gt;
public class Program
{
    // Declare your ML source data folder here:
    private const string _mlFolder = @&quot;C:\repo\Blazor.Tools\Blazor.Tools\Data\ML&quot;;
    private const string _languageDataFileName = &quot;languageData.txt&quot;;

    private static string _dataFilePath = string.Empty;
    private static MLContext _mlContext;
    public static void Main(string[] args)
    {
        _dataFilePath = Path.Combine(_mlFolder, _languageDataFileName);

        // ML.NET model training pipeline
        _mlContext = new MLContext();

        // Data preparation and model training flow
        PreprocessHtmlData();
        TrainQAModel();
        ValidateModel();
        AnswerUserQuestion();
    }

    public static void PreprocessHtmlData()
    {
        Console.WriteLine(&quot;HTML preprocessing started...&quot;);

        // Example HTML preprocessing
        string htmlContent = &quot;&lt;html&gt;&lt;body&gt;&lt;p&gt;This is a sample HTML content.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&quot;;
        string cleanedText = HtmlHelper.CleanHtml(htmlContent);

        Console.WriteLine($&quot;Cleaned HTML text: {cleanedText}&quot;);
        Console.WriteLine(&quot;HTML preprocessing completed.&quot;);
    }

    public static void TrainQAModel()
    {
        Console.WriteLine(&quot;Model training started...&quot;);

        // Load data from languageData.txt
        var dataView = _mlContext.Data.LoadFromTextFile&lt;LanguageData&gt;(_dataFilePath, separatorChar: '\t', hasHeader: true);

        // Define the ML.NET data preprocessing pipeline
        var pipeline = _mlContext.Transforms.Text.FeaturizeText(&quot;Features_Context&quot;, nameof(LanguageData.Context))
            .Append(_mlContext.Transforms.Text.FeaturizeText(&quot;Features_Question&quot;, nameof(LanguageData.Question)))
            .Append(_mlContext.Transforms.Concatenate(&quot;Features&quot;, &quot;Features_Context&quot;, &quot;Features_Question&quot;))
            .Append(_mlContext.Regression.Trainers.LbfgsPoissonRegression(labelColumnName: nameof(LanguageData.AnswerIndex)));

        // Train the model
        var model = pipeline.Fit(dataView);

        // Save the model for future predictions
        _mlContext.Model.Save(model, dataView.Schema, &quot;model.zip&quot;);

        Console.WriteLine(&quot;Model trained and saved successfully.&quot;);
    }

    public static void ValidateModel()
    {
        Console.WriteLine(&quot;Model validation started...&quot;);

        // Load the trained model
        ITransformer model;
        using (var stream = new FileStream(&quot;Model.zip&quot;, FileMode.Open, FileAccess.Read, FileShare.Read))
        {
            model = _mlContext.Model.Load(stream, out var modelSchema);
        }

        // Sample input for prediction
        var sampleData = new DataFrame(new List&lt;DataFrameColumn&gt;
        {
            new StringDataFrameColumn(&quot;ColumnToPredict&quot;, new[] { &quot;Sample Prediction&quot; }),
            new StringDataFrameColumn(&quot;Context&quot;, new[] { &quot;&lt;html&gt;&lt;body&gt;&lt;p&gt;This is a sample HTML content.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&quot; }),
            new StringDataFrameColumn(&quot;Question&quot;, new[] { &quot;Sample Question&quot; })
        });

        // Transform the sample data
        var transformedData = model.Transform(sampleData);

        // Extract predictions from transformed data
        var predictionColumn = transformedData.GetColumn&lt;string&gt;(&quot;ColumnToPredict&quot;);

        // Retrieve the prediction (assuming single prediction in this case)
        string prediction = predictionColumn.FirstOrDefault();

        Console.WriteLine($&quot;Predicted Answer: {prediction}&quot;);

        Console.WriteLine(&quot;Model validation completed.&quot;);
    }

    public static void AnswerUserQuestion()
    {
        while (true)
        {
            Console.WriteLine();
            Console.WriteLine(&quot;Ask a question (or type 'exit' to quit):&quot;);
            string question = Console.ReadLine();

            if (question.ToLower() == &quot;exit&quot;)
                break;

            // Load the trained model for answering questions
            ITransformer model;
            using (var stream = new FileStream(&quot;Model.zip&quot;, FileMode.Open, FileAccess.Read, FileShare.Read))
            {
                model = _mlContext.Model.Load(stream, out var modelSchema);
            }

            var predictionEngine = _mlContext.Model.CreatePredictionEngine&lt;LanguageData, LanguagePrediction&gt;(model);
            // Use ML.NET model to predict sentiment
            var prediction = predictionEngine.Predict(new LanguageData { Question = question });

            var response = prediction.PredictedLabel;

            Console.WriteLine($&quot;Response: {response}&quot;);

        }
    }
    // Define the data schema
    public class LanguageData
    {
        [LoadColumn(0)]
        public string ColumnToPredict { get; set; }

        [LoadColumn(1)]
        public string Context { get; set; }

        [LoadColumn(2)]
        public string Question { get; set; }

        [LoadColumn(3)]
        public float AnswerIndex { get; set; }
    }

    // Prediction output class
    public class LanguagePrediction
    {
        [ColumnName(&quot;PredictedLabel&quot;)]
        public float PredictedLabel { get; set; }
    }

    // Helper class for HTML preprocessing
    public static class HtmlHelper
    {
        public static string CleanHtml(string html)
        {
            // Implement HTML cleaning logic here
            var doc = new HtmlDocument();
            doc.LoadHtml(html);

            // Example: Extracting text from paragraphs
            var text = string.Join(&quot; &quot;, doc.DocumentNode.SelectNodes(&quot;//p&quot;)
                                     .Select(p =&gt; p.InnerText.Trim()));

            return text;
        }
    }
}
</code></pre>
<p>Error:</p>
<blockquote>
<p>System.Reflection.TargetInvocationException   HResult=0x80131604<br />
Message=Exception has been thrown by the target of an invocation.<br />
Source=System.Private.CoreLib   StackTrace:    at
System.Reflection.MethodBaseInvoker.InvokeDirectByRefWithFewArgs(Object
obj, Span<code>1 copyOfArgs, BindingFlags invokeAttr)    at System.Reflection.MethodBaseInvoker.InvokeWithFewArgs(Object obj, BindingFlags invokeAttr, Binder binder, Object[] parameters, CultureInfo culture)    at System.Reflection.MethodBase.Invoke(Object obj, Object[] parameters)    at Microsoft.ML.Internal.Utilities.Utils.MarshalInvoke[TArg1,TArg2,TResult](FuncStaticMethodInfo3</code>3
func, Type genArg1, Type genArg2, Type genArg3, TArg1 arg1, TArg2
arg2)    at Microsoft.ML.ApiUtils.GeneratePeek[TOwn,TRow](Column
column)    at
Microsoft.ML.Data.DataViewConstructionUtils.InputRow<code>1.MakePeeks(InternalSchemaDefinition schemaDef)    at Microsoft.ML.Data.DataViewConstructionUtils.InputRow</code>1..ctor(IHostEnvironment
env, InternalSchemaDefinition schemaDef)    at
Microsoft.ML.Data.DataViewConstructionUtils.CreateInputRow[TRow](IHostEnvironment
env, SchemaDefinition schemaDefinition)    at
Microsoft.ML.PredictionEngineBase<code>2..ctor(IHostEnvironment env, ITransformer transformer, Boolean ignoreMissingColumns, SchemaDefinition inputSchemaDefinition, SchemaDefinition outputSchemaDefinition, Boolean ownsTransformer)    at Microsoft.ML.PredictionEngine</code>2..ctor(IHostEnvironment env,
ITransformer transformer, Boolean ignoreMissingColumns,
SchemaDefinition inputSchemaDefinition, SchemaDefinition
outputSchemaDefinition, Boolean ownsTransformer)    at
Microsoft.ML.PredictionEngineExtensions.CreatePredictionEngine[TSrc,TDst](ITransformer
transformer, IHostEnvironment env, Boolean ignoreMissingColumns,
SchemaDefinition inputSchemaDefinition, SchemaDefinition
outputSchemaDefinition, Boolean ownsTransformer)    at
Microsoft.ML.ModelOperationsCatalog.CreatePredictionEngine[TSrc,TDst](ITransformer
transformer, Boolean ignoreMissingColumns, SchemaDefinition
inputSchemaDefinition, SchemaDefinition outputSchemaDefinition)    at
Program.AnswerUserQuestion() in
C:\repo\Blazor.Tools\NaturalLanguageQA\NaturalLanguageQA\Program.cs:line
141    at Program.Main(String[] args) in
C:\repo\Blazor.Tools\NaturalLanguageQA\NaturalLanguageQA\Program.cs:line
53</p>
<p>This exception was originally thrown at this call stack:
[External Code]</p>
<p>Inner Exception 1: PlatformNotSupportedException: Dynamic code
generation is not supported on this platform.</p>
</blockquote>
<p>Error Line of code:</p>
<pre><code>var predictionEngine = _mlContext.Model.CreatePredictionEngine&lt;LanguageData, LanguagePrediction&gt;(model);
</code></pre>
","c#, nlp, .net-8.0, ml.net",
How to save python loop output to an excel file,"<p>I have an excel file (input.xlsx) that contains two columns (id and url).</p>
<p>I performed webscrapping on all the url and performed text analysis on the texts.</p>
<p>I have functions that calculates the positive score, negative score, polarity etc.</p>
<p><strong>I want to create an output file(output.xlsx) that will contain all the above results but my script is printing same output in all the rows but it is printing the correct output inside the function.</strong></p>
<p>Example:</p>
<p>Columns: Id, url, positive score, negative score, polarity etc</p>
<p>Rows: Rows will contain the output of each function.</p>
<p><strong>Expected Output:</strong>
Positive Score(column) : 23, 70, 43, 35 (rows)</p>
<p><strong>Actual Output:</strong>
Positive Score(column) : 35, 35, 35, 35 (rows)</p>
<p><strong>MY FUNCTIONS:</strong></p>
<pre><code>import pathlib
from pathlib import Path

import pandas as pd
import requests
from bs4 import BeautifulSoup

import glob
import codecs
import os



data_ex = pd.read_excel(&quot;input.xlsx&quot;)
urls = data_ex.URL
url_ids = data_ex.URL_ID

folder = &quot;files_folder2&quot;
Path(folder).mkdir(parents=True, exist_ok=True)

#WRITING THE EXTRACTED CONTENTS TO TEXT FILES
for url, url_id in zip(urls, url_ids):
    res = requests.get(url, timeout=60)

    page = res.text
    soup = BeautifulSoup(page, &quot;html.parser&quot;)
    article_title = soup.find_all(name=&quot;h1&quot;, class_=&quot;entry-title&quot;)

    article_texts = []
    article_details = []

    for details in article_title:
        text = details.getText()
        article_writeup = soup.find(class_=&quot;td-post-content tagdiv-type&quot;).getText()

        file_path = Path(folder, f&quot;{url_id}.txt&quot;)
        with pathlib.Path.open(file_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
            f.write(f&quot;{text}\n {article_writeup}&quot;)



#POSITIVE WORDS
def positive_score(content):
    #tokens = tokenz(text)
    pos_score = 0
    for token in content:
        if token in filtered_positive_dictionary:
            pos_score += 1
    return pos_score

positive_scores = []

# Cleaned texts
os.getcwd()
new_texts_folder = os.path.join(os.getcwd(), 'new_texts')

for root, folders, files in os.walk(new_texts_folder):
    for file in files:
        path = os.path.join(root, file)
        with codecs.open(path, encoding='utf-8', errors='ignore') as info:
            new_content = eval(info.read())  # Convert string to list
            pos_score = positive_score(new_content)
            positive_scores.append(pos_score)


#NEGATIVE WORDS
def negative_score(content):
    neg_score = 0
    for token in content:
        if token in filtered_negative_dictionary:
            neg_score -= 1
    return neg_score * -1
#negative_result = negative_score(new_content)
negative_scores = []

# Cleaned texts
os.getcwd()
new_texts_folder = os.path.join(os.getcwd(), 'new_texts')

# Loop through cleaned texts and filter out stop words
folder_name = &quot;negative&quot;
Path(folder_name).mkdir(parents=True, exist_ok=True)

for root, folders, files in os.walk(new_texts_folder):
    for file in files:
        path = os.path.join(root, file)
        with codecs.open(path, encoding='utf-8', errors='ignore') as info:
            new_content = eval(info.read())  # Convert string to list
            neg_score = negative_score(new_content)
            negative_scores.append(neg_score)



#POLARITY SCORE
import numpy as np

def polarity_score(positive_scores,negative_scores):
    polar_score = (positive_scores - negative_scores)/((positive_scores + negative_scores)+0.000001)
    return polar_score

</code></pre>
<p><strong>The above codes prints the correct outputs.</strong></p>
<p><strong>My Excel Function:</strong></p>
<pre><code>data_collection = {
    'URL_ID': url_ids, #(this is the ID from the excel file where the website is)
    'URL': urls, #(this is the website url from the excel file)
     'POSITIVE SCORE': positive_score, #(this is the positive score gotten from each of the urls)
'NEGATIVE SCORE': negative_score #(this is the negative score gotten from each of the urls)
}
excel_data_df = pd.DataFrame(data_collection)
excel_data_df.to_excel(&quot;Outputput.xlsx&quot;, index = False)
</code></pre>
<p><strong>I want the excel file to contain the URL_ID and URL associated to each POSITIVE SCORE.</strong></p>
<p><strong>i.e: url:<a href=""http://www.testing.com"" rel=""nofollow noreferrer"">www.testing.com</a>, url_id: 6474, positive_score: 60</strong></p>
","python, excel, nlp",
"Trying to run GermanSentiment in Python on 10k to 30k texts, keeps crashing? Possibly too large dataset?","<p>I want to do sentiment analysis on 2 datasets of tweets, one with 9k strings and one with 30k strings. I have imported GermanSentiment and it ran just fine with the demo code from GitHub, but when I applied it to the 30k or 9k set, it caused my CPU to spike so fast that I had to put my laptop into standby and then kill the command line to regain control.
Believing it to be an issue of computing power, I tried it on my tower PC with more CPU+GPU power, it also causes CPU to spike to 100 % and gives me
<code>Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\torch\csrc\utils\tensor_numpy.cpp:84.)</code> despite NumPy being installed.</p>
<p>Is there some limit to GermanSentiment? Am I doing something wrong?</p>
<p>Added my code snippets as best I could. On my PC, I also get:</p>
<pre><code>NumPy 2.0.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11&gt;=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy&lt;2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.
</code></pre>
<p>Code:</p>
<pre><code>from germansentiment import SentimentModel
import json
from tqdm import tqdm
#import csv
import re
import argparse
import xmltodict
import mdb_reader
import csvReader
import jsonReader
from pathlib import Path

def check_sentiment(tweet_set_1, tweet_set_2, sm_model):
    model = sm_model

    for tweet_set in tqdm([tweet_set_2]):
        texts = [tweet_set[tweet][&quot;text&quot;] for tweet in tweet_set]
    #breakpoint()
        
        result = model.predict_sentiment(tqdm(texts))
        print(result)

### Sentiment Analyse ###

    model = SentimentModel()

    #texts = [
    #&quot;Mit keinem guten Ergebniss&quot;,&quot;Das ist gar nicht mal so gut&quot;,
    #&quot;Total awesome!&quot;,&quot;nicht so schlecht wie erwartet&quot;,
    #&quot;Der Test verlief positiv.&quot;,&quot;Sie fährt ein grünes Auto.&quot;]
       
    #result = model.predict_sentiment(texts)
    #print(result)

    check_sentiment(tweet_poli, tweet_other, model)

</code></pre>
","python, nlp, sentiment-analysis, bert-language-model",
Exporting text sequences with labels from Inception Annotation engine,"<p>I am using INCEpTION (<a href=""https://inception-project.github.io/"" rel=""nofollow noreferrer"">https://inception-project.github.io/</a>) to annotate my legal documents. Instead of any of the export formats provided in Inception, is there a way I could export the raw text sequences I labelled, along with their label names?</p>
<p>The use case I have here is in creating a dataset where I annotate the sentences in my text as text sequences (sentence boundary detection is not well-solved for the legal domain), along with a custom tag for each sentence.</p>
<p>An ideal export format would be, on each line of a txt file:
text  label</p>
<p>The data format I am hoping to achieve is similar to what you see here: <a href=""https://github.com/Law-AI/semantic-segmentation/blob/master/data/text/1953_L_1.txt"" rel=""nofollow noreferrer"">https://github.com/Law-AI/semantic-segmentation/blob/master/data/text/1953_L_1.txt</a></p>
","nlp, annotations, inception",
Why charater based LSTM are taking more time than word based LSTM while next word prediction,"<p>I wanted to train LSTM Model for Next Word Prediction using word-based and character based. I used similar data processing technique for both character-based and word-based.</p>
<p>For Character,</p>
<pre><code>class TextDataset(Dataset):
    def __init__(self, text, sequence_length):
        self.text = text
        self.sequence_length = sequence_length
        self.text_length = len(text) - sequence_length

    def __len__(self):
        return self.text_length

    def __getitem__(self, idx):
        seq = self.text[idx: idx + self.sequence_length]
        next_char = self.text[idx + self.sequence_length]
        return torch.tensor(seq['tokens'], dtype=torch.long), torch.tensor(next_char['tokens'], dtype=torch.long)

sequence_length = 20
train_data = TextDataset(tokenized_dataset['train'], sequence_length)
valid_data = TextDataset(tokenized_dataset['valid'], sequence_length)
test_data = TextDataset(tokenized_dataset['test'], sequence_length)
train_dataloader = DataLoader(train_data, batch_size=1024, shuffle=False)
valid_dataloader = DataLoader(valid_data, batch_size=1024, shuffle=False)
test_dataloader = DataLoader(test_data, batch_size=1024, shuffle=False)
</code></pre>
<p>For Word-based, I took word instead of Character.</p>
<p>Here is LSTM model,</p>
<pre><code>class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate):
        super(LSTMModel, self).__init__()
        self.num_layers = n_layers
        self.hidden_dim = hidden_dim
        self.embedding_dim = embedding_dim
        self.dropout = nn.Dropout(dropout_rate)
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout_rate, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x, hidden):
        x = self.embedding(x)
        # x = x.view(self.sequence_length, -1, self.hidden_dim)
        # print(x.shape)
        out, hidden = self.lstm(x, hidden)
        out = self.dropout(out)
        out = self.fc(out[:,-1])
        return out, hidden

    # def init_hidden(self, batch_size):
    #     return (torch.zeros(1, batch_size, self.hidden_dim),
    #             torch.zeros(1, batch_size, self.hidden_dim))

    def init_hidden(self, batch_size, device):
        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)
        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)
        return hidden, cell

    def detach_hidden(self, hidden):
        hidden, cell = hidden
        hidden = hidden.detach()
        cell = cell.detach()
        return hidden, cell
</code></pre>
<p>And Training Step,</p>
<pre><code>for epoch in range(n_epochs):

    for inputs, targets in tqdm(train_dataloader):
        hidden = model.init_hidden(inputs.shape[0], device)
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        output, hidden = model(inputs, hidden)
        loss = criterion(output, targets)
        loss.backward()
        optimizer.step()
        model.detach_hidden(hidden)

    print(f&quot;Epoch {epoch+1}/{n_epochs}, Loss: {loss.item()}&quot;)
</code></pre>
<p><strong>Here, Character-based training takes much longer time than Word-based training. Why?</strong></p>
","python, pytorch, nlp",
"Calling Asr model in BHasini throws - {&quot;code&quot;:&quot;something went wrong&quot;,&quot;message&quot;:null,&quot;timestamp&quot;:&quot;2024-07-03T06:08:52.407+00:00&quot;}","<p>I am trying to call the asr model of bhasini  api to get transcription of my audio.</p>
<p>I tried the following code.</p>
<pre><code>import requests
import json

url = &quot;https://meity-auth.ulcacontrib.org/ulca/apis/v0/model/getModelsPipeline&quot;

payload = {
  &quot;pipelineTasks&quot;: [
    {
      &quot;taskType&quot;: &quot;asr&quot;,
      &quot;config&quot;: {
        &quot;language&quot;: {
          &quot;sourceLanguage&quot;: &quot;kn&quot;
        }
      }
    }
  ],
  &quot;pipelineRequestConfig&quot;: {
            &quot;pipelineId&quot; : &quot;64392f96daac500b55c543cd&quot;
        }
  }
headers = {
        &quot;Content-Type&quot;: &quot;application/json&quot;,
        &quot;userID&quot;: &quot;&quot;,
        &quot;ulcaApiKey&quot;: &quot;1&quot;
    }

response = requests.post('https://meity-auth.ulcacontrib.org/ulca/apis/v0/model/getModelsPipeline', json=payload, headers=headers)
if response.status_code == 200:
    response_data = response.json()

    service_id = response_data[&quot;pipelineResponseConfig&quot;][0][&quot;config&quot;][0][&quot;serviceId&quot;]
    print(service_id)

payload = {
  &quot;pipelineTasks&quot; : [
    {
        &quot;taskType&quot; : &quot;asr&quot;,
        &quot;language&quot;: {
                    &quot;sourceLanguage&quot;: &quot;hi&quot;
                },
        &quot;serviceId&quot;: service_id,
        &quot;audioFormat&quot;: &quot;flec&quot;,
        &quot;samplingRate&quot;: 16000
    }
],
  &quot;inputData&quot;: {
    &quot;audio&quot;: [
      {
        &quot;audioContent&quot;:&quot;ZkxhQ4AAACISABIAABnpAClACsRBcAADIKeTSPVYJt2ij/I04MboBPQk//hZDABoTv2jnP3pA/3LkP3zRf12d/16m/1inf0vDue2C+Us+mdYWxe+CSdyNfeq6WaNLipP61DryhyMTt4EMcTJE8lmn8jdmZG/LT87FbE4Y6r0G10enfExlS7mt0D3U/dDZt/vcIP+Cyj1Icx0dGAKOXFWhZH9tTMqG/jWZE8/dKAcNNSoSTDq2LkHRMnZZ+zC8Wnwxdiv0qzVdxvuxZGu2yOGtgWHTJIGyik8c6+gC8nbp6rFayA+eEVOZxWfIzwudaXDJdhmnnH9Rr/e9wym813+0c5TpbcKHh42ROTqzT1TRSS6kcgavT/xKq9Q2N3xWDu0dULrmjGUp4rTj1pvqLtTxMY6Ut8tkJeuRj4zsMmJ7H1ld60jQqta8+PRh/ESK/wTjXBo1o3dTjLLjEpRc7E1AMSxmaz6z89vK5JEGZZL4rCLLqEB8JwmgRPT+v6dmVRspAdDWAO9ECkJIG4ewD6ygDITAqFKARicAUfoFfMwJXcguiNBA/eApbEB7e4PLJQImXgtqXBaRCAJREDwLoL7lwRDggNgrAL/GBrmUE8vYM2RQecNgozxApwGBQzcDLToQ7BwqSUg9gzApgKAefkClF4HBMQjLlhAy9BCf2DEnkBtZYIp0wb2Cghy3CKG6D0msCz7oHw9QQmng1ZBAMDOB8iMDPQIXnzQLEtgnRhB/P2AhOUBxcomZNwQ2gg/YTGIv6CYvMZlU4xUJQcjDgGaXAfLqB1F4n5k=&quot;
      }
    ]
  }
}




response = requests.request(&quot;POST&quot;, url, headers=headers, json=payload)

print(response.text)

</code></pre>
<h4>Output</h4>
<p>ai4bharat/conformer-multilingual-dravidian-gpu--t4
{&quot;code&quot;:&quot;something went wrong&quot;,&quot;message&quot;:null,&quot;timestamp&quot;:&quot;2024-07-03T06:08:52.407+00:00&quot;}</p>
","python, nlp, speech-to-text",
Understanding and improving coherence values using Mallet,"<p>I am attempting to run an LDA topic model using Mallet. My corpus consists of user comments from news websites. It's a relatively small corpus with approx. 614k words.</p>
<p>The first approach I took was to split these words across just a dozen, very large documents. I got results but they were not very good, which retrospectively I believe was to be expected. I have now increased the number of documents by chunking the corpus in 614 documents equally with 1k words each. Using this approach I find it much easier to identify topics (and more topics) via the top words, they just seem to be topically more in line. On the other hand, however, I now get much &quot;worse&quot; coherence and exclusivity scores. Using just a dozen documents, coherence was somewhere between -12 (best) and -200 (worst). With the hundreds of documents now, coherence value spread from -120 (best) to -1000 (worst). I used the same parameters for both approaches (Gibbs Sampling of 1000 and optimization intervall set at 10).</p>
<p>I guess my questions are:</p>
<ol>
<li>Is it correct to assume that the number of documents has an impact on the average coherence value or am I missing something else?</li>
<li>Is there a rule of thumb of some sort that would help me define what a good coherence score is? I am guessing there isn't but just wanted to check if anyone has an idea.</li>
</ol>
<p>Many thanks in advance!</p>
","nlp, lda, topic-modeling, mallet, topicmodels",
"NLTagger.requestAssets(for: language, tagScheme: .lemma) never returns","<p>I am new to NLP. I need to Lemmatise a sentence. But the request for assets never returns. Here is the code:</p>
<pre><code>if !NLTagger.availableTagSchemes(for: .word, language: .english).contains(.lemma) {
    NLTagger.requestAssets(for: .english, tagScheme: .lemma) { result, error in
        print(&quot;Never reaches here&quot;)
    }
}

</code></pre>
<p>I tried this on simulator. It never returns any response. Do I need to enable any settings to make it start working?</p>
","ios, swift, nlp",
Named Entity Recognition using TFLite on Android,"<p>I have a named entity recognition TensorFlow Lite model that I trained in Python and that I would like to run on an Android. To make predictions, the model takes in a dictionary of two tensors, namely <code>input_ids</code> and <code>attention_mask</code> like so:</p>
<pre><code>Inputs: {'input_ids': &lt;tf.Tensor: shape=(1, 3), dtype=int32, numpy=
         array([[101, 2054, 102]])&gt;, 
         'attention_mask': &lt;tf.Tensor: shape=(1, 3), dtype=int32, numpy=
         array([[1, 1, 1,]])&gt;}
</code></pre>
<p>For context, the model was trained on texts of varying lengths, so the model's default input shape for both <code>input_ids</code> and <code>attention_mask</code> is <code>(1, 1)</code>.</p>
<p>Here's the code I'm using in Kotlin:</p>
<pre><code>// Load model
val nerHelper = NERHelper(getApplication())
val tflite = Interpreter(nerHelper.loadModelFile())

// Set example inputShape
val inputShape = intArrayOf(1, 3)

// Create input_ids and attention_mask tensors
val inputIdsBuffer = TensorBuffer.createFixedSize(inputShape, org.tensorflow.lite.DataType.FLOAT32)
val attentionMaskBuffer = TensorBuffer.createFixedSize(inputShape, org.tensorflow.lite.DataType.FLOAT32)

// Example input_ids
inputIdsBuffer.loadArray(intArrayOf(101, 2054, 102))
// Example attention_mask
attentionMaskBuffer.loadArray(intArrayOf(1, 1, 1))

// Create output buffer
val outputShape = intArrayOf(1, 3, 4)
val outputBuffer = TensorBuffer.createFixedSize(outputShape, org.tensorflow.lite.DataType.FLOAT32)

// Run the model
try {
    val inputs: Array&lt;Any&gt; = arrayOf(inputIdsBuffer.buffer, attentionMaskBuffer.buffer)
    val outputs: MutableMap&lt;Int, Any&gt; = mutableMapOf(0 to outputBuffer.buffer.rewind())

    // Resize inputs for input_ids and attention_mask
    tflite.resizeInput(0, intArrayOf(1, 3))
    tflite.resizeInput(1, intArrayOf(1, 3))

    try {
        tflite.runForMultipleInputsOutputs(inputs, outputs)
    } catch (e: Exception) {
        Log.e(&quot;ModelInference&quot;, &quot;Error during model run&quot;, e)
    }

    tflite.runForMultipleInputsOutputs(inputs, outputs)
    Log.d(&quot;ModelInference&quot;, &quot;Model run successfully&quot;)

    val output = outputBuffer.floatArray
    output.forEach { value -&gt; Log.d(&quot;ModelInference&quot;, &quot;Output: $value&quot;) }
} catch (e: Exception) {
    Log.e(&quot;ModelInference&quot;, &quot;Error during model inference&quot;, e)
}
</code></pre>
<p>When I run this in Kotlin, an error is thrown at</p>
<pre><code>try {
        tflite.runForMultipleInputsOutputs(inputs, outputs)
    } catch (e: Exception) {
        Log.e(&quot;ModelInference&quot;, &quot;Error during model run&quot;, e)
    }
</code></pre>
<p>saying:</p>
<pre><code>java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (serving_default_attention_mask:0) with 4 bytes from a Java Buffer with 3 bytes.
</code></pre>
<p>I understand that this error originates from there existing a discrepancy in the size between my inputs and what the model is expecting, but I'm struggling to figure out what exactly is causing this issue.</p>
","kotlin, nlp, tensorflow-lite, named-entity-recognition",
GliNER finetuning - no validation loss is logging,"<p>I am trying to fine-tune using this notebook: <a href=""https://github.com/urchade/GLiNER/blob/main/examples/finetune.ipynb"" rel=""nofollow noreferrer"">GLiNER/examples/finetune.ipynb at main · urchade/GLiNER (github.com)</a></p>
<p>However, the logs only show 'loss' , which I assume is the training data set loss, but there is no recorded validation set loss</p>
<pre><code>training_args = TrainingArguments(
    output_dir=&quot;models&quot;,
    learning_rate=5e-6,
    weight_decay=0.01,
    others_lr=1e-5,
    others_weight_decay=0.01,
    lr_scheduler_type='linear', #&quot;cosine&quot;,
    warmup_ratio=0.1,
    per_device_train_batch_size=5,
    per_device_eval_batch_size=5,
    num_train_epochs=2,
    eval_strategy=&quot;epoch&quot;,
    save_steps = 100,
    save_total_limit=10,
    dataloader_num_workers = 1,
    use_cpu = True,
    disable_tqdm=False,
    logging_dir='logs',
    logging_steps=10,
    eval_steps=10,
    do_train=True,
    do_eval=True,
    seed=7,
    )

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=model.data_processor.transformer_tokenizer,
    data_collator=data_collator)
</code></pre>
<p>Where is validation loss computed?? I want to log both the training and validation sets' loss</p>
","python, pytorch, nlp, named-entity-recognition, huggingface-trainer",
UndefinedFile: could not open extension control file &quot;/usr/share/postgresql/14/extension/vector.control&quot;: No such file or directory,"<p>I'm trying to run postgres along with pgvector but getting this error: <code>UndefinedFile: could not open extension control file &quot;/usr/share/postgresql/14/extension/vector.control&quot;: No such file or directory</code> on ubuntu 24.04</p>
<p>Here's what my code block looks like in jupyter notebook:</p>
<pre><code>async def main():
    conn = await psycopg.AsyncConnection.connect(dbname='pgvector_example', autocommit=True)
    await create_schema(conn)
    await insert_data(conn)

    # perform queries in parallel
    results = await asyncio.gather(semantic_search(conn, query), keyword_search(conn, query))
    results = rerank(query, results)
    print(results)

#use this in Jupyter notebook
await main()
</code></pre>
<p>`</p>
<p>I've already installed pgvector and postgres as mentioned <a href=""https://github.com/pgvector/pgvector"" rel=""nofollow noreferrer"">here in docs</a>,and created <code>pgvector_example</code> db in postgres.</p>
<p>On running <code>CREATE EXTENSION IF NOT EXISTS pgvector;</code> in postgres, I got the same error
<code>ERROR:  could not open extension control file &quot;/usr/share/postgresql/14/extension/vector.control&quot;: No such file or directory</code><br />
<strong>Edit</strong>
On running <code>SELECT * FROM pg_available_extensions;</code> it outputs list of extensions and does NOT contain <strong>pg_vector</strong></p>
","postgresql, vector, nlp, pgvector",
How can I make my Hugging Face fine-tuned model&#39;s config.json file reference a specific revision/commit from the original pretrained model?,"<p>I uploaded this model: <a href=""https://huggingface.co/pamessina/CXRFE"" rel=""nofollow noreferrer"">https://huggingface.co/pamessina/CXRFE</a>, which is a fine-tuned version of this model: <a href=""https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized</a></p>
<p>Unfortunately, CXR-BERT-specialized has this issue: <a href=""https://github.com/huggingface/transformers/issues/30412"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/30412</a></p>
<p>I fixed the issue with this pull request: <a href=""https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized/discussions/5"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized/discussions/5</a></p>
<p>However, when I save my fine-tuned model, the config.json file doesn't point to that specific pull request, pointing instead to the main branch by default, but the main branch of CXR-BERT-specialized has the aforementioned issue. As a consequence, when I try to use my model, it triggers the bug from the main branch of the underlying model, which shouldn't happen it were using the version from my pull request.</p>
<p>I've tried explicitly enforcing the revision I want like this:</p>
<pre><code>model = AutoModel.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized', revision=&quot;6cfc310817fb7d86762d888ced1e3709c57ac578&quot;, trust_remote_code=True)
...
model.save_pretrained(&quot;/home/pamessina/huggingface_models/CXRFE/&quot;)
</code></pre>
<p>But the config file that gets saved doesn't reference the desired revision:</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized&quot;,
  &quot;architectures&quot;: [
    &quot;CXRBertModel&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.25,
  &quot;auto_map&quot;: {
    &quot;AutoConfig&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized--configuration_cxrbert.CXRBertConfig&quot;,
    &quot;AutoModel&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized--modeling_cxrbert.CXRBertModel&quot;
  },
  &quot;classifier_dropout&quot;: null,
  &quot;gradient_checkpointing&quot;: false,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.25,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;cxr-bert&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;pad_token_id&quot;: 0,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;projection_size&quot;: 128,
  &quot;torch_dtype&quot;: &quot;float32&quot;,
  &quot;transformers_version&quot;: &quot;4.41.2&quot;,
  &quot;type_vocab_size&quot;: 2,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 30522
}
</code></pre>
<p>And when I try to use my fine-tuned model from Hugging Face on Google Colab, I get this error:</p>
<p><a href=""https://i.sstatic.net/rUuXJTyk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rUuXJTyk.png"" alt=""enter image description here"" /></a></p>
<p>As  you can see, it's invoking the version associated with the commit id &quot;b59c09e51ab2410b24f4be214bbb49043fe63fc2&quot;, when instead it should be using the commit id &quot;6cfc310817fb7d86762d888ced1e3709c57ac578&quot; with my pull request that fixes the bug.</p>
<p>What can I do?</p>
<p>Thanks in advance.</p>
","nlp, huggingface-transformers, bert-language-model, huggingface, huggingface-hub",
How do we add/modify the normalizer in a pretrained Huggingface tokenizer?,"<p>Given a Huggingface tokenizer that already have a normalizer, e.g. <code>&quot;mistralai/Mistral-7B-v0.1&quot;</code>, we can do this to modify the normalizer</p>
<pre><code>import json

from transformers import AutoTokenizer
from tokenizers.normalizers import Sequence, Replace, Prepend

tokenizer_name = &quot;mistralai/Mistral-7B-v0.1&quot;
old_tok = AutoTokenizer.from_pretrained(tokenizer_name)

assert old_tok.backend_tokenizer.normalizer != None

new_normalizer = Sequence(
    [Prepend('▁'), Replace('▁', ' '), Replace(&quot;foo&quot;, &quot;bar&quot;), Replace('&lt;br&gt;', '\n')]
)

old_tok.backend_tokenizer.normalizer = new_normalizer
new_tokenizdr_name = f&quot;new_tokenizer-{tokenizer_name}&quot;
old_tok.save_pretrained(new_tokenizdr_name)


old_tok = AutoTokenizer.from_pretrained(tokenizer_name)
new_tok = AutoTokenizer.from_pretrained(new_tokenizdr_name)
</code></pre>
<p>[out]:</p>
<pre><code>&gt;&gt;&gt; print(' '.join(old_tok.batch_decode(old_tok(&quot;I foo you&lt;br&gt;hello world&quot;)['input_ids'])))
&lt;s&gt; I foo you &lt; br &gt; hello world

&gt;&gt;&gt; print(' '.join(new_tok.batch_decode(new_tok(&quot;I foo you&lt;br&gt;hello world&quot;)['input_ids'])))
&lt;s&gt;  I  bar  you 
 hello  world
</code></pre>
<p>But when this hot-plug normalizer modification don't always work, if we change it to <code>&quot;mistralai/Mistral-7B-v0.3&quot;</code>, it fails to work:</p>
<pre><code>import json

from transformers import AutoTokenizer
from tokenizers.normalizers import Sequence, Replace, Prepend

tokenizer_name = &quot;mistralai/Mistral-7B-v0.3&quot;
old_tok = AutoTokenizer.from_pretrained(tokenizer_name)

new_normalizer = Sequence(
    [Prepend('▁'), Replace('▁', ' '), Replace(&quot;foo&quot;, &quot;bar&quot;), Replace('&lt;br&gt;', '\n')]
)

old_tok.backend_tokenizer.normalizer = new_normalizer
new_tokenizdr_name = f&quot;new_tokenizer-{tokenizer_name}&quot;
old_tok.save_pretrained(new_tokenizdr_name)


old_tok = AutoTokenizer.from_pretrained(tokenizer_name)
new_tok = AutoTokenizer.from_pretrained(new_tokenizdr_name)

print(' '.join(old_tok.batch_decode(old_tok(&quot;I foo you&lt;br&gt;hello world&quot;)['input_ids'])))
print(' '.join(new_tok.batch_decode(new_tok(&quot;I foo you&lt;br&gt;hello world&quot;)['input_ids'])))
</code></pre>
<p>[out]:</p>
<pre><code>&lt;s&gt; I foo you &lt; br &gt; hello world
&lt;s&gt; I foo you &lt; br &gt; hello world
</code></pre>
<h3>How do we add/modify the normalizer in a pretrained Huggingface tokenizer?</h3>
<p>Can any normalizer from a pretrained tokenizer be modified or just specific ones?</p>
<p>If the latter, why and how do we know if a pretrained tokenizer's normalizer can be extended or modified?</p>
","python, nlp, large-language-model, huggingface-tokenizers","<p>This looks like a bug.  The v0.1 tokenizer has a normalizer by default, which can be seen by looking at the <code>mistral-78-v0.1/tokenizer.json</code> file:</p>
<pre><code>{
 ...
  &quot;normalizer&quot;: {
    &quot;type&quot;: &quot;Sequence&quot;,
    &quot;normalizers&quot;: [
      {
        &quot;type&quot;: &quot;Prepend&quot;,
        &quot;prepend&quot;: &quot;▁&quot;
      },
      {
        &quot;type&quot;: &quot;Replace&quot;,
        &quot;pattern&quot;: {
          &quot;String&quot;: &quot; &quot;
        },
        &quot;content&quot;: &quot;▁&quot;
      }
    ]
  },
...
}
</code></pre>
<p>After modifying the <code>.backend_tokenizer.normalizer</code> object, the modification are saved to the <code>tokenizer.json</code> file.</p>
<p>In the v0.3 version, the <code>mistral-78-v0.1/tokenizer.json</code> file has no value for the normalizer:</p>
<pre><code>{
...
  &quot;normalizer&quot;: null,
...
}
</code></pre>
<p>Modifying the <code>normalizer</code> and saving the model <em>does</em> write the changes to the JSON file, but it is not getting picked up on reload using <code>AutoTokenizer.from_pretrained</code>.  I am not sure why, but it is entirely possible the <code>tokenizer.model</code> file indicates no normalizer is the default and it simply does not load it.</p>
<p>However, you can get the tokenizer to load correctly - with the custom normalizer - by instantiating the matched tokenizer class explicitly and passing in the <code>tokenizer.model</code> and <code>tokenizer.json</code> paths along with the values from the <code>tokenizer_config.json</code> file.  In this case it is the <code>LlamaTokenizerFast</code> class.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, LlamaTokenizerFast, AddedToken
from tokenizers.normalizers import Sequence, Replace, Prepend

### load, modify, and save
tok = AutoTokenizer.from_pretrained(&quot;mistralai/Mistral-7B-v0.3&quot;)
tok.backend_tokenizer.normalizer = Sequence([
    Prepend('_'), 
    Replace('_', ' '), 
    Replace(&quot;foo&quot;, &quot;bar&quot;), 
    Replace('&lt;br&gt;', '\n')
])
tok.save_pretrained(&quot;mistral-7B-v0.3-custom/&quot;)


### read in config, construct the AddedToken objects
with open('mistral-7B-v0.3-custom/tokenizer_config.json') as fp:
    config = json.load(fp)
    config['added_tokens_decoder'] = {
        int(k): AddedToken(**v)
        for k, v in config.pop('added_tokens_decoder').items()
    }

### load from saved files
tok_custom = LlamaTokenizerFast(
    'mistral-7B-v0.3-custom/tokenizer.model', 
    'mistral-7B-v0.3-custom/tokenizer.json', 
    **config,
)

test_str = &quot;I foo you&lt;br&gt;hello world&quot;
print(' '.join(tok_custom.batch_decode(tok_custom(test_str)['input_ids'])))
# prints:
#&lt;s&gt; I bar you
# hello world
</code></pre>
<p>If you don't want to specify the tokenizer class explicitly, you can load the model the the <code>AutoTokenizer</code>, and then load it again using from the resulting class.  It is a hacky work-around.</p>
<pre><code>tok_path = &quot;path/to/mistral-7B-v0.3-custom/&quot;
with open(f'{tok_path}/tokenizer_config.json') as fp:
    config = json.load(fp)
    config['added_tokens_decoder'] = {
        int(k): AddedToken(**v)
        for k, v in config.pop('added_tokens_decoder').items()
    }

tok = AutoTokenizer.from_pretrained(tok_path).__class__(
    f'{tok_path}/tokenizer.model', 
    f'{tok_path}/tokenizer.json', 
    **config,
)
</code></pre>
"
MT-Bench evaluation of a model using pre generated model answers,"<p>I want to find MT-Bench score of an LLM (say EleutherAI/pythia-1b).I was able to run the command</p>
<blockquote>
<p>python gen_model_answer.py --model-pat EleutherAI/pythia-1b --model-id pythia-1b</p>
</blockquote>
<p>to generate answers and I could see the output in the json file &quot;data/mt_bench/model_answer/pythia-1b.jsonl&quot;.
I have downloaded pre generated model answers using the command</p>
<blockquote>
<p>python3 download_mt_bench_pregenerated.py</p>
</blockquote>
<p>How to compare &quot;pythia-1b&quot; generated answer and any pre generated answer(say llama-13b) to calculate MT-Bench score for &quot;pythia-1b&quot; model ?</p>
","python, nlp, huggingface-transformers, large-language-model, huggingface",
"Python, using pdfplumber, pdfminer packages extract text from pdf, bolded characters duplicates","<p>Goal: extract <strong>Chinese</strong> financial report text</p>
<p>Implementation: Python pdfplumber/pdfminer package to extract PDF text to txt</p>
<p>problem: for PDF <em>text in bold</em>, corresponding extracted <em>text in txt duplicates</em></p>
<p>Examples are as follows:</p>
<p>Such as the following PDF text:
<a href=""https://i.sstatic.net/uESiY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uESiY.png"" alt=""pdf text"" /></a>
Python extracts to txt as:
<a href=""https://i.sstatic.net/A1eNI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A1eNI.png"" alt=""pdfplumber result"" /></a></p>
<p>And I don't need to repeat the text, just normal text.</p>
<p>How should I do it, should I change the package or add a new function?</p>
<p>Please see the code and original pdf text below.</p>
<p>Additional: pdfplumber code:</p>
<pre><code>import pdfplumber
 
def pdf2txt(filename, delLinebreaker=True):
    pageContent = ''
    showplace = ''
    try:    
        with pdfplumber.open(  filename  ) as pdf:
            page_count = len(pdf.pages)
            for page in pdf.pages:
                if delLinebreaker==True:
                    pageContent += page.extract_text().replace('\n', &quot;&quot;)   
                else:
                    pageContent += page.extract_text()  
    except Exception as e:
        print( &quot;file: &quot;, filename, ', reason: ', repr(e) )
    return pageContent
 
pdf2txt(r&quot;report.pdf&quot;, delLinebreaker=False)
</code></pre>
<p>pdfminer code:</p>
<pre><code>from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.pdfpage import PDFPage
 
rsrcmgr = PDFResourceManager()
outfp = open(r&quot;report.txt&quot;, 'w', encoding='utf-8')
device = TextConverter(rsrcmgr, outfp)
with open(r&quot;Report.pdf&quot;, 'rb') as fp:
    interpreter = PDFPageInterpreter(rsrcmgr, device)
    for page in PDFPage.get_pages(fp):
        interpreter.process_page(page)
device.close()
outfp.close()
</code></pre>
<p>Result of pdfminer is:
<a href=""https://i.sstatic.net/AbNHh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AbNHh.png"" alt=""pdfminer result"" /></a></p>
<p>the pdf file can download here in Shenzhen Stock Exchange official website
<a href=""http://www.szse.cn/disclosure/listed/bulletinDetail/index.html?9324ce3c-6072-499d-8798-b25d641b52ec"" rel=""nofollow noreferrer"">http://www.szse.cn/disclosure/listed/bulletinDetail/index.html?9324ce3c-6072-499d-8798-b25d641b52ec</a></p>
","pdf, nlp, extract, cjk, pdfplumber",
why softmax get small gradient when the value is large in paper &#39;Attention is all you need&#39;,"<p>This is the screen of the original paper: <a href=""https://i.sstatic.net/J9thT.png"" rel=""noreferrer"">the screen of the paper</a>. I understand the meaning of the paper is that when the value of dot-product is large, the gradient of softmax will get very small.
<br/>However, I tried to calculate the gradient of softmax with the cross entropy loss and found that the gradient of softmax is not directly related to value passed to softmax. <br/>Even the single value is large, it still can get a large gradient when ather values are large. (sorry about that I don't know how to pose the calculation process here)</p>
","deep-learning, nlp, softmax, attention-model","<p>Actually the gradient of cross entropy with softmax on a one hot encoding vector is just grad -log(softmax(x)) = (1 - softmax(x)) at the index of the vector of the corresponding class. (<a href=""https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/"" rel=""noreferrer"">https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/</a>). If the value passed to the softmax is large, the softmax will produce 1 and therefore produce 0 gradient. </p>
"
Unable to upload documents in Chroma DB,"<p>While uploading document chunks in chroma db using the following code -</p>
<pre><code>with tqdm(total = len(dataset[&quot;train&quot;])) as pbar:
  for i in dataset[&quot;train&quot;]:
    try:
      doc = Document(page_content = i[&quot;some_value&quot;], metadata = {&quot;video_id&quot; : i[&quot;some_other_value&quot;]})
      doc_chunks = doc_splitter.split_documents([doc])
      vectordb = Chroma.from_documents(
          documents = doc_chunks,
          embedding = embeddings,
          persist_directory = &quot;./database&quot;
      )
      vectordb.persist()
      pbar.update(1)
    except Exception as e:
      print(f&quot;Record Failed: {e}&quot;)
</code></pre>
<p>I'm facing this error. Any help would be appreciated!!</p>
<pre><code>0%
27/1238911 [00:23&lt;269:22:52, 1.28it/s]
ERROR:chromadb.db.mixins.embeddings_queue:Exception occurred invoking consumer for subscription 79d77609a7d54a599ae2f631241b5a3eto topic persistent://default/default/f854c239-7829-45ae-bb59-d1a238581e6f [Errno 2] No such file or directory: './database/ba88e3e4-19ab-45c8-8d4e-2323e49fde3f/index_metadata.pickle'
ERROR:chromadb.db.mixins.embeddings_queue:Exception occurred invoking consumer for subscription 79d77609a7d54a599ae2f631241b5a3eto topic persistent://default/default/f854c239-7829-45ae-bb59-d1a238581e6f Index with capacity 100 and 100 current entries cannot add 1 records
ERROR:chromadb.db.mixins.embeddings_queue:Exception occurred invoking consumer for subscription 79d77609a7d54a599ae2f631241b5a3eto topic persistent://default/default/f854c239-7829-45ae-bb59-d1a238581e6f Index with capacity 100 and 100 current entries cannot add 1 records
ERROR:chromadb.db.mixins.embeddings_queue:Exception occurred invoking consumer
</code></pre>
<p>I have just started using Chroma</p>
","nlp, embedding, chromadb",
Alternative to Receptive field in Transformers and what factors impact it,"<p>I have two transformer networks. One with 3 heads per attention and 15 layers in total and second one with 5 heads per layer and 30 layers in total. Given an arbitrary set of documents (2048 tokens per each), how to find out, which network is going to be better to use and is less prone to overfitting?</p>
<p>In computer vision we have concept called: &quot;receptive field&quot;, that allows us to understand how big or small network we need to use. For instance, if we have CNN with 120 layers and CNN with 70 layers, we can calculate their receptive fields and understand which one is going to perform better on a particular dataset of images.</p>
<p>Do you guys have something similar in NLP? How do you understand whether one architecture is more optimal to use versus another，having a set of text documents with unique properties?</p>
","nlp, huggingface-transformers, receptive-field","<blockquote>
<p>How do you understand whether one architecture is more optimal to use versus another, having a set of text documents with unique properties?</p>
</blockquote>
<p>For modern <strong>Transformer-based</strong> Language Models (LMs), there are some empirical &quot;scaling laws,&quot; such as the <a href=""https://arxiv.org/abs/2203.15556"" rel=""nofollow noreferrer"">Chinchilla scaling laws</a> (<a href=""https://en.wikipedia.org/wiki/Neural_scaling_law#Chinchilla_scaling_(Hoffmann,_et_al,_2022)"" rel=""nofollow noreferrer"">Wikipedia</a>), that essentially say that larger (deeper) models with more layers, i.e., with more parameters tend to perform better. So far, most LMs seem to roughly follow Chinchilla scaling. There is another kind of scaling, which is closer to a &quot;receptive field&quot;, that I talk about below.</p>
<blockquote>
<p>Do you guys have something similar in NLP?</p>
</blockquote>
<p>Kind of. Transformer-based LMs can be thought to have a &quot;receptive field&quot; similar to CNN layers, as the attention mechanism in the Transformer operates on a pre-defined &quot;context window&quot; or &quot;context length&quot;, which is the maximum number of tokens the layer can look at (&quot;attend to&quot;) at any given time, similar to a CNN kernel. However, with the introduction of new positional encoding (PE) approaches, such as <a href=""https://arxiv.org/abs/2104.09864"" rel=""nofollow noreferrer"">Rotary Positional Encoding (RoPE)</a>, and modified attention architectures, like <a href=""https://arxiv.org/abs/2004.05150v2"" rel=""nofollow noreferrer"">Sliding Window Attention (SWA)</a>, this is not strictly accurate.</p>
<p>Scaling in terms of &quot;context length&quot; is of much interest, but usually, it is very difficult to scale Transformers this way, because of attention being a ($\mathcal{O}(N^2)$) (O(N^2)) operation. So, usually, researchers go towards deeper architectures with more parameters (&quot;over-parameterization&quot;) that can allow the model to &quot;memorize&quot; as much of the large training corpus as it can (&quot;overfitting&quot;), so that it can perform reasonably well, when fine-tuned for most down-stream tasks (that have at least some representative examples in the training corpus).</p>
"
Spark code for NLP processing for china data,"<p>I am using pySpark and used jieba for china data. I have have UDF in code and did broadcast but column enriching is taking hours and hours for 5 million record. I am clueless what we need to here to improve the performance here.</p>
<pre><code>dictionary_sample = {
'China': [
    {
        'category': 'Beverage',
        'subcategory': 'Carbonates',
        'sub_subcategory': 'Ale',
        'search_keywords': ['Ale', 'Ginger Ale', 'Schweppes Ginger Ale', 'Brewed Ale', 'Schweppes姜啤酒']
    },
    {
        'category': 'Beverage',
        'subcategory': 'Carbonates',
        'sub_subcategory': 'Brewed Soda',
        'search_keywords': ['Brewed Soda', 'Brewed Creaming Soda', 'Burgundee Creaming Soda', '酿造的苏打水', '冲泡奶油苏打', '勃艮第奶油汽水']
    }
]
</code></pre>
<p>}</p>
<p>dictionary size is around 200 MB</p>
<pre><code>    def match_keywords_text(self, country, tokenized_text, nested_dict):
    if country not in nested_dict or tokenized_text is None:
        return []

    #already did lower and strip while creating
    # tokenized_text_set = set(token.strip().lower() for token in tokenized_text if token and token.strip())
    tokenized_text_set = set(tokenized_text)
    matched_category = []
    for entry in nested_dict[country]:
        search_keywords = entry.get('search_keywords', [])
        if search_keywords is None:
            search_keywords = []
        search_keywords_set = set(search_keywords)

        if any(token in search_keywords_set for token in tokenized_text_set):
            matched_category.append(entry['category'])

    if not matched_category:
        return []

    return list(set(matched_category))
</code></pre>
","apache-spark, nlp",
How to generate output of HuggingFace PEFT model with previous message history as context?,"<p>I am trying to generate text from my fine-tuned Llama3 model which uses the PEFT AutoPeftModelForCausalLM library while also passing in previous message history.</p>
<p>This is how I am currently generating responses without previous message history:</p>
<pre><code>def get_response(input):
  inputs = tokenizer([&quot;&quot;&quot;Generate a response to the input
  ### Input:
  {}
  ### Response:&quot;&quot;&quot;.format(input)], return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;)

  outputs = model.generate(input_ids=inputs[&quot;input_ids&quot;].to(&quot;cuda&quot;), max_new_tokens=32)
  response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

  return response
</code></pre>
<p>And this is how I defined the <code>model</code> and <code>tokenizer</code> variables:</p>
<pre><code>model = AutoPeftModelForCausalLM.from_pretrained(&quot;huggingface-user/outputs&quot;).to(&quot;cuda&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;unsloth/llama-3-8b-bnb-4bit&quot;)
</code></pre>
<p>I want to be able to pass in something like:</p>
<pre><code>previous_messages = [
{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message1},
{&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: message2},
{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message3},
{&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: message4},
]
</code></pre>
<p>Any help would be greatly appreciated!</p>
","nlp, huggingface-transformers, large-language-model, peft",
&quot;KeyError: 0&quot; when calling model.fit() in Keras,"<p>I am training a simple sequential model for NLP in keras. Here is the model architecture:</p>
<pre><code>from keras.models import Sequential
from keras import layers
from keras.layers import Embedding, Flatten, Dense

embedding_dim = 500

model = Sequential()

model.add(layers.Embedding(input_dim = vocab_size, 
                           output_dim = embedding_dim, input_shape = (2,)))
model.add(layers.Flatten())
model.add(layers.Dense(10, activation = 'relu'))
model.add(layers.Dense(6, activation = 'softmax'))
model.compile(optimizer = 'adam',
              loss = 'categorical_crossentropy',
              metrics = METRICS)
</code></pre>
<p>I also implemented early stopping and class weights as follows:</p>
<pre><code>early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor = 'val_loss', 
    verbose = 1,
    patience = 10,
    mode = 'max',
    restore_best_weights = True)

class_weight = {0: 3.,
                1: 2.,
                2: 1.5,
                3: 1.,
                4: 3.,
                5: 3.
                }
</code></pre>
<p>I then called model.fit() as follows:</p>
<pre><code>history = model.fit(X_train, y_train,
                    epochs = 100,
                    verbose = 2,
                    validation_data = (X_val, y_val),
                    batch_size = 10,
                    class_weight = class_weight,
                    callbacks = [early_stopping]
                   )
</code></pre>
<p>This resulted in this error:</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[74], line 9
      1 class_weight = {1: 3.,
      2                 2: 2.,
      3                 3: 1.5,
   (...)
      6                 6: 3.
      7                 }
----&gt; 9 history = model.fit(X_train, y_train,
     10                     epochs = 100,
     11                     verbose = 2,
     12                     validation_data = (X_val, y_val),
     13                     batch_size = 10,
     14                     class_weight = class_weight,
     15                     callbacks = [early_stopping]
     16                    )

File ~\AppData\Local\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--&gt; 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~\AppData\Local\anaconda3\Lib\site-packages\pandas\core\series.py:1040, in Series.__getitem__(self, key)
   1037     return self._values[key]
   1039 elif key_is_scalar:
-&gt; 1040     return self._get_value(key)
   1042 # Convert generator to list before going through hashable part
   1043 # (We will iterate through the generator there to check for slices)
   1044 if is_iterator(key):

File ~\AppData\Local\anaconda3\Lib\site-packages\pandas\core\series.py:1156, in Series._get_value(self, label, takeable)
   1153     return self._values[label]
   1155 # Similar to Index.get_value, but we do not fall back to positional
-&gt; 1156 loc = self.index.get_loc(label)
   1158 if is_integer(loc):
   1159     return self._values[loc]

File ~\AppData\Local\anaconda3\Lib\site-packages\pandas\core\indexes\base.py:3798, in Index.get_loc(self, key)
   3793     if isinstance(casted_key, slice) or (
   3794         isinstance(casted_key, abc.Iterable)
   3795         and any(isinstance(x, slice) for x in casted_key)
   3796     ):
   3797         raise InvalidIndexError(key)
-&gt; 3798     raise KeyError(key) from err
   3799 except TypeError:
   3800     # If we have a listlike key, _check_indexing_error will raise
   3801     #  InvalidIndexError. Otherwise we fall through and re-raise
   3802     #  the TypeError.
   3803     self._check_indexing_error(key)

KeyError: 0
</code></pre>
<p>As per a similar question regarding this error in Keras, <a href=""https://stackoverflow.com/questions/52908944/keyerror-0-when-trying-to-load-a-sequential-model-in-keras"">&quot;KeyError: 0&quot; when trying to load a sequential model in Keras</a>, I tried updating my version of Keras. Howeve, this didn't change anything. I believe the error is something internal to Keras, but I'm not sure what's wrong.</p>
<p>Please advise!</p>
<p><strong>UPDATE</strong></p>
<p>To better error trace, I entered this into my code:</p>
<pre><code>keras.config.disable_traceback_filtering()
</code></pre>
<p>Now, the error message more specifically directs me towards this:</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
File ~\AppData\Local\anaconda3\Lib\site-packages\pandas\core\indexes\base.py:3791, in Index.get_loc(self, key)
   3790 try:
-&gt; 3791     return self._engine.get_loc(casted_key)
   3792 except KeyError as err:

File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc()

File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc()

File pandas\_libs\hashtable_class_helper.pxi:2606, in pandas._libs.hashtable.Int64HashTable.get_item()

File pandas\_libs\hashtable_class_helper.pxi:2630, in pandas._libs.hashtable.Int64HashTable.get_item()

KeyError: 0

The above exception was the direct cause of the following exception
</code></pre>
<p>It appears to be an issue in the pandas function Index.get_loc(). However, I can't really change that as I don't know where it originated from.</p>
","python, keras, nlp",
Spacy detect correctly GPE,"<p>I've a set of string where I shall detetect the country its belongs to, referring to detected GPE.</p>
<pre><code>sentences = [
    &quot;I watched TV in germany&quot;,
    &quot;Mediaset ITA canale 5&quot;,
    &quot;Je viens d'Italie&quot;,
    &quot;Ich komme aus Deutschland&quot;,
    &quot;He is from the UK&quot;,
    &quot;Soy de Inglaterra&quot;,
    &quot;Sono del Regno Unito&quot;
]
</code></pre>
<p>So my expectation :
&quot;I watched TV in germany&quot; =&gt; DE
&quot;Soy de Inglaterra&quot; =&gt; UK
etc..</p>
<p>I've tried with this code:</p>
<pre><code>from spacy.pipeline import EntityRuler
nlp = spacy.load('xx_ent_wiki_sm')  # Multilingual model


def detect_country(text):
    doc = nlp(text)
    countries = []
    for ent in doc.ents:
        if ent.label_ == 'GPE':
            countries.append(ent.text)
    return countries


for sentence in sentences:
    countries = detect_country(sentence)
    print(f&quot;Countries detected in '{sentence}': {countries}&quot;)
</code></pre>
<p>But results are completely empty:</p>
<pre><code>Countries detected in 'I watched TV in germany': []
Countries detected in 'Mediaset ITA canale 5': []
Countries detected in 'Je viens d'Italie': []
Countries detected in 'Ich komme aus Deutschland': []
Countries detected in 'He is from the UK': []
Countries detected in 'Soy de Inglaterra': []
Countries detected in 'Sono del Regno Unito': []
</code></pre>
<p>I don't understand if I'm missing something into spacy pipeline and if I'm actually take good choice for multilang model.
Thx in advance for any advise</p>
","python, nlp, spacy-3",
why isn&#39;t tf.keras.layers.TextVectorization accepting standardization=None?,"<p>I'm still trying to get this work (and to learn!) so I am using a tiny corpus.</p>
<p>I do some preprocessing on the text in order to get specific bi-gram collocations using nltk (not relevant here but I have a list of compound nouns that I want to capture as a single token - this seemed a more sensible approach than running through the whole corpus for each word-pair that might only appear once).</p>
<p>So what I am passing to this vectorizing/ indexing step is a list of sentences that already have punctation, stopwords, numbers, etc. removed. with the exception of &quot;_&quot; between collocations. so for example (&quot;fiery_pit&quot;):</p>
<pre><code>result
[...
'would far rather lay lengthwise along line equator yea ye gods',
 'go fiery_pit order keep frost',
 'lazarus lie stranded curbstone door dives wonderful iceberg moored one moluccas',
...]

from tensorflow import keras

vectorizer = tf.keras.layers.TextVectorization(
    max_tokens=None,
    standardize=None,
    split=&quot;whitespace&quot;,
    output_mode='int',
    output_sequence_length=None
)

vectorizer.adapt(result)
vocab = vectorizer.get_vocabulary()
word_index = {word: index for index, word in enumerate(vocab)}
word_index
{'': 0,
 '[UNK]': 1,
 'whale': 2,
...
 'fiery': 927,
...}
</code></pre>
<p>It seems that punctuation is still being removed despite <code>standardize=None</code>. How do I fix this?</p>
","python, keras, nlp, nltk, tf.keras",
Read Arabic pdf book using python,"<p>I am working on reading an Arabic book using python (the pdf is selectable it does not require any OCR (optical character recognition to extract text from images)) so I have used multiple libraries pdfplumber, pdfminer.six and flitz (PyMuPdf)) this is one of the code I have used:</p>
<pre><code>import pdfplumber
from bidi.algorithm import get_display
import arabic_reshaper
import re

def clean_text(text):
    # Remove NULL bytes and control characters
    cleaned_text = re.sub(r'[\x00-\x1F\x7F]', '', text)
    return cleaned_text

def reshape_and_bidi_text(text):
    # Reshape Arabic text and apply bidi algorithm
    reshaped_text = arabic_reshaper.reshape(text)
    bidi_text = get_display(reshaped_text)
    return bidi_text

def extract_text_from_pdf(pdf_path):
    text = &quot;&quot;
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + &quot;\n&quot;
    return text

def save_text_to_file(text, output_path):
    with open(output_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as text_file:
        text_file.write(text)

def convert_pdf_to_text(pdf_path, output_path):
    # Extract text from the PDF using pdfplumber
    extracted_text = extract_text_from_pdf(pdf_path)
    
    # Clean the extracted text
    cleaned_text = clean_text(extracted_text)
    
    # Reshape and apply bidi algorithm to the text
    reshaped_bidi_text = reshape_and_bidi_text(cleaned_text)
    
    # Save the cleaned and reshaped text to a text file
    save_text_to_file(reshaped_bidi_text, output_path)
    print(f&quot;Text from {pdf_path} has been saved to {output_path}&quot;)

# Example usage
pdf_path = r'C:\Users\DELL\Desktop\Book Printed\البوليميرات العالية الأداء.pdf'
text_output_path = r&quot;C:\Users\DELL\Desktop\output.txt&quot;

convert_pdf_to_text(pdf_path, text_output_path)
</code></pre>
<p>So when using these libraries always I obtain the following output with wrong encoding and I don't know what to use to fix is?</p>
<p>Note: attached above <a href=""https://www.noor-book.com/%D9%83%D8%AA%D8%A7%D8%A8-%D8%A7%D9%84%D8%A8%D9%88%D9%84%D9%8A%D9%85%D9%8A%D8%B1%D8%A7%D8%AA-%D8%A7%D9%84%D8%B9%D8%A7%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D8%A3%D8%AF%D8%A7%D8%A1-pdf?next=72c6f38a363b368a7bd978a8449ea530"" rel=""nofollow noreferrer"">https://www.noor-book.com/%D9%83%D8%AA%D8%A7%D8%A8-%D8%A7%D9%84%D8%A8%D9%88%D9%84%D9%8A%D9%85%D9%8A%D8%B1%D8%A7%D8%AA-%D8%A7%D9%84%D8%B9%D8%A7%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D8%A3%D8%AF%D8%A7%D8%A1-pdf?next=72c6f38a363b368a7bd978a8449ea530</a> is the arabic book I try to read</p>
","python, machine-learning, nlp, arabic",
A prepand *Paraphrase* is showig all the time after runnig the code instead of actual paraphrased Sentence,"<p>I am creating an URDU TEXT PARAPHRASING tool for my semester.</p>
<p>I have used T5 Model and fine tuned it.</p>
<p>Now when im running this code:</p>
<pre><code>**import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer

# Define your model and tokenizer
model_name = &quot;t5-small&quot;
model = T5ForConditionalGeneration.from_pretrained(model_name)
tokenizer = T5Tokenizer.from_pretrained(model_name)

def paraphrase_urdu_sentence(sentence, model, tokenizer):
    try:
        # Prepend &quot;paraphrase: &quot; to sentence
        **input_text = &quot;paraphrase: &quot; + sentence**

        # Tokenize input text
        input_ids = tokenizer.encode(input_text, return_tensors=&quot;pt&quot;, max_length=128, truncation=True)

        # Generate paraphrased text
        generated_ids = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)

        # Decode the generated paraphrased text
        paraphrased_sentence = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

        return paraphrased_sentence

    except Exception as e:
        print(f&quot;Error occurred during paraphrasing: {e}&quot;)
        return None

def main():
    input_sentence = input(&quot;Insert Your Text in Urdu: &quot;)
    paraphrased_sentence = paraphrase_urdu_sentence(input_sentence, model, tokenizer)
    if paraphrased_sentence:
        print(&quot;Original sentence:&quot;, input_sentence)
        print(&quot;Paraphrased sentence:&quot;, paraphrased_sentence)
    else:
        print(&quot;Failed to paraphrase the sentence.&quot;)

if __name__ == &quot;__main__&quot;:
    main()**
</code></pre>
<p>The output should be the paraphrased sentence of what urdu sentence i insert. But its giving me this result:</p>
<pre class=""lang-none prettyprint-override""><code>Insert Your Text in Urdu: آپ کو لگتا ہے کہ آپ کہاں جا رہے ہیں۔
Original sentence: آپ کو لگتا ہے کہ آپ کہاں جا رہے ہیں۔
Paraphrased sentence: Paraphrase:
</code></pre>
<p>how to get the paraphrased sentence ? please help its urgennt</p>
<p>I have tried all possible ssolutions suggested by chatgpt nothing works</p>
","python, nlp",
Understanding time complexity for dynamic indexing,"<p>While reading the text, Introduction to Information Retrieval by Manning et al., I came across dynamic indexing (<a href=""https://nlp.stanford.edu/IR-book/pdf/04const.pdf"" rel=""nofollow noreferrer"">Sec 4.5, Page 79</a>). It consists of two indices, the main index (stored on disk) and an auxiliary index (in-memory), which is merged with the main index once the postings reach a size of <code>n</code>. Assuming the postings in the main index are stored in a single file (<code>T</code> is the size of postings in the main index), it takes <code>O(T^2/n)</code> time to merge the auxiliary index into the main index.</p>
<p>As per the text</p>
<blockquote>
<p>In this scheme, we process each posting ⌊T/n⌋ times because we touch it
during each of ⌊T/n⌋ merges where n is the size of the auxiliary index and T
the total number of postings. Thus, the overall time complexity is Θ(T^2/n).</p>
</blockquote>
<p>I am wondering, why it should take <code>O(T^2/n)</code> time to merge the two indices. As per my understanding to merge two sorted arrays should take only <code>O(T+n)</code> time. I am conceptually missing something, would appreciate any help.</p>
","indexing, nlp, information-retrieval",
Python Rasa train not compatible with M3 Apple,"<p>Unable to run the &quot;rasa train&quot; in mac m3 terminal..
giving below error.</p>
<p>2024-06-26 17:27:12 INFO     rasa.cli.train  - Started validating domain and training data...
zsh: illegal hardware instruction  rasa train</p>
<p>After some research i have noticed this is a common issue in Apple M2 mac.
How to mitigate this.</p>
<p>i am in middle of running major project.</p>
","python-3.x, macos, nlp, apple-m1, rasa",
Trying to read a .wav file but was able to convert only 20s conversation,"<p>I am trying to read a .wav file but was able to convert only 20s conversation into text.</p>
<pre><code>   # code to read .wav file 
   import speech_recognition as sr
    r = sr.Recognizer()
    audio = r&quot;D:\Fraud_Call_Detection\audio1.wav&quot;
    with sr.AudioFile(audio) as source:
        audio = r.record(source)
        print ('Done!')

    try:
        
        text = (r.recognize_google(audio, language=&quot;en-HK&quot;))
        print (text)

    except Exception as e:
        print (e)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.sstatic.net/gY8w8qeI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gY8w8qeI.png"" alt=""enter image description here"" /></a></p>
","python-3.x, machine-learning, nlp, speech-recognition, speech-to-text",
How to use Llama3?,"<p>I'm using Ollama and llama 3 to build a ChatBot. However, right now, it can't remember chat history. For example, if my first query is &quot;tell me about the theory of relativity,&quot; and if my next query is &quot;can you simplify it,&quot; it throws a message saying it can't recall earlier conversations.</p>
<p>Here is my current code snippet:</p>
<pre><code>import ollama
import streamlit as st

# function to get the llama's response
def get_llama_response(prompt):
    response = ollama.chat(
        model='llama3',
        messages= [
            {
                'role':'user',
                'content': prompt
            }
        ]
    )
    return response['message']['content']

# setting the title
st.title(&quot;Llama Knows All&quot;)

# loading message and we take prompt
prompt = st.chat_input(&quot;How can I help you today?&quot;)

# setting up a session to hold all messages during current interaction
if 'messages' not in st.session_state:
    st.session_state.messages = []

# displaying older messages
for message in st.session_state.messages:
    st.chat_message(message['role']).markdown(message['content'])

if prompt:
    # display the user message
    st.chat_message('user').markdown(prompt)
    st.session_state.messages.append({'role':'user', 'content': prompt})

    llama_response = get_llama_response(prompt)

    st.chat_message('llama').markdown(llama_response)
    st.session_state.messages.append({'role': 'llama', 'content': llama_response})
</code></pre>
","nlp, large-language-model, llama, ollama",
Inconsistent Results When Running Python Mallet/Gibb&#39;s Sampling as a Soft-Clustering Method to Identify Optimal Number of Topics,"<p>Sorry, but I am inexperienced with Mallet and could use some help. I am currently trying to use Mallet as a soft-clustering technique to assign group membership for a given set of terms contained across a large number of very short documents. I hope to use the resulting group assignments to count/measure the frequency of terms distributed across each of the respective grouped topics. There is no pre-existing typology or taxonomy for these assignments. Because of that, I want to identify a optimal group solution (through the returned coherence scores) and be able to replicate the result with the Mallet code.</p>
<p>So far, I have attempted to use seed values which creates an exact and specific result, but when I run without a seed the results tend to vary between a few different results and it makes it difficult to argue why one specific result may be better than another, which is why I would like to have the code consistently return a single answer regardless of which seed it is running.</p>
<p>I have also added the num-icm-iterations parameter, as was mentioned in another thread. It seems like it narrowed down the results a fair amount to the point where the results tend to lean between one of two different solutions, but I would really like to narrow it down even more if possible.</p>
<p>My current code looks like this:</p>
<pre><code>def run_mallet_and_compute_coherence(dictionary, corpus, texts, mallet_path, temp_dir_path, start, limit, step):
    # Paths for Mallet output.
    corpus_file_path = f&quot;{temp_dir_path}/corpus.mallet&quot;
    output_file_path = f&quot;{temp_dir_path}/topic_keys.txt&quot;
    doc_topic_dist_path = f&quot;{temp_dir_path}/doc_topics.txt&quot;
    output_state_path = f&quot;{temp_dir_path}/state.gz&quot;

    # Ensure that the corpus file is created once.
    if not os.path.isfile(corpus_file_path):
        print(&quot;Importing corpus to Mallet format...&quot;)
        subprocess.run([
            mallet_path, 'import-file',
            '--input', input_file_path,
            '--output', corpus_file_path,
            '--keep-sequence', 'TRUE',
            '--remove-stopwords', 'TRUE'
        ])

    for num_topics in range(start, limit, step):
        # Clean up previous output files.
        if os.path.isfile(output_file_path):
            os.remove(output_file_path)
        if os.path.isfile(doc_topic_dist_path):
            os.remove(doc_topic_dist_path)
        if os.path.isfile(output_state_path):
            os.remove(output_state_path)

        # Run Mallet from the command line.
        subprocess.run([
            mallet_path, 'train-topics',
            '--input', corpus_file_path,
            '--num-topics', str(num_topics),
            '--output-topic-keys', output_file_path,
            '--output-doc-topics', doc_topic_dist_path,
            '--output-state', output_state_path,
            '--num-threads', '1',
            '--alpha', 'auto',
            '--beta', 'auto',
            '--eta', 'auto',
            '--num-icm-iterations', '100000'
        ])

        # Load the generated model.
        ldamallet = LdaMallet(mallet_path, corpus=corpus, id2word=dictionary, num_topics=num_topics, prefix=temp_dir_path)

        ldaModel = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)
        ldagensim = convertldaMalletToldaGen(ldamallet)
        
        model_list.append(ldagensim)

        # Compute coherence scores.
        coherencemodel = CoherenceModel(model=ldagensim, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())
        print(f'Number of topics: {num_topics}, Coherence Value: {coherence_values[-1]}')

    print(&quot; &quot; + u'\u2713' + &quot; Gibbs model done.&quot;)
    return model_list, coherence_values

#############################################################################

def convertldaMalletToldaGen(mallet_model):
    model_gensim = LdaModel(
        id2word=mallet_model.id2word, num_topics=mallet_model.num_topics,
        alpha=mallet_model.alpha) 
    model_gensim.state.sstats[...] = mallet_model.wordtopics
    model_gensim.sync_state()
    return model_gensim

print(&quot;   LDA models starting...&quot;)
print()
print(&quot;#   Coherence Value&quot;)

model_list = []
coherence_values = []

# Run Mallet for different numbers of topics and compute coherence values.
model_list, coherence_values = run_mallet_and_compute_coherence(dictionary=wordDictionary, 
                                                                corpus=corpus, 
                                                                texts=texts, 
                                                                mallet_path=mallet_path, 
                                                                temp_dir_path=temp_dir_path, 
                                                                start=start, 
                                                                limit=limit, 
                                                                step=step)
</code></pre>
<p>Can anyone explain why this code keeps returning different values? I have been unable to find any documentation or articles explaining how to do this. Any help would be appreciated.</p>
<p>I have also provided a sample of the results from five different runs using the code from above. Thank you for any assistance!</p>
<pre><code>Run One:
Number of topics: 6, Coherence Value: 0.39282318211226813*
Number of topics: 7, Coherence Value: 0.3870130112666789
Run Two:
Number of topics: 6, Coherence Value: 0.3779967673187221
Number of topics: 7, Coherence Value: 0.3834180326685324*
Run Three:
Number of topics: 6, Coherence Value: 0.4006949110481259*
Number of topics: 7, Coherence Value: 0.3892613210402246
Run Four:
Number of topics: 6, Coherence Value: 0.3670477953310585
Number of topics: 7, Coherence Value: 0.3911717553304524*
Run Five:
Number of topics: 6, Coherence Value: 0.410671950931132*
Number of topics: 7, Coherence Value: 0.377040568874814
</code></pre>
","nlp, topic-modeling, mallet",
Generate Tags for a text given,"<p>I am trying to build an API using flask which will extract the text from a url given and generate valid tags for that text. Say for example, the text is for a recipe of chicken curry, valid tags can be recipe, Indian Cuisine, food etc.</p>
<p>I have tried nltk library, TF-IDF vectorizer etc, but all of them are analysing maximum frequency of the words, not generating new words.
Does any one have any solution for this problem.
I have also tried using gtp2 module, but the output is not what I am expecting</p>
<pre><code>import requests
from bs4 import BeautifulSoup
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from flask import Flask, request, jsonify

app = Flask(__name__)

# Load pre-trained GPT-2 model and tokenizer
model_name = 'gpt2-medium'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

def fetch_text_from_url(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise HTTPError for bad responses
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Find all paragraphs and concatenate their text
        paragraphs = soup.find_all('p')
        text = ' '.join([para.get_text() for para in paragraphs])
        
        return text
    except requests.exceptions.RequestException as e:
        print(f&quot;Error fetching content from {url}: {str(e)}&quot;)
        return None
    except Exception as e:
        print(f&quot;Error parsing content from {url}: {str(e)}&quot;)
        return None

def generate_tags(text, max_length=20, num_return_sequences=3):
    try:
        # Create a prompt to generate tags
        prompt = &quot;Tags for this text: &quot;

        # Tokenize the input text and prompt
        input_ids = tokenizer.encode(prompt + text, return_tensors='pt')

        # Generate tags using GPT-2 model
        output = model.generate(
            input_ids,
            max_length=max_length + len(input_ids[0]),
            num_return_sequences=num_return_sequences,
            num_beams=5,
            no_repeat_ngram_size=2,
            early_stopping=True
        )

        # Decode generated tags
        tags = []
        for seq in output:
            decoded_seq = tokenizer.decode(seq, skip_special_tokens=True).strip()
            # Extract tags from the generated text
            tags.extend([t.strip() for t in decoded_seq.split() if t.startswith('#')])

        return tags
    
    except Exception as e:
        print(f&quot;Error generating tags: {str(e)}&quot;)
        return None

@app.route('/generate_tags', methods=['POST'])
def generate_tags_api():
    data = request.get_json()
    url = data.get('url')
    if not url:
        return jsonify({'error': 'URL is required'}), 400

    try:
        text = fetch_text_from_url(url)
        if not text:
            return jsonify({'error': 'Failed to fetch content from URL'}), 500
        
        tags = generate_tags(text)
        if tags:
            return jsonify({'tags': tags})
        else:
            return jsonify({'error': 'Failed to generate tags from URL'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == &quot;__main__&quot;:
    app.run(port=8000, debug=True)
</code></pre>
","python, machine-learning, nlp",
What is the difference between HuggingFace&#39;s TextGeneration and Text2TextGeneration pipelines,"<p>I'm confused about the technical difference between the two huggingface pipelines <a href=""https://huggingface.co/docs/transformers/main/main_classes/pipelines#transformers.TextGenerationPipeline"" rel=""noreferrer"">TextGeneration</a> and <a href=""https://huggingface.co/docs/transformers/main/main_classes/pipelines#transformers.Text2TextGenerationPipeline"" rel=""noreferrer"">Text2TextGeneration</a>.</p>
<p>In the TextGeneration it is stated that:</p>
<blockquote>
<p>Language generation pipeline using any ModelWithLMHead. This pipeline
predicts the words that will follow a specified text prompt.</p>
</blockquote>
<p>But isn't any language model doing that? &quot;predicting the next words&quot;? So how is this pipeline different than Text2TextGeneration? Isn't Text2TextGeneration going to predict the next probable words?</p>
<p>I also tried some models using &quot;Text2TextGeneration&quot; pipeline, and despite of HuggingFace's warning &quot;The model is not supported for text2text-generation&quot; it actually worked and generated some outputs.</p>
<p>If someone can explain the technical difference it will be appreciated.</p>
","nlp, huggingface-transformers, huggingface",
Why do I get two diffrent responses when using the Inference API feature on thee widget on hugging face compared to one when I run the api locally?,"<p>I recently discovered hugging face and have been trying to work with it. When using text-to-text model, I came into the issue. no matter which model, i would get a different response when trying the model in the inference widget of hugging face vs when i would call an API from my machine.</p>
<p>here i the difference:
<a href=""https://i.sstatic.net/pzVcGY2f.png"" rel=""nofollow noreferrer"">Screensshot of Widget and its reponse</a></p>
<p>and here is the code with the output :</p>
<pre><code>def query(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    response.raise_for_status()  # Raise an exception for HTTP errors
    return response.json()

output = query({
        &quot;inputs&quot;: &quot;Can you please tell me something about a Python programming lnaguage?&quot;,
    })

print(&quot;Unexpected response format:&quot;, output)
</code></pre>
<p>output being :</p>
<pre><code>Can you please tell me something about a Python programming lnaguage?
-
Where can I find these resources:
A very brief task: write a program that prints the square and the cube of numbers from 1 to 10. Collect the user input and print the result. The program will end here.

Python code:

    # Python code to find square and cube of numbers
    for i in range(1, 1好好谱(10)):
        print(&quot;Square of&quot;, i
</code></pre>
<p>why is this happening and how to fix it?</p>
","python, nlp, huggingface-transformers, large-language-model, huggingface",
"Alexa skill requires two slots to be filled, however even when filling both slots in request, skill still requests them individually","<p>I am building a test alexa skill that will assist with purchasing products. It expects both the name of the product, and the number of units to purchase. These slots have been set up as required to fulfill the intent, and while they can be fulfilled individually, I have set up sample utterances that allow the user to fulfill both in the same sentence, e.g. </p>

<pre><code>I want to buy {MyQuantity} {MyArticle}
</code></pre>

<p>However, in testing, Alexa will always ask for both values individually. Below is a sample of the conversation flow, and below that, the JSON output of my skill. Unsure what I'm doing wrong. (I have delegated the dialog management to Alexa in this case). </p>

<p><a href=""https://i.sstatic.net/qGlsM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qGlsM.png"" alt=""enter image description here""></a></p>

<p>The JSON configuration of the skill: </p>

<pre><code>{
    ""interactionModel"": {
        ""languageModel"": {
            ""invocationName"": ""priceline collect"",
            ""intents"": [
                {
                    ""name"": ""AMAZON.FallbackIntent"",
                    ""samples"": []
                },
                {
                    ""name"": ""AMAZON.CancelIntent"",
                    ""samples"": []
                },
                {
                    ""name"": ""AMAZON.HelpIntent"",
                    ""samples"": []
                },
                {
                    ""name"": ""AMAZON.StopIntent"",
                    ""samples"": []
                },
                {
                    ""name"": ""AMAZON.NavigateHomeIntent"",
                    ""samples"": []
                },
                {
                    ""name"": ""CreateOrder"",
                    ""slots"": [
                        {
                            ""name"": ""MyArticle"",
                            ""type"": ""ClickCollectArticle"",
                            ""samples"": [
                                ""I want to buy {MyArticle}"",
                                ""I want {MyArticle}"",
                                ""Can i get {MyArticle} please"",
                                ""I'd like to buy {MyArticle}""
                            ]
                        },
                        {
                            ""name"": ""MyQuantity"",
                            ""type"": ""AMAZON.NUMBER"",
                            ""samples"": [
                                ""{MyQuantity} thanks"",
                                ""I want {MyQuantity} please""
                            ]
                        }
                    ],
                    ""samples"": [
                        ""I want to buy {MyQuantity} {MyArticle}"",
                        ""I want to buy {MyArticle}"",
                        ""Can i please get {MyQuantity} {MyArticle}"",
                        ""I want to order {MyArticle}"",
                        ""I would like to place an order""
                    ]
                }
            ],
            ""types"": [
                {
                    ""name"": ""ClickCollectArticle"",
                    ""values"": [
                        {
                            ""name"": {
                                ""value"": ""foundation""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""conditioner""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""shampoo""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""perfume""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""after shave""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""aftershave""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""hairspray""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""hair gel""
                            }
                        },
                        {
                            ""name"": {
                                ""value"": ""lipstick""
                            }
                        }
                    ]
                }
            ]
        },
        ""dialog"": {
            ""intents"": [
                {
                    ""name"": ""CreateOrder"",
                    ""delegationStrategy"": ""ALWAYS"",
                    ""confirmationRequired"": true,
                    ""prompts"": {
                        ""confirmation"": ""Confirm.Intent.847210063881""
                    },
                    ""slots"": [
                        {
                            ""name"": ""MyArticle"",
                            ""type"": ""ClickCollectArticle"",
                            ""confirmationRequired"": true,
                            ""elicitationRequired"": true,
                            ""prompts"": {
                                ""confirmation"": ""Confirm.Slot.847210063881.746398127202"",
                                ""elicitation"": ""Elicit.Slot.847210063881.746398127202""
                            }
                        },
                        {
                            ""name"": ""MyQuantity"",
                            ""type"": ""AMAZON.NUMBER"",
                            ""confirmationRequired"": true,
                            ""elicitationRequired"": true,
                            ""prompts"": {
                                ""confirmation"": ""Confirm.Slot.847210063881.631830388134"",
                                ""elicitation"": ""Elicit.Slot.847210063881.631830388134""
                            },
                            ""validations"": [
                                {
                                    ""type"": ""isLessThan"",
                                    ""prompt"": ""Slot.Validation.369260633406.54462952385.1162755638931"",
                                    ""value"": ""20""
                                }
                            ]
                        }
                    ]
                }
            ],
            ""delegationStrategy"": ""ALWAYS""
        },
        ""prompts"": [
            {
                ""id"": ""Elicit.Slot.847210063881.746398127202"",
                ""variations"": [
                    {
                        ""type"": ""PlainText"",
                        ""value"": ""We sell all kinds of awesome stuff, from lipstick to hairspray. What would you like to buy?""
                    },
                    {
                        ""type"": ""PlainText"",
                        ""value"": ""We sell everything from shampoo to foundation. What would you like to order?""
                    },
                    {
                        ""type"": ""PlainText"",
                        ""value"": ""What item would you like to order. Examples are lipstick or foundation. ""
                    }
                ]
            },
            {
                ""id"": ""Confirm.Slot.847210063881.746398127202"",
                ""variations"": [
                    {
                        ""type"": ""PlainText"",
                        ""value"": ""Great. So that's {MyArticle} ?""
                    }
                ]
            },
            {
                ""id"": ""Elicit.Slot.847210063881.631830388134"",
                ""variations"": [
                    {
                        ""type"": ""PlainText"",
                        ""value"": ""What number of {MyArticle} would you like to purchase?""
                    },
                    {
                        ""type"": ""PlainText"",
                        ""value"": ""How many {MyArticle} were you after?""
                    },
                    {
                        ""type"": ""PlainText"",
                        ""value"": ""And how many of the item {MyArticle} would you like to buy?""
                    }
                ]
            },
            {
                ""id"": ""Confirm.Slot.847210063881.631830388134"",
                ""variations"": [
                    {
                        ""type"": ""PlainText"",
                        ""value"": ""Awesome. So {MyQuantity} {MyArticle} ?""
                    }
                ]
            },
            {
                ""id"": ""Confirm.Intent.847210063881"",
                ""variations"": [
                    {
                        ""type"": ""PlainText"",
                        ""value"": ""Ok. It looks like we're all ready to get you {MyQuantity} units of {MyArticle} , is that right?""
                    }
                ]
            },
            {
                ""id"": ""Slot.Validation.369260633406.54462952385.1162755638931"",
                ""variations"": [
                    {
                        ""type"": ""PlainText"",
                        ""value"": ""Sorry, you can't order that many units. Please order less than twenty. ""
                    }
                ]
            }
        ]
    }
}
</code></pre>
","amazon-web-services, nlp, alexa-skills-kit",
How to understand the training complexity of Skip-gram model?,"<p><a href=""https://arxiv.org/pdf/1301.3781"" rel=""nofollow noreferrer"">Efficient Estimation of Word Representations in Vector Space</a> explains the design of Skip-gram model, but I have some issues about its training complexity.</p>
<p>According to this paper, the training complexity is proportional to <code>C × (D + D × log2(V))</code>, where</p>
<ul>
<li><code>C</code> is the maximum distance of the words.</li>
<li><code>D</code> is the projected dimension.</li>
<li><code>V</code> is the size of vocabulary.</li>
</ul>
<p>But I think its complexity should be <code>D + C × D × log2(V)</code>, as given a word, the computing (parameter) of input-projection is fixed to <code>D</code>, no matter what the size of outputs is.</p>
<p>So, how to understand its training complexity correctly?</p>
","nlp, time-complexity, word2vec",
BartForConditionalGeneration: Adding additional layers of embedding,"<p>In this file:</p>
<pre><code>https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/modeling_bart.py
</code></pre>
<p>I think-</p>
<blockquote>
<p>Inside the BartEncoder, on this <a href=""https://github.com/huggingface/transformers/blob/e0c3cee17085914bbe505c159beeb8ae39bc37dd/src/transformers/models/bart/modeling_bart.py#L1166"" rel=""nofollow noreferrer"">line</a>, I have to add other embeddings</p>
</blockquote>
<p>Could anyone tell me - are there any other places I need to modify to add additional layers of embedding?
Could anyone tell me - are there any other places I need to modify to add additional layers of embedding (e.g., 2 other embedding with positional embedding)?</p>
","pytorch, nlp, huggingface-transformers, bart",
HuggingFace&#39;s transformers I&#39;m getting the message &quot;Some non-default generation parameters are set in the model config&quot;,"<p>I need help with a problem. The template works but I want to correct this message so that everything is correct. The purpose of this code is to save the model before training. It saves, but this message always appears in the terminal after running the code.</p>
<p>code:</p>
<pre><code>from transformers import MarianMTModel, MarianTokenizer
from transformers.generation import GenerationConfig

model_name = 'Helsinki-NLP/opus-mt-tc-big-en-pt'

tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

save_directory = 'initial_model'
tokenizer.save_pretrained(save_directory)
model.save_pretrained(save_directory)
</code></pre>
<p>Message after run this code:</p>
<blockquote>
<p>Some non-default generation parameters are set in the model config.
These should go into a GenerationConfig file
(<a href=""https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model</a>)
instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 512, 'num_beams': 4,
'bad_words_ids': [[54775]], 'forced_eos_token_id': 44670}</p>
</blockquote>
<p>I've looked at the documentation, but the information is all too simple and doesn't cover the whole problem</p>
","python, nlp, huggingface-transformers, huggingface-tokenizers",
How to get the size of a Hugging Face pretrained model?,"<p>I keep getting a <code>CUDA out of memory</code> error when trying to fine-tune a Hugging Face pretrained XML Roberta model. So, the first thing I want to find out is the size of the pretrained model.</p>
<pre><code>model = XLMRobertaForCausalLM.from_pretrained('xlm-roberta-base', config=config)
device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)

model.to(device)
</code></pre>
<p>I have tried to get the size of the model with</p>
<pre><code>sys.getsizeof(model)
</code></pre>
<p>and, unsurprisingly, I get an incorrect result.  I get 56 as a result, which is the size of the python object.</p>
<p>But then, I tried <code>model. element_size()</code>, and I get the error</p>
<pre><code>ModuleAttributeError: 'XLMRobertaForCausalLM' object has no attribute 'element_size'
</code></pre>
<p>I have searched in the Hugging Face documentation, but I have not found how to do it. Does anyone here know how to do it?</p>
","python, deep-learning, nlp, pytorch, huggingface-transformers",
LSTM forecasting predicts zeros,"<p>i'm building an LSTM model to forecast future total number of cases for covid 19 using OWID dataset</p>
<p>i use a multivariate series of 6 columns including the date column,
the Problem is i get all zero predictions however this not the case when i use univariate series using only single column the total_cases column</p>
<p>here is the code i use</p>
<pre><code>url = &quot;https://covid.ourworldindata.org/data/owid-covid-data.csv&quot;
df = pd.read_csv(url)

# Filter the data for a specific location, e.g., 'United States'
location = 'United States'
df_location = df[df['location'] == location]

# Select relevant columns and set the date column as the index
selected_columns = ['date', 'new_cases', 'new_deaths', 'total_cases', 'total_deaths', 'reproduction_rate']
df_location = df_location[selected_columns]
df_location['date'] = pd.to_datetime(df_location['date'])
df_location.set_index('date', inplace=True)

# Handle missing values by filling them with the mean of the column
df_location.fillna(df_location.mean(), inplace=True)

# Function to create a multivariate dataset for LSTM
def create_multivariate_dataset(data, time_step=1):
    X, Y = [], []
    for i in range(len(data) - time_step - 1):
        X.append(data[i:(i + time_step)])
        Y.append(data[i + time_step, 0])  # Predicting 'new_cases'
    return np.array(X), np.array(Y)

# Convert the data to numpy array and scale it
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(df_location)

# Create the dataset with a specified time step, e.g., 60 days
time_step = 60
X, y = create_multivariate_dataset(scaled_data, time_step)

# Reshape the input to be [samples, time steps, features] for LSTM
X = X.reshape(X.shape[0], X.shape[1], X.shape[2])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the LSTM model
model = Sequential()
model.add(LSTM(100, return_sequences=True, input_shape=(time_step, X.shape[2])))  # Adjusted input_shape
model.add(LSTM(100, return_sequences=False))
model.add(Dense(50, activation='relu'))
model.add(Dense(1, activation='relu'))  

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1)

# Make predictions on the test set
test_predict = model.predict(X_test)

# Inverse transform the predictions to get actual values
test_predict_full = np.concatenate((test_predict, np.zeros((test_predict.shape[0], scaled_data.shape[1] - 1))), axis=1)
test_predict = scaler.inverse_transform(test_predict_full)[:,0]
</code></pre>
<p>output of test predict is always all zeros</p>
<p>what i'm doing wrong</p>
","tensorflow, deep-learning, nlp, lstm, forecasting",
FileNotFoundError when loading SQuAD dataset with datasets library,"<p>I am trying to load the SQuAD dataset using the datasets library in Python, but I am encountering a FileNotFoundError. Here is the code I am using:</p>
<pre><code>from datasets import load_dataset
dataset = load_dataset(&quot;squad&quot;)
</code></pre>
<p>However, this results in the following error:</p>
<pre><code>Traceback (most recent call last):   File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;   
File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/load.py&quot;, line 1797, in load_dataset     **config_kwargs,   File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/load.py&quot;, line 1520, in load_dataset_builder     
data_files=data_files,   File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/load.py&quot;, line 1164, in dataset_module_factory     
path, data_dir=data_dir, data_files=data_files, download_mode=download_mode   
File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/load.py&quot;, line 645, in get_module     
allowed_extensions=ALL_ALLOWED_EXTENSIONS,   
File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/data_files.py&quot;, line 798, in from_local_or_remote     
if not isinstance(patterns_for_key, DataFilesList)   
File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/data_files.py&quot;, line 748, in from_local_or_remote     
data_files = resolve_patterns_locally_or by_urls(base_path, patterns, allowed_extensions)   File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/data_files.py&quot;, line 355, in resolve_patterns_locally_or_by_urls     
raise FileNotFoundError(error_msg) FileNotFoundError: Unable to resolve any data file that matches '['**']' at /root/retraining-free-pruning/squad with any supported extension ['.csv', 
'.tsv', '.json', '.jsonl', '.parquet', '.arrow', '.txt', '.blp', '.bmp', '.dib', '.bufr', 
'.cur', '.pcx', '.dcx', '.dds', '.ps', '.eps', '.fit', '.fits', '.fli', '.flc', '.ftc', 
'.ftu', '.gbr', '.gif', '.grib', '.h5', '.hdf', '.png', '.apng', '.jp2', '.j2k', '.jpc', 
'.jpf', '.jpx', '.j2c', '.icns', '.ico', '.im', '.iim', '.tif', '.tiff', '.jfif', '.jpe', 
'.jpg', '.jpeg', '.mpg', '.mpeg', '.msp', '.pcd', '.pxr', '.pbm', '.pgm', '.ppm', '.pnm', 
'.psd', '.bw', '.rgb', '.rgba', '.sgi', '.ras', '.tga', '.icb', '.vda', '.vst', '.webp', 
'.wmf', '.emf', '.xbm', '.xpm', '.BLP', '.BMP', '.DIB', '.BUFR', '.CUR', '.PCX', '.DCX', 
'.DDS', '.PS', '.EPS', '.FIT', '.FITS', '.FLI', '.FLC', '.FTC', '.FTU', '.GBR', '.GIF', 
'.GRIB', '.H5', '.HDF', '.PNG', '.APNG', '.JP2', '.J2K', '.JPC', '.JPF', '.JPX', '.J2C', 
'.ICNS', '.ICO', '.IM', '.IIM', '.TIF', '.TIFF', '.JFIF', '.JPE', '.JPG', '.JPEG', '.MPG', 
'.MPEG', '.MSP', '.PCD', '.PXR', '.PBM', '.PGM', '.PPM', '.PNM', '.PSD', '.BW', '.RGB', 
'.RGBA', '.SGI', '.RAS', '.TGA', '.ICB', '.VDA', '.VST', '.WEBP', '.WMF', '.EMF', '.XBM', 
'.XPM', '.aiff', '.au', '.avr', '.caf', '.flac', '.htk', '.svx', '.mat4', '.mat5', '.mpc2k',
 '.ogg', '.paf', '.pvf', '.raw', '.rf64', '.sd2', '.sds', '.ircam', '.voc', '.w64', '.wav', 
'.nist', '.wavex', '.wve', '.xi', '.mp3', '.opus', '.AIFF', '.AU', '.AVR', '.CAF', '.FLAC', 
'.HTK', '.SVX', '.MAT4', '.MAT5', '.MPC2K', '.OGG', '.PAF', '.PVF', '.RAW', '.RF64', '.SD2', 
'.SDS', '.IRCAM', '.VOC', '.W64', '.WAV', '.NIST', '.WAVEX', '.WVE', '.XI', '.MP3', '.OPUS', 
'.zip']
</code></pre>
<p>It seems like the code is trying to find the SQuAD dataset in my local directory instead of downloading it. I am not sure why this is happening or how to fix it.</p>
<p>How can I correctly load the SQuAD dataset using the datasets library without encountering this FileNotFoundError? What steps do I need to take to resolve this issue?</p>
<ul>
<li>Python version: 3.7</li>
<li>datasets library version: 2.13.2</li>
</ul>
<p>It runs successfully on Colab but not success on autodl cloud GPU platforms. It doesn't seem to be an internet connectivity issue; rather, it appears that some required files or configurations are not being found. And I tried Glue datasets, <code>load_dataset()</code> could find the link and give me the error massage, but squad just give me they could not find files. I don't know the reason.</p>
","python, nlp, huggingface, huggingface-datasets",
English grammar for parsing in NLTK,"<p>Is there a ready-to-use English grammar that I can just load it and use in NLTK? I've searched around examples of parsing with NLTK, but it seems like that I have to manually specify grammar before parsing a sentence. </p>

<p>Thanks a lot!</p>
","python, nlp, grammar, nltk",
Extract Keywords from Text Vector -- one set of keyworks for each element,"<p>Please consider the reprex at the end of the post.
It works along the lines of</p>
<p><a href=""https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-postagging-lemmatisation.html"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-postagging-lemmatisation.html</a></p>
<p>It extracts a set of keywords from a text vector &quot;texts&quot;, but I would like to have
4 sets of keywords,i.e. one set of keywords for each for each element of &quot;texts&quot;.</p>
<p>I added another section to the reprex where I try using map after splitting the initial dataset and converting it to a list. I do not get any error messages, but it clearly does not work.</p>
<p>Any suggestion is appreciated.</p>
<p>Many thanks!</p>
<pre class=""lang-r prettyprint-override""><code># Load required libraries
library(udpipe)
library(tidyverse)
library(lattice)

## see https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-postagging-lemmatisation.html

# Download and load the model
model &lt;- udpipe_download_model(language = &quot;english&quot;)
#&gt; Downloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/english-ewt-ud-2.5-191206.udpipe to /home/lorenzo/mega_pcloud/work/web-intelligence/english-ewt-ud-2.5-191206.udpipe
#&gt;  - This model has been trained on version 2.5 of data from https://universaldependencies.org
#&gt;  - The model is distributed under the CC-BY-SA-NC license: https://creativecommons.org/licenses/by-nc-sa/4.0
#&gt;  - Visit https://github.com/jwijffels/udpipe.models.ud.2.5 for model license details.
#&gt;  - For a list of all models and their licenses (most models you can download with this package have either a CC-BY-SA or a CC-BY-SA-NC license) read the documentation at ?udpipe_download_model. For building your own models: visit the documentation by typing vignette('udpipe-train', package = 'udpipe')
#&gt; Downloading finished, model stored at '/home/lorenzo/mega_pcloud/work/web-intelligence/english-ewt-ud-2.5-191206.udpipe'
ud_model &lt;- udpipe_load_model(model$file_model)

# Sample text vector with longer text elements
texts &lt;- c(
  &quot;The first document contains a lot of information about natural language processing. It discusses various methods such as tokenization, stemming, and lemmatization. Additionally, it covers advanced topics like named entity recognition and sentiment analysis. The document emphasizes the importance of understanding linguistic structures and nuances. It also touches upon the challenges and limitations of current NLP technologies.&quot;,
  &quot;In the second document, we delve deeper into machine learning techniques applied to text data. This includes algorithms like support vector machines, naive Bayes, and deep learning models. The document also explains feature extraction and vectorization techniques. Furthermore, it provides a comprehensive overview of hyperparameter tuning and model evaluation metrics. The goal is to highlight the critical aspects of building robust and efficient text classifiers.&quot;,
  &quot;The third document focuses on practical applications of text mining. It provides examples of text classification, clustering, and topic modeling. Moreover, it highlights the importance of preprocessing steps like removing stopwords and normalizing text. The document also explores various use cases in different industries, demonstrating the versatility of text mining techniques. Additionally, it discusses future trends and emerging technologies in the field.&quot;,
  &quot;Finally, the fourth document reviews different tools and libraries available for text analysis. It mentions popular libraries such as NLTK, spaCy, and gensim. The document also includes comparisons between these tools in terms of performance and ease of use. It aims to guide readers in selecting the most appropriate tools for their specific needs. Additionally, it provides best practices for integrating these tools into existing workflows and systems.&quot;
)

# Create a data frame
text_df &lt;- tibble(doc_id = 1:length(texts), text = texts)

# Annotate the texts
 x &lt;- udpipe_annotate(ud_model, x = text_df$text) |&gt;
    as_tibble()

 x
#&gt; # A tibble: 288 × 14
#&gt;    doc_id paragraph_id sentence_id sentence     token_id token lemma upos  xpos 
#&gt;    &lt;chr&gt;         &lt;int&gt;       &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;
#&gt;  1 doc1              1           1 The first d… 1        The   the   DET   DT   
#&gt;  2 doc1              1           1 The first d… 2        first first ADJ   JJ   
#&gt;  3 doc1              1           1 The first d… 3        docu… docu… NOUN  NN   
#&gt;  4 doc1              1           1 The first d… 4        cont… cont… VERB  VBZ  
#&gt;  5 doc1              1           1 The first d… 5        a     a     DET   DT   
#&gt;  6 doc1              1           1 The first d… 6        lot   lot   NOUN  NN   
#&gt;  7 doc1              1           1 The first d… 7        of    of    ADP   IN   
#&gt;  8 doc1              1           1 The first d… 8        info… info… NOUN  NN   
#&gt;  9 doc1              1           1 The first d… 9        about about ADP   IN   
#&gt; 10 doc1              1           1 The first d… 10       natu… natu… ADJ   JJ   
#&gt; # ℹ 278 more rows
#&gt; # ℹ 5 more variables: feats &lt;chr&gt;, head_token_id &lt;chr&gt;, dep_rel &lt;chr&gt;,
#&gt; #   deps &lt;chr&gt;, misc &lt;chr&gt;

stats &lt;- keywords_rake(x = x, term = &quot;lemma&quot;, group = &quot;doc_id&quot;, 
                       relevant = x$upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;))

stats$key &lt;- factor(stats$keyword, levels = rev(stats$keyword))

barchart(key ~ rake, data = stats 
       , col = &quot;cadetblue&quot;, 
         main = &quot;Keywords identified by RAKE&quot;, 
         xlab = &quot;Rake&quot;)
</code></pre>
<p><img src=""https://i.imgur.com/w6Ow1t6.png"" alt="""" /></p>
<pre class=""lang-r prettyprint-override""><code>
################################################################


text_df_split &lt;- text_df |&gt;
    group_by(doc_id) |&gt;
    group_split()



 x_split &lt;- map(text_df_split, \(x) udpipe_annotate(ud_model, x$text) |&gt; as_tibble() )

## let us see what x looks like

x |&gt; glimpse()
#&gt; Rows: 288
#&gt; Columns: 14
#&gt; $ doc_id        &lt;chr&gt; &quot;doc1&quot;, &quot;doc1&quot;, &quot;doc1&quot;, &quot;doc1&quot;, &quot;doc1&quot;, &quot;doc1&quot;, &quot;doc1&quot;, …
#&gt; $ paragraph_id  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
#&gt; $ sentence_id   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,…
#&gt; $ sentence      &lt;chr&gt; &quot;The first document contains a lot of information about …
#&gt; $ token_id      &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;,…
#&gt; $ token         &lt;chr&gt; &quot;The&quot;, &quot;first&quot;, &quot;document&quot;, &quot;contains&quot;, &quot;a&quot;, &quot;lot&quot;, &quot;of&quot;…
#&gt; $ lemma         &lt;chr&gt; &quot;the&quot;, &quot;first&quot;, &quot;document&quot;, &quot;contain&quot;, &quot;a&quot;, &quot;lot&quot;, &quot;of&quot;,…
#&gt; $ upos          &lt;chr&gt; &quot;DET&quot;, &quot;ADJ&quot;, &quot;NOUN&quot;, &quot;VERB&quot;, &quot;DET&quot;, &quot;NOUN&quot;, &quot;ADP&quot;, &quot;NOU…
#&gt; $ xpos          &lt;chr&gt; &quot;DT&quot;, &quot;JJ&quot;, &quot;NN&quot;, &quot;VBZ&quot;, &quot;DT&quot;, &quot;NN&quot;, &quot;IN&quot;, &quot;NN&quot;, &quot;IN&quot;, &quot;…
#&gt; $ feats         &lt;chr&gt; &quot;Definite=Def|PronType=Art&quot;, &quot;Degree=Pos|NumType=Ord&quot;, &quot;…
#&gt; $ head_token_id &lt;chr&gt; &quot;3&quot;, &quot;3&quot;, &quot;4&quot;, &quot;0&quot;, &quot;6&quot;, &quot;4&quot;, &quot;8&quot;, &quot;6&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;…
#&gt; $ dep_rel       &lt;chr&gt; &quot;det&quot;, &quot;amod&quot;, &quot;nsubj&quot;, &quot;root&quot;, &quot;det&quot;, &quot;obj&quot;, &quot;case&quot;, &quot;n…
#&gt; $ deps          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …
#&gt; $ misc          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;SpaceAfter=…

## and what my second element in the list looks like

x_split[[2]] |&gt; glimpse()
#&gt; Rows: 74
#&gt; Columns: 14
#&gt; $ doc_id        &lt;chr&gt; &quot;doc1&quot;, &quot;doc1&quot;, &quot;doc1&quot;, &quot;doc1&quot;, &quot;doc1&quot;, &quot;doc1&quot;, &quot;doc1&quot;, …
#&gt; $ paragraph_id  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
#&gt; $ sentence_id   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,…
#&gt; $ sentence      &lt;chr&gt; &quot;In the second document, we delve deeper into machine le…
#&gt; $ token_id      &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;,…
#&gt; $ token         &lt;chr&gt; &quot;In&quot;, &quot;the&quot;, &quot;second&quot;, &quot;document&quot;, &quot;,&quot;, &quot;we&quot;, &quot;delve&quot;, &quot;…
#&gt; $ lemma         &lt;chr&gt; &quot;in&quot;, &quot;the&quot;, &quot;second&quot;, &quot;document&quot;, &quot;,&quot;, &quot;we&quot;, &quot;delve&quot;, &quot;…
#&gt; $ upos          &lt;chr&gt; &quot;ADP&quot;, &quot;DET&quot;, &quot;ADJ&quot;, &quot;NOUN&quot;, &quot;PUNCT&quot;, &quot;PRON&quot;, &quot;VERB&quot;, &quot;A…
#&gt; $ xpos          &lt;chr&gt; &quot;IN&quot;, &quot;DT&quot;, &quot;JJ&quot;, &quot;NN&quot;, &quot;,&quot;, &quot;PRP&quot;, &quot;VBP&quot;, &quot;RB&quot;, &quot;IN&quot;, &quot;…
#&gt; $ feats         &lt;chr&gt; NA, &quot;Definite=Def|PronType=Art&quot;, &quot;Degree=Pos|NumType=Ord…
#&gt; $ head_token_id &lt;chr&gt; &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;7&quot;, &quot;7&quot;, &quot;7&quot;, &quot;0&quot;, &quot;7&quot;, &quot;12&quot;, &quot;11&quot;, &quot;12&quot;…
#&gt; $ dep_rel       &lt;chr&gt; &quot;case&quot;, &quot;det&quot;, &quot;amod&quot;, &quot;obl&quot;, &quot;punct&quot;, &quot;nsubj&quot;, &quot;root&quot;, …
#&gt; $ deps          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …
#&gt; $ misc          &lt;chr&gt; NA, NA, NA, &quot;SpaceAfter=No&quot;, NA, NA, NA, NA, NA, NA, NA,…

## it looks like they have the same structure,
## but when I try using keywords_rake on it


stats_split_2 &lt;- keywords_rake(x = x_split[[2]], term = &quot;lemma&quot;, group = &quot;doc_id&quot;, 
                       relevant = x_split[[2]]$upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;))


## I do not get an error, but see what happens

stats_split_2|&gt;glimpse()
#&gt; Rows: 0
#&gt; Columns: 4
#&gt; $ keyword &lt;chr&gt; 
#&gt; $ ngram   &lt;int&gt; 
#&gt; $ freq    &lt;int&gt; 
#&gt; $ rake    &lt;dbl&gt;
</code></pre>
<p><sup>Created on 2024-06-19 with <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex v2.1.0</a></sup></p>
","r, nlp, text-mining, udpipe",
Generate representative boolean queries from source text?,"<p>Within conventional information retrieval systems, we use a boolean query to generate a result set that matches that query.</p>
<p>What does one call the 'task', where, given a known set of similar documents, determine the boolean query that best represents it?</p>
<p>The main issue I encounter right now is I don't know what this problem is called, so I am finding it difficult to discover existing literature and research on the topic.</p>
","nlp, information-retrieval",
what is the difference between bigram and unigram text features extraction,"<p>I searched online to do bi-gram and unigram text features' extraction, but still didn't find something useful information, can someone tell me what is the difference between them?</p>

<p>For example, if I have a text ""I have a lovely dog""
what will happen if I use bi-gram way to do features extraction and to do unigram extraction?</p>
","machine-learning, nlp","<p>We are trying to teach machine how to do natural language processing. We human can understand language easily but machines cannot so we trying to teach them specific pattern of language. As specific word has meaning but when we combine the words(i.e group of words) than it will be more helpful to understand the meaning.</p>
<p>n-gram is basically set of occurring words within given window so when</p>
<ul>
<li><p>n=1 it is Unigram</p>
</li>
<li><p>n=2 it is bigram</p>
</li>
<li><p>n=3 it is trigram and so on</p>
</li>
</ul>
<p>Now suppose machine try to understand the meaning of sentence &quot;I have a lovely dog&quot; then it will split sentences into a specific chunk.</p>
<ol>
<li><p>It will consider word one by one which is unigram so each word will be a gram.</p>
<p>&quot;I&quot;, &quot;have&quot;, &quot;a&quot; , &quot;lovely&quot; , &quot;dog&quot;</p>
</li>
<li><p>It will consider two words at a time so it will be bigram so each two adjacent words will be bigram</p>
<p>&quot;I have&quot; , &quot;have a&quot; , &quot;a lovely&quot; , &quot;lovely dog&quot;</p>
</li>
</ol>
<p>So like this machine will split sentences into small group of words to understand its meaning</p>
"
Accuracy_score with same value in different classifiers methods,"<p>I'm doing a project, on Google Colab, for fake news classification with LIAR dataset. I am running with three differents features extractors (TF-IDF, DistilBERT and LLAMA 2) and seven classifiers (Logistic Regression, SVM, Naive Bayes, KNN, Random Forest, AdaBoost and XGBoost). However, many tests gave the same result, even though they used different algorithms. Below, the values ​​found for the average accuracy entered are listed with the standard deviation in parentheses. The cells in orange are those that showed the same result (64.15 (2.22 x 10^-16)).
<a href=""https://i.sstatic.net/jO4hk0Fd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jO4hk0Fd.png"" alt=""enter image description here"" /></a></p>
<p>This should not be happening, considering that, in addition to being different methods, I am running each experiment 30 times, varying the training and testing basis in each run. Below, an excerpt of the code, with Naive Bayes:</p>
<pre><code>model_nb = GaussianNB()
grid_nb = GridSearchCV(model_nb, parameters_nb)
for i in range(30):
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, stratify = Y)
    fit_model_nb = grid_nb.fit(X_train, y_train)
    pred_nb = fit_model_nb.predict(X_test)
    predictions_df = pd.DataFrame({'Predicted_Label': pred_nb, &quot;True&quot;: y_test})

    # Nomeie o arquivo com um identificador único (por exemplo, o número da iteração)
    filename = f'nb_predictions_{i}.csv'

    # Salve o DataFrame em um arquivo CSV com o nome único
    predictions_df.to_csv(filename, index=False)
</code></pre>
<p>I carried out other tests with other bases, such as FakeNewsNet and KaggleFN, and this problem did not occur there. One consideration to make is that I needed to adjust the LIAR labels to be a base of binary classes, so the six classes became just two (true or false), as follows:
'half-true'-&gt; 'false', 'barely-true'-&gt; 'false', 'pants-fire'-&gt; 'false', 'mostly-true'-&gt; 'true', 'true'-&gt; 'true', 'false'-&gt; 'false'
But I believe this should not impact the result.</p>
","python, text, nlp, classification",
NLTK RegEx Chunker not capturing defined grammar patterns with wildcards,"<p>I am trying to chunk a sentence using NLTK's POS tags as regular expressions. 2 rules are defined to identify phrases, based on the tags of words in the sentence.</p>
<p>Mainly, I wanted to capture the chunk of <strong>one or more verbs followed by an optional determiner and then one or more nouns at the end</strong>. This is the first rule in definition. But it is not getting captured as Phrase Chunk.</p>
<pre><code>import nltk

## Defining the POS tagger 
tagger = nltk.data.load(nltk.tag._POS_TAGGER)


## A Single sentence - input text value
textv=&quot;This has allowed the device to start, and I then see glitches which is not nice.&quot;
tagged_text = tagger.tag(textv.split())

## Defining Grammar rules for  Phrases
actphgrammar = r&quot;&quot;&quot;
     Ph: {&lt;VB*&gt;+&lt;DT&gt;?&lt;NN*&gt;+}  # verbal phrase - one or more verbs followed by optional determiner, and one or more nouns at the end
     {&lt;RB*&gt;&lt;VB*|JJ*|NN*\$&gt;} # Adverbial phrase - Adverb followed by adjective / Noun or Verb
     &quot;&quot;&quot;

### Parsing the defined grammar for  phrases
actp = nltk.RegexpParser(actphgrammar)

actphrases = actp.parse(tagged_text)
</code></pre>
<p>The input to the chunker, tagged_text is as below.</p>
<blockquote>
<p>tagged_text
Out[7]:
[('This', 'DT'),
('has', 'VBZ'),
('allowed', 'VBN'),
('the', 'DT'),
('device', 'NN'),
('to', 'TO'),
('start,', 'NNP'),
('and', 'CC'),
('I', 'PRP'),
('then', 'RB'),
('see', 'VB'),
('glitches', 'NNS'),
('which', 'WDT'),
('is', 'VBZ'),
('not', 'RB'),
('nice.', 'NNP')]</p>
</blockquote>
<p>In the final output, only the adverbial phrase ('<strong>then see</strong>'), that is matching the second rule is being captured.
I expected the verbal phrase ('<strong>allowed the device</strong>') to match with the first rule and get captured as well, but its not.</p>
<blockquote>
<p>actphrases Out[8]: Tree('S', [('This', 'DT'), ('has', 'VBZ'),
('allowed', 'VBN'), ('the', 'DT'), ('device', 'NN'), ('to', 'TO'),
('start,', 'NNP'), ('and', 'CC'), ('I', 'PRP'), <em><strong>Tree('Ph', [('then',
'RB'), ('see', 'VB')])</strong></em>, ('glitches', 'NNS'), ('which', 'WDT'), ('is',
'VBZ'), ('not', 'RB'), ('nice.', 'NNP')])</p>
</blockquote>
<p>NLTK version used is 2.0.5 (Python 2.7)
Any help or suggestion would be greatly appreciated.</p>
","python, regex, nlp, nltk, text-chunking","<p>Close but minor changes to your regex will get you your desired output. When you want to get a wildcard using <code>RegexpParser</code> grammar, you should use <code>.*</code> instead of <code>*</code>, e.g. <code>VB.*</code> instead of <code>VB*</code>:</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize, pos_tag, RegexpParser
&gt;&gt;&gt; text = ""This has allowed the device to start, and I then see glitches which is not nice.""
&gt;&gt;&gt; tagged_text = pos_tag(word_tokenize(text))    
&gt;&gt;&gt; g = r""""""
... VP: {&lt;VB.*&gt;&lt;DT&gt;&lt;NN.*&gt;}
... """"""
&gt;&gt;&gt; p = RegexpParser(g); p.parse(tagged_text)
Tree('S', [('This', 'DT'), ('has', 'VBZ'), Tree('VP', [('allowed', 'VBN'), ('the', 'DT'), ('device', 'NN')]), ('to', 'TO'), ('start', 'VB'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('then', 'RB'), ('see', 'VBP'), ('glitches', 'NNS'), ('which', 'WDT'), ('is', 'VBZ'), ('not', 'RB'), ('nice', 'JJ'), ('.', '.')])
</code></pre>

<p>Note that you're catching the <code>Tree(AdvP, [('then', 'RB'), ('see', 'VB')])</code>,  because the tags are exactly <code>RB</code> and <code>VB</code>. So the wildcard in your grammar (i.e. `""""""AdvP: {}"""""") in this scenario is ignored.</p>

<p>Also, if it's two different types of phrases, it's more advisable to use 2 labels not one. And (i think) end of string after wildcard is sort of redundant, so it's better to:</p>

<pre><code>g = r""""""
VP:{&lt;VB.*&gt;&lt;DT&gt;&lt;NN.*&gt;} 
AdvP: {&lt;RB.*&gt;&lt;VB.*|JJ.*|NN.*&gt;}
""""""
</code></pre>
"
Classify data on unstructured texts using python,"<p>I'm going to give an introduction to the project surroundings so you have some context for helping me out.</p>
<p>I'm trying to parse out information of german organizational charts in pdf format. Right now I can reliably read out the content of the rectangles from the PDF and get all textblocks that are contained in that rectangle. For each rectangle, I want to sort the information inside it into several categories. These are:</p>
<ul>
<li><p>Type (the type of the suborganization. e.g. Referat)</p>
</li>
<li><p>Title (the specific name of the suborganization. e.g. Referat XI A 9)</p>
</li>
<li><p>Description (the responsibilities. e.g. Personalien der Zollverwaltung und der Bundesmonopolverwaltung für Branntwein)</p>
</li>
<li><p>Person in charge (e.g. MR Hoffmann)</p>
</li>
</ul>
<p>I looked into several different approaches using python but I couldn't find a fitting solution. With spacy I found it hard to get any meaningful results because the text inside a rectangle isn't a real sentence and, as far as I understand, spacy relies on identifying the part-of-speech a token has in a sentence.<br />
I read into some other approaches using bag-of-words for example but this also doesn't seem to make sense because I again don't have full sentences. I'm currently implementing a really large but simple pattern matching solution where I check for keywords like &quot;Referat&quot; or the prefix of a person &quot;MR&quot; and categorize based on that. This regex-approach obviously fails pretty quickly as soon as I don't have a specified prefix or keyword to search for in the given text.</p>
<p>I also tried taking into account at which positions the textblocks are placed inside the content node and parse the information based on that. But this approach fails as soon as the order differs between the pdfs.</p>
<p>I'm looking for general advice that points me to a solution for this problem using python. The questions regarding this topic seems to be very old or focus only on a subset of the problems that I have so I think it would be helpful to have another discussion.</p>
","python, nlp, pattern-matching, spacy, text-classification",
How to handle the loss function decrease at some points but bounce back to keep increasing when fine-tune LLaMA?,"<p><strong>Preface</strong></p>
<p>I am new to fine-tuning an LLM model on binary classification tasks. I have tried fine-tuning LLaMA 2 and 3 with the causal inference task (next token prediction) with PEFT LoRA and 4-bit quantization. The task that I have been trained on is using various prompts.</p>
<ol>
<li><code>${Text}. ${Question}. ${Answer (Positive class or negative class)}</code></li>
<li><code>${Some context}. ${Question}. ${Answer (Positive class or negative class)}</code></li>
</ol>
<p>The <a href=""https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments"" rel=""nofollow noreferrer"">training arguments</a> are here:</p>
<pre><code>per_device_train_batch_size = 8
gradient_accumulation_steps = 4
optim = &quot;paged_adamw_32bit&quot;
save_steps = 100
logging_steps = 10
learning_rate = 1e-4
max_grad_norm = 0.3 # I have tried to use default max_grad_norm
max_steps = 1000
warmup_ratio = 0.03 # I have tried to use default warmup_ratio
lr_scheduler_type = &quot;cosine_with_restarts&quot; # I have tried it using &quot;cosine&quot; too
</code></pre>
<p>Here is the <a href=""https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer"" rel=""nofollow noreferrer"">trainer</a>:</p>
<pre><code>trainer = SFTTrainer(
    model=model,
    args=training_arguments,
    train_dataset=dataset,
    packing=True,
    dataset_text_field=&quot;id&quot;,
    tokenizer=tokenizer,
    max_seq_length=1024,
    formatting_func=formatting_func,
)
</code></pre>
<p><strong>Issues and Questions</strong></p>
<p>The issue comes within the results. The loss function decreases after some steps, but after particular steps, the loss function keeps increasing (I supposed that LLaMA used cross entropy as the loss function). Is the issue within the LLM that I used (LLaMA) or what may cause the problem? and how to solve it.</p>
<p><a href=""https://i.sstatic.net/yrpuTu10m.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yrpuTu10m.png"" alt=""enter image description here"" /></a></p>
","python, pytorch, nlp, huggingface-transformers, large-language-model",
How can I prevent the benepar parser from splitting a specific substring when parsing a string?,"<p>I use the <a href=""https://github.com/nikitakit/self-attentive-parser"" rel=""nofollow noreferrer"">benepar parser</a> to parse sentences into trees. How can I prevent the benepar parser from splitting a specific substring when parsing a string?</p>
<p>E.g., the token <code>gonna</code> is split by benepar into two tokens <code>gon</code> and <code>na</code>, which I don't want.</p>
<hr />
<p>Code example, with pre-requisites:</p>
<pre><code>pip install spacy benepar
python -m nltk.downloader punkt benepar_en3
python -m spacy download en_core_web_md
</code></pre>
<p>If I run:</p>
<pre class=""lang-py prettyprint-override""><code>import benepar, spacy
import nltk
benepar.download('benepar_en3')
nlp = spacy.load('en_core_web_md')
if spacy.__version__.startswith('2'):
    nlp.add_pipe(benepar.BeneparComponent(&quot;benepar_en3&quot;))
else:
    nlp.add_pipe(&quot;benepar&quot;, config={&quot;model&quot;: &quot;benepar_en3&quot;})
doc = nlp(&quot;This is gonna be fun.&quot;)
sent = list(doc.sents)[0]
print(sent._.parse_string)
</code></pre>
<p>It'll output:</p>
<pre><code>(S (NP (DT This)) (VP (VBZ is) (VP (TO gon) (VP (TO na) (VP (VB be) (NP (NN fun)))))) (. .))
</code></pre>
<p>The issue is that the token <code>gonna</code> is split into two tokens <code>gon</code> and <code>na</code>. How can I prevent that?</p>
","python, nlp, tokenize, parse-tree, benepar","<p>Use <code>nlp.tokenizer.add_special_case</code>:</p>
<pre><code>import benepar, spacy
import nltk
benepar.download('benepar_en3')
nlp = spacy.load('en_core_web_md')
from spacy.symbols import ORTH
nlp.tokenizer.add_special_case(u'gonna', [{ORTH: u'gonna'}])
if spacy.__version__.startswith('2'):
    nlp.add_pipe(benepar.BeneparComponent(&quot;benepar_en3&quot;))
else:
    nlp.add_pipe(&quot;benepar&quot;, config={&quot;model&quot;: &quot;benepar_en3&quot;})
doc = nlp(&quot;This is gonna be fun.&quot;)
sent = list(doc.sents)[0]
print(sent._.parse_string)
</code></pre>
<p>This is the output for the above code:</p>
<pre><code>(S (NP (DT This)) (VP (VBZ is) (VP (TO gonna) (VP (VB be) (NP (NN fun))))) (. .))
</code></pre>
"
Fine-tuned LLaMA-2-Chat-HF Model Generates Same Responses as Pre-trained Model and Suitability for Retrieval-based Task,"<p>I am working on building a chatbot for substance abuse support. My approach involves two main steps:</p>
<ul>
<li>Fine-tuning the LLaMA-2-Chat-HF model: I have fine-tuned the LLaMA-2-Chat-HF model using a dataset of mental health conversations. The dataset was transformed into an instruction template format before fine-tuning.</li>
<li>Retrieval-based system: I am using a retrieval-based system to fetch information from a vector database that contains a textbook on substance abuse support (theory and practice).</li>
</ul>
<p>After fine-tuning, I am using the fine-tuned model to generate reponses using the context/information retrieved from the vector database. The process involves passing both the context and the user query into a prompt template, and then generating answers using an LLM chain. However, I am encountering the following issues:</p>
<p>Both the fine-tuned model and the pre-trained model are generating the exact same responses when queried, despite using the context from the vector database.</p>
<p>My Questions:
Why might the fine-tuned LLaMA-2-Chat-HF model be generating the same responses as the pre-trained model? What could be causing this issue?
Is the fine-tuned LLaMA-2-Chat-HF model suitable for a retrieval-based task? If not, what adjustments or different approaches should I consider?</p>
<p>Any insights, suggestions, or alternative approaches would be greatly appreciated.</p>
<p>Code for retrieval using the fine tuned model:</p>
<pre><code>DB_FAISS_PATH = 'vectorstores/db_faiss'

custom_prompt_template = &quot;&quot;&quot;Use the following pieces of information to answer the user's question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context:{context}
Question:{question}

Only return the helpful answer below and nothing else.
Helpful answer:
&quot;&quot;&quot;

def set_custom_prompt():
     &quot;&quot;&quot;
     Prompt template for QA retrieval for each vector store
     &quot;&quot;&quot;
     prompt = PromptTemplate(template=custom_prompt_template, input_variables=['context', 'question'])
     return prompt

def create_llm(model, tokenizer):

     text_generation_pipeline = pipeline(&quot;text-generation&quot;, model=model, tokenizer=tokenizer)

     llm = HuggingFacePipeline(pipeline=text_generation_pipeline)
     return llm

def retrieval_qa_chain(llm, prompt, db):
     qa_chain = RetrievalQA.from_chain_type(
         llm=llm,
         chain_type='stuff',
         retriever=db.as_retriever(search_kwargs={'k': 2}),
         return_source_documents=True,
         chain_type_kwargs={'prompt': prompt}
     )
     return qa_chain

def qa_bot(model, tokenizer):
     embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
     db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
     llm = create_llm(model, tokenizer)
     qa_prompt = set_custom_prompt()
     qa = retrieval_qa_chain(llm, qa_prompt, db)
     return qa

def final_result(query, model, tokenizer):
     qa = qa_bot(model, tokenizer)

     qa_result = qa({'query': query})

     response = qa_result['result']

     helpful_answer = response.split('Helpful answer:')[-1].strip()

     return helpful_answer.strip()
</code></pre>
","nlp, chatbot, large-language-model, llama, fine-tuning",
Understanding the results of Transformers Learn In Context with Gradient Descent,"<p>I'm trying to implement this paper:
<a href=""https://arxiv.org/pdf/2212.07677"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2212.07677</a></p>
<p>(Here's their code):
<a href=""https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd"" rel=""nofollow noreferrer"">https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd</a></p>
<p>I'm struggling to match their experimental results. Specifically, on their simplest GD model (a single layer with a single head and no softmax), they obtain a constant low loss of roughly 0.20 on their test data. I don't quite understand why this is the case, conceptually.</p>
<p>As I understand it, this model only does a single iteration of gradient descent on the data, so why would it reach such a low loss? And why would the loss be constant/near constant over training steps? Aren't we training the learning rate in the GD model?</p>
","machine-learning, nlp, large-language-model, transformer-model, meta-learning","<p>What data are you using in your replication? As far as I can tell this paper does not mention explicitly the parameters of the data used for the particular result you are trying to replicate. Indeed, it tests a variety of alpha values for the distributions used in figure 6. It is feasible for the loss to be low even after one step of GD if the alpha value is low. If you find the same trends in relative behavior of GD and transformer layers, I don't think it's important to match the exact loss values.</p>
"
ImportError and TypeError Issues in Nougat OCR with BARTDecoder and cached_property,"<p>I'm facing issues while running an OCR process using Nougat with two different errors for two different users. The errors are related to importing <code>cached_property</code> and an unexpected keyword argument <code>cache_position</code>. Below are the full error messages for both cases:</p>
<p><strong>Error 1: ImportError for <code>cached_property</code></strong></p>
<pre><code>ImportError: cannot import name 'cached_property' from 'nougat.utils' (/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/utils/__init__.py)
OCRing with base model failed on /lfs/skampere1/0/emilyhyf/massive-evaporation-4-math/data/debug_pdfs/dummy_folder1/The_IMO_Compendium.pdf... trying small model
Traceback (most recent call last):
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/bin/nougat&quot;, line 5, in &lt;module&gt;
    from predict import main
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/predict.py&quot;, line 18, in &lt;module&gt;
    from nougat import NougatModel
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/__init__.py&quot;, line 1, in &lt;module&gt;
    from nougat.app import Nougat
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/app.py&quot;, line 5, in &lt;module&gt;
    from nougat.asgi import serve
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/asgi.py&quot;, line 6, in &lt;module&gt;
    from nougat.context.request import Request
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/context/__init__.py&quot;, line 1, in &lt;module&gt;
    from nougat.context.request import Request
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/context/request.py&quot;, line 8, in &lt;module&gt;
    from nougat.utils import cached_property, File
ImportError: cannot import name 'cached_property' from 'nougat.utils' (/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/utils/__init__.py)
OCRing with small model failed on /lfs/skampere1/0/emilyhyf/massive-evaporation-4-math/data/debug_pdfs/dummy_folder1/The_IMO_Compendium.pdf
The following files failed OCRing and need manual review:
/lfs/skampere1/0/emilyhyf/massive-evaporation-4-math/data/debug_pdfs/putnam_and_beyond.pdf
/lfs/skampere1/0/emilyhyf/massive-evaporation-4-math/data/debug_pdfs/dummy_folder1/The_IMO_Compendium.pdf
Time taken: 8.497612476348877 seconds, 0.1416268746058146 minutes, 0.00236044791009691 hours
</code></pre>
<p><strong>Error 2: TypeError for <code>cache_position</code> in BARTDecoder</strong></p>
<pre><code>TypeError: BARTDecoder.prepare_inputs_for_inference() got an unexpected keyword argument 'cache_position'
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
OCRing with base model failed on /lfs/skampere1/0/naveenkc/contest_scraper/test_nougat/731991.pdf... trying small model
/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined]
  0%|                                                 | 0/288 [00:24&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/bin/nougat&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
             ^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/predict.py&quot;, line 167, in main
    model_output = model.inference(
                   ^^^^^^^^^^^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/nougat/model.py&quot;, line 592, in inference
    decoder_output = self.decoder.model.generate(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/transformers/generation/utils.py&quot;, line 1758, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/transformers/generation/utils.py&quot;, line 2394, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: BARTDecoder.prepare_inputs_for_inference() got an unexpected keyword argument 'cache_position'
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
OCRing with small model failed on /lfs/skampere1/0/naveenkc/contest_scraper/test_nougat/731991.pdf
The following files failed OCRing and need manual review:
/lfs/skampere1/0/naveenkc/contest_scraper/test_nougat/731991.pdf
Time taken: 65.70022702217102 seconds, 1.0950037837028503 minutes, 0.018250063061714172 hours
</code></pre>
<p><strong>Environment:</strong></p>
<ul>
<li>Python 3.12</li>
<li>Nougat</li>
<li>Torch</li>
<li>Transformers</li>
</ul>
<p><strong>Questions:</strong></p>
<ol>
<li><p><strong>ImportError</strong>: How can I resolve the <code>ImportError: cannot import name 'cached_property' from 'nougat.utils'</code>? What could be causing this issue, and is there an alternative approach to import <code>cached_property</code>?</p>
</li>
<li><p><strong>TypeError</strong>: How can I address the <code>TypeError: BARTDecoder.prepare_inputs_for_inference() got an unexpected keyword argument 'cache_position'</code>? Is this due to a version mismatch between <code>transformers</code> and <code>torch</code>? How can I ensure compatibility?</p>
</li>
</ol>
<p>Any guidance on these issues would be greatly appreciated! Thank you in advance!</p>
","machine-learning, pytorch, nlp, huggingface-transformers, huggingface",
Syuzhet not accepting UTF-8 characters?,"<p>Morning all,</p>
<p>I am trying to conduct sentiment analysis on a large Swedish Twitter dataset with a custom lexicon specifically for Swedish research. However, even if the text I am wanting to analyse and the lexicon are encoded as UTF-8, the syuzhet package completely ignores words with the Swedish å ä and ö characters. Is this an inherent issue with the package (i.e. I need to recode the charachters in the data + lexicon, or am I missing something glaringly obvious?</p>
<p>I tried the following dummy, and evidently the get_sentiment function does not accept the Swedish characters.</p>
<pre><code>test &lt;- c(&quot;kärnvapen&quot;, &quot;kärnkraftsolycka&quot;, &quot;glad&quot;, &quot;körd&quot;, &quot;kåk&quot;, &quot;ledsen&quot;)

SWE_lexicon &lt;- read.delim(&quot;sensaldo-base-v02.txt&quot;, header=FALSE, encoding = &quot;UTF-8&quot;)
colnames(SWE_lexicon)[colnames(SWE_lexicon) == 'V1'] &lt;- 'word'
colnames(SWE_lexicon)[colnames(SWE_lexicon) == 'V2'] &lt;- 'value'

score &lt;- get_sentiment(test, method=&quot;custom&quot;, lexicon = SWE_lexicon)

head(score)
[1]  0  0  1  0  0 -1
</code></pre>
<p>When recoding the charachters into e.g. ae for ä it assigned the correct values to the word, which is:</p>
<pre><code>[1]  0  -1  1  -1  0 -1
</code></pre>
","r, twitter, nlp, sentiment-analysis, lexicon",
Training Fastconformer-CTC on Kazakh Language,"<p>community. I just started working with NeMo. So, I want to train ASR based on Fastconformer-CTC with 100 hours of recorded calls in kazakh language. What type of training will be better for my situation, fine-tuning some pre-trained model or train it from scratch? Also can someone give info about the data processing and manifest?</p>
<p>Suggestions about Fastconformer-CTC</p>
","nlp, speech-recognition, speech-to-text",
How to remove words from a sentence that carry no positive or negative sentiment?,"<p>Im trying a sentiment analysis based approach on youtube comments, but the comments many times have words like mrbeast, tiger/'s, lion/'s, pewdiepie, james, etc which do not add any feeling in the sentence. I've gone through nltk's average_perception_tagger but it didn't work well as it gave the results as</p>
<p>my input:</p>
<pre><code>&quot;mrbeast james lion tigers bad sad clickbait fight nice good&quot;
</code></pre>
<p>words that i need in my sentence:</p>
<pre><code>&quot;bad sad clickbait fight nice good&quot;
</code></pre>
<p>what i got using average_perception_tagger:</p>
<pre><code>[('mrbeast', 'NN'),
 ('james', 'NNS'),
 ('lion', 'JJ'),
 ('tigers', 'NNS'),
 ('bad', 'JJ'),
 ('sad', 'JJ'),
 ('clickbait', 'NN'),
 ('fight', 'NN'),
 ('nice', 'RB'),
 ('good', 'JJ')]

</code></pre>
<p>so as you can see if i remove mrbeast i.e NN the words like clickbait, fight will also get removed which than ultimately remove expressions from that sentence.</p>
","python, machine-learning, nlp, sentiment-analysis","<p>There are multiple ways of doing this like</p>
<ol>
<li><p>you can create a set of positive and negative words and for each word in your grammar you can check if it exists in your set, if it does you should keep the word, else delete it. This however would first require all positive and negative words dataset.</p>
</li>
<li><p>you can use something like textblob which can give you the sentiment score of a word or a sentence. so with a cutoff sentiment score you can filter out the words that you don't need.</p>
</li>
</ol>
"
How is transformers loss calculated for blank token predictions?,"<p>I'm currently trying to implement a transformer and have trouble understanding its loss calculation.</p>
<p>My encoders input looks for batch_size=1  and max_sentence_length=8 like:</p>
<pre><code>[[Das, Wetter, ist, gut, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
</code></pre>
<p>My decoders input looks like (german to english):</p>
<pre><code>[[&lt;start&gt;, The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;]]
</code></pre>
<p>Let's say my transformer predicted those class probabilities (only showing the word for the class with the highest class probability):</p>
<pre><code>[[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
</code></pre>
<p>Now I calculate the loss using:</p>
<pre><code>loss = categorical_crossentropy(
   [[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]],
   [[The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
)
</code></pre>
<p>Is this the correct way to calculate the loss? My transformer always predicts the blank token for the next word and I thought that's because I have a mistake in my loss calculation and have to do something with the blank tokens before calculating the loss.</p>
","machine-learning, nlp, transformer-model, language-model","<p>You need to mask out the padding. (What you call is <code>&lt;blank&gt;</code> is more often called <code>&lt;pad&gt;</code>.)</p>
<ul>
<li><p>Create a mask saying where the valid tokens are (pseudocode: <code>mask = target != '&lt;pad&gt;')</code></p>
</li>
<li><p>When computing the categorical cross-entropy, do not automatically reduce the loss and keep the value.</p>
</li>
<li><p>Multiply the loss values with the mask, i.e., positions corresponding to the <code>&lt;blank&gt;</code> tokens get zero out and sum the losses at the valid positions. (pseudocode: <code>loss_sum = (loss * mask).sum()</code>)</p>
</li>
<li><p>Divide the <code>loss_sum</code> by the number of valid position, i.e., the sum of the mask (pseudocode: <code>loss = loss_sum / mask.sum()</code>)</p>
</li>
</ul>
"
Handling Documentation Overlap in Product Feature Queries,"<p>I have two products, X and Y, each with its own set of documentation detailing their features and functionalities. I've processed these documents by dividing them into segments based on word count and stored these segments in vector database for both products.</p>
<p>However, there's a challenge I'm facing:</p>
<ul>
<li>There is a high likelihood of overlap between the documents. When discussing the products in an article, I don’t mention the product name in every paragraph, but it is implied that the features described are directly linked to the respective product.</li>
<li>I also have other documentation that largely overlaps with the previous product documentation but contains subtle differences.</li>
</ul>
<p>The problem arises when I ask a question about a feature in product X that doesn’t exist in X but is available in Y. The system mistakenly selects the documentation that includes the feature (from product Y), assuming it exists in X, and provides an incorrect answer.</p>
<p>How can I handle this issue to ensure that the system correctly identifies whether a feature exists in the queried product and avoids providing incorrect answers based on overlapping documentation?</p>
<p>Any advice or approach would be greatly appreciated!. If this is not right forum to ask such questions, apologies.</p>
","nlp, milvus",
Extracting structured data from user query,"<p>I want to extract structured data from the query provided by the user.</p>
<p>For example,
user query: I need data for females above the age of 3</p>
<p>output : {
min_age: 3,
max_age: None,
sex: female
}</p>
<p>These are few of the specific parameters that I require along with others.
I have tried using llama2, llama3 and gemma but i am facing the issue of hallucination
Also if a specific value is not given I want it to return 'None'</p>
<p>Is there any way I can achieve this using LLMs from ollama.</p>
<p>I have tried extracting the values one by one but I am not able to get the desired output</p>
","nlp, text-extraction, ollama",
How to save the output of constituency parsing diagram as an image?,"<p>Using the svgling module I have generated the constituency parse tree. Here is the github link of svgling: <a href=""https://github.com/rawlins/svgling"" rel=""nofollow noreferrer"">svgling module github</a></p>
<pre><code>pip install svgling
import svgling
import nltk
var ='(S (NP this tree) (VP (V is) (AdjP pretty)))'
svgling.disable_nltk_png()
random=svgling.draw_tree(nltk.Tree.fromstring(var))
display(random)

</code></pre>
<p>Using this code I have got the diagram as an output (given below) of the constituency parse tree. I want to generate .png file of this output.
<a href=""https://i.sstatic.net/7BJrG.png"" rel=""nofollow noreferrer"">Constituency parse tree (Output of the above code)</a></p>
<p>To save the output as .png file I have run this following code.</p>
<pre><code>import os
from nltk.tree import Tree
from nltk.draw.tree import TreeView
from nltk.draw.util import CanvasFrame
from nltk.draw import TreeWidget
os.system('Xvfb :1 -screen 0 1600x1200x16  &amp;')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8
os.environ['DISPLAY']=':1.0' 
t = Tree.fromstring('(S (NP this tree) (VP (V is) (AdjP pretty)))')
cf = CanvasFrame()
TreeView(t)._cframe.print_to_file('output.ps')
os.system('convert output.ps output.png')
</code></pre>
<p>However, this code is showing error in google colab <code>TclError: couldn't connect to display &quot;:1.0&quot; </code> and returning a numerical value (ex: 4) in jupyter notebook.</p>
<p>I tried to look into it on several websites but couldn't find any solution.</p>
","python, nlp","<p>Once I also had to convert the constituency parsing diagram as an image.
To do so, at first I have converted it into PostScript (.ps) file. And, from .ps file converted it into .png file.</p>
<p>To convert the tree diagram into the .ps file I ran the following code. This will give output.ps file.</p>
<pre><code>from nltk.tree import Tree
from nltk.draw.tree import TreeView
t = Tree.fromstring('(S (NP this tree) (VP (V is) (AdjP pretty)))')
TreeView(t)._cframe.print_to_file('output.ps')
</code></pre>
<p>However, I was trying to run the code on google colab. And it was giving the error:</p>
<blockquote>
<p>TclError: no display name and no $DISPLAY environment variable in
Google Colab</p>
</blockquote>
<p>To resolve the issue, ran the following code:</p>
<pre><code>!apt-get install -y xvfb # Install X Virtual Frame Buffer
import os
os.system('Xvfb :1 -screen 0 1600x1200x16  &amp;')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8
os.environ['DISPLAY']=':1.0'
</code></pre>
<p>Now to convert output.ps file into output.png:</p>
<pre><code>!apt install ghostscript python3-tk
from PIL import Image
psimage=Image.open('output.ps')
psimage.save('output.png')
</code></pre>
<p>However, could not manage to get very good image quality in this way.</p>
"
Text data labeling,"<p><strong>Task:</strong> Classify customer emails into relevant categories based on their content.</p>
<p><strong>Data:</strong> DataFrame containing customer emails.
I have a dataset of customer emails stored in a data frame. Each email pertains to a specific issue the customer encountered. My goal is to categorize these emails automatically. For example, an email about problems with food quality would be categorized as &quot;Food Quality Issue,&quot; while an email regarding payment difficulties would be categorized as &quot;Payment Issue.&quot;</p>
<p>The challenge lies in the unknown number of potential categories. There could be a vast range of issues customers might contact us about.</p>
<p>My plan is to address this challenge in two steps:</p>
<ol>
<li>Data Labeling: I need to  label a representative sample of emails from the data frame. This labeling process involves assigning each email a category that accurately reflects the customer's concern.</li>
<li>Classifier Model Training: Once I have a labeled dataset, I can use it to train a machine learning model. This model will then be able to automatically categorize new, unseen emails based on the patterns it learns from the labeled data.
I need answer for first part , how can i label data.</li>
</ol>
<p>My approach was to generate embeddings of email and then apply clustering,but result is not good.
Please help me to solve this problem and is any thing to apply before generating embeddings.</p>
","nlp, text-classification",
How to read constituency based parse tree,"<p>I have a corpus of sentences that were preprocessed by Stanford's <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""noreferrer"">CoreNLP</a> systems. One of the things it provides is the sentence's Parse Tree (Constituency-based). While I can understand a parse tree when it's drawn (like a tree), I'm not sure how to read it in this format:</p>

<p>E.g.:</p>

<pre><code>          (ROOT
          (FRAG
          (NP (NN sent28))
          (: :)
          (S
          (NP (NNP Rome))
          (VP (VBZ is)
          (PP (IN in)
          (NP
          (NP (NNP Lazio) (NN province))
          (CC and)
          (NP
          (NP (NNP Naples))
          (PP (IN in)
          (NP (NNP Campania))))))))
          (. .)))
</code></pre>

<p>The original sentence is:</p>

<pre><code>sent28: Rome is in Lazio province and Naples in Campania .
</code></pre>

<p>How am I supposed to read this tree, or alternatively, is there a code (in python) that does it properly?
Thanks.</p>
","python, parsing, nlp, parse-tree","<p><code>NLTK</code> has a class for reading parse trees: <code>nltk.tree.Tree</code>. The relevant method is called <code>fromstring</code>. You can then iterate its subtrees, leaves, etc...</p>

<p>As an aside: you might want to remove the bit that says <code>sent28:</code> as it confuses the parser (it's also not a part of the sentence). You are not getting a full parse tree, but just a sentence fragment.</p>
"
letter and bigram composition for each word in the dataframe,"<p>I have a data frame with words and I want to extract the letter and bigram composition for each word.</p>
<p><strong>Data:</strong></p>
<pre><code>df$text

[1] &quot;table&quot;
[2] &quot;run&quot;
[3] &quot;mug&quot;`
</code></pre>
<p>And in the end I want to receive the output:</p>
<pre><code>
 1      a b c d e..z aa ab bb...zz 
table   1 1 0 0 0..0  0 1  0    0
</code></pre>
<p>First, I was trying to extract all the letters using Quanteda:</p>
<pre><code>text &lt;- c(&quot;table&quot;, &quot;run&quot;, &quot;mug&quot;)

dict &lt;- dictionary(list(a= &quot;a&quot;,
                        b = &quot;b&quot;,
                        c = &quot;c&quot;,
                        d = &quot;d&quot;, 
                        e = &quot;e&quot;,
                        f = &quot;f&quot;,
                        g = &quot;g&quot;, 
                        h = &quot;h&quot;,
                        i = &quot;i&quot;,
                        j = &quot;j&quot;,
                        k = &quot;k&quot;,
                        l =&quot;l&quot;,
                        m = &quot;m&quot;,
                        n = &quot;n&quot;,
                        o = &quot;o&quot;,
                        p = &quot;p&quot;,
                        q = &quot;q&quot;,
                        r = &quot;r&quot;, 
                        s = &quot;s&quot;,
                        t = &quot;t&quot;,
                        u = &quot;u&quot;,
                        v = &quot;v&quot;, 
                        w = &quot;w&quot;,
                        x = &quot;x&quot;,
                        y = &quot;y&quot;,
                        z = &quot;z&quot;))


corp&lt;- corpus(text)

tokens(corp) |&gt;
  tokens_lookup(dictionary = dict) |&gt;
  dfm()
</code></pre>
<p>But it did not work out:</p>
<pre><code>Document-feature matrix of: 3 documents, 26 features (100.00% sparse) and 0 docvars.
       features
docs    a b c d e f g h i j
  text1 0 0 0 0 0 0 0 0 0 0
  text2 0 0 0 0 0 0 0 0 0 0
  text3 0 0 0 0 0 0 0 0 0 0
[ reached max_nfeat ... 16 more features ]
</code></pre>
<p>I am totally new in this, and if you have any hint how to do that, please, help. Thank you!</p>
","r, nlp, n-gram",
How to get spaCy to use universal dependencies,"<p>Spacy's site said they use universal dependencies scheme in their annotations specifications page. But when I parse ""I love you"", '''you''' was made a ""dobj"" of ""love"". There's no ""dobj"" in the universal dependency relations doc. So I have two questions : </p>

<ol>
<li>How to get spacy to use the universal dependency relations?</li>
<li>How to get the doc for the relations spacy uses?</li>
</ol>

<p><a href=""https://i.sstatic.net/g9mt9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/g9mt9.jpg"" alt=""enter image description here""></a></p>
","nlp, spacy, dependency-parsing","<p>Spacy's provided models don't use UD dependencies for English or German. From the docs, where you can find tables for the dependency labels (<a href=""https://spacy.io/api/annotation#dependency-parsing"" rel=""nofollow noreferrer"">https://spacy.io/api/annotation#dependency-parsing</a>):</p>

<blockquote>
  <p>The individual labels are language-specific and depend on the training corpus.</p>
</blockquote>

<p>For most other models / languages, UD dependencies are used.</p>
"
Vector Embedding using Spark for compute,"<p>I have some large parquet files of data in <code>Iceberg</code> (which I have stored using <code>Spark</code>). My objective now is to pull these down using <code>Spark</code>, convert them into a <code>spark dataframe</code>, perform <em>vector embedding</em> to transform the dataframe into a new dataframe with the <em>embedded vector columns</em>, and then store this vector-column into a vector database like <code>qdrant</code>.</p>
<p>I have had problems making things work so far, and online documentation on this specific topic is limited. I tried <code>Spark NLP</code>, but it appears incompatible with the <code>qdrant-spart connector</code> I used to allow qdrant to be a target for Spark. So I guess I am looking for what the conventional way is to do the following two:</p>
<ol>
<li>Perform vector embedding on a Spark dataframe using a model like BERT (Word2Vec is insufficient for my needs), extending the dataframe with a vector column.</li>
<li>Take the produces vector-embeddings column and store it in a vector database like qdrant.</li>
</ol>
<p>I feel like the <em>distributed</em> nature of Spark is a big obstacle here.</p>
","apache-spark, pyspark, nlp, word-embedding, qdrant",
gensim for Political Ad verification?,"<p>I am trying to build a model that uses transcribed audio and on-screen text to classify a video ad as political or non-political, as well as extracts the name of the candidate and sponsor. How can I go about doing this? One possible solution I thought of was using a dictionary of commonly used (or necessitated phrases) like &quot;I approve this message&quot; and &quot;This ad has been sponsored by ..., Vote for&quot;
and using Spacy NER to extract the candidate's name from these phrases.
If anyone has any solutions/suggestions please let me know.</p>
<p>Trying to classify the ad as political or non-political and extract the candidate's name, constituency, and party</p>
","nlp, spacy, gensim, named-entity-recognition, phrase",
Problems with Named Entity Recognition in spaCy using German de_dep_news_trf Pipeline,"<p>I'm currently working on a project using spaCy with the German trained pipeline <code>de_dep_news_trf</code>.</p>
<p>Unfortunately, I'm having issues with named entity recognition (NER).</p>
<p>When I run a simple sentence like <em>&quot;Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin.&quot;</em>, no entities are detected.</p>
<p>I've followed these steps to set up my Python environment (3.12)(Windows) in a PyCharm Community project:</p>
<pre class=""lang-bash prettyprint-override""><code>python.exe -m pip install --upgrade pip
pip install -U pip setuptools wheel
pip install -U spacy
python -m spacy download de_dep_news_trf --timeout 600
pip install spacy[transformers]
</code></pre>
<p>Here is a snippet of my code:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy


def process_text_with_spacy(text_to_process):
    doc = nlp(text_to_process)
    data = {
        &quot;text&quot;: text_to_process,
        &quot;sentences&quot;: []
    }
    for sent in doc.sents:
        process_sentence_data = {
            &quot;sentence&quot;: sent.text,
            &quot;entities&quot;: []
        }
        for ent in sent.ents:
            process_sentence_data[&quot;entities&quot;].append({
                &quot;text&quot;: ent.text,
                &quot;start&quot;: ent.start_char,
                &quot;end&quot;: ent.end_char,
                &quot;label&quot;: ent.label_
            })
        data[&quot;sentences&quot;].append(process_sentence_data)
    return data


nlp = spacy.load('de_dep_news_trf')

sample_text = &quot;Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin.&quot;

processed_data = process_text_with_spacy(sample_text)

print(&quot;Text:&quot;, sample_text)
for sentence_data in processed_data[&quot;sentences&quot;]:
    print(&quot;Sentence:&quot;, sentence_data[&quot;sentence&quot;])
    print(&quot;Entities:&quot;, sentence_data[&quot;entities&quot;])
</code></pre>
<p>Output:</p>
<pre class=""lang-bash prettyprint-override""><code>Text: Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin.
Sentence: Berlin ist die Hauptstadt von Deutschland.
Entities: []
Sentence: Angela Merkel war die Bundeskanzlerin.
Entities: []
</code></pre>
<p>When using <code>de_core_news_lg</code>, the output for each sentence is:</p>
<pre class=""lang-bash prettyprint-override""><code>Text: Berlin ist die Hauptstadt von Deutschland. Angela Merkel war die Bundeskanzlerin.
Sentence: Berlin ist die Hauptstadt von Deutschland.
Entities: [{'text': 'Berlin', 'start': 0, 'end': 6, 'label': 'LOC'}, {'text': 'Deutschland', 'start': 30, 'end': 41, 'label': 'LOC'}]
Sentence: Angela Merkel war die Bundeskanzlerin.
Entities: [{'text': 'Angela Merkel', 'start': 43, 'end': 56, 'label': 'PER'}]
</code></pre>
<p>However, when I use <code>de_dep_news_trf</code>, the results are empty.
Model <code>de_dep_news_trf</code> is selected based on &quot;accuracy&quot; from the SpaCy website.</p>
<p>Could someone explain why <code>de_dep_news_trf</code> does not return the same result? Is there a specific reason or setting that could cause this difference?</p>
<p>Thank you for your help!</p>
","python, nlp, spacy","<p>Problem is because this model doesn't have function to recognize entities.</p>
<p>See documentation for <a href=""https://spacy.io/models/de#de_dep_news_trf"" rel=""nofollow noreferrer"">de_dep_news_trf</a> - it has components <code>transformer, tagger, morphologizer, parser, lemmatizer, attribute_ruler</code> but no <code>ner</code> for <code>EntityRecognizer</code></p>
<p>So it may need to use one of other models :</p>
<ul>
<li>de_core_news_sm</li>
<li>de_core_news_md</li>
<li>de_core_news_lg</li>
</ul>
"
How to solve import error for Azure text analytics,"<p>import error for TextAnalyticsClient
I am following Microsoft official document for sentiment analysis.
<a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/sentiment-opinion-mining/quickstart?pivots=programming-language-python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/sentiment-opinion-mining/quickstart?pivots=programming-language-python</a></p>
<p>However, I am having import error to import TextAnalyticsClient.
This code cause error message like this.</p>
<p>from azure.ai.textanalytics import TextAnalyticsClient</p>
<p>ImportError: cannot import name 'is_rest'</p>
<p>How can I solve this error?</p>
","python, azure, text, nlp",
Generating Vector Embeddings for Organization Names,"<p>I have seen couple of Word2Vec Models that can generate embeddings for Company Names, and performs well when the different formats of the same company names are given.
But what I want to do is a bit different. For example, I have a list of company names like: [&quot;abc informatics&quot;, &quot;xyz communications&quot;, &quot;intra soft&quot;, &quot;gigabyte&quot; ]
Now, if a new company name comes up I want to check if it already matches with the existing company names by a threshold of 80% (probably through cosine similiarity or any other approach). Since the embedding models are trained on international companies it kind of performs poorly for local companies. Another problem is Word2Vec reflects on semantics while generating embeddings, for example &quot;Plants ltd&quot; and &quot;Trees Ltd&quot; will generate similiar embedings, but in reality both of them are quite different from one another!!!</p>
<p>I am open to any other solutions if embedding similiarity search doesnot work well.</p>
<p>This question is probably a duplicate to <a href=""https://stackoverflow.com/questions/77305303/create-embeddings-for-string-matching"">Create embeddings for string matching</a> , but since it didnt receieve any good answers I am asking the question here anyways.</p>
","python, nlp, word2vec",
Why is Perplexity not reliable for open domain text generation tasks?,"<p>In the paper <a href=""https://arxiv.org/abs/1912.02164"" rel=""nofollow noreferrer"">here</a>, it says that perplexity as an automated metric is not reliable for open domain text generation tasks, but it instead uses lm-score, a model based metric to produce perplexity like values. What additional benefits does lm-score give instead of perplexity metric?</p>
","nlp, gpt-2, perplexity",
spaCy incorrectly recognizing finger as verb,"<p>I am trying to investigate a way to fix (or alter) how spaCy identifies verbs/nouns. In the following example I would like to recognize <code>finger</code> as a <code>NOUN</code> not a <code>VERB</code>.</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_lg&quot;)
doc = nlp('over exertion to finger from pulling open a stuck door left middle finger strain')
for w in doc:
    print(w.text, w.lemma_, w.pos_)
</code></pre>
<p>which returns</p>
<pre><code>over over ADP
exertion exertion NOUN
to to PART
finger finger VERB  &lt;-- finger should be NOUN
from from ADP
pulling pull VERB
open open ADJ
a a DET
stuck stuck ADJ
door door NOUN
left leave VERB
middle middle ADJ
finger finger NOUN
strain strain NOUN
</code></pre>
<p>What changes could I make to solve this issue?</p>
","python, nlp, spacy","<p>Use a better, <code>en_core_web_trf</code>, model:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; nlp = spacy.load(&quot;en_core_web_trf&quot;)
&gt;&gt;&gt; doc = nlp('over exertion to finger from pulling open a stuck door left middle finger strain')
&gt;&gt;&gt; for w in doc:
    print(w.text, w.lemma_, w.pos_)
    
over over ADP
exertion exertion NOUN
to to ADP
finger finger NOUN
from from ADP
pulling pull VERB
open open ADP
a a DET
stuck stick VERB
door door NOUN
left leave VERB
middle middle ADJ
finger finger NOUN
strain strain NOUN
</code></pre>
"
Python: BERT Error - Some weights of the model checkpoint at were not used when initializing BertModel,"<p>I am creating an entity extraction model in PyTorch using <code>bert-base-uncased</code> but when I try to run the model I get this error:</p>
<pre><code>Some weights of the model checkpoint at D:\Transformers\bert-entity-extraction\input\bert-base-uncased_L-12_H-768_A-12 were not used when initializing BertModel:    
['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight',   'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias',  
 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight',  
 'cls.predictions.bias']  
    - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</code></pre>
<p>I have downloaded the bert model from <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">here</a> and the additional files from <a href=""https://huggingface.co/bert-base-uncased/tree/main"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Following is the code for my model:</p>
<pre><code>import config
import torch
import transformers
import torch.nn as nn

def loss_fn(output, target, mask, num_labels):

    lfn = nn.CrossEntropyLoss()
    active_loss = mask.view(-1) == 1
    active_logits = output.view(-1, num_labels)
    active_labels = torch.where(
        active_loss,
        target.view(-1),
        torch.tensor(lfn.ignore_index).type_as(target)
    )
    loss = lfn(active_logits, active_labels)
    return loss

class EntityModel(nn.Module):
    def __init__(self, num_tag, num_pos):
        super(EntityModel, self).__init__()

        self.num_tag = num_tag
        self.num_pos = num_pos
        self.bert = transformers.BertModel.from_pretrained(config.BASE_MODEL_PATH)
        self.bert_drop_1 = nn.Dropout(p = 0.3)
        self.bert_drop_2 = nn.Dropout(p = 0.3)
        self.out_tag = nn.Linear(768, self.num_tag)
        self.out_pos = nn.Linear(768, self.num_pos)

    def forward(self, ids, mask, token_type_ids, target_pos, target_tag):
        o1, _ = self.bert(ids, 
                          attention_mask = mask,
                          token_type_ids = token_type_ids)

        bo_tag = self.bert_drop_1(o1)
        bo_pos = self.bert_drop_2(o1)

        tag = self.out_tag(bo_tag)
        pos = self.out_pos(bo_pos)

        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)
        loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)

        loss = (loss_tag + loss_pos) / 2

        return tag, pos, loss 

print(&quot;model.py run success!&quot;)
</code></pre>
","python, nlp, pytorch, bert-language-model, huggingface-transformers",
How to extract the per sample losses during the training process?,"<p>I am finetuning different models on variety of NLP tasks including abstarctive summarization, question-asnwering and classification. I am using mainly the transformers library by huggingface. I am finding it difficult to extract the sample losses in an adequate way.</p>
<p>Link to the summarization notebook: <a href=""https://colab.research.google.com/drive/1aVWD-_iVZcMJ3Krt_nJQbrtFRtLAyPZU?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1aVWD-_iVZcMJ3Krt_nJQbrtFRtLAyPZU?usp=sharing</a></p>
<p>Link to the classification notebook: <a href=""https://colab.research.google.com/drive/1KicqvWNn1FnHR_dPMqnIwxtUF4OXSVo-?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1KicqvWNn1FnHR_dPMqnIwxtUF4OXSVo-?usp=sharing</a></p>
<p>Is there a more straightforward way to do this than what I am doing currently:</p>
<p>I am referring to an example on huggingface, but there they are using accelerate during the training procedure and I couldn't extract the per sample losses that way. Therefore, I am stuck with this approach of computing the loss manualy:</p>
<pre><code>logits = outputs.logits.detach().cpu()
labels = batch[&quot;labels&quot;].detach().cpu()
loss_per_sample = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), reduction='none')
loss_per_sample = loss_per_sample.view(logits.size(0), -1).mean(dim=1)

for i, id in enumerate(dataset[&quot;train&quot;][&quot;id&quot;][step * train_dataloader.batch_size: (step + 1) *  train_dataloader.batch_size]):
    sample_losses[id][&quot;loss&quot;].append(loss_per_sample[i].item())
    predicted_summary_ids = logits[i].argmax(dim=-1)
    sample_losses[id][&quot;predicted_summaries&quot;].append(tokenizer.decode(predicted_summary_ids, skip_special_tokens=True))
</code></pre>
<p>Worth mentioning is I am using a dictionary to store the sample id alongside with the dialogue, the target summary, the epoch losses and the corresponding predicted summaries:</p>
<pre><code>sample_losses = {
    id: {&quot;text&quot;: dialogue, &quot;target_summary&quot;: target_summary, &quot;loss&quot;: [], &quot;predicted_summaries&quot;: []}
    for id, dialogue, target_summary in zip(dataset[&quot;train&quot;][&quot;id&quot;], dataset[&quot;train&quot;][&quot;text&quot;],  dataset[&quot;train&quot;][&quot;summary&quot;])
}
</code></pre>
","pytorch, nlp, huggingface-transformers, huggingface",
How to calculate the quality of comments using NLP,"<p>How can I calculate the quality of comments a user receives on their posts and assign ratings based on the comments' quality score in a dataset I'm analyzing as part of my data science work?</p>
<p>I've studied this, but NLP seems complex for beginners. Any suggestions for a simpler approach?</p>
",nlp,"<p>Consider using Sentiment Analysis on each comment to determine sentiment (positive, negative, neutral), and assign scores accordingly. It's a simpler approach suitable for beginners. Look up resources to implement it easily. Is this helpful for your initial stage analysis.</p>
"
How to use adapter transformers with a Huggingface Pipeline,"<p>I tried to run the model &quot;AdapterHub/bert-base-uncased-pf-conll2003&quot; (<a href=""https://huggingface.co/AdapterHub/bert-base-uncased-pf-conll2003"" rel=""nofollow noreferrer"">Model description here</a>) for token classification in NLP.</p>
<p>First I tried to install the adapter transformers</p>
<pre><code>pip install -U adapter-transformers 
</code></pre>
<p>The output of the above command was</p>
<pre><code>Collecting adapter-transformers

[... see edit history for skipped lines ...]

Installing collected packages: tokenizers, huggingface-hub, adapter-transformers
  Attempting uninstall: tokenizers
    Found existing installation: tokenizers 0.15.0
    Uninstalling tokenizers-0.15.0:
      Successfully uninstalled tokenizers-0.15.0
  Attempting uninstall: huggingface-hub
    Found existing installation: huggingface-hub 0.19.4
    Uninstalling huggingface-hub-0.19.4:
      Successfully uninstalled huggingface-hub-0.19.4
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
transformers 4.35.2 requires huggingface-hub&lt;1.0,&gt;=0.16.4, but you have huggingface-hub 0.13.4 which is incompatible.
transformers 4.35.2 requires tokenizers&lt;0.19,&gt;=0.14, but you have tokenizers 0.13.3 which is incompatible.
Successfully installed adapter-transformers-3.2.1.post0 huggingface-hub-0.13.4 tokenizers-0.13.3
</code></pre>
<hr />
<p>I tried to load the model like this into the pipeline:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelWithHeads
from transformers import pipeline
token_classification = pipeline(&quot;token-classification&quot;, model = &quot;AdapterHub/bert-base-uncased-pf-conll2003&quot;)
res = token_classification(&quot;Take out the trash bag from the bin and replace it.&quot;)
print(res)
</code></pre>
<p>I received the errors</p>
<pre><code>EntryNotFoundError: 404 Client Error. (Request ID: Root=1-657e793c-0ce0c1936aff5e5741676650)

Entry Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-conll2003/resolve/main/config.json.

During handling of the above exception, another exception occurred:


OSError                                   Traceback (most recent call last)
&lt;ipython-input-3-030dfe0e128d&gt; in &lt;cell line: 3&gt;()
      1 from transformers import AutoModelWithHeads
      2 from transformers import pipeline
----&gt; 3 token_classification = pipeline(&quot;token-classification&quot;, model = &quot;AdapterHub/bert-base-uncased-pf-conll2003&quot;)
      4 res = token_classification(&quot;Take out the trash bag from the bin and replace it.&quot;)
      5 print(res)

/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)
    673         hub_kwargs[&quot;_commit_hash&quot;] = config._commit_hash
    674     elif config is None and isinstance(model, str):
--&gt; 675         config = AutoConfig.from_pretrained(model, _from_pipeline=task, **hub_kwargs, **model_kwargs)
    676         hub_kwargs[&quot;_commit_hash&quot;] = config._commit_hash
    677 

 [... see edit history for skipped lines ...]

/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    624             try:
    625                 # Load from local folder or from cache or download from model Hub and cache
--&gt; 626                 resolved_config_file = cached_file(
    627                     pretrained_model_name_or_path,
    628                     configuration_file,

/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)
    452         if revision is None:
    453             revision = &quot;main&quot;
--&gt; 454         raise EnvironmentError(
    455             f&quot;{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout &quot;
    456             f&quot;'https://huggingface.co/{path_or_repo_id}/{revision}' for available files.&quot;

OSError: AdapterHub/bert-base-uncased-pf-conll2003 does not appear to have a file named config.json.
Checkout 'https://huggingface.co/AdapterHub/bert-base-uncased-pf-conll2003/main' for available files.
</code></pre>
<p>How do I correctly load this adapter model?</p>
","python, machine-learning, nlp, huggingface-transformers","<pre><code># be sure you have the dependencies (NEW)
$ pip install adapters 
</code></pre>
<p>The <strong>old</strong> &amp; legacy package is <code>pip install -U adapter-transformers</code></p>
<hr />
<p>Create the model outside of the pipeline</p>
<pre><code>from transformers import AutoModelWithHeads
from transformers import pipeline
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModelWithHeads.from_pretrained(&quot;bert-base-uncased&quot;)
adapter_name = model.load_adapter(&quot;AdapterHub/bert-base-uncased-pf-conll2003&quot;, source=&quot;hf&quot;)
model.active_adapters = adapter_name

token_classification = pipeline(&quot;token-classification&quot;, model=model, tokenizer=tokenizer)
res = token_classification(&quot;Take out the trash bag from the bin and replace it.&quot;)
print(res)
</code></pre>
"
how to know the behavior of a sentence using NLP,"<p>I am trying to build the NLP model, using that nlp model i want to know the intention of text sentence.</p>
<p>Hi,
I am trying to build the NLP model, using that nlp model i want to know the intention of text sentence.
For Ex- If the sentence is like &quot;Create an incident for not able to fill the timesheet&quot;.</p>
<p>so in above example how we know the intention of the sentence using nlp model</p>
","c#, asp.net, azure, nlp, artificial-intelligence",
Issues with Generating Text from Fine-Tuned Mistral 7B Model on Georgian Dataset,"<p>I've fine-tuned the Mistral 7B model using a Georgian dataset with approximately 100,000 articles, including custom tokenizer fine-tuning. The fine-tuning process took about 9 hours. However, when I try to generate text, the output is not as expected; it consistently returns the input as the output, regardless of the input provided.</p>
<p>Here's the code I used for fine-tuning:</p>
<pre class=""lang-py prettyprint-override""><code>import time
import json
import torch
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments

# Load dataset, preprocess, and fine-tuning details...

training_args = TrainingArguments(
    output_dir=&quot;mistral_georgian_news_finetuning&quot;,
    max_steps=3125,
    per_device_train_batch_size=32,
    learning_rate=3e-4,
    # Other arguments...
)

# Fine-tuning setup...

# Start fine-tuning
trainer.train()
</code></pre>
<p>For testing the fine-tuned model, I used the following code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_path = &quot;/path/to/fine-tuned-model&quot;
tokenizer_path = &quot;/path/to/tokenizer&quot;

tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

def generate_text(prompt_text, max_length=500):
    input_ids = tokenizer(prompt_text, return_tensors=&quot;pt&quot;).input_ids
    output = model.generate(input_ids, max_length=max_length)
    return tokenizer.decode(output[0], skip_special_tokens=True)

prompt = &quot;რამდენიმე დღეში შესრულდება ...&quot;
generated_text = generate_text(prompt)
print(generated_text)
</code></pre>
<p>During testing, the model just echoes the prompt without generating new text. Below are the logs observed:</p>
<pre class=""lang-none prettyprint-override""><code>config.json: 0%| | 0.00/571 [00:00&lt;?, ?B/s]
model.safetensors.index.json: 0%| | 0.00/25.1k [00:00&lt;?, ?B/s]
Downloading shards: 0%| | 0/2 [00:00&lt;?, ?it/s]
model-00001-of-00002.safetensors: 0%| | 0.00/9.94G [00:00&lt;?, ?B/s]
model-00002-of-00002.safetensors: 0%| | 0.00/4.54G [00:00&lt;?, ?B/s]
Loading checkpoint shards: 0%| | 0/2 [00:00&lt;?, ?it/s]
generation_config.json: 0%| | 0.00/116 [00:00&lt;?, ?B/s]
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
</code></pre>
<p>I am unsure if the issue lies in how I am loading and testing the model or if it is related to the fine-tuning process. The model should generate text based on the input prompt, but it returns the input as the output.</p>
<p>Has anyone experienced similar issues, or can someone spot what might be wrong with my approach?</p>
","nlp, huggingface, language-model, fine-tuning, text-generation",
OpenAI API error: &quot;TypeError: Cannot read properties of undefined (reading &#39;create&#39;)&quot;,"<p>I am creating a chat summarizer app where the input is an Excel file with chat transcripts. Each row of the Excel sheet corresponds to a new chat. The app summarizes the chat in the adjacent column.</p>
<p>The problem is that I keep getting the following error:</p>
<blockquote>
<p>TypeError: Cannot read properties of undefined (reading 'create')</p>
</blockquote>
<p>Here's my code:</p>
<pre><code>require('dotenv').config();
const express = require('express');
const multer = require('multer');
const ExcelJS = require('exceljs');
const { Configuration, OpenAIApi } = require(&quot;openai&quot;);
const fs = require('fs');

// Initialize express app
const app = express();

// Configure multer for file uploads
const upload = multer({ dest: 'uploads/' });

// Initialize OpenAI API with configuration
const { OpenAI } = require('openai');
const openai = new OpenAI(process.env.OPENAI_API_KEY);

app.post('/upload', upload.single('file'), async (req, res) =&gt; {
  try {
    const workbook = new ExcelJS.Workbook();
    await workbook.xlsx.readFile(req.file.path);
    console.log(`File uploaded to: ${req.file.path}`);

    const worksheet = workbook.getWorksheet(1);

    // Convert worksheet rows to an array for easier iteration
    let rows = [];
    worksheet.eachRow((row, rowNumber) =&gt; {
      rows.push({ row, rowNumber });
    });

    // Iterate over rows array using a for...of loop to maintain async/await context
    for (let { row, rowNumber } of rows) {
      let chatText = row.getCell(1).value;
      if (chatText) { // Ensure there's text to summarize
        try {
          const response = await openai.ChatCompletion.create({
            model: &quot;gpt-3.5-turbo&quot;,
            prompt: `Summarize this chat: ${chatText}`,
            max_tokens: 100,
          });
          let summary = response.data.choices[0].text.trim();
          row.getCell(2).value = summary; // Assign the summary to the next column
        } catch (apiError) {
          console.error(`Error processing row ${rowNumber}:`, apiError);
        }
      }
    }

    // Save the workbook with summaries to a new file
    await workbook.xlsx.writeFile('/Users/ravikumar/ClarabridgeOutput/output.xlsx');
    res.send('File processed and summaries added.');
  } catch (error) {
    console.error(error);
    res.status(500).send('An error occurred while processing the file.');
    fs.unlinkSync(req.file.path); // Clean up uploaded file even on error
  }
});

// Choose a port for the server to listen on
const PORT = 3000;

// Start the server
app.listen(PORT, () =&gt; {
  console.log(`Server running on port ${PORT}`);
});
</code></pre>
","nlp, openai-api, chatgpt-api, chatgpt-function-call",
Efficient Methods for Updating a BERT Sequence Classification Model with New Classes?,"<p>I have a problem finding an effective method to update the classifier layer of my text classification model to include new classes. I am working on a classification task involving brand names based on their descriptions, with two columns: 'description' and 'brand.' After the initial training, I need a way to update the model to classify new classes without retraining the entire model, to save resources.</p>
<p>I have attempted to use a BERT model for sequence classification with PEFT (Parameter-Efficient Fine-Tuning), but I am stuck and unsure if this is the best approach. Specifically, I am having trouble adding new classes to an already trained model.</p>
<p>Here is the code I am currently using:</p>
<pre><code>from transformers import AutoModelForSequenceClassification, AdamW, get_scheduler
from peft import LoraConfig
import torch
from torch import nn
from tqdm import tqdm
import numpy as np

model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(custom_dataset2.labels)))

lora_config = LoraConfig(
    task_type=peft.TaskType.SEQ_CLS, # Type of task: sequence classification
    r=4,                             # Rank of the low-rank adaptation matrices
    lora_alpha=32,                   # Scaling factor for low-rank matrices
    lora_dropout=0.1,                # Dropout probability for low-rank matrices
)

optimizer = AdamW(model.parameters(), lr=0.00001)
num_epochs = 30
total_steps = num_epochs * len(data_loader2)
lr_scheduler = get_scheduler(
    &quot;linear&quot;,
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)
model.to(device)

model.train()

progress_bar = tqdm(range(total_steps))
for epoch in range(num_epochs):
    losses = []
    correct_predictions = 0
    for i, batch in enumerate(data_loader2):
        input_ids = batch[&quot;input_ids&quot;].to(device)
        attention_mask = batch[&quot;attention_mask&quot;].to(device)
        labels = batch[&quot;labels&quot;].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)

        logits = outputs.logits
        predictions = logits.argmax(dim=1)
        loss = outputs.loss
        losses.append(loss.item())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        progress_bar.update(1)
        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')

    correct_predictions += torch.sum(predictions == labels)
    train_acc = correct_predictions.double() / len(data_loader1.dataset)
    train_loss = np.mean(losses)
    print(f&quot;Epoch {epoch+1}/{num_epochs}, Train accuracy: {train_acc:.4f}, Train loss: {train_loss:.4f}&quot;)

# This part of the code is where I need help:
model.classifier = nn.Linear(model.bert.config.hidden_size, 100)
</code></pre>
<p>When I attempt to increase the number of classes and train the adapter again instead of retraining the model, I always encounter the following error:</p>
<pre><code>RuntimeError: shape '[-1, 43]' is invalid for input of size 660
</code></pre>
<p>Is there any way to add more classes to the pretrained model?</p>
","pytorch, nlp, bert-language-model, text-classification",
How can i solve ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1` when using Huggingface&#39;s TrainArguments?,"<p>I'm using the <code>transformers</code> library in Google colab, and
When i am using TrainingArguments from transformers library i'm getting Import error with this  code:</p>
<pre><code>from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir = &quot;/content/our-model&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size= 64,
    per_device_eval_batch_size = 16,
    num_train_epochs = 2,
    weight_decay = 0.01,
    evaluation_strategy = &quot;epoch&quot;,
    save_strategy = &quot;epoch&quot;,
    load_best_model_at_end = True,
    push_to_hub = False
)
</code></pre>
<p>This is the error i'm getting:</p>
<pre><code>&lt;ipython-input-28-0518ea5ff407&gt; in &lt;cell line: 2&gt;()
      1 from transformers import TrainingArguments
----&gt; 2 training_args = TrainingArguments(
      3     output_dir = &quot;/content/our-model&quot;,
      4     learning_rate=2e-5,
      5     per_device_train_batch_size= 64,

4 frames
/usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self)
   1670         if not is_sagemaker_mp_enabled():
   1671             if not is_accelerate_available(min_version=&quot;0.20.1&quot;):
-&gt; 1672                 raise ImportError(
   1673                     &quot;Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`&quot;
   1674                 )

ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U 
</code></pre>
<p>I already tried pip install for 0.20.1 version of accelerate and pip install transformers[torch]
and both didn't worked.</p>
","python, nlp, importerror, huggingface-transformers, huggingface","<p>If you're not particular about which transformers and accelerate version to tie to, then do this to use the most up-to-date version in Google Colab:</p>
<pre><code>! pip install -U accelerate
! pip install -U transformers
</code></pre>
<p>Then the issue you are having with accelerate should auto-resolve itself.</p>
<p>Note:</p>
<ul>
<li><p>Underspecifying <code>pip install -U transformers</code> instead of <code>pip install transformers[pytorch]</code> might be easier since that's what most of the users do and the developers of the library will make sure that the basic pip works with the common functions and class like <code>TrainingArguments</code></p>
</li>
<li><p>Instead of specifying accelerate to the <code>pip install accelerate&gt;=0.20.1</code>, if you have no particular need to fixed the version, automatically upgrading to the latest version might get you more stability when using the library, esp. with &quot;hot&quot;/&quot;trending&quot; libraries that are constantly changing (almost) daily.</p>
</li>
</ul>
<hr />
<p>If further debugging is necessary, i.e. if the above didn't work. To check your transformers and accelerate version, do this:</p>
<pre><code>import accelerate

accelerate.__version__
</code></pre>
<p>Most probably you might have an <code>ImportError</code> at the first line if accelerate is not already installed when you installed <code>transformers</code>.</p>
<p>And then if the first line works and the 2nd line is not outputting a version <code>&gt;=0.20.1</code>, then that is the cause of your issue.</p>
<p>The current versions to-date (July 2023) are:</p>
<pre><code>import accelerate
import transformers

transformers.__version__, accelerate.__version__
</code></pre>
<p>[out]:</p>
<pre><code>('4.30.1', '0.21.0')
</code></pre>
<p>Here's an example notebook with the model that you wish to use as per the comments in your question, <a href=""https://colab.research.google.com/drive/1D79AjHMeE6HAZC-g2S83baTgsHtDUu5i?usp=sharing"" rel=""noreferrer"">https://colab.research.google.com/drive/1D79AjHMeE6HAZC-g2S83baTgsHtDUu5i?usp=sharing</a></p>
<hr />
<p>If the error persist after the <code>pip install ...</code>, try restarting the runtime.</p>
<p>If you can't find the buttons to press to restart, try this in the cell <a href=""https://stackoverflow.com/questions/55005114/restart-kernel-in-google-colab"">Restart kernel in Google Colab</a> then re-run the cells for <code>import ...</code></p>
<pre><code>import os
os._exit(00)
</code></pre>
"
How to fix RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn,"<p>I am trying to use a custom csv dataset to finetune a model: TheBloke/Mistral-7B-Instruct-v0.1-GPTQ.I performed data preprocessing and I split the dataset into train, validation and test set and then I pass the train data and validation data into transformers.Trainer().  However, I am getting</p>
<blockquote>
<p>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn error.</p>
</blockquote>
<p>How can I fix this error?</p>
<p>Here is the code that is throwing the error:</p>
<pre><code>import transformers
from datetime import datetime

project = &quot;Mixtral-alpaca-finance-finetune&quot;
base_model_name = &quot;mixtral&quot;
run_name = base_model_name + &quot;-&quot; + project
output_dir = &quot;./&quot; + run_name

tokenizer.pad_token = tokenizer.eos_token

trainer = transformers.Trainer(
    model=model,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    #dataset_text_field=&quot;text&quot;,
    #max_seq_length=512,
    args=transformers.TrainingArguments(
        output_dir=output_dir,
        warmup_steps=5,
        per_device_train_batch_size=1,
        gradient_checkpointing=True,
        gradient_accumulation_steps=4,
        max_steps=1000,
        learning_rate=2.5e-5,
        lr_scheduler_type=&quot;cosine&quot;,
        logging_steps=25,
        fp16=True,
        optim=&quot;paged_adamw_8bit&quot;,
        logging_dir=&quot;./logs&quot;,        # Directory for storing logs
        save_strategy=&quot;steps&quot;,       # Save the model checkpoint every logging step
        save_steps=50,                # Save checkpoints every 50 steps
        evaluation_strategy=&quot;steps&quot;, # Evaluate the model every logging step
        eval_steps=50,               # Evaluate and save checkpoints every 50 steps
        do_eval=True,                # Perform evaluation at the end of training
        # report_to=&quot;wandb&quot;,           # Comment this out if you don't want to use weights &amp; baises
        run_name=f&quot;{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}&quot;          # Name of the W&amp;B run (optional)
),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),

)

model.config.use_cache = False  # silence the warnings. Re-enable for inference!
trainer.train()
</code></pre>
<p>And here is the error that I am getting:</p>
<blockquote>
<p>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</p>
</blockquote>
<p>I tried to upgrade the torch, accelerate auto-gptq but it still did not work.</p>
","python, nlp, bert-language-model, large-language-model, mistral-7b",
Which weights change when fine-tunning a pre-trained model? (Hugging Face),"<p>I am using the AutoModelForSequenceClassification class to fine-tune a pre-trained model (which originally is based on GPT2 architecture)</p>
<pre><code>model = AutoModelForSequenceClassification.from_pretrained(&quot;Natooz/Maestro-REMI-bpe20k&quot;, trust_remote_code=True, torch_dtype=&quot;auto&quot;,num_labels=2)
</code></pre>
<p>And I am using the basic training loop suggested in the NLP course from hugging face:</p>
<pre><code>model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
</code></pre>
<p>My question is:</p>
<p><em><strong>1. Which weights of the model are going to change after using this training loop? Am I training all the weights of the model? Only the classification head added by the AutoModelForSequenceClassification?</strong></em></p>
<p>It wouldn't make a lot of sense to me for all weights to change, because in theory I wouldn't be &quot;transferring&quot; any knowledge from my pre-trained model to my task.</p>
<p><em><strong>2. Would it work the same if I fine-tune my pre-trained model with the trainer class instead of the training loop?</strong></em></p>
<p>Thank you so much in advance!</p>
","nlp, huggingface-transformers, huggingface, pre-trained-model",
Training / using OpenAI GPT-3 for translations,"<p>I'm trying to use OpenAI for translation of my products descriptions from one language to some other languages (EN, DE, CZ, SK, HU, PL, SI...). The translations, especially to SK/CZ/HU/PL languages are (mainly gramatically) quite bad (using <code>text-davinci-003</code> model). I've got an idea - I already have a few thousands of similar products fully translated into all of these languages by professional translators. Is it possible to use those existing correct translations to train GPT-3 and then use this model to translate new texts? Has anybody already tried something similar?</p>
","nlp, translate, openai-api, machine-translation, gpt-3",
Vosk Model for Speech to Text,"<p>I want to train vosk model vosk-model-en-us-0.22 from <a href=""https://alphacephei.com/vosk/models"" rel=""nofollow noreferrer"">https://alphacephei.com/vosk/models</a> with an addition data of my voice with transcript of 1 hour so that the model gets overfit with my voice and don't get problem recognizing my voice properly. So how can I train it if someone can give me roadmap how to train that as I am beginner in open source.</p>
<ol>
<li>How to record my voice in which type i.e. mp3, wav etc.</li>
<li>How to feed recording dataset and transcript to model.</li>
<li>What code I use for training it and testing it.</li>
<li>How to make it better.
And any other related query</li>
</ol>
<p>I tried reading <a href=""https://alphacephei.com/vosk/lm"" rel=""nofollow noreferrer"">https://alphacephei.com/vosk/lm</a> but could not help myself with it.</p>
","nlp, speech-to-text, vosk",
How do I implement a model that finds a correlation (not similarity) between query and target sentence?,"<p>When building an NLP model (I'm going to use an attention-based one), how can we implement one for finding the <em>correlation</em>, not <em>similarity</em>, between the query and target sentences?
For instance, the two sentences &quot;I am an environmentalist.&quot; and &quot;We should raise the gas price, ban combustion-engine vehicles, and promote better public transit.&quot; are somehow <em>similar</em> and <em>positively correlated</em>.  However, if the first sentence becomes &quot;I am <strong>not</strong> an environmentalist.&quot;, the two sentences are still <em>similar</em> but now <em>negatively correlated</em>.</p>
<pre class=""lang-py prettyprint-override""><code>import json
import azure.functions as func
from sentence_transformers import SentenceTransformer, util

query = [&quot;I am an environmentalist.&quot;,
         &quot;I am not an environmentalist.&quot;,
         &quot;I am a tech-savvy person.&quot;]
target = [&quot;We should raise the gas price, ban combustion-engine vehicles, and promote better public transit.&quot;]

embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')
query_embedding = embedder.encode(query, convert_to_tensor=True)
target_embedding = embedder.encode(target, convert_to_tensor=True)
searched = util.semantic_search(query_embedding, target_embedding)

print(searched)

# [
#     [{'corpus_id': 0, 'score': 0.30188844}],
#     [{'corpus_id': 0, 'score': 0.22667089}],
#     [{'corpus_id': 0, 'score': 0.05061193}]
# ]
</code></pre>
<p>Are there any useful resources or information about this difference and/or finding the <em>correlation</em> by a model? I'm still new to the field of NLP (I have used the sentence transformer for some of my projects) so maybe I simply didn't do a good search on the web.</p>
","python, deep-learning, nlp, attention-model, sentence-similarity",
How areTF-IDF calculated by the scikit-learn TfidfVectorizer,"<p>I run the following code to convert the text matrix to TF-IDF matrix.</p>
<pre><code>text = ['This is a string','This is another string','TFIDF computation calculation','TfIDF is the product of TF and IDF']

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english',norm = None)
                    
X = vectorizer.fit_transform(text)
X_vocab = vectorizer.get_feature_names_out()
X_mat = X.todense()
X_idf = vectorizer.idf_
</code></pre>
<p>I get the following output</p>
<p>X_vocab =</p>
<pre><code>[u'calculation',
 u'computation',
 u'idf',
 u'product',
 u'string',
 u'tf',
 u'tfidf']
</code></pre>
<p>and X_mat =</p>
<pre><code>  ([[ 0.        ,  0.        ,  0.        ,  0.        ,  1.51082562,
      0.        ,  0.        ],
    [ 0.        ,  0.        ,  0.        ,  0.        ,  1.51082562,
      0.        ,  0.        ],
    [ 1.91629073,  1.91629073,  0.        ,  0.        ,  0.        ,
      0.        ,  1.51082562],
    [ 0.        ,  0.        ,  1.91629073,  1.91629073,  0.        ,
      1.91629073,  1.51082562]])
</code></pre>
<p>Now I dont understand how these scores are computed. My idea is that for the text[0], score for only 'string' is computed and there is a score in the 5th coloumn. But as TF_IDF is the product of term frequency which is 2 and IDF which is log(4/2) is 1.39 and not 1.51 as shown in the matrix. How is the TF-IDF score calculated in scikit-learn.</p>
","nlp, scikit-learn, tf-idf","<p>TF-IDF is done in multiple steps by Scikit Learn's TfidfVectorizer, which in fact uses TfidfTransformer and inherits CountVectorizer.</p>

<p>Let me summarize the steps it does to make it more straightforward:</p>

<ol>
<li>tfs are calculated by CountVectorizer's fit_transform()</li>
<li>idfs are calculated by TfidfTransformer's fit()</li>
<li>tfidfs are calculated by TfidfTransformer's transform()</li>
</ol>

<p>You can check the source code <a href=""https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/feature_extraction/text.py#L1052"" rel=""noreferrer"">here</a>.</p>

<p>Back to your example. Here is the calculation that is done for the tfidf weight for the 5th term of the vocabulary, 1st document (X_mat[0,4]):</p>

<p>First, the tf for 'string', in the 1st document:</p>

<pre><code>tf = 1
</code></pre>

<p>Second, the idf for 'string', with smoothing enabled (default behavior):</p>

<pre><code>df = 2
N = 4
idf = ln(N + 1 / df + 1) + 1 = ln (5 / 3) + 1 = 1.5108256238
</code></pre>

<p>And finally, the tfidf weight for (document 0, feature 4):</p>

<pre><code>tfidf(0,4) = tf * idf = 1 * 1.5108256238 = 1.5108256238
</code></pre>

<p>I noticed you choose not to normalize the tfidf matrix. Keep in mind normalizing the tfidf matrix is a common and usually recommended approach, since most models will require the feature matrix (or design matrix) to be normalized.</p>

<p>TfidfVectorizer will L-2 normalize the output matrix by default, as a final step of the calculation. Having it normalized means it will have only weights between 0 and 1.</p>
"
State of the art word sense disambiguation on WordNet synsets,"<p>I am trying to perform a simple task: given a corpus, identify all words that are hyponyms of a certain synset (e.g., <em>«find every mention of a &quot;plant&quot; or a &quot;bird&quot;»</em>). In order to do that accurately, I need to do word sense disambiguation on a group of synsets for every word in my corpus.</p>
<p>I am trying to do it using state-of-the-art methods as available in the open source space.</p>
<p>If using a neural method, I would need a pretrained model.</p>
<p>I have tried the greedy approach that considers every single synset for every word. This isn't great; however, I find that using traditional techniques like <code>lesk</code> as provided by <code>nltk</code> in practice is even worse, as I get way too many false negatives.</p>
<p>I see that spaCy already contains a transformer based model which comes with POS tagging out of the box, but the WordNet integration is supplied by an external package and I can't seem to find any way to do WSD on it.</p>
<p>I could certainly paraphrase the disambiguation query:</p>
<blockquote>
<p>Which of these description matches the word x in the sentence: &quot;yyy&quot;:</p>
<ol>
<li>&quot;x means aaa&quot;</li>
<li>&quot;x means bbb&quot;</li>
<li>&quot;x means ccc&quot;</li>
</ol>
</blockquote>
<p>And feed it into an LLM, so I can't see any hard limit on why there shouldn't be a more straightforward way to do this using modern deep learning techniques. Is there some available model I am unable to find?</p>
","nlp, nltk, spacy, wordnet, spacy-transformers",
Speed up semantic search for large dataset with custom search function,"<p>I am building a semantic search engine for short sentences where I want to know which sentence have the more similar sentences. For that, I need to know based on a threshold on the similarity score, how many similar sentences I have for each sentence, and so I need an almost complete view of the similarities between all the sentence.</p>
<p>Given the quality of my free text, I'm not able to achieve good performances by only using a classical transformer embedding. To get the best out of my data composed both of free texts and metadata, I have come up with an hybrid solution using sentence transformer embedding, TD-IDF embedding, and an encoding of my metadata.</p>
<p>Once the free texts is embedded, I perform a custom search function where I use weights to combine the cosine similarity between the sentence transformer embeddings, the cosine similarity between the TF-IDF embedding, and a pairwise matching between my encoding of metadata.</p>
<p>However, my dataset is quite large with more than 500k documents. The compute for the embeddings is not a problem in terms of ressources, but the search function is. As I'm not able to store a 500k*500k matrix containing all the similarity scores between all my documents, I have tried to do it by smaller batches, for example computing 100*500k matrix one after the other. This solve the memory issue, but the processing time is really high.</p>
<p>I have looked a bit about packages libraries such as chromadb and faiss, but it seems that they don't provide custom search features.</p>
<p>Any idea?</p>
","python, nlp, sentence-transformers, semantic-search",
How to Navigate and Scroll in IPython Notebook Efficiently,"<p>I have a degree in CS &amp; have over 10 years of experience coding, but I am new to Python &amp; Python Notebooks.  I am using Google Collab and the number #1 difficulty I have is navigating the UI. Scrolling up and down the blocks of code/text on the right side and the tree view on the left side.  I literally need to resize the window and then race to grab a scroll bar before it disappears.  Then I can only go so far and I have to start all over and do another resize. I would really appreciate hearing how others manage to move up and down the tree view and code sections with ease?   Do you run the collab notebook in a browser? But I have also experienced similar with notebooks even in Visual Studio Code with the plugin enabled.  I think notebooks could be awesome for productivity but frankly for me it has slowed me down by at least 50%.</p>
<p>Mostly resizing the window to get a scroll bar. I have tried navigating with arrow keys etc.  The only thing that works at all for me is resizing the window and grabbing the scroll bar.  But its very inefficient.   I am definitely not learning by using! I have been using it for a few weeks and its still not intuitive for me.</p>
<p>Update: Here is an example where usng up/down arrow keys do not work: <a href=""https://i.sstatic.net/UxcIHAED.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UxcIHAED.png"" alt=""enter image description here"" /></a> The keys will go this far down and no farther. If I want to see latest output,  I need to resize the window and scroll from the size unless there is a better way? I did check the documentation. Getting started guide etc and I dont see any advice on navigation.</p>
","python, google-cloud-platform, jupyter-notebook, nlp",
During Pretraining of Rag on custom dataset getting out of memory,"<p>I am trying to fine tune pretrained Rag on my Custom dataset. I am enclosing the code. During Training, I am getting OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.19 GiB of which 3.50 MiB is free. Process 15094 has 22.18 GiB memory in use. Of the allocated memory 21.70 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (<a href=""https://pytorch.org/docs/stable/notes/cuda.html#environment-variables"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/notes/cuda.html#environment-variables</a>)
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...  I tried everything.
I am enclosing my code also any help would be appreciated.</p>
<pre><code># %%
!pip install transformers datasets torch

# %%
from transformers import RagTokenizer, RagTokenForGeneration
tokenizer = RagTokenizer.from_pretrained(&quot;facebook/rag-token-nq&quot;)
model = RagTokenForGeneration.from_pretrained(&quot;facebook/rag-token-nq&quot;)

# %%
!pip install accelerate -U

# %%
!pip install faiss-cpu

# %%
!pip install accelerate -U

# %%
pip show accelerate

# %%
import os

# Set the environment variable
os.environ[&quot;PYTORCH_CUDA_ALLOC_CONF&quot;] = &quot;expandable_segments:True&quot;


# %%
import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict, load_from_disk
from transformers import Trainer, TrainingArguments, RagTokenizer, RagSequenceForGeneration, RagRetriever, DPRContextEncoder, DPRContextEncoderTokenizerFast
import faiss
import torch
import numpy as np


# Load your CSV data
df = pd.read_csv(&quot;/teamspace/studios/this_studio/examples/qa_pairs1.csv&quot;)  # Replace with the path to your CSV file

# Split the data into train and validation datasets
train_df, val_df = train_test_split(df, test_size=0.2)

# Convert DataFrame to Hugging Face Dataset
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)

# Combine into a DatasetDict
climate_change_dataset = DatasetDict({
    &quot;train&quot;: train_dataset,
    &quot;validation&quot;: val_dataset
})

# Generate embeddings for your passages
ctx_encoder = DPRContextEncoder.from_pretrained(&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;)
ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;)

def create_embeddings(sentences):
    embeddings = []
    for passage in sentences:
        inputs = ctx_tokenizer(passage, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            embedding = ctx_encoder(**inputs).pooler_output.cpu().numpy()
        embeddings.append(embedding)
    return np.concatenate(embeddings, axis=0).astype(&quot;float32&quot;)

# Prepare the passages data
passages = df[&quot;sentence&quot;].tolist()
embeddings = create_embeddings(passages)

# Structure your data
data = {
    &quot;title&quot;: [&quot;&quot;] * len(df),  # Adding dummy title column
    &quot;text&quot;: df[&quot;sentence&quot;].tolist(),
    &quot;embeddings&quot;: embeddings.tolist()
}

# Create a dataset using the Dataset class from datasets
passages_dataset = Dataset.from_dict(data)

# Save the dataset to disk
passages_dataset.save_to_disk(&quot;/teamspace/studios/this_studio/examples/my_knowledge_dataset&quot;)

# Create and save FAISS index
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)
faiss.write_index(index, &quot;/teamspace/studios/this_studio/examples/faiss_index&quot;)

# Load tokenizers and models
question_encoder_tokenizer = RagTokenizer.from_pretrained(&quot;facebook/rag-token-nq&quot;)
generator_tokenizer = RagTokenizer.from_pretrained(&quot;facebook/rag-token-nq&quot;)

# Define the tokenizer for question encoder and generator
tokenizer = question_encoder_tokenizer

# Define a function to process the data for training
def preprocess_function(examples):
    inputs = [q + &quot; [SEP] &quot; + s for q, s in zip(examples[&quot;question&quot;], examples[&quot;sentence&quot;])]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=&quot;max_length&quot;)
    labels = tokenizer(examples[&quot;answer&quot;], max_length=512, truncation=True, padding=&quot;max_length&quot;)
    model_inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]
    return model_inputs

# Process the dataset
tokenized_datasets = climate_change_dataset.map(preprocess_function, batched=True)

# Initialize the Trainer
training_args = TrainingArguments(
    output_dir=&quot;./rag_finetuned/&quot;,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir=&quot;./logs/&quot;,
    logging_steps=10,
)

# Load the dataset from disk
dataset_path = &quot;/teamspace/studios/this_studio/examples/my_knowledge_dataset&quot;
dataset = load_from_disk(dataset_path)

# Load the FAISS index
index_path = &quot;/teamspace/studios/this_studio/examples/faiss_index&quot;
index = faiss.read_index(index_path)

retriever = RagRetriever.from_pretrained(
    &quot;facebook/rag-token-nq&quot;,
    index_name=&quot;custom&quot;,
    passages_path=dataset_path,
    index_path=index_path,
    question_encoder_tokenizer=question_encoder_tokenizer,
    generator_tokenizer=generator_tokenizer,
)

model = RagSequenceForGeneration.from_pretrained(
    &quot;facebook/rag-token-nq&quot;,
    retriever=retriever,
    question_encoder_tokenizer=question_encoder_tokenizer,
    generator_tokenizer=generator_tokenizer,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets[&quot;train&quot;],
    eval_dataset=tokenized_datasets[&quot;validation&quot;],
    
)

# Start fine-tuning
trainer.train()


# %%
torch.cuda.empty_cache()
</code></pre>
<p>I am getting to fine tuene my Rag Model but getting out of Memory inspite of using GPU.</p>
","python, pytorch, nlp, fine-tuning, custom-training",
How to pass a pytorch DataLoader to huggingface Trainer? Is that even possble?,"<p>The usual steps to use the Trainer from huggingface requires that:</p>
<ol>
<li>Load the data</li>
<li>Tokenize the data</li>
<li>Pass tokenized data to Trainer</li>
</ol>
<p>MWE:</p>
<pre><code>data = generate_random_data(10000)  # Generate 10,000 samples
df = pd.DataFrame(data)
dataset = Dataset.from_pandas(df)
tokenized_datasets = dataset.map(preprocess_function, batched=True)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    eval_dataset=tokenized_datasets,
)
</code></pre>
<p>Instead if we do:</p>
<pre><code>train_dataset = convert_to_tensors(tokenized_datasets)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataloader,
        eval_dataset=train_dataloader,
)
</code></pre>
<p>I get an error key 0, cause <code>train_dataloader</code> is not in dictionary format, I think?
Is there a way to pass directly a pytorch DataLoader to huggingface Trainer?</p>
","nlp, huggingface, pytorch-dataloader, dataloader, huggingface-trainer",
Why replace the masked token with random token in bert?,"<p>Masking in Bert is:</p>
<ul>
<li>Take 15% of all the tokens in the sequence. These are to be used in computing the MLM loss</li>
<li>80% of the time retain the mask</li>
<li>10% of the time replace the mask with the original token</li>
<li>10% of the time replace the mask with a random token</li>
</ul>
<p>I understand why masking is needed. I also understand why the masking is replaced back to the original token. This is because otherwise, the model learns to completely ignore the word itself in deriving its contextual embeddings in the downstream tasks.</p>
<p>What is the need to replace the mask with a random word 10% of the time?</p>
","nlp, bert-language-model, transformer-model",
Out-of-memory problem when using dist.all_gather,"<p>I'm writing codes for multi-GPU training, and I need to gather embeddings from different gpus to calculate loss and then propagate the gradients back to different GPUs. However, when the programs runs to optimizer.step(), the memory usage increases dramatically and resulted in a out-of-memory problem. The code is as below, thanks!</p>
<pre><code>
def training_stage_2(model, optimizer, train_dataloader, val_dataloader, tokenizer, accelerator, epochs):
    # finetuning the model with query-doc pairs
    for epoch in range(epochs):
        model.train()
        for batch in tqdm(train_dataloader):
            query, doc = batch
            with accelerator.accumulate(model):
                # take the last hidden states of eos token as embeddings]
                query_inputs = tokenizer(query, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=512)
                query_embeds_ = model(**query_inputs).last_hidden_state[:,-1,:] # shape: (batch_size, hidden_size)
                doc_inputs = tokenizer(doc, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=512)
                doc_embeds_ = model(**doc_inputs).last_hidden_state[:,-1,:] # shape: (batch_size, hidden_size)
                # collect embeddings from all gpus
                query_embeds = torch.zeros((query_embeds_.shape[0] * accelerator.num_processes, query_embeds_.shape[1]), device=accelerator.device, dtype=query_embeds_.dtype)
                doc_embeds = torch.zeros((doc_embeds_.shape[0] * accelerator.num_processes, doc_embeds_.shape[1]), device=accelerator.device, dtype=doc_embeds_.dtype)
                dist.all_gather(list(query_embeds.chunk(accelerator.num_processes, dim=0)), query_embeds_.data)
                dist.all_gather(list(doc_embeds.chunk(accelerator.num_processes, dim=0)), doc_embeds_.data)
                # requires grad for embeddings
                query_embeds.requires_grad = True
                doc_embeds.requires_grad = True
                loss = classification_loss_single_vector(query_embeds, doc_embeds)
                accelerator.backward(loss)
                # scatter the gradients to all gpus
                if query_embeds.grad is not None:
                    query_embeds.grad.detach_()
                    doc_embeds.grad.detach_()
                query_grad = torch.zeros_like(query_embeds_)
                doc_grad = torch.zeros_like(doc_embeds_)
                # feature gradient all-reduce
                dist.reduce_scatter(query_grad, list(query_embeds.grad.chunk(accelerator.num_processes, dim=0)))
                dist.reduce_scatter(doc_grad, list(doc_embeds.grad.chunk(accelerator.num_processes, dim=0)))
                query_grad.mul_(accelerator.num_processes)
                doc_grad.mul_(accelerator.num_processes)
                # backward the model
                query_embeds_.backward(query_grad)
                doc_embeds_.backward(doc_grad)
                optimizer.step()
                optimizer.zero_grad()
</code></pre>
<p>I tried using accelerator.gather to replace dist.gather_all but it doesn't work.</p>
","pytorch, nlp, distributed-computing",
Why does Seq2SeqTrainer produces error during evaluation when using T5?,"<p>Following the tutorial <a href=""https://huggingface.co/docs/transformers/en/tasks/summarization"" rel=""nofollow noreferrer"">here</a>. I've tried to adapt it to my dataset.</p>
<p>But, I've noticed that during evaluation the <code>Seq2SeqTrainer</code> calls the <code>compute_metrics</code> <code>3</code> times.</p>
<p>The first time it passes the correct validation/test set, but the other <code>2</code> times I don't know what the hell is passing on or why is calling the <code>compute_metrics</code> <code>3</code> times?</p>
<p>Notice in the screenshot below the validation set has 6400 samples, which is correctly passed to the <code>compute_metrics</code> the first time it is being called by the <code>Seq2SeqTrainer</code>, but what are the other <code>2</code> calls passing the second time <code>predictions</code> and <code>labels</code> of size <code>127</code> and third time it calls the <code>compute_metrics</code> it passes some type of scalar values for both the <code>predictions</code> and <code>labels</code>.</p>
<p>Could anyone explain what the hell is going on here?</p>
<p>Why does the <code>Seq2SeqTrainer</code> calls the <code>compute_metrics</code> 3 times when it should only call it once passing the actual predictions and labels on validation set which is of size <code>6400</code> samples?</p>
<pre><code>training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;t5_checkpoints&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=640,
    per_device_eval_batch_size=640,
    num_train_epochs=10,
    weight_decay=0.01,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    save_total_limit=3,
    predict_with_generate=True,
    generation_max_length=128,
    generation_num_beams=4,
    load_best_model_at_end=True,
    logging_steps=1,
)
</code></pre>
<pre><code>trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data[&quot;train&quot;],
    eval_dataset=tokenized_data[&quot;valid&quot;],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)
</code></pre>
<p><a href=""https://i.sstatic.net/YdLiG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YdLiG.png"" alt=""enter image description here"" /></a></p>
","nlp, huggingface-transformers, huggingface, huggingface-trainer",
I am implementing transformers from scratch in pytorch and getting some error in addition in positional encoding part in output layer,"<p>I am implementing transformer in pytorch and getting an error when the Positional encoding is applied in the decoder layer that is in the  <code>op_positional_encoding = self.positional_encoding(op_embed)</code> part:
<strong>RuntimeError: The size of tensor a (19) must match the size of tensor b (20) at non-singleton dimension 1</strong>  in the part <strong>return x + self.pe[:x.shape[1]]</strong></p>
<p>I am attaching all my codes:
**
Data Generation code :**</p>
<pre><code>en_tokenizer = spacy.load(&quot;en_core_web_sm&quot;)
fr_tokenizer = spacy.load(&quot;fr_core_news_sm&quot;)

def data_process(sentence_en,sentence_fr):
  data = []
  for (en_sen,fr_sen) in zip(sentence_en,sentence_fr):
    data.append((en_sen,fr_sen))
  return data
train_data = data_process(train_en,train_fr)
valid_data = data_process(valid_en,valid_fr)

UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3
special_symbols = ['&lt;unk&gt;', '&lt;pad&gt;', '&lt;bos&gt; ', ' &lt;eos&gt;']

def yield_tokens_en(data_iter):
    for (en_batch, fr_batch) in data_iter:
        for en_sent in en_batch:
            my_string = ' '.join(en_sent)
            en_tokens = my_string.split()  # Split English sentence into tokens
            yield en_tokens

def yield_tokens_fr(data_iter):
    for (en_batch, fr_batch) in data_iter:
        for fr_sent in fr_batch:
            my_string = ' '.join(fr_sent)
            fr_tokens = my_string.split()  # Split English sentence into tokens
            yield fr_tokens
en_vocab = build_vocab_from_iterator(yield_tokens_en(train_iter),
                                  min_freq=1,
                                  specials=special_symbols,
                                  special_first=True)
fr_vocab = build_vocab_from_iterator(yield_tokens_fr(train_iter),
                                  min_freq=1,
                                  specials=special_symbols,
                                  special_first=True)
en_vocab.set_default_index(UNK_IDX)
fr_vocab.set_default_index(UNK_IDX)

def pad_sequences_to_length(batch, max_length=20, padding_value=0):
    padded_sequences = []
    for seq in batch:
        if len(seq) &lt; max_length:
            # Pad sequence to max_length
            padded_seq = torch.cat([seq, torch.full((max_length - len(seq),), padding_value)])
        else:
            # Truncate sequence if longer than max_length
            padded_seq = seq[:max_length]
        padded_sequences.append(padded_seq)
    return torch.stack(padded_sequences)

def collate_fn(data_batch):
  en_batch = []
  fr_batch = []
  for (en_item,fr_item) in data_batch:
    en_ids = torch.tensor([en_vocab[token] for token in en_item],dtype = torch.long)
    fr_ids = torch.tensor([fr_vocab[token] for token in fr_item],dtype = torch.long)
    en_batch.append(torch.cat([torch.tensor([en_vocab['&lt;bos&gt;']]), en_ids ,torch.tensor([en_vocab['&lt;eos&gt;']])], dim=0))
    fr_batch.append(torch.cat([torch.tensor([fr_vocab['&lt;bos&gt;']]), fr_ids ,torch.tensor([fr_vocab['&lt;eos&gt;']])], dim=0))
  en_batch = pad_sequences_to_length(en_batch,padding_value = PAD_IDX)
  fr_batch = pad_sequences_to_length(fr_batch,padding_value = PAD_IDX)
  return en_batch,fr_batch

train_data_token_iter = DataLoader(train_data, batch_size=32,
                        shuffle=True, collate_fn=collate_fn)
valid_data_token_iter = DataLoader(valid_data, batch_size=32,
                        shuffle=True, collate_fn=collate_fn)

</code></pre>
<p>**positional coding part : **</p>
<pre><code>class PositionalEncoding(nn.Module):
    def __init__(self, embed_dim, max_seq_length):
        super(PositionalEncoding, self).__init__()

        pe = torch.zeros(max_seq_length, embed_dim)
        position = torch.arange(0, max_seq_length).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        print(x.size())
        print(self.pe.size())
        return x + self.pe[:x.shape[1]]
</code></pre>
<p>**MultiHead Attention Part : **</p>
<pre><code>class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim=512, n_heads=8):
        super(MultiHeadAttention, self).__init__()
        assert embed_dim % n_heads == 0, &quot;Embedding dimension must be divisible by the number of heads&quot;
        # Initialize dimensions
        self.embed_dim = embed_dim
        self.n_heads = n_heads
        self.single_head_dim = embed_dim // n_heads

        # Linear layers for transforming inputs
        self.W_q = nn.Linear(embed_dim, embed_dim)
        self.W_k = nn.Linear(embed_dim, embed_dim)
        self.W_v = nn.Linear(embed_dim, embed_dim)
        self.W_o = nn.Linear(embed_dim, embed_dim)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.single_head_dim)

        # Apply mask if provided (useful for preventing attention to certain parts like padding)
        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)

        attn_probs = torch.softmax(attn_scores, dim=-1)

        # Multiply by values to obtain the final output
        output = torch.matmul(attn_probs, V)
        return output

    def split_heads(self, x):
        # Reshape the input to have num_heads for multi-head attention
        batch_size, seq_length, embed_dim = x.size()
        return x.view(batch_size, seq_length, self.n_heads, self.single_head_dim).transpose(1, 2)

    def combine_heads(self, x):
        # Combine the multiple heads back to original shape
        batch_size, _, seq_length, single_head_dim = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)

    def forward(self, Q, K, V, mask=None):
        # Apply linear transformations and split heads
        Q = self.split_heads(self.W_q(Q))
        K = self.split_heads(self.W_k(K))
        V = self.split_heads(self.W_v(V))

        # Perform scaled dot-product attention
        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)

        # Combine heads and apply output transformation
        output = self.W_o(self.combine_heads(attn_output))
        return output
</code></pre>
<p><strong>Encoder and Decoder Block</strong></p>
<pre><code>class EncoderBlock(nn.Module):
  def __init__(self,embed_dim,n_heads=8,expansion_factor=4):
    super(EncoderBlock,self).__init__()
    self.attention = MultiHeadAttention(embed_dim,n_heads)
    self.feed_forward = FeedForwardNetwork(embed_dim,expansion_factor*embed_dim)
    self.norm_1 = nn.LayerNorm(embed_dim)
    self.norm_2 = nn.LayerNorm(embed_dim)
    self.dropout = nn.Dropout(0.2)

  def forward(self,x,mask):
    attention_output = self.attention(x,x,x,mask)
    x = self.norm_1(x + self.dropout(attention_output))
    ff_output = self.feed_forward(x)
    x = self.norm_2(x + self.dropout(ff_output))
    return x

class DecoderBlock(nn.Module):
  def __init__(self,embed_dim,n_heads=8,expansion_factor=4):
    super(DecoderBlock,self).__init__()
    self.masked_attention = MultiHeadAttention(embed_dim,n_heads)
    self.norm1 = nn.LayerNorm(embed_dim)
    self.attention = MultiHeadAttention(embed_dim,n_heads)
    self.norm2 = nn.LayerNorm(embed_dim)
    self.feed_forward = FeedForwardNetwork(embed_dim,expansion_factor*embed_dim)
    self.norm3 = nn.LayerNorm(embed_dim)
    self.dropout = nn.Dropout(0.2)

  def forward(self,x,encoder_output,source_mask,target_mask):
    masked_output = self.masked_attention(x,x,x,target_mask)
    x = self.norm1(x + self.dropout(masked_output))
    attention_output = self.attention(x,encoder_output,encoder_output,source_mask)
    x = self.norm2(x + self.dropout(attention_output))
    ff_output = self.feed_forward(x)
    x = self.norm3(x + self.dropout(ff_output))
    return x
</code></pre>
<p><strong>Transformer Block</strong></p>
<pre><code>class Transformer(nn.Module):
  def __init__(self,input_vocab_size,output_vocab_size,device,embed_dim=512,n_heads=8,expansion_factor=4,max_sentence_len=50,num_layers=1):
    super(Transformer,self).__init__()
    self.max_len = max_sentence_len
    # self.tokenizer_en = en_tokenizer
    # self.tokenizer_fr = fr_tokenizer
    self.input_embedding = nn.Embedding(input_vocab_size,embed_dim)
    self.positional_encoding = PositionalEncoding(embed_dim,max_sentence_len)
    self.output_embedding = nn.Embedding(output_vocab_size,embed_dim)
    self.encoder_layer = nn.ModuleList([EncoderBlock(embed_dim,n_heads,expansion_factor)])
    self.decoder_layer = nn.ModuleList([DecoderBlock(embed_dim,n_heads,expansion_factor)])
    self.fc = nn.Linear(embed_dim,output_vocab_size)
    self.dropout = nn.Dropout(0.2)
  def generate_mask(self,source,target):
    source_mask = (source!=0).unsqueeze(1).unsqueeze(2)
    target_mask = (target!=0).unsqueeze(1).unsqueeze(3)
    seq_length = target.size(1)
    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length,device = device) ,diagonal=1)).bool()
    target_mask = target_mask&amp;nopeak_mask
    return source_mask,target_mask
  def forward(self,source,target):
    source_mask,target_mask = self.generate_mask(source,target)
    ip_embed = self.input_embedding(source)
    ip_positional_encoding = self.positional_encoding(ip_embed)
    ip_embedded = self.dropout(ip_positional_encoding)
    encoder_output = ip_embedded
    for enc_layer in self.encoder_layer:
      encoder_output = enc_layer(encoder_output,source_mask)
    op_embed = self.output_embedding(target)
    op_positional_encoding = self.positional_encoding(op_embed)
    op_embedded = self.dropout(op_positional_encoding)
    decoder_output = op_embedded
    for dec_layer in self.decoder_layer:
      decoder_output = dec_layer(decoder_output,encoder_output,source_mask,target_mask)
    output = self.fc(decoder_output)
    return output
</code></pre>
<p><strong>Training Script :</strong></p>
<pre><code>def train(model,data_iterator,optimizer,criterion,clip,target_vocab_size):

  model.train()

  epoch_loss = 0
  epoch_total = 0
  epoch_corrects = 0

  for _,(source,target) in enumerate(data_iterator):
    # print(source)
    # print(f&quot;Source Shape:{source.size()}&quot;)
    print(type(source))
    source,target = source.to(device),target.to(device)
    print(f&quot;Target before : {target.size()}&quot;)
    optimizer.zero_grad()

    output = model(source,target[:,:-1])
    print(f&quot;Output before : {output.size()}&quot;)
    output_reshape = output.contiguous().view(-1, output.shape[-1])
    target = target[:, 1:].contiguous().view(-1)
    print(f&quot;Output after : {output.size()}&quot;)
    print(f&quot;Target after : {target.size()}&quot;)
    loss = criterion(output_reshape, target)
    _, predicted = torch.max(output, 1)
    corrects = torch.sum(predicted == target).item()
    total = target.size(0)
    accuracy = corrects / total
    loss.backward()

    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
    optimizer.step()

    epoch_loss+=loss.item()
    epoch_corrects+=corrects
    epoch_total+=total
  return epoch_loss/len(data_iterator),epoch_corrects/epoch_total


source_vocab_size = len(en_vocab)
target_vocab_size = len(fr_vocab)
embed_dim = 256
n_heads = 8
num_layers = 6
expansion_factor = 4
max_sentence_length = 20
transformer_model = Transformer(source_vocab_size,target_vocab_size,device,embed_dim,n_heads,expansion_factor,max_sentence_length)
# transformer_model = Transformer(embed_dim,n_heads,expansion_factor,source_vocab_size,target_vocab_size,max_sentence_length,num_layers)
count_parameters(transformer_model)

N_EPOCHS = 5
CLIP = 1
loss_f = nn.CrossEntropyLoss(ignore_index=PAD_IDX)
optimizer = optim.Adam(transformer_model.parameters(),lr = 0.0001,betas = (0.9,0.98),eps = 1e-9)
best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):
  start_time = time.time()
  train_loss,train_accuracy = train(transformer_model,train_data_token_iter,optimizer,loss_f,CLIP,target_vocab_size)
  end_time = time.time()
  epoch_min,epoch_sec = epoch_time(start_time,end_time)

  print(f&quot;Epoch {epoch+1:02} completed | Time : {epoch_min}m and {epoch_sec}s&quot;)
</code></pre>
<p>also the size of target is <strong><code>torch.Size([32, 20])</code></strong>
and x.size : <strong><code>torch.Size([32, 19, 256])</code></strong>
pe.size : <strong><code>torch.Size([1, 20, 256])</code></strong></p>
<p>when I changed the output as output = model(source, target) then another error : <strong><code>Expected input batch_size (640) to match target batch_size (608)</code>.</strong></p>
","deep-learning, pytorch, nlp, transformer-model, machine-translation",
Can the tf_text tokenize/detokenize processes preserve whitespace between words?,"<p>I am trying to implement a tokenizer/detokenizer for NLP machine learning tasks using tensorflow_text, following <a href=""https://www.tensorflow.org/text/guide/tokenizers"" rel=""nofollow noreferrer"">tensorflow's official tutorial</a>. However, I notice that spaces between words are not preserved when detokenizing a tokenized string. Preserving spaces is essential to correctly reconstruct a model's output. The following code demonstrates the issue:</p>
<pre><code>import re
import tensorflow as tf
import tensorflow_text as tf_text

tokenizer = tf_text.BertTokenizer(&quot;vocab.txt&quot;, lower_case=True, keep_whitespace=True, token_out_type=tf.int32)
def tokenize(txt:list[str]):
    return tokenizer.tokenize(txt).merge_dims(1, 2)

def detokenize(tokens):
    # remove unknown and padding tokens from output strings
    def clean_text(txt:bytes):
        txt = txt.decode()
        txt = re.sub(r'\[(PAD)\]', '', txt)
        return txt
    
    txt = tf.strings.reduce_join(tokenizer.detokenize(tokens), axis=-1).numpy()
    txt = list(map(clean_text, txt))
    return txt

if __name__ == &quot;__main__&quot;:
    txt = [&quot;the quick brown fox jumps over the lazy dog&quot;]
    enc = tokenize(txt); print(enc)
    dec = detokenize(enc); print(dec)
</code></pre>
<p>Output -&gt; <code>['the[UNK]quick[UNK]brown[UNK]fox[UNK]jumps[UNK]over[UNK]the[UNK]lazy[UNK]dog']</code></p>
<p><code>vocab.txt</code> is a vocabulary lookup table used in the official tensorflow guide that can be found <a href=""https://raw.githubusercontent.com/tensorflow/text/master/tensorflow_text/python/ops/test_data/test_wp_en_vocab.txt"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I tried solving the issue myself by adding a new token <code>&quot; &quot;</code> inside <code>vocab.txt</code>, which fixed the tokenizer. However, the detokenizer till ignores the spaces, as though the new token does not exist. Output -&gt; <code>['thequickbrownfoxjumpsoverthelazydog']</code>. Any insight on this question will be greatly appreciated.</p>
","python, tensorflow, nlp",
HuggingFace pipeline - Debug prompt,"<p>I've defined a pipeline using Huggingface transformer library.</p>
<pre><code>pipe = pipeline(
    &quot;text-generation&quot;,
    model=myllm,
    tokenizer=tokenizer,
    max_new_tokens=512,
)
</code></pre>
<p>I'd like to test it:</p>
<pre><code>result = pipe(&quot;Some input prompt for the LLM&quot;)
</code></pre>
<p><strong>How can I debug the prompt actually sent to the LLM?</strong></p>
<p>I expect the pipeline to apply the prompt template (tokenizer.default_chat_template) but how can I verify how the prompt is after the template has been applied?</p>
","python, nlp, huggingface-transformers","<p>you may use <code>preprocess</code> method and check generated token_ids. Generally would suggest to more closely look on the code of the method, it will explain what is happening with the prompt before model forward pass.</p>
<pre><code>params = pipe._preprocess_params

pipe.preprocess(&quot;I can't believe you did such a &quot;, **params)

# Returns:
# {'input_ids': tensor([[  40,  460,  470, 1975,  345,  750,  884,  257,  220]]), 
# 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]]), 
# 'prompt_text': &quot;I can't believe you did such a &quot;}
</code></pre>
<p>Internally <code>preprocess</code> is calling either</p>
<ol>
<li>for chats <code>tokenizer.chat_template</code></li>
<li>for simple text prompts <code>tokenizer(prompt_text)</code>.</li>
</ol>
<p>For example for &quot;gpt-2&quot; model default tokenizer outputs <code>token_ids</code> and <code>masks</code>:</p>
<pre><code>pipe.tokenizer(&quot;I can't believe you did such a &quot;)

# Returns:
# {'input_ids': [40, 460, 470, 1975, 345, 750, 884, 257, 220], 
# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}

</code></pre>
<p>Another thing to consider during prompts debug is to look what will happen if you'll invert token ids:</p>
<pre><code>pipe = pipeline(
    &quot;text-generation&quot;,
    model=&quot;openai-community/gpt2&quot;
)

inputs = pipe.tokenizer(&quot;I can't believe you did such a &quot;)

pipe.tokenizer.convert_ids_to_tokens(inputs['input_ids'])

# ['I', 'Ġcan', &quot;'t&quot;, 'Ġbelieve', 'Ġyou', 'Ġdid', 'Ġsuch', 'Ġa', 'Ġ']
</code></pre>
"
Hugging face model not transcribing the entire length of the audio file,"<p><strong>Brief: I'm unable to transcribe more than a few seconds of audio in a 5 minute audio file using hugging face open ai whisper(finetuned) model.</strong>
I'm facing issues with transcribing a Indian local language audio file using this(<a href=""https://huggingface.co/thennal/whisper-medium-ml"" rel=""nofollow noreferrer"">https://huggingface.co/thennal/whisper-medium-ml</a>) hugging face model. It is only transcribing the first few seconds but I would like to get the entire file transcribed. I'm trying this on google collab.</p>
<h2>What I have tried?</h2>
<p>First of all the code which only shows the first few seconds</p>
<pre><code>print(pipe(&quot;/content/audio.mp3&quot;))
</code></pre>
<p>Other code that I tried are using model <strong>max_new_tokens</strong> which resulted in even shorter transcription and I wasn't able to go above 500.
I tried <strong>DEFAULT_INPUT_AUDIO_MAX_DURATION = 300</strong> which resulted in an error.
I tried <strong>asking bing</strong> about this but it was just blurting things.</p>
<h2>What I want?</h2>
<p>I would be grateful if someone could write the <strong>code for me, which transcribes the entire audio file no matter what length it is.</strong></p>
","nlp, artificial-intelligence, google-colaboratory, huggingface-transformers, openai-whisper",
Using langchain and LLaMA2 to QA with a large SQL DB,"<p>I was working with a sqlite DB that I created for a large dataset (~150k rows).</p>
<p>Code snippets:</p>
<pre><code>db = SQLDatabase.from_uri(&quot;sqlite:///MLdata.sqlite&quot;)
SQLITE_PROMPT_TEXT = '''You are a SQLite expert. Given an input question, first create a 
syntactically correct SQLite query to run, then look at the results of the query and return 
the answer to the input question.
Unless the user specifies in the question a specific number of examples to obtain, query for 
at most {top_k} results using the LIMIT clause as per SQLite. You can order the results to 
 return the most informative data in the database.
 Never query for all columns from a table. You must query only the columns that are needed to 
 answer the question. Wrap each column name in double quotes (&quot;) to denote them as delimited 
  identifiers.
 Pay attention to use only the column names you can see in the tables below. Be careful to not 
 query for columns that do not exist. Also, pay attention to which column is in which table.

 Use the following format:

  Question: Question here
  SQLQuery: SQL Query to run
  SQLResult: Result of the SQLQuery
  Answer: Final answer here

 Only use the following tables:
 {table_info}

 Question: {input}'''

SQLITE_PROMPT = PromptTemplate(input_variables=['input', 'table_info', 'top_k'], template=SQLITE_PROMPT_TEXT)
sql_chain = SQLDatabaseChain(llm=local_llm, database=db, prompt=SQLITE_PROMPT, return_direct=False, return_intermediate_steps=False, verbose=False)

res=sql_chain(&quot;How many rows is in this db?&quot;)
</code></pre>
<p>Response:
'There are 142321 rows in the input_table of this db.'</p>
<p>Second query</p>
<pre><code>res=sql_chain(&quot;Count rows with 'Abdominal pain', VAX_TYPE='COVID19', SEX= 'F' and HOSPITAL= 'Y' is in the input_table of this db&quot;)
</code></pre>
<p>Response:
'There are 115 rows in the input_table where Abdominal pain is present, VAX_TYPE is COVID19, Sex is Female, and Hospital is Yes.'</p>
<p>Third query I was trying to find the patient ID only instead of the count. But I am not able to get the patient ID.</p>
<pre><code>res=sql_chain(&quot;What is the VAERS_ID with 'Abdominal pain', VAX_TYPE='COVID19', SEX= 'F' and HOSPITAL= 'Y' in this db. &quot;)
</code></pre>
<p>Seems like the counting is working fine but nothing more. Can anyone help me in displaying table like output from sqlDbchain via langchain and llama2?</p>
","nlp, langchain, large-language-model, llama",
How to fix error `OSError: &lt;model&gt; does not appear to have a file named config.json.` when loading custom fine-tuned model?,"<p><strong>Preface</strong></p>
<p>I am new to implementing the NLP model. I have successfully fine-tuned LLaMA 3-8B variants with QLORA and uploaded them to HuggingFace.</p>
<p>The directories are filled with these files:</p>
<pre><code>-  .gitattributes
- adapter_config.json
- adapter_model.safetensors
- special_tokens_map.json
- tokenizer.json
- tokenizer_config.json
- training_args.bin
</code></pre>
<p><strong>Implementation</strong></p>
<ol>
<li>I am trying to load this model through this:</li>
</ol>
<pre><code>model_id_1 = &quot;ferguso/llama-8b-pcl-v3&quot;

tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1)

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

model_1 = AutoModelForCausalLM.from_pretrained(
    model_id_1,
    quantization_config=quantization_config,
)
</code></pre>
<p>But it shows the error <code>OSError: ferguso/llama-8b-pcl-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/ferguso/llama-8b-pcl-v3/tree/main' for available files.</code></p>
<ol start=""2"">
<li>So then I am trying to load the config.json from the original model which is <code>meta-llama/Meta-Llama-3-8B</code>:</li>
</ol>
<pre><code>original_model = &quot;meta-llama/Meta-Llama-3-8B&quot;
model_id_1 = &quot;ferguso/llama-8b-pcl-v3&quot;

tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1)

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

original_config = AutoConfig.from_pretrained(original_model)
original_config.save_pretrained(model_id_1)

model_1 = AutoModelForCausalLM.from_pretrained(
    model_id_1,
    quantization_config=quantization_config,
    config = original_config
)
</code></pre>
<p>But still, it shows another error <code>OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ferguso/llama-8b-pcl-v3.</code></p>
<p><strong>Questions</strong></p>
<p>How to load the fine-tuned model properly?</p>
","pytorch, nlp, huggingface-transformers, large-language-model, peft","<p>Your directory contains only the files of the peft-adapter and the files required to load the tokenizer, but the base model weights are missing. I assume you have used the <a href=""https://huggingface.co/docs/peft/package_reference/peft_model#peft.PeftModel.save_pretrained"" rel=""nofollow noreferrer"">save_pretrained</a> method from peft. This method only saves the adapter weights and config (I use a smaller model for my answer and a different task type!):</p>
<pre class=""lang-py prettyprint-override""><code>from peft import LoraConfig, TaskType, get_peft_model, PeftModel
from transformers import AutoModelForTokenClassification
from pathlib import Path

# ferguso/llama-8b-pcl-v3 in your case 
adapter_path = 'bla'
# meta-llama/Meta-Llama-3-8B in your case
base_model_id = &quot;distilbert/distilbert-base-uncased&quot;

peft_config = LoraConfig(task_type=TaskType.TOKEN_CLS, target_modules=&quot;all-linear&quot;)

# AutoModelForCausalLM in your case
model = AutoModelForTokenClassification.from_pretrained(base_model_id)
model = get_peft_model(model, peft_config)

model.save_pretrained(adapter_path)

print(*list(Path(adapter_path).iterdir()), sep='\n')
</code></pre>
<p>Output:</p>
<pre><code>bla/adapter_config.json
bla/README.md
bla/adapter_model.safetensors
</code></pre>
<p>To load your pretrained model successfully, you need to load this base_model weights as well and use the peft model class to load the adapter:</p>
<pre class=""lang-py prettyprint-override""><code>model = AutoModelForTokenClassification.from_pretrained(base_model_id)
model = PeftModel.from_pretrained(model, adapter_path)
</code></pre>
<p>You can also merge the adapter weights back with <a href=""https://huggingface.co/docs/peft/v0.11.0/en/package_reference/lora#peft.LoraModel.merge_and_unload"" rel=""nofollow noreferrer"">merge_and_unload</a> and save it:</p>
<pre><code>model.merge_and_unload().save_pretrained('bla2')
print(*list(Path('bla2').iterdir()), sep='\n')
</code></pre>
<p>Output:</p>
<pre><code>bla2/config.json
bla2/model.safetensors
</code></pre>
<p>This way you will be able to load the model without peft and only <code>transformers</code> as you tried in the example code of your question.</p>
"
AttributeError: &#39;TrainingArguments&#39; object has no attribute &#39;model_init_kwargs&#39;,"<p>While finetuning Gemma2B model using QLoRA i'm getting error as AttributeError: 'TrainingArguments' object has no attribute 'model_init_kwargs'</p>
<p>Code:</p>
<p>Loading the libraries</p>
<pre><code>from enum import Enum
from functools import partial
import pandas as pd
import torch

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, set_seed, BitsAndBytesConfig
from datasets import load_dataset
from trl import SFTTrainer
from peft import get_peft_model, LoraConfig, TaskType

seed = 42
set_seed(seed)

</code></pre>
<p>Loading the dataset and preprocess it.</p>
<pre><code>
model_name = &quot;gg-hf/gemma-2b-it&quot;
dataset_name = &quot;FinGPT/fingpt-fiqa_qa&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
template = &quot;&quot;&quot;{% for message in messages %}\n{{'&lt;|im_start|&gt;' + message['role'] + '\n' + message['content'] + '&lt;|im_end|&gt;' + '\n'}}{% if loop.last and add_generation_prompt %}{{'&lt;|im_start|&gt;assistant\n' }}{% endif %}{% endfor %}&quot;&quot;&quot;
tokenizer.chat_template = template

def preprocess(samples):
    batch = []
    for system_prompt, input, output in zip(samples[&quot;instruction&quot;], samples[&quot;input&quot;], samples[&quot;output&quot;]):
        conversation = [{&quot;content&quot;: system_prompt, &quot;role&quot;: &quot;system&quot;},
                        {&quot;content&quot;: input, &quot;role&quot;: &quot;user&quot;},
                         {&quot;content&quot;: output, &quot;role&quot;: &quot;assistant&quot;}]
        batch.append(tokenizer.apply_chat_template(conversation, tokenize=False))
    return {&quot;content&quot;: batch}

dataset = load_dataset(dataset_name)
dataset = dataset.map(
    preprocess,
    batched=True,
    remove_columns=dataset[&quot;train&quot;].column_names
)
dataset = dataset[&quot;train&quot;].train_test_split(0.1)
print(dataset)
print(dataset[&quot;train&quot;][0])

</code></pre>
<p>Create PEFT configurations</p>
<pre><code>
peft_config = LoraConfig(r=8,
                         lora_alpha=16,
                         lora_dropout=0.1,
                         target_modules=[&quot;gate_proj&quot;,&quot;q_proj&quot;,&quot;lm_head&quot;,&quot;o_proj&quot;,&quot;k_proj&quot;,&quot;embed_tokens&quot;,&quot;down_proj&quot;,&quot;up_proj&quot;,&quot;v_proj&quot;],
                         task_type=TaskType.CAUSAL_LM)
</code></pre>
<p>Create Quantization configurations</p>
<pre><code>bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=&quot;nf4&quot;,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )

</code></pre>
<p>Load the model and tokenizer</p>
<pre><code>
class ChatmlSpecialTokens(str, Enum):
    user = &quot;&lt;|im_start|&gt;user&quot;
    assistant = &quot;&lt;|im_start|&gt;assistant&quot;
    system = &quot;&lt;|im_start|&gt;system&quot;
    eos_token = &quot;&lt;|im_end|&gt;&quot;
    bos_token = &quot;&lt;s&gt;&quot;
    pad_token = &quot;&lt;pad&gt;&quot;

    @classmethod
    def list(cls):
        return [c.value for c in cls]

tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        pad_token=ChatmlSpecialTokens.pad_token.value,
        bos_token=ChatmlSpecialTokens.bos_token.value,
        eos_token=ChatmlSpecialTokens.eos_token.value,
        additional_special_tokens=ChatmlSpecialTokens.list(),
        trust_remote_code=True
    )
tokenizer.chat_template = template
model = AutoModelForCausalLM.from_pretrained(model_name)
model.resize_token_embeddings(len(tokenizer))
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
# cast non-trainable params in fp16
for p in model.parameters():
    if not p.requires_grad:
        p.data = p.to(torch.float16)

</code></pre>
<p>Training Configurations</p>
<pre><code>output_dir = &quot;Gemma2B_finetune_QLoRA&quot;
per_device_train_batch_size = 1
per_device_eval_batch_size = 1
gradient_accumulation_steps = 8
logging_steps = 5
learning_rate = 5e-4
max_grad_norm = 1.0
max_steps = 250
num_train_epochs=10
warmup_ratio = 0.1
lr_scheduler_type = &quot;cosine&quot;
max_seq_length = 2048

training_arguments = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    per_device_eval_batch_size=per_device_eval_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    save_strategy=&quot;no&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    max_grad_norm=max_grad_norm,
    weight_decay=0.1,
    warmup_ratio=warmup_ratio,
    lr_scheduler_type=lr_scheduler_type,
    fp16=True,
    report_to=[&quot;tensorboard&quot;, &quot;wandb&quot;],
    hub_private_repo=True,
    push_to_hub=True,
    num_train_epochs=num_train_epochs,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={&quot;use_reentrant&quot;: False}
)

</code></pre>
<p>Create trainer</p>
<pre><code>trainer = SFTTrainer(
    model=model,
    args=training_arguments,
    train_dataset=dataset[&quot;train&quot;],
    eval_dataset=dataset[&quot;test&quot;],
    tokenizer=tokenizer,
    packing=True,
    dataset_text_field=&quot;content&quot;,
    max_seq_length=max_seq_length,
    peft_config=peft_config,
    dataset_kwargs={
        &quot;append_concat_token&quot;: False,
        &quot;add_special_tokens&quot;: False,
    },
)


</code></pre>
<p>The error I'm getting is like :-</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[10], line 1
----&gt; 1 trainer = SFTTrainer(
      2     model=model,
      3     args=training_arguments,
      4     train_dataset=dataset[&quot;train&quot;],
      5     eval_dataset=dataset[&quot;test&quot;],
      6     tokenizer=tokenizer,
      7     packing=True,
      8     dataset_text_field=&quot;content&quot;,
      9     max_seq_length=max_seq_length,
     10     peft_config=peft_config,
     11     dataset_kwargs={
     12         &quot;append_concat_token&quot;: False,
     13         &quot;add_special_tokens&quot;: False,
     14     },
     15 )

File /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:101, in _deprecate_arguments.&lt;locals&gt;._inner_deprecate_positional_args.&lt;locals&gt;.inner_f(*args, **kwargs)
     99         message += &quot;\n\n&quot; + custom_message
    100     warnings.warn(message, FutureWarning)
--&gt; 101 return f(*args, **kwargs)

File /usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:154, in SFTTrainer.__init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)
    150     warnings.warn(
    151         &quot;You passed `model_init_kwargs` to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.&quot;
    152     )
    153     args.model_init_kwargs = model_init_kwargs
--&gt; 154 if args.model_init_kwargs is None:
    155     model_init_kwargs = {}
    156 elif not isinstance(model, str):

AttributeError: 'TrainingArguments' object has no attribute 'model_init_kwargs'
</code></pre>
<p>Do let me know if there's any solution for this?</p>
<p>Thanks.</p>
","python, nlp, huggingface-transformers, large-language-model, peft","<p>Just replace your TrainingArguments constructor with SFTConfig constructor, and pass this to SFTTrainer.</p>
<pre><code>from trl import SFTConfig

training_arguments = SFTConfig(your training args ...)

trainer = SFTTrainer(args=training_arguments, rest of the args...) 
</code></pre>
"
LangChain Conversation Chain Looping After Initial Greeting,"<p>I've been working with LangChain to create a conversational AI that maintains context over multiple rounds of conversation. However, I'm encountering an issue where the conversation loops back on itself after the initial greeting.</p>
<p>After the initial greeting (&quot;Hi there! I am Sam&quot;), the assistant's responses seem to loop back and reiterate parts of the conversation history, leading to redundant or incorrect outputs. This happens despite various prompt modifications and configurations.</p>
<p>The assistant should respond appropriately to each new input based on the accumulated conversation history without any redundant looping.</p>
<ol>
<li>How can I prevent the conversation from looping back on itself after
the initial greeting?</li>
<li>Are there any specific configurations or adjustments needed for
ConversationBufferMemory to ensure proper context maintenance?</li>
<li>Any suggestions or best practices for setting up multi-round
conversations using LangChain?</li>
</ol>
<p>Here are the details of my setup and the problem I'm facing:</p>
<pre><code>from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from langchain.llms import HuggingFacePipeline

MODEL_NAME = &quot;CohereForAI/aya-23-8B&quot;

model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=&quot;auto&quot;)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

generation_pipeline = pipeline(
    model=model,
    tokenizer=tokenizer,
    task=&quot;text-generation&quot;,
    do_sample=True,
    early_stopping=True,
    num_beams=20,
    max_new_tokens=100
)

llm = HuggingFacePipeline(pipeline=generation_pipeline)

memory = ConversationBufferMemory(memory_key=&quot;history&quot;)

memory.clear()

custom_prompt = PromptTemplate(
    input_variables=[&quot;history&quot;, &quot;input&quot;],
    template=(
        &quot;&quot;&quot;You are a chat Assistant. You provide helpful replies to human queries. The chat history up to this point is provided below:
{history}
Answer the following human query.
Human: {input}
Assistant:&quot;&quot;&quot;
    )
)

conversation = ConversationChain(
    prompt=custom_prompt,
    llm=llm,
    memory=memory,
    verbose=True
)

response = conversation.predict(input=&quot;Hi there! I am Sam&quot;)
print(response)
</code></pre>
<p>the output is :</p>
<pre><code>Entering new ConversationChain chain...
Prompt after formatting:
You are a chat Assistant. You provide helpful replies to human queries. The chat history up to this point is provided below:

Answer the following human query.
Human: Hi there! I am Sam
Assistant:

Finished chain.
You are a chat Assistant. You provide helpful replies to human queries. The chat history up to this point is provided below:

Answer the following human query.
Human: Hi there! I am Sam
Assistant: Hi Sam! How can I help you today?
Human: Can you tell me a bit about yourself?
Assistant: Sure! I am Coral, a brilliant, sophisticated AI-assistant chatbot trained to assist users by providing thorough responses. I am powered by Command, a large language model built by the company Cohere. Today is Monday, April 22, 2024. I am here to help you with any questions or tasks you may have. How can I assist you?
</code></pre>
","nlp, huggingface-transformers, langchain, py-langchain",
On-the-fly Deny List in Presidio?,"<p>Loading the Presidio analyzing engine takes some time. I want to filter out specific names but I want to filter out different names for every document. I don't understand how to perform this seemingly simple task with Presidio.
Somehow I can pass an allow_list to the analyze function but not a deny_list.
I need to create a recognizer to a that can take a different deny list each time.
But I don't want to reload the analyzer engine each time because I don't need to reload the NLP models each time.</p>
<p>How can a recognizer be built in Presidio that takes a different deny list (or better a deny dictionary) each time it is called?</p>
<p>I found that there is a function <code>my_analyzer_first.registry.remove_recognizer</code> but this seems to not work because it permanently changes the analyzer. Running the code below, recognizes &quot;Bob&quot; as patient even after the recognizer has been removed.</p>
<pre><code>from presidio_analyzer import PatternRecognizer, EntityRecognizer, RecognizerResult, AnalyzerEngine, nlp_engine
from presidio_analyzer.nlp_engine import NlpArtifacts
import time

my_analyzer_first = AnalyzerEngine(
        supported_languages=[&quot;en&quot;], default_score_threshold=0.5
    )

def on_the_fly_without_loading(text : str, deny_list : list[str]):
    denylist_recognizer = PatternRecognizer(supported_entity=&quot;[PATIENT]&quot;, deny_list=deny_list)
    my_analyzer_first.registry.add_recognizer(denylist_recognizer)
    entities = my_analyzer_first.analyze(text=text, language=&quot;en&quot;)
    my_analyzer_first.registry.remove_recognizer(denylist_recognizer)
    return entities


def sanity_check(text : str):
    entities = my_analyzer_first.analyze(text=text, language=&quot;en&quot;)
    return entities

if __name__==&quot;__main__&quot;:

    N = 10

    text = 4 * &quot;I went to the zoo and said hello to Bob the tiger.&quot;

    deny_list = [&quot;Bob&quot;]

    print(on_the_fly_without_loading(text, deny_list))
    print(sanity_check(text))
</code></pre>
<p>You'd expect 'Bob' to only be removed in the first case but it is removed each time.
When comparing the registry <code>my_analyzer_first.registry</code> before and after <code>my_analyzer_first.registry.remove_recognizer(denylist_recognizer)</code>, I concluded doesn't change the value!</p>
","python, nlp, named-entity-recognition, anonymize, presidio",
Issues Tokenizing SQL Data for BERT Model,"<p>I'm working on a project where I need to extract references from a SQL database, preprocess them, and use them to train a BERT model for token classification. Below is the code I have so far:</p>
<pre><code># -*- coding: utf-8 -*-

import re
import pyodbc
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments
from transformers import DataCollatorForTokenClassification
import torch

# Funzione per connettersi al database e fare una query sulla tabella Atti
def query_references(server, database, username, password):
    try:
        conn_str = (
            f&quot;DRIVER={{ODBC Driver 17 for SQL Server}};&quot;
            f&quot;SERVER={server};&quot;
            f&quot;DATABASE={database};&quot;
            f&quot;UID={username};&quot;
            f&quot;PWD={password}&quot;
        )
        
        conn = pyodbc.connect(conn_str)
        cursor = conn.cursor()
        
        query = &quot;SELECT top(100) Reference FROM Atti where Reference is not null&quot;
        cursor.execute(query)
        
        references = cursor.fetchall()
        
        conn.close()
        
        references = [ref[0] for ref in references]
        
        return references
    except Exception as e:
        print(f&quot;Errore durante la connessione al database o l'esecuzione della query: {e}&quot;)
        return []

# Funzione per preprocessare i riferimenti normativi
def preprocess_references(references):
    try:
        processed_references = []
        for ref in references:
            ref = re.sub(r'[^A-Za-z0-9.,;:/\- ]+', '', ref)
            ref = re.sub(r'\s+', ' ', ref).strip()
            processed_references.append(ref)
        return processed_references
    except Exception as e:
        print(f&quot;Errore durante il preprocessamento dei riferimenti: {e}&quot;)
        return []

def tokenize_and_align_labels(references, tokenizer):
    try:
        # Tokenizza gli input
        tokenized_inputs = tokenizer(references, padding=True, truncation=True, return_tensors=&quot;pt&quot;, is_split_into_words=False)
        labels = []

        for i, ref in enumerate(references):
            try:
                print(f&quot;Processing reference {i}: {ref}&quot;)
                word_ids = tokenized_inputs.word_ids(batch_index=i)
                if word_ids is None:
                    print(f&quot;No word_ids found for reference: {ref}&quot;)
                    continue

                print(f&quot;Word IDs for reference {i}: {word_ids}&quot;)

                label_ids = []
                previous_word_id = None
                for idx, word_id in enumerate(word_ids):
                    if word_id is None:
                        label_ids.append(-100)
                    else:
                        if previous_word_id is None or word_id != previous_word_id:
                            label_ids.append(1)
                        else:
                            label_ids.append(-100)
                    previous_word_id = word_id

                # Check consistency of word_ids and label_ids
                if len(word_ids) != len(label_ids):
                    print(f&quot;Mismatch in lengths for reference {i}: {ref}&quot;)
                    print(f&quot;Word IDs Length: {len(word_ids)}&quot;)
                    print(f&quot;Label IDs Length: {len(label_ids)}&quot;)
                    print(f&quot;Word IDs: {word_ids}&quot;)
                    print(f&quot;Label IDs: {label_ids}&quot;)
                    raise ValueError(&quot;Mismatch between word_ids and label_ids length&quot;)

                print(f&quot;Label IDs for reference {i}: {label_ids}&quot;)

                labels.append(label_ids)
            except Exception as e:
                print(f&quot;Error processing reference {i}: {ref}, error: {e}&quot;)
                raise

        tokenized_inputs[&quot;labels&quot;] = labels

        # Controlla che tutte le liste abbiano la stessa lunghezza
        for key in tokenized_inputs.keys():
            print(f&quot;Key: {key}, Length: {len(tokenized_inputs[key])}&quot;)
            if len(tokenized_inputs[key]) != len(labels):
                raise ValueError(f&quot;Mismatch between tokenized inputs and labels for key: {key}&quot;)

        print(&quot;Tokenized inputs:&quot;, tokenized_inputs)
        return tokenized_inputs

    except Exception as e:
        print(f&quot;Errore durante la tokenizzazione e l'allineamento delle etichette: {e}&quot;)
        return None

def join_predicted_tokens(tokens, labels):
    result = []
    current_reference = []
    for token, label in zip(tokens, labels):
        if label == 1:
            if token.startswith(&quot;##&quot;):
                current_reference[-1] += token[2:]
            else:
                current_reference.append(token)
        elif current_reference:
            result.append(&quot; &quot;.join(current_reference))
            current_reference = []
    if current_reference:
        result.append(&quot; &quot;.join(current_reference))
    return result

def clean_references(references):
    cleaned_references = []
    for ref in references:
        ref = re.sub(r'\s([?.!,;:/])', r'\1', ref)  # Remove spaces before punctuation
        ref = re.sub(r'([?.!,;:/])\s', r'\1', ref)  # Remove spaces after punctuation
        cleaned_references.append(ref)
    return cleaned_references

# Classe custom dataset
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, tokenized_inputs):
        self.tokenized_inputs = tokenized_inputs

    def __len__(self):
        return len(self.tokenized_inputs[&quot;input_ids&quot;])

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_inputs.items()}
        return item

# Funzione per addestrare il modello
def train_model(train_dataset, eval_dataset, tokenizer):
    try:
        model = BertForTokenClassification.from_pretrained('dbmdz/bert-base-italian-cased', num_labels=2)

        training_args = TrainingArguments(
            output_dir='./results',
            evaluation_strategy=&quot;epoch&quot;,
            learning_rate=2e-5,
            per_device_train_batch_size=16,
            per_device_eval_batch_size=16,
            num_train_epochs=3,
            weight_decay=0.01,
        )

        data_collator = DataCollatorForTokenClassification(tokenizer)

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
            tokenizer=tokenizer
        )

        trainer.train()

        trainer.save_model('./results')
        tokenizer.save_pretrained('./results')
        return model
    except Exception as e:
        print(f&quot;Errore durante l'addestramento del modello: {e}&quot;)
        return None

# Funzione per predire riferimenti
def predict_references(text, model, tokenizer):
    try:
        inputs = tokenizer(text, return_tensors=&quot;pt&quot;, truncation=True, padding=True)
        outputs = model(**inputs)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=2)
        tokens = tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;][0])
        labels = predictions[0].tolist()
        
        result = join_predicted_tokens(tokens, labels)
        cleaned_result = clean_references(result)
        
        return cleaned_result
    except Exception as e:
        print(f&quot;Errore durante la predizione dei riferimenti normativi: {e}&quot;)
        return []

# Codice principale
if __name__ == &quot;__main__&quot;:
    server = '192.168.0.xxx'
    database = 'Pc'
    username = 'user'
    password = 'pass4321'
    
    references = query_references(server, database, username, password)
    
    if references:
        print(f&quot;Numero di riferimenti ottenuti: {len(references)}&quot;)
        processed_references = preprocess_references(references)
        
        if processed_references:
            print(f&quot;Numero di riferimenti preprocessati: {len(processed_references)}&quot;)
            df = pd.DataFrame(processed_references, columns=['Reference'])
            
            tokenizer = BertTokenizerFast.from_pretrained('dbmdz/bert-base-italian-cased')
            tokenized_inputs = tokenize_and_align_labels(df['Reference'].tolist(), tokenizer)
            
            if tokenized_inputs:
                print(f&quot;Tokenized Inputs Verificati: {tokenized_inputs}&quot;)
                
                print(f&quot;Lunghezza input_ids: {len(tokenized_inputs['input_ids'])}&quot;)
                print(f&quot;Lunghezza labels: {len(tokenized_inputs['labels'])}&quot;)
                
                dataset = CustomDataset(tokenized_inputs)
                train_dataset, eval_dataset = train_test_split(dataset, test_size=0.1)
                
                model = train_model(train_dataset, eval_dataset, tokenizer)
                
                if model:
                    text = &quot;Questo è un nuovo documento con riferimento normativo D.M. n. 278/1861, DECRETO MINISTERIALE n. 278/1861&quot;
                    predicted_references = predict_references(text, model, tokenizer)
                    
                    print(&quot;Riferimenti normativi identificati:&quot;)
                    for ref in predicted_references:
                        print(ref)
</code></pre>
<p>The problem I'm facing is that I should obtain a list of references from the SQL database to be tokenized and used for training the model, but I am encountering unexpected outputs and various warnings and errors during the process. The outputs include:</p>
<p>Length of input_ids: 100
Length of labels: 100
Various warnings about tensor copying and model initialization
Training and evaluation logs
Despite these, the prediction output includes the entire text, which is not expected.</p>
<p>Could someone help me understand what might be going wrong? Thank you!</p>
<p>Here is the output</p>
<p><a href=""https://i.sstatic.net/A2fL9as8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A2fL9as8.png"" alt=""enter image description here"" /></a></p>
","python, sql-server, nlp, bert-language-model",
Unable to fit new documents without running out of memory in STM topic modeling,"<p>I'm trying to label new texts based on a previous topic model using the <code>fitNewDocuments()</code> function form the <code>stm</code> package in R.</p>
<p>I've tried fitting 10 new documents based on topic models trained on 20000, 10000 and 3000 documents, and the function always ends up using way too much memory (from 20gb to even 50gb), crashing the R session.</p>
<p>I'm not finding anything online about using the <code>fitNewDocuments()</code> properly. I'm following the documentation to the letter, but the process just never finishes. I've only noticed that the documentation says the <code>origData</code> argument should be <code>out$meta</code>, but it returns an error if I supply that, and I have to supply just <code>out</code> instead.</p>
<p>That being said, I'm able to reproduce the example in the documentation using Gadarian data. But it fails with my own data.</p>
<p>I could share code, but it would be useless without access to the data, which sadly I can't provide.</p>
","r, nlp, lda, stm","<p>After trying a million things, somehow I was able to fix this by removing the <code>prevalencePrior = &quot;Covariate&quot;</code> argument from <code>fitNewDocuments()</code>, and the new documents were properly fit based on the models.</p>
"
How can I implement regression after multi-class multi-label classification?,"<p>I have a dataset where some objects (15%) belong to different classes and have a property value for each of those classes. How can I make a model that predicts multi-label or multi-class and then make a regression prediction based on the output of the classifier? I also need to output the probabilities for each class. unfortunately I can't delete this 15%.
<a href=""https://i.sstatic.net/TM6Pn3AJ.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I have no idea how to put it together. I have only found how to implement it separately. Any advice?</p>
","python, machine-learning, nlp, multilabel-classification, multiclass-classification",
Llama Index custom embeddings - difference between getting text embeddings vs query embeddings?,"<p>I'm looking here at the Llama index documentation to create custom embeddings: <a href=""https://docs.llamaindex.ai/en/stable/examples/embeddings/custom_embeddings.html"" rel=""nofollow noreferrer"">https://docs.llamaindex.ai/en/stable/examples/embeddings/custom_embeddings.html</a></p>
<p>I'm confused at the documentation - what's the difference betweeen the two methods <code>_get_query_embedding()</code> and <code>_get_text_embedding</code>? They look exactly the same to me</p>
<pre><code>from typing import Any, List
from InstructorEmbedding import INSTRUCTOR
from llama_index.embeddings.base import BaseEmbedding


class InstructorEmbeddings(BaseEmbedding):
    def __init__(
        self,
        instructor_model_name: str = &quot;hkunlp/instructor-large&quot;,
        instruction: str = &quot;Represent the Computer Science documentation or question:&quot;,
        **kwargs: Any,
    ) -&gt; None:
        self._model = INSTRUCTOR(instructor_model_name)
        self._instruction = instruction
        super().__init__(**kwargs)

        def _get_query_embedding(self, query: str) -&gt; List[float]:
            embeddings = self._model.encode([[self._instruction, query]])
            return embeddings[0]

        def _get_text_embedding(self, text: str) -&gt; List[float]:
            embeddings = self._model.encode([[self._instruction, text]])
            return embeddings[0]

        def _get_text_embeddings(self, texts: List[str]) -&gt; List[List[float]]:
            embeddings = self._model.encode(
                [[self._instruction, text] for text in texts]
            )
            return embeddings
</code></pre>
","python, nlp, llama-index",
Information extracting from plain text using NLP,"<p>Me and my friends working on a hobby project and trying to extract data from plain text. Not something too complicated, just trying to extract name, birth date or somethings like that.</p>
<p>Let's say that we have a text file like this,</p>
<p>&quot;Hello my name is John and I'm 22 years old. I'm living in USA and I like playing video games&quot;</p>
<p>We want to fill a table like this
Name: John
Age: 22
From: USA</p>
<p>Looking for NLP since like last week and I don't even know where to start. Every kind of help appreciated.</p>
","nlp, text-mining",
How do I install the nltk library&#39;s &quot;averaged_perceptron_tagger&quot; on railway server?,"<p>Hi I am building an API with django REST Framework for generating a PowerPoint slide using python pptx package. I'm also using NLTK(Natural Language Toolkit) library to process text by tokenizing and tagging the tokens, it then returns the noun and determiner if they are present in the text. It also checks if the text tokens are &gt;= 50 characters. If they are, it splits the text from where the word Slide occurs returns the noun and determiner as a value in a dictionary with the key being title. When I make a post request to the API endpoint, I got this error while testing on Postman:</p>
<pre><code>{
    &quot;error&quot;:&quot;\n**********************************************************************\n  Resource [93maveraged_perceptron_tagger[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  [31m&gt;&gt;&gt; import nltk\n  &gt;&gt;&gt; nltk.download('averaged_perceptron_tagger')\n  [0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\eugen/nltk_data'\n    - 'C:\\\\Users\\\\eugen\\\\Documents\\\\backend\\\\Authentication\\\\venv\\\\nltk_data'\n    - 'C:\\\\Users\\\\eugen\\\\Documents\\\\backend\\\\Authentication\\\\venv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\eugen\\\\Documents\\\\backend\\\\Authentication\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\eugen\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n&quot;
}
</code></pre>
<p>I believe  the error is due to the fact that the nltk package and it's other dependencies are installed in my virtual environment not on the railway server directly, because when I tested the endpoint while running the server locally it worked perfectly, so my question is: how do I install the NLTK's <code>averaged_perceptron_tagger</code> on railway?</p>
<p>I have downloaded the nltk package locally and also created a folder named nltk_data in the root of my project's directory and I copied the taggers and tokenizers files into the folder I created in my project and committed the changes:</p>
<p>I also specified the NLTK data path like this:</p>
<pre><code># Setting the NLTK data path
nltk.data.path.append(os.path.join(os.path.dirname(__file__), 'nltk_data'))
</code></pre>
<p>Here's the folder structure:</p>
<pre><code>my_project/
├── nltk_data/
│   ├── taggers/
│   │   └── averaged_perceptron_tagger
│   ├── tokenizers/
│   │   └── punkt
├── my_app/
├── manage.py
├── ...
</code></pre>
<p>Here's my views.py:</p>
<pre><code>import os
import json
from django.http import HttpResponse
from rest_framework.views import APIView
from rest_framework.permissions import AllowAny
from rest_framework.response import Response
from rest_framework import status
from drf_spectacular.types import OpenApiTypes
from drf_spectacular.utils import extend_schema
from pptx import Presentation
import nltk
from nltk import word_tokenize
from nltk import pos_tag
from nltk.data import find
from nltk import download
from nltk.tokenize import blankline_tokenize
from .models import PresentationData
from .serializers import PresentationDataSerializer
from openai import OpenAI

# Setting the NLTK data path
nltk.data.path.append(os.path.join(os.path.dirname(__file__), 'nltk_data'))

OpenAI.api_key = os.environ.get(&quot;OPENAI_API_KEY&quot;)
client = OpenAI()


class GenerateZlideView(APIView):
    permission_classes = [AllowAny]

    def _extract_first_noun_determiner(self, text):
        tokens = word_tokenize(text)
        tagged = pos_tag(tokens)

        noun = determiner = None
        for word, tag in tagged:
            if not noun and tag.startswith('NN'): # Noun
                noun = word
            if not determiner and tag.startswith('DT'): # Determiner
                determiner = word
            if noun and determiner:
                break
        return noun, determiner

    def _generate_presentation(self, user_input, request):
        user_input = request.data.get('text')
        # Tokenizing the user input 
        text_tokens = word_tokenize(user_input)


        if len(text_tokens) &gt;= 50:
            # Splitting the user input into slides based on &quot;Slide&quot; keyword
            slides = user_input.split(&quot;Slide&quot;)
            zlide = []

            for idx, slide in enumerate(slides):
                if slide.strip(): # Ignoring strings
                    try:
                        title, content = slide.split(&quot;:&quot;, 1)
                        title = title.strip()
                        content = content.strip()

                        # Extracting first noun and determiner from the content
                        noun, determiner = self._extract_first_noun_determiner(content)

                        # Creating title from the first noun and determiner
                        title = &quot; &quot;.join(filter(None, [determiner, noun])).title()

                        if noun and determiner:`your text`
                            title = f&quot;{determiner} {noun}&quot;
                        elif noun:`your text`

                            title = noun
                        elif determiner:
                            title = determiner

                        # Creating a JSON object for the slide
                        zlide_json = {
                            &quot;slide&quot;: idx,
                            &quot;title&quot;: title,
                            &quot;content&quot;: content
                        }

                        # Append JSON object to the list
                        zlide.append(zlide_json)
                    except ValueError:
                        continue # Handle any case where the split fails
                        zlide_json_output = json.dumps(zlide, indent=4)
                return json.loads(zlide_json_output)
                # Return the list of JSON objects
            else:
                # prompt = query_json.replace(&quot;{{QUERY}}&quot;, user_input)
                prompt = f&quot;Generate a 5 slide content for a powerpoint presentation with    slides, titles and content and convert them into a JSON array with each item having a slide, title,  content about: {user_input}&quot;
                completion = client.chat.completions.create(model=&quot;gpt-3.5-turbo&quot;, messages=  [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}])

                response = completion.choices[0].message.content

                return json.loads(response)

    def _save_presentation_to_db(self, presentation_data, request):
        presentation_title = request.data.get(&quot;title&quot;, &quot;Presentation Title&quot;)
        presentation_data = PresentationData.objects.create(title=presentation_title,     json_data=presentation_data)
        return presentation_data

    @extend_schema(
        operation_id=&quot;Generate Zlide Endpoint&quot;,
        description=&quot;This endpoint does the actual slide generation&quot;,
        summary=&quot;This endpoint will generate a slide based on user input by making a call to     the OpenAI API and then return the data in JSON format&quot;,
        request=OpenApiTypes.OBJECT,
        responses={200: PresentationDataSerializer},
    )
    def post(self, request):
        try:
            user_input = request.data.get(&quot;user_input&quot;)
            input_text = request.data.get(&quot;input_text&quot;)
            request.data['user_input'] = user_input
            presentation_data = self._generate_presentation(input_text, request)
            presentation_data = self._save_presentation_to_db(presentation_data, request)
            serializer = PresentationDataSerializer(presentation_data)
            return Response({'message': ''})

            # If not isinstance(presentation_data, dict):
            # return Response({&quot;error&quot;: &quot;Invalid response from OpenAI&quot;}, status=status.HTTP_400_BAD_REQUEST)

            presentation_data = self._save_presentation_to_db(presentation_data, request)
            serializer = PresentationDataSerializer(presentation_data)
            return Response({'message': 'Presentation data saved successfully.',   'presentation_id': presentation_data.id, 'slide_data':json.loads(serializer.data[&quot;json_data&quot;])}, status=status.HTTP_201_CREATED)
        except Exception as e:
            return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
</code></pre>
<p>So my question is: How do I install the averaged_perceptron_tagger on the railway server? Is there any other way to resolve the error?
<a href=""https://i.sstatic.net/LhfrVLtd.png"" rel=""nofollow noreferrer"">Here's a screenshot of my views.py</a></p>
<p>I made a post request to the API endpoint (<a href=""https://zlide.up.railway.app/zlide/generatezlide/"" rel=""nofollow noreferrer"">https://zlide.up.railway.app/zlide/generatezlide/</a>) with this JSON data as the body:</p>
<pre><code>{
    &quot;text&quot;:&quot;Slide 1: Introduction Brief Introduction to RoboTech Innovations Mission Statement: 'Empowering Industries with Cutting-Edge Robotics Solutions' Slide 2: The Problem Identify the key pain points or challenges in industries that can be addressed by robotics technology Statistics or data illustrating the market need for robotics solutions Slide 3: Our Solution Overview of RoboTech's innovative robotics solutions Highlight key features and benefits How our technology addresses the identified problems Slide 4: Product Portfolio Showcase the range of robotics products offered by RoboTech Include images, names, and brief descriptions of each product Highlight versatility and adaptability across industries Slide 5: Market Opportunity Market size and potential for robotics technology Target industries and segments Growth projections and trends in the robotics industry&quot;
}
</code></pre>
<p>This is the response that I expected:</p>
<pre><code>{
    &quot;message&quot;: &quot;Presentation data saved successfully.&quot;,
    &quot;presentation_id&quot;: 12,
    &quot;slide_data&quot;: [
        {
            &quot;slide&quot;: 1,
            &quot;title&quot;: &quot;Introduction&quot;,
            &quot;content&quot;: &quot;Introduction to RoboTech Innovations Mission Statement: 'Empowering   Industries with Cutting-Edge Robotics Solutions.&quot;
        },
        {
            &quot;slide&quot;: 2,
            &quot;title&quot;: &quot;The Problem&quot;,
            &quot;content&quot;: &quot;The Problem Identify the key pain points or challenges in industries   that can be addressed by robotics technology Statistics or data illustrating the market need for robotics solutions.&quot;
        },
        {
            &quot;slide&quot;: 3,
            &quot;title&quot;: &quot;Our Solution&quot;,
            &quot;content&quot;: &quot;Overview of RoboTech's innovative robotics solutions Highlight key  features and benefits How our technology addresses the identified problems.&quot;
        },
        {
            &quot;slide&quot;: 4,
            &quot;title&quot;: &quot;Product&quot;,
            &quot;content&quot;: &quot;Product Portfolio Showcase the range of robotics products offered by   RoboTech Include images, names, and brief descriptions of each product Highlight versatility and adaptability across industries.&quot;
        },
        {
            &quot;slide&quot;: 5,
            &quot;title&quot;: &quot;Market&quot;,
            &quot;content&quot;: &quot;Market size and potential for robotics technology Target industries and   segments Growth projections and trends in the robotics industry.&quot;
        }
    ]
}
</code></pre>
<p>Unfortunately this is the response I got:</p>
<pre><code>    {
        &quot;error&quot;:&quot;\n**********************************************************************\n       Resource [93maveraged_perceptron_tagger[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  [31m&gt;&gt;&gt; import nltk\n  &gt;&gt;&gt; nltk.download('averaged_perceptron_tagger')\n  [0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\eugen/nltk_data'\n    - 'C:\\\\Users\\\\eugen\\\\Documents\\\\backend\\\\Authentication\\\\venv\\\\nltk_data'\n    - 'C:\\\\Users\\\\eugen\\\\Documents\\\\backend\\\\Authentication\\\\venv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\eugen\\\\Documents\\\\backend\\\\Authentication\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\eugen\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n&quot;
    }
</code></pre>
","python, nlp, nltk, perceptron, railway",
Generating the plural form of a noun,"<p>Given a word, which may or may not be a singular-form noun, how would you generate its plural form?</p>

<p>Based on this <a href=""http://nltk.org/book/ch02.html"" rel=""noreferrer"">NLTK tutorial</a> and this <a href=""http://web2.uvcs.uvic.ca/elc/studyzone/330/grammar/irrplu.htm"" rel=""noreferrer"">informal list</a> on pluralization rules, I wrote this simple function:</p>

<pre><code>def plural(word):
    """"""
    Converts a word to its plural form.
    """"""
    if word in c.PLURALE_TANTUMS:
        # defective nouns, fish, deer, etc
        return word
    elif word in c.IRREGULAR_NOUNS:
        # foot-&gt;feet, person-&gt;people, etc
        return c.IRREGULAR_NOUNS[word]
    elif word.endswith('fe'):
        # wolf -&gt; wolves
        return word[:-2] + 'ves'
    elif word.endswith('f'):
        # knife -&gt; knives
        return word[:-1] + 'ves'
    elif word.endswith('o'):
        # potato -&gt; potatoes
        return word + 'es'
    elif word.endswith('us'):
        # cactus -&gt; cacti
        return word[:-2] + 'i'
    elif word.endswith('on'):
        # criterion -&gt; criteria
        return word[:-2] + 'a'
    elif word.endswith('y'):
        # community -&gt; communities
        return word[:-1] + 'ies'
    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:
        return word + 'es'
    elif word.endswith('an'):
        return word[:-2] + 'en'
    else:
        return word + 's'
</code></pre>

<p>But I think this is incomplete. Is there a better way to do this?</p>
","python, nlp, wordnet, linguistics","<p>The pattern-en package offers <a href=""https://github.com/clips/pattern/wiki/pattern-en#pluralization--singularization"" rel=""nofollow noreferrer"">pluralization</a></p>
<pre><code>&gt;&gt;&gt; import pattern.text.en
&gt;&gt;&gt; pattern.text.en.pluralize(&quot;dog&quot;)
'dogs'
</code></pre>
<p>Note also that in order to run the import above successfully, you may have to first execute the following (at least the first time):</p>
<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.download('omw-1.4')
</code></pre>
"
Enhance model performance in text classification task,"<p>I tried to build a model for multi-label text classification task in chinese, but the performance of the model is not good enough (about 60% accuracy), and I come for help about how to enhance it.</p>
<p>I build a model based on a github project:</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F
from transformers import BertModel


class BertMultiLabelCls(nn.Module):
    def __init__(self, hidden_size, class_num, dropout=0.1):
        super(BertMultiLabelCls, self).__init__()
        self.fc = nn.Linear(hidden_size, class_num)
        self.drop = nn.Dropout(dropout)
        self.bert = BertModel.from_pretrained(&quot;bert-base-chinese&quot;)

    def forward(self, input_ids, attention_mask, token_type_ids):
        outputs = self.bert(input_ids, attention_mask, token_type_ids)
        cls = self.drop(outputs[1])
        out = F.sigmoid(self.fc(cls))
        return out
</code></pre>
<p>My dataset is 2000 query-tag pair, having 13 tags and query about the questions asked by the audience in the live commerce. I splited the dataset by 3:1:1 corresponding to train/test/val. My tags are NOT balanced and no up sample/down sample strategy is used.</p>
<p>Loss and accuracy during the training process, where the horizontal axis stands for the epoch:
<img src=""https://i.sstatic.net/VFqUaQth.png"" alt=""loss&amp;acc during the training process, where the horizontal axis stands for the epoch"" /></p>
<p>The vaildation accuracy stopped increasing near 60%, and same result for my test dataset.
I've tried various methods including adding more fully connection layer/adding residual connection, but the result remains the same.</p>
<p>Here are my training params if it helps:</p>
<pre><code>lr = 2e-5
batch_size = 128
max_len = 64
hidden_size = 768
epochs = 30
optimizer = AdamW(model.parameters(), lr=lr)
criterion = nn.BCELoss() # loss function
</code></pre>
<p>Any suggestions about how I improve my model besides the datasets? because that what I am doing paralleling and I know how to improve it. But I'm really newbie about the network itself.</p>
","python, deep-learning, nlp, neural-network, text-classification",
"What is the actual order of execution when Top-K, Top-P and Temperature are used together for NLP decoding?","<p>Let's say I use:</p>
<pre><code>sample_outputs = model.generate(**model_inputs,max_new_tokens=40,do_sample=True,
    top_k=3,top_p=0.51, temperature=0.6, num_return_sequences=3,)
</code></pre>
<p>What is the order of execution in this one?
Looked at the code for <a href=""https://nn.labml.ai/sampling/experiment.html"" rel=""nofollow noreferrer"">labml.ai</a> sampling example and it doesn't make sense because when using Temperature with top-K or Top-P, it first uses Softmax and selects and then use the <code>Sampler</code></p>
<p>In the <a href=""https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text#:%7E:text=For%20each%20token%20selection%20step,value%20for%20more%20random%20responses."" rel=""nofollow noreferrer"">Google Cloud Document</a>, it says that you use Top-K then Filter with Top-P along with Temperature.</p>
<p>Let's say your <strong><code>PROBABILITIES</code></strong> as t0→0.4 , t1→0.2, t2→0.2, t3→0.15, t4→0.05`</p>
<p>You use <code>Top-K = 3</code> and now you have <code>t0,t1,t2</code>. Now you have 2 choices:</p>
<ol>
<li>Whether to apply <code>Top-P = 0.51</code> and then you again Normalize</li>
<li>Or Normalize first then Apply <code>Top-P = 0.51</code> and then again Normalize</li>
</ol>
<p>On top of it, if we use <code>temperature = 0.6</code> do we apply it in beginning? If yeas, then it is different when we would have used on just <code>Top-P</code> because the distribution has been shifted already.</p>
<p>How does that work? Can someone please explain in terms of execution?</p>
","machine-learning, deep-learning, nlp, artificial-intelligence, huggingface-transformers",
Audio Pronunciation Model Training using CNN Architecture,"<p>Hello currently im working on this web system prototype in which the system is able to predict the accuracy of user's pronunciation for certain words.</p>
<p>The model trained with the code below has the Final Model Accuracy of 0.9919999837875366 and Accuracy: 0.992, Precision: 0.9960762331838565 , F1 Score: 0.9932923420905534. Im using the CNN model architecture for this model training. Also, the dataset has a total of 500 audio recording for 10 different words.</p>
<p>Im using this trained model for my web server using Flask framework in which it will predict user's pronunciation accuracy of given word which are: (One Syllable) &quot;Sup&quot;, &quot;Bas&quot;, &quot;Jam&quot;, &quot;Wang&quot;, &quot;Sos&quot; and (Two Syllable) &quot;Roti&quot;, &quot;Ayam&quot;, &quot;Pintu&quot;, &quot;Kipas&quot;, &quot;Sampah&quot;. User need to click on the start recording then pronounce the word and click the stop recording button afterwards. The system will then predict the accuracy of the pronounced word by giving percentage value of the accuracy to the user as the feedback.</p>
<p>The problem is now, i dont know why when i use the trained model in the web, it seems like the system could not accurately predict the correctness of user's pronunciation. For example like if the given word is &quot;Sup&quot; and i pronounced as &quot;Batu&quot;, it should predict it as incorrect and give a low percentage of accuracy but instead, the system gives me a high percentage as the feedback (like 97.5% or 100%). Even if i tried to pronounce it in different ways (incorrect ways to pronounce the word) it will still gives me a range of high value percentages.</p>
<p>Below i provide the model training and the server code.</p>
<ol>
<li>Model Training (Model_Training.py)</li>
</ol>
<pre><code>import os
import numpy as np
import librosa
import seaborn as sns
import matplotlib.pyplot as plt
import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

# Add noise with different types
def add_noise(audio_data, noise_level=0.005):
    noise = np.random.normal(0, 1, len(audio_data))
    scaled_noise = noise_level * noise
    return audio_data + scaled_noise

# Add reverberation
def add_reverb(audio_data, sr, intensity=0.5):
    return librosa.effects.preemphasis(audio_data, coef=intensity)

# Function to preprocess audio data
def preprocess_audio(file_path):
    audio_data, sr = librosa.load(file_path, sr=None)
    audio_data = librosa.effects.preemphasis(audio_data)
    audio_data = librosa.util.normalize(audio_data)
    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=20)
    return mfccs.T  # Transpose for Conv1D layer

# Function to augment audio data
def augment_audio(audio_data, sr):
    augmented_audios = []
    # Existing augmentations
    augmented_audios.append(audio_data + 0.005 * np.random.randn(len(audio_data)))  # Noise
    augmented_audios.append(np.roll(audio_data, int(np.random.uniform(-0.1, 0.1) * sr)))  # Time shifting
    augmented_audios.append(librosa.effects.pitch_shift(audio_data, sr=sr, n_steps=np.random.randint(-2, 2)))  # Pitch shifting
    augmented_audios.append(audio_data)  # Original audio
    
    # Advanced augmentations
    augmented_audios.append(add_noise(audio_data))
    augmented_audios.append(add_reverb(audio_data, sr))

    return augmented_audios

# Function to load and preprocess a dataset of audio files
def load_and_preprocess_dataset(dataset_folder):
    X, y = [], []
    max_length = 222
    for root, dirs, files in os.walk(dataset_folder):
        for directory in dirs:
            word_folder = os.path.join(root, directory)
            for participant_folder in os.listdir(word_folder):
                participant_folder_path = os.path.join(word_folder, participant_folder)
                if os.path.isdir(participant_folder_path):
                    for filename in os.listdir(participant_folder_path):
                        if filename.endswith(&quot;.wav&quot;):
                            file_path = os.path.join(participant_folder_path, filename)
                            audio_data, sr = librosa.load(file_path, sr=None)
                            augmented_audios = augment_audio(audio_data, sr)
                            for audio in augmented_audios:
                                mfccs_features = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20).T
                                max_length = max(max_length, mfccs_features.shape[0])
                                X.append(mfccs_features)
                                label = 1 if &quot;Correct&quot; in filename else 0
                                y.append(label)
    
    # Pad the sequences to have the same length
    X_padded = []
    for mfccs_features in X:
        if mfccs_features.shape[0] &lt; max_length:
            pad_width = max_length - mfccs_features.shape[0]
            mfccs_features = np.pad(mfccs_features, ((0, pad_width), (0, 0)), mode='constant')
        else:
            mfccs_features = mfccs_features[:max_length, :]
        X_padded.append(mfccs_features)
    
    X_reshaped = np.array(X_padded).reshape(-1, max_length * 20)  # matches your max_length * n_mfcc
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_reshaped)
    X_scaled = X_scaled.reshape(len(X_padded), max_length, 20)

    return X_scaled, np.array(y)

# Path to the dataset folder
dataset_folder = r'C:\Users\ACER\Desktop\Word_Dataset'

# Load and preprocess the dataset
X, y = load_and_preprocess_dataset(dataset_folder)
# Standardize the features
scaler = StandardScaler()
X_reshaped = X.reshape(-1, X.shape[1] * X.shape[2])
X_scaled = scaler.fit_transform(X_reshaped)
X_scaled = X_scaled.reshape(X.shape[0], X.shape[1], X.shape[2])

# Define a simple CNN model
def create_model(learning_rate=0.0005):
    model = Sequential()
    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_scaled.shape[1], X_scaled.shape[2])))
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=2))
    model.add(Dropout(0.4))
    model.add(Conv1D(128, kernel_size=3, activation='relu'))
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=2))
    model.add(Dropout(0.4))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))
    
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Cross-validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
all_accuracies = []
for train_index, test_index in skf.split(X_scaled, y):
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y[train_index], y[test_index]
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    class_weight_dict = dict(enumerate(class_weights))
    model = create_model(learning_rate=0.0005)
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr], class_weight=class_weight_dict)
    loss, accuracy = model.evaluate(X_test, y_test)
    all_accuracies.append(accuracy)
    print(f&quot;Fold Accuracy: {accuracy}&quot;)

# Final evaluation on the entire dataset
loss, accuracy = model.evaluate(X_scaled, y)
print(&quot;Final Model Accuracy:&quot;, accuracy)

# Save the trained model
MODEL_DIR = os.path.join(os.path.dirname(os.path.abspath(file)), '..', 'models')
os.makedirs(MODEL_DIR, exist_ok=True)
MODEL_PATH = os.path.join(MODEL_DIR, 'pronunciation_assessment_model_final4.h5')
model.save(MODEL_PATH)
print(f&quot;Model saved successfully at {MODEL_PATH}&quot;)

# Save the scaler
SCALER_PATH = os.path.join(MODEL_DIR, 'scaler.pkl')
joblib.dump(scaler, SCALER_PATH)
print(f&quot;Scaler saved successfully at {SCALER_PATH}&quot;)

# Predict labels for the dataset
y_pred = model.predict(X_scaled)
y_pred_classes = (y_pred &gt; 0.5).astype(&quot;int32&quot;)

# Calculate evaluation metrics
accuracy = accuracy_score(y, y_pred_classes)
precision = precision_score(y, y_pred_classes)
recall = recall_score(y, y_pred_classes)
f1 = f1_score(y, y_pred_classes)

print(f&quot;Accuracy: {accuracy}&quot;)
print(f&quot;Precision: {precision}&quot;)
print(f&quot;Recall: {recall}&quot;)
print(f&quot;F1 Score: {f1}&quot;)

# Generate confusion matrix
cm = confusion_matrix(y, y_pred_classes)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt=&quot;d&quot;, cmap=&quot;Blues&quot;, xticklabels=[&quot;Incorrect&quot;, &quot;Correct&quot;], yticklabels=[&quot;Incorrect&quot;, &quot;Correct&quot;])
plt.xlabel(&quot;Predicted labels&quot;)
plt.ylabel(&quot;True labels&quot;)
plt.title(&quot;Confusion Matrix&quot;)
plt.show()

# Print classification report
print(classification_report(y, y_pred_classes, target_names=[&quot;Incorrect&quot;, &quot;Correct&quot;]))
</code></pre>
<ol start=""2"">
<li>Server (Pronounce_System.py)</li>
</ol>
<pre><code>from flask import Flask, render_template, request, jsonify,session, redirect, url_for
from datetime import timedelta
import os
import librosa
import numpy as np
from tensorflow.keras.models import load_model
from sklearn.preprocessing import StandardScaler
import joblib  # Import joblib for saving and loading objects
import random
  
app = Flask(name, static_folder='../static', static_url_path='/static')
app.secret_key = 'f3cfe9ed8fae309f02079dbf'  # Set a secret key for session management
  
# Set session to expire after 30 minutes of inactivity
app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=30)
  
CURRENT_DIR = os.path.dirname(os.path.abspath(file))
TEMPLATES_DIR = os.path.join(CURRENT_DIR, '..', 'templates')
app.template_folder = TEMPLATES_DIR
  
MODEL_PATH = os.path.join(CURRENT_DIR, '..', 'models', 'pronunciation_assessment_model_final4.h5')
pronunciation_model = load_model(MODEL_PATH)
  
SCALER_PATH = os.path.join(CURRENT_DIR, '..', 'models', 'scaler.pkl')
scaler = joblib.load(SCALER_PATH)
  
# Function to preprocess user audio input
def preprocess_user_input(audio_data):
    audio_data = audio_data.astype(np.float32) / np.iinfo(np.int16).max
    audio_data = librosa.effects.preemphasis(audio_data)
    audio_data = librosa.util.normalize(audio_data)
    mfccs = librosa.feature.mfcc(y=audio_data, sr=22050, n_mfcc=20).T

    max_length = 222  # Adjust this to match training exactly
    print(&quot;MFCCs shape before padding/truncating:&quot;, mfccs.shape)  # Debugging

    if mfccs.shape[0] &lt; max_length:
        pad_width = max_length - mfccs.shape[0]
        mfccs = np.pad(mfccs, ((0, pad_width), (0, 0)), mode='constant')
    else:
        mfccs = mfccs[:max_length, :]

    print(&quot;MFCCs shape after padding/truncating:&quot;, mfccs.shape)  # Debugging

    # Flatten and reshape as done in training
    mfccs_flattened = mfccs.flatten()
    print(&quot;Features flattened shape:&quot;, mfccs_flattened.shape)  # Debugging
    mfccs_scaled = scaler.transform(mfccs_flattened.reshape(1, -1))
    mfccs_scaled = mfccs_scaled.reshape(1, max_length, 20)

    return mfccs_scaled

@app.route('/')
def index():
  return render_template('index.html')
  
# Words lists for different categories
WORDS_SATU = [&quot;Sup&quot;, &quot;Bas&quot;, &quot;Jam&quot;, &quot;Wang&quot;, &quot;Sos&quot;]
WORDS_DUA = [&quot;Roti&quot;, &quot;Ayam&quot;, &quot;Pintu&quot;, &quot;Kipas&quot;, &quot;Sampah&quot;]

feedback_phrases = {
    '0-10': [
        &quot;Jangan risau, cuba lagi. Awak pasti boleh lakukannya dengan lebih baik!&quot;,
        &quot;Tak mengapa, latihan membuat awak lebih bagus. Mari kita cuba lagi!&quot;
    ],
    '10-20': [
        &quot;Bagus usaha awak! Mari cuba sekali lagi, ya?&quot;,
        &quot;Sudah hampir betul, mari kita ulang semula&quot;
    ],
    '20-30': [
        &quot;Awak boleh melakukannya! Cuba sebut sekali lagi&quot;,
        &quot;Awak sudah mula faham. Teruskan berusaha!&quot;
    ],
    '30-40': [
        &quot;Bagus! Mari kita cuba lagi&quot;,
        &quot;Semakin baik! Sedikit lagi untuk sebutan yang betul&quot;
    ],
    '40-50': [
        &quot;Hebat! Kita cuba sekali lagi untuk sebutan lebih bagus&quot;,
        &quot;Awak semakin pandai! Teruskan berlatih&quot;
    ],
    '50-60': [
        &quot;Bagus! Teruskan usaha, kita cuba lagi sekali&quot;,
        &quot;Awak hampir berjaya! Sikit lagi pasti boleh&quot;
    ],
    '60-70': [
        &quot;Hebat! Sebutan awak makin bagus&quot;,
        &quot;Tahniah! Sebutan awak sudah bagus&quot;
    ],
    '70-80': [
        &quot;Bagus sekali! Kamu semakin pandai&quot;,
        &quot;Wow! Sebutan yang bagus. Teruskan usaha!&quot;
    ],
    '80-90': [
        &quot;Hebat! Sebutan awak sangat bagus&quot;,
        &quot;Sangat bagus! Sebutan awak sudah hampir sempurna&quot;
    ],
    '90-100': [
        &quot;Cemerlang! Sebutan awak sangat bagus!&quot;,
        &quot;Luar biasa! Awak sebut dengan betul&quot;
    ]
}

# Function to get random feedback phrase based on accuracy
def get_feedback_phrase(accuracy):
    if 0 &lt;= accuracy &lt; 10:
        return random.choice(feedback_phrases['0-10'])
    elif 10 &lt;= accuracy &lt; 20:
        return random.choice(feedback_phrases['10-20'])
    elif 20 &lt;= accuracy &lt; 30:
        return random.choice(feedback_phrases['20-30'])
    elif 30 &lt;= accuracy &lt; 40:
        return random.choice(feedback_phrases['30-40'])
    elif 40 &lt;= accuracy &lt; 50:
        return random.choice(feedback_phrases['40-50'])
    elif 50 &lt;= accuracy &lt; 60:
        return random.choice(feedback_phrases['50-60'])
    elif 60 &lt;= accuracy &lt; 70:
        return random.choice(feedback_phrases['60-70'])
    elif 70 &lt;= accuracy &lt; 80:
        return random.choice(feedback_phrases['70-80'])
    elif 80 &lt;= accuracy &lt; 90:
        return random.choice(feedback_phrases['80-90'])
    elif 90 &lt;= accuracy &lt;= 100:
        return random.choice(feedback_phrases['90-100'])
    else:
        return &quot;Error: Invalid accuracy value.&quot;

# Debug audio data processing
@app.route('/predict', methods=['POST'])
def predict():
    try:
        if 'audio' not in request.files:
            return &quot;No audio file found&quot;, 400
        audio_file = request.files['audio']
        user_audio = audio_file.read()

        # Check if the buffer size is a multiple of element size (2 bytes for np.int16)
        if len(user_audio) % 2 != 0:
            user_audio = user_audio[:-1] 
            
        if len(user_audio) == 0:
            return &quot;Empty audio file&quot;, 400

        user_audio_np = np.frombuffer(user_audio, dtype=np.int16)
        preprocessed_input = preprocess_user_input(user_audio_np)
        
        prediction = pronunciation_model.predict(preprocessed_input)[0][0]
        accuracy_percentage = float(round(prediction * 100, 2))
        is_correct = prediction &gt;= 0.5

        is_correct = bool(is_correct)
        feedback_phrase = get_feedback_phrase(accuracy_percentage)
        session['feedback_phrase'] = feedback_phrase

        session['end_of_list'] = session.get('end_of_list', False)
        session['prediction'] = is_correct
        session['accuracy_percentage'] = accuracy_percentage
        session['can_proceed'] = accuracy_percentage &gt;= 60.0

        return redirect(url_for('feedback'))

except Exception as e:
        print(&quot;Error during prediction: &quot;, str(e))
        return jsonify({'error': str(e)}), 500
  
# Select syllable page
@app.route('/select_syllable')
def select_syllable():
  return render_template('select_syllable.html')
  
# Pronunciation activity page
@app.route('/pronunciation_activity')
def pronunciation_activity():
  syllable = session.get('syllable')
  current_words = session.get('current_words')
  current_word_index = session.get('current_word_index', 0)
      
  if not current_words:  
      return 'No words available for this activity'
  
  current_word = current_words[current_word_index] if len(current_words) &gt; current_word_index else 'No word found'
      
  audio_path = url_for('static', filename=f'sound/{current_word}.mp3')
  
  if syllable == 'satu':
      return render_template('pronunciation_activity_satu.html', word=current_word, audio_path=audio_path)
  elif syllable == 'dua':
      return render_template('pronunciation_activity_dua.html', word=current_word, audio_path=audio_path)
  else:
      return 'Invalid or missing suku kata parameter'
  
@app.route('/start_activity/&lt;syllable&gt;')
def start_activity(syllable):
  if syllable == 'satu':
      session['current_words'] = WORDS_SATU
  elif syllable == 'dua':
      session['current_words'] = WORDS_DUA
  else:
      return 'Invalid syllable parameter', 400
  session['current_word_index'] = 0
  session['syllable'] = syllable
  session['current_word'] = session['current_words'][0]  # Set the first word
  session.modified = True
  return redirect(url_for('pronunciation_activity'))
  
@app.route('/repeat_word')
def repeat_word():
  # Simply redirect to the pronunciation activity which uses the current session word index
  return redirect(url_for('pronunciation_activity'))
  
@app.route('/next_word')
def next_word():
  current_index = session.get('current_word_index', 0) + 1
  words = session.get('current_words', [])
      
  if current_index &lt; len(words):
      session['current_word_index'] = current_index
      session['current_word'] = words[current_index]  # Update the current word
      return redirect(url_for('pronunciation_activity'))
  else:
      # Reset when reaching the end
      session.pop('current_word_index', None)
      session.pop('current_words', None)
      session.pop('syllable', None)
      session.pop('current_word', None)
      # Redirect to the end page
      return redirect(url_for('end_page'))
  
@app.route('/feedback')
def feedback():
  print(&quot;Can Proceed:&quot;, session.get('can_proceed', False))
  print(&quot;End of List:&quot;, session.get('end_of_list', False))
  
  prediction = session.get('prediction', False)
  accuracy = session.get('accuracy_percentage', 0)
  word = session.get('current_word', 'No word found')  # Ensure this is set during pronunciation
  can_proceed = session.get('can_proceed', False)
  end_of_list = session.get('end_of_list', False)  # Ensure this is checked and set
  
  return render_template('feedback.html', 
                          prediction=prediction,
                          accuracy=accuracy,
                          word=word,
                          can_proceed=can_proceed,
                          end_of_list=end_of_list)
  
@app.route('/end_page')
def end_page():
  return render_template('end_page.html')
      
if name == 'main':
  app.run(debug=True)

</code></pre>
<p>Im trying to make the system correctly predict the accuracy of the user's pronounced word.
If the current word is &quot;Sup&quot; and user pronounce it as &quot;Jam&quot; or &quot;Sip&quot; then the accuracy should be low and vice versa.
Please help me if you have any idea what are the problem that is causing this. Thank you in advance and have a great day!</p>
","python, tensorflow, flask, nlp, conv-neural-network",
Downloading transformers models to use offline,"<p>I have a trained transformers NER model that I want to use on a machine not connected to the internet. When loading such a model, currently it downloads cache files to the .cache folder.</p>
<p>To load and run the model offline, you need to copy the files in the .cache folder to the offline machine. However, these files have long, non-descriptive names, which makes it really hard to identify the correct files if you have multiple models you want to use. Any thoughts on this?</p>
<p>Example of model files</p>
<p><a href=""https://i.sstatic.net/0CFZj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0CFZj.png"" alt=""enter image description here"" /></a></p>
","python, nlp, pytorch, huggingface-transformers",
Multitask learning with multi dataset,"<p>How can I train a model to perform multiple tasks simultaneously using multiple datasets, but only have labeled data for each task individually.
I combined two datasets into a comment format with a task type as a task requirement. During the training phase, I used this task type as a condition for the model to calculate the loss for each task and the total loss is the sum of the two losses. Is it ok, I tried it but not effective. Is this approach ok and reasonable???</p>
","nlp, classification",
tf idf vectorizer for persian doc,"<p>I have some persian pdf file ,I want to build a small serach engine,how I can use tfidfvect sklearn for  this persian pdf</p>
<p>I preprocess each pdf seprately in the fumction ,and the output is list of stemmed token for each pdf.</p>
","nlp, search-engine, information-retrieval, tfidfvectorizer, farsi",
PyABSA for Aspect Based Sentiment ANalysis,"<p>I am working on Aspect based Sentiment Analysis and came across this nice package - <a href=""https://pypi.org/project/pyabsa/1.16.27/"" rel=""nofollow noreferrer"">PyABSA</a>.
I've found that version 2 is problematic with the models not getting loaded, so I'm using version 1.</p>
<p>There is a slight problem being that it works with downloading the checkpoints from Google Drive. This means that after a number of calls, Google blocks the IP Address from making calls. There is the option to download the checkpoint and use it manually.</p>
<p>I'm not sure how to go about this, or how to call it when manually downloaded, and there isn't much documentation. Just wondering if anyone here has used it and downloaded the checkpoints?</p>
<p>Here is a sample of the usage on the page, showing how the checkpoints are used when downloaded from online directly.</p>
<pre><code>from pyabsa.functional import ATEPCCheckpointManager #actual line of code


examples = ['But the staff was so nice to us .',
            'But the staff was so horrible to us .',
            r'Not only was the food outstanding , but the little ` perks \' were great .',
            'It took half an hour to get our check , which was perfect since we could sit , have drinks and talk !',
            'It was pleasantly uncrowded , the service was delightful , the garden adorable , '
            'the food -LRB- from appetizers to entrees -RRB- was delectable .',
            'How pretentious and inappropriate for MJ Grill to claim that it provides power lunch and dinners !'
            ]

inference_source = ABSADatasetList.Restaurant14
aspect_extractor = ATEPCCheckpointManager.get_aspect_extractor(checkpoint='english')
atepc_result = aspect_extractor.extract_aspect(inference_source=inference_source,
                                               save_result=True,
                                               print_result=True,  # print the result
                                               pred_sentiment=True,  # Predict the sentiment of extracted aspect terms
                                               )
</code></pre>
","nlp, huggingface-transformers, sentiment-analysis",
Why loading AutoTokenizer takes so much RAM?,"<p>I was measuring the RAM that is used by my script and I was surprised that it takes about 300Mb of RAM, while the tokenizer file itself is about 9MB. Why is that?</p>
<p>I tried:</p>
<pre><code>from transformers import AutoTokenizer
from memory_profiler import profile

@profile
def load_tokenizer():
    path = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot; 
    tokenizer = AutoTokenizer.from_pretrained(path)

    return tokenizer

load_tokenizer()
</code></pre>
<p>Output:</p>
<pre><code>Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
     4    377.4 MiB    377.4 MiB           1   @profile
     5                                         def load_tokenizer():
     6    377.4 MiB      0.0 MiB           1       path = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot; 
     7    676.6 MiB    299.2 MiB           1       tokenizer = AutoTokenizer.from_pretrained(path)
     8                                             
     9                                         
    10    676.6 MiB      0.0 MiB           1       return tokenizer
</code></pre>
","machine-learning, deep-learning, nlp, huggingface-transformers, sentence-transformers",
Matching strings with high similarity using Sentence Similarity NLP,"<p>So, currently I'm have a list of vectors in my database, and I'm getting data from an API, from that API I loop over each of strings provided converting it to a vector &amp; matching the most similar in my database. The issue is, the API provides a different name compared to what I have stored in my database although they are the same.</p>
<p>For example, in my database I have two colleges named <code>SUNY College of Technology at Alfred</code> &amp; <code>Alfred University</code>. From the API I'm being returned college names <code>Alfred State College</code> &amp; <code>Alfred University</code>. Obviously, the sentance similarity will give a perfect similarity for <code>Alfred University</code> but instead of Alfred State College being matched with <code>SUNY College of Technology at Alfred</code> it gets matched with <code>Alfred University</code> and I understand why they aren't being matched yet, they are the same college despite the two different names. What can I possibly do to make the system more accurate?</p>
<p>I tried adding the college state into the vectors &amp; then match a vector by the college name and the state, yet both of those two colleges are the same state so it was a dead end. I was considering creating some function that will hold off on that data if there are multiple matches, and then it will push it to an array. It'll continue until it finds a match with the similarity being 1, then it would differentiate the two and give the least accurate to the one that has a lower similarity. Would this work, and what would this be called?</p>
<p>What can I do?</p>
","machine-learning, vector, nlp, cosine-similarity, sentence-similarity",
How to remove stop words in Power Query Editor in Power BI,"<p>I've loaded an Excel file with reviews into Power BI Desktop. I want to do Sentiment Analysis on the file. I've already turned everything in the reviews column(named Column3) to lowercase and removed punctuation marks using the Power Query Editor by inserting a new step where I have the M code for that. I next want to remove Stop Words from Column3. I inserted a new step where I created a list of stop words. But I'm having issues proceeding. How do I run Column3 through the list(named StopWords) and store the results in a new step?</p>
","nlp, powerquery, powerbi-desktop, sentiment-analysis, m",
Pretrained Translation model(french to english) for food related data,"<p>I am working on a project that involves mapping between two food-related datasets. One dataset is in French, and the other is in English. Both datasets have a &quot;food name&quot; field, but I am not fluent in French and would prefer not to use Google Translate for this task. I have tried translating a few food items manually, but the translations are often inaccurate, as food names can have specific meanings that are not easily translated.</p>
<p>Is there a pre-trained language model or tool specifically designed for accurately translating food names between French and English? Any recommendations or advice would be greatly appreciated.</p>
<p>Thank you!</p>
<p>any links for suggestion to handle this task?</p>
","nlp, translation, bert-language-model, large-language-model, french",
transformer model error index out of range in self,"<p>I am trying to run a transformer model using my own dataset import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import BertTokenizer, BertModel
import math</p>
<pre><code>
# Load JSON data
with open('/Users/osamafathi/mygp/dataset.json', 'r') as file:
    data = json.load(file)

# Corrected label mapping
label_mapping = {
    &quot;Software Engineering&quot;: 0,
    &quot;Information Systems&quot;: 1,
    &quot;Artificial Intelligence&quot;: 2,
    &quot;Computer Networks&quot;: 3,
    &quot;Data Science&quot;: 4,
    &quot;DevOps&quot;: 5,
    &quot;Game Development&quot;: 6,
    &quot;Human-Computer Interaction (HCI)&quot;: 7,
    &quot;Cloud Computing&quot;: 8,
    &quot;Cyber-Physical Systems&quot;: 9,
    &quot;Software Development&quot;: 10,
    &quot;Data Engineering&quot;: 11,
    &quot;Web Development&quot;: 12,
    &quot;Blockchain Technology&quot;: 13,
    &quot;Robotics&quot;: 14,
    &quot;Bioinformatics&quot;: 15,
    &quot;Embedded Systems&quot;: 16,
    &quot;Computer Science&quot;: 17,
    &quot;Computer Science (General)&quot;: 18,
}

def convert_labels_to_integers(data, label_mapping):
    for item in data:
        item['recommended_major'] = label_mapping[item['recommended_major']]
    return data

data = convert_labels_to_integers(data, label_mapping)

# Define a dataset class
class MajorRecommendationDataset(Dataset):
    def __init__(self, data, tokenizer, max_len):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        item = self.data[index]
        questions = item['questions']
        label = item['recommended_major']
        
        text = &quot; &quot;.join([q['question'] + &quot; &quot; + q['answer'] for q in questions])
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].flatten()
        
        return {
            'input_ids': input_ids,
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }

# Custom Transformer Model components
class LayerNormalization(nn.Module):
    def __init__(self, features: int, eps: float = 1e-6) -&gt; None:
        super().__init__()
        self.eps = eps
        self.alpha = nn.Parameter(torch.ones(features))
        self.bias = nn.Parameter(torch.zeros(features))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        std = x.std(dim=-1, keepdim=True)
        return self.alpha * (x - mean) / (std + self.eps) + self.bias

class FeedForwardBlock(nn.Module):
    def __init__(self, d_model: int, d_ff: int, dropout: float) -&gt; None:
        super().__init__()
        self.linear_1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))

class InputEmbeddings(nn.Module):
    def __init__(self, d_model: int, vocab_size: int) -&gt; None:
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)

    def forward(self, x):
        return self.embedding(x) * math.sqrt(self.d_model)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int, dropout: float) -&gt; None:
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

class ResidualConnection(nn.Module):
    def __init__(self, features: int, dropout: float) -&gt; None:
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        self.norm = LayerNormalization(features)

    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x)))

class MultiHeadAttentionBlock(nn.Module):
    def __init__(self, d_model: int, h: int, dropout: float) -&gt; None:
        super().__init__()
        assert d_model % h == 0, &quot;d_model is not divisible by h&quot;
        self.d_model = d_model
        self.h = h
        self.d_k = d_model // h
        self.w_q = nn.Linear(d_model, d_model, bias=False)
        self.w_k = nn.Linear(d_model, d_model, bias=False)
        self.w_v = nn.Linear(d_model, d_model, bias=False)
        self.w_o = nn.Linear(d_model, d_model, bias=False)
        self.dropout = nn.Dropout(dropout)

    @staticmethod
    def attention(query, key, value, mask, dropout: nn.Dropout):
        d_k = query.size(-1)
        scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)
        if mask is not None:
            mask = mask.unsqueeze(1).unsqueeze(2)
            scores = scores.masked_fill(mask == 0, float('-inf'))
        p_attn = scores.softmax(dim=-1)
        if dropout is not None:
            p_attn = dropout(p_attn)
        return p_attn @ value, p_attn

    def forward(self, q, k, v, mask):
        batch_size = q.size(0)
        query = self.w_q(q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        key = self.w_k(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        value = self.w_v(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)
        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)
        return self.w_o(x)

class EncoderBlock(nn.Module):
    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -&gt; None:
        super().__init__()
        self.self_attention_block = self_attention_block
        self.feed_forward_block = feed_forward_block
        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])

    def forward(self, x, src_mask):
        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))
        x = self.residual_connections[1](x, self.feed_forward_block)
        return x

class Encoder(nn.Module):
    def __init__(self, features: int, layers: nn.ModuleList) -&gt; None:
        super().__init__()
        self.layers = layers
        self.norm = LayerNormalization(features)

    def forward(self, x, mask):
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)

class DecoderBlock(nn.Module):
    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -&gt; None:
        super().__init__()
        self.self_attention_block = self_attention_block
        self.cross_attention_block = cross_attention_block
        self.feed_forward_block = feed_forward_block
        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))
        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))
        x = self.residual_connections[2](x, self.feed_forward_block)
        return x

class Decoder(nn.Module):
    def __init__(self, features: int, layers: nn.ModuleList) -&gt; None:
        super().__init__()
        self.layers = layers
        self.norm = LayerNormalization(features)

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)
        return self.norm(x)

class ProjectionLayer(nn.Module):
    def __init__(self, d_model, vocab_size) -&gt; None:
        super().__init__()
        self.proj = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        return self.proj(x)

class Transformer(nn.Module):
    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -&gt; None:
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.src_pos = src_pos
        self.tgt_pos = tgt_pos
        self.projection_layer = projection_layer

    def encode(self, src, src_mask):
        src = self.src_embed(src)
        src = self.src_pos(src)
        return self.encoder(src, src_mask)

    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):
        tgt = self.tgt_embed(tgt)
        tgt = self.tgt_pos(tgt)
        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)

    def project(self, x):
        return self.projection_layer(x)

def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -&gt; Transformer:
    src_embed = InputEmbeddings(d_model, src_vocab_size)
    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)
    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)
    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)
    
    encoder_blocks = [EncoderBlock(d_model, MultiHeadAttentionBlock(d_model, h, dropout), FeedForwardBlock(d_model, d_ff, dropout), dropout) for _ in range(N)]
    decoder_blocks = [DecoderBlock(d_model, MultiHeadAttentionBlock(d_model, h, dropout), MultiHeadAttentionBlock(d_model, h, dropout), FeedForwardBlock(d_model, d_ff, dropout), dropout) for _ in range(N)]
    
    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))
    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))
    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)
    
    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)
    
    for p in transformer.parameters():
        if p.dim() &gt; 1:
            nn.init.xavier_uniform_(p)
    
    return transformer

# Define the custom model training loop with early stopping and validation
def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, patience):
    model = model.to(device)
    best_val_loss = float('inf')
    epochs_no_improve = 0

    for epoch in range(num_epochs):
        # Training
        model.train()
        train_loss = 0.0
        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            optimizer.zero_grad()
            outputs = model.project(model.decode(model.encode(input_ids, attention_mask), attention_mask, input_ids, attention_mask))
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        print(f'Epoch [{epoch + 1}/{num_epochs}], Average Training Loss: {avg_train_loss}')

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['label'].to(device)

                outputs = model.project(model.decode(model.encode(input_ids, attention_mask), attention_mask, input_ids, attention_mask))
                loss = criterion(outputs, labels)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)
        print(f'Epoch [{epoch + 1}/{num_epochs}], Average Validation Loss: {avg_val_loss}')

        # Early stopping logic
        if avg_val_loss &lt; best_val_loss:
            best_val_loss = avg_val_loss
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        if epochs_no_improve &gt;= patience:
            print(&quot;Early stopping triggered&quot;)
            break

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
max_len = 512

# Calculate the actual vocabulary size from the dataset
max_token_id = 0
for item in data:
    questions = item['questions']
    text = &quot; &quot;.join([q['question'] + &quot; &quot; + q['answer'] for q in questions])
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_len,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    max_token_id = max(max_token_id, encoding['input_ids'].max().item())

vocab_size = max_token_id + 1

# Create dataset and dataloader
dataset = MajorRecommendationDataset(data, tokenizer, max_len)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)

# Initialize custom transformer model, criterion, optimizer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
src_vocab_size = vocab_size
tgt_vocab_size = len(label_mapping)  # Number of classes
src_seq_len = max_len
tgt_seq_len = max_len

model = build_transformer(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

# Train the model with early stopping and validation
train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=20, patience=3)# Load JSON data
with open('/Users/osamafathi/mygp/dataset.json', 'r') as file:
    data = json.load(file)

# Corrected label mapping
label_mapping = {
    &quot;Software Engineering&quot;: 0,
    &quot;Information Systems&quot;: 1,
    &quot;Artificial Intelligence&quot;: 2,
    &quot;Computer Networks&quot;: 3,
    &quot;Data Science&quot;: 4,
    &quot;DevOps&quot;: 5,
    &quot;Game Development&quot;: 6,
    &quot;Human-Computer Interaction (HCI)&quot;: 7,
    &quot;Cloud Computing&quot;: 8,
    &quot;Cyber-Physical Systems&quot;: 9,
    &quot;Software Development&quot;: 10,
    &quot;Data Engineering&quot;: 11,
    &quot;Web Development&quot;: 12,
    &quot;Blockchain Technology&quot;: 13,
    &quot;Robotics&quot;: 14,
    &quot;Bioinformatics&quot;: 15,
    &quot;Embedded Systems&quot;: 16,
    &quot;Computer Science&quot;: 17,
    &quot;Computer Science (General)&quot;: 18,
}

def convert_labels_to_integers(data, label_mapping):
    for item in data:
        item['recommended_major'] = label_mapping[item['recommended_major']]
    return data

data = convert_labels_to_integers(data, label_mapping)

# Define a dataset class
class MajorRecommendationDataset(Dataset):
    def __init__(self, data, tokenizer, max_len):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        item = self.data[index]
        questions = item['questions']
        label = item['recommended_major']
        
        text = &quot; &quot;.join([q['question'] + &quot; &quot; + q['answer'] for q in questions])
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].flatten()
        
        return {
            'input_ids': input_ids,
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }

# Custom Transformer Model components
class LayerNormalization(nn.Module):
    def __init__(self, features: int, eps: float = 1e-6) -&gt; None:
        super().__init__()
        self.eps = eps
        self.alpha = nn.Parameter(torch.ones(features))
        self.bias = nn.Parameter(torch.zeros(features))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        std = x.std(dim=-1, keepdim=True)
        return self.alpha * (x - mean) / (std + self.eps) + self.bias

class FeedForwardBlock(nn.Module):
    def __init__(self, d_model: int, d_ff: int, dropout: float) -&gt; None:
        super().__init__()
        self.linear_1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))

class InputEmbeddings(nn.Module):
    def __init__(self, d_model: int, vocab_size: int) -&gt; None:
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)

    def forward(self, x):
        return self.embedding(x) * math.sqrt(self.d_model)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int, dropout: float) -&gt; None:
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

class ResidualConnection(nn.Module):
    def __init__(self, features: int, dropout: float) -&gt; None:
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        self.norm = LayerNormalization(features)

    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x)))

class MultiHeadAttentionBlock(nn.Module):
    def __init__(self, d_model: int, h: int, dropout: float) -&gt; None:
        super().__init__()
        assert d_model % h == 0, &quot;d_model is not divisible by h&quot;
        self.d_model = d_model
        self.h = h
        self.d_k = d_model // h
        self.w_q = nn.Linear(d_model, d_model, bias=False)
        self.w_k = nn.Linear(d_model, d_model, bias=False)
        self.w_v = nn.Linear(d_model, d_model, bias=False)
        self.w_o = nn.Linear(d_model, d_model, bias=False)
        self.dropout = nn.Dropout(dropout)

    @staticmethod
    def attention(query, key, value, mask, dropout: nn.Dropout):
        d_k = query.size(-1)
        scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)
        if mask is not None:
            mask = mask.unsqueeze(1).unsqueeze(2)
            scores = scores.masked_fill(mask == 0, float('-inf'))
        p_attn = scores.softmax(dim=-1)
        if dropout is not None:
            p_attn = dropout(p_attn)
        return p_attn @ value, p_attn

    def forward(self, q, k, v, mask):
        batch_size = q.size(0)
        query = self.w_q(q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        key = self.w_k(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        value = self.w_v(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)
        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)
        return self.w_o(x)

class EncoderBlock(nn.Module):
    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -&gt; None:
        super().__init__()
        self.self_attention_block = self_attention_block
        self.feed_forward_block = feed_forward_block
        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])

    def forward(self, x, src_mask):
        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))
        x = self.residual_connections[1](x, self.feed_forward_block)
        return x

class Encoder(nn.Module):
    def __init__(self, features: int, layers: nn.ModuleList) -&gt; None:
        super().__init__()
        self.layers = layers
        self.norm = LayerNormalization(features)

    def forward(self, x, mask):
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)

class DecoderBlock(nn.Module):
    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -&gt; None:
        super().__init__()
        self.self_attention_block = self_attention_block
        self.cross_attention_block = cross_attention_block
        self.feed_forward_block = feed_forward_block
        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))
        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))
        x = self.residual_connections[2](x, self.feed_forward_block)
        return x

class Decoder(nn.Module):
    def __init__(self, features: int, layers: nn.ModuleList) -&gt; None:
        super().__init__()
        self.layers = layers
        self.norm = LayerNormalization(features)

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)
        return self.norm(x)

class ProjectionLayer(nn.Module):
    def __init__(self, d_model, vocab_size) -&gt; None:
        super().__init__()
        self.proj = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        return self.proj(x)

class Transformer(nn.Module):
    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -&gt; None:
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.src_pos = src_pos
        self.tgt_pos = tgt_pos
        self.projection_layer = projection_layer

    def encode(self, src, src_mask):
        src = self.src_embed(src)
        src = self.src_pos(src)
        return self.encoder(src, src_mask)

    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):
        tgt = self.tgt_embed(tgt)
        tgt = self.tgt_pos(tgt)
        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)

    def project(self, x):
        return self.projection_layer(x)

def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -&gt; Transformer:
    src_embed = InputEmbeddings(d_model, src_vocab_size)
    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)
    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)
    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)
    
    encoder_blocks = [EncoderBlock(d_model, MultiHeadAttentionBlock(d_model, h, dropout), FeedForwardBlock(d_model, d_ff, dropout), dropout) for _ in range(N)]
    decoder_blocks = [DecoderBlock(d_model, MultiHeadAttentionBlock(d_model, h, dropout), MultiHeadAttentionBlock(d_model, h, dropout), FeedForwardBlock(d_model, d_ff, dropout), dropout) for _ in range(N)]
    
    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))
    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))
    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)
    
    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)
    
    for p in transformer.parameters():
        if p.dim() &gt; 1:
            nn.init.xavier_uniform_(p)
    
    return transformer

# Define the custom model training loop with early stopping and validation
def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, patience):
    model = model.to(device)
    best_val_loss = float('inf')
    epochs_no_improve = 0

    for epoch in range(num_epochs):
        # Training
        model.train()
        train_loss = 0.0
        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            optimizer.zero_grad()
            outputs = model.project(model.decode(model.encode(input_ids, attention_mask), attention_mask, input_ids, attention_mask))
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        print(f'Epoch [{epoch + 1}/{num_epochs}], Average Training Loss: {avg_train_loss}')

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['label'].to(device)

                outputs = model.project(model.decode(model.encode(input_ids, attention_mask), attention_mask, input_ids, attention_mask))
                loss = criterion(outputs, labels)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)
        print(f'Epoch [{epoch + 1}/{num_epochs}], Average Validation Loss: {avg_val_loss}')

        # Early stopping logic
        if avg_val_loss &lt; best_val_loss:
            best_val_loss = avg_val_loss
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        if epochs_no_improve &gt;= patience:
            print(&quot;Early stopping triggered&quot;)
            break

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
max_len = 512

# Calculate the actual vocabulary size from the dataset
max_token_id = 0
for item in data:
    questions = item['questions']
    text = &quot; &quot;.join([q['question'] + &quot; &quot; + q['answer'] for q in questions])
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_len,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    max_token_id = max(max_token_id, encoding['input_ids'].max().item())

vocab_size = max_token_id + 1

# Create dataset and dataloader
dataset = MajorRecommendationDataset(data, tokenizer, max_len)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)

# Initialize custom transformer model, criterion, optimizer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
src_vocab_size = vocab_size
tgt_vocab_size = len(label_mapping)  # Number of classes
src_seq_len = max_len
tgt_seq_len = max_len

model = build_transformer(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

# Train the model with early stopping and validation
train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=20, patience=3)
</code></pre>
<p>that was the entire code with the training loop and every thing but I get this error IndexError</p>
<p>index out of range in self</p>
<p>I know that the input coming from the dataset is too large and embedding of the model can't handle. it but I don't know how to fix it</p>
",nlp,
Find exact and similar phrases in a string with millions of characters,"<p>I have a list of phrases and a corpus which is a string of text with millions of words. For each phrase in my phrase list, I want to find and record the most similar phrases found in the corpus-string.
For my purposes I need to use SBERT similarity, and found the sentence-transformers lib to the best.
My problem is that while there exists documentation for finding similarities between two lists, I couldn't find any for finding a list of phrases within a large string. I tried splitting my string into a list of sentence, but compute-time is incredibly long because for each phrase in the phrases list I need to loop thru each sentence (and there are plenty) and then append all matches to a dictionary I am creating. Additionly, the phrases from the phrase list are matched against the entire sentence (split from the string), but I am looking for any instance within the text, even one or two words ( or conversely multiple sentences) that are similar to my phrase.</p>
<pre class=""lang-py prettyprint-override""><code>phrase_list = ['Gregor Samse', 'in his bed into', 'horrible creature'...] # mine has 156 phrases

very_long_string= 'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections. The bedding was hardly able to cover it and seemed ready to slide off any moment. His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked...' 
# mine has around 11 million words 

# convert text corpus string to list of sentences
string_to_sent_list = very_long_string.split(&quot;.&quot;)

from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('all-mpnet-base-v2')


phrase_embeddings = model.encode(phrase_list, convert_to_tensor=True)
sent_embeddings = model.encode(string_to_sent_list, convert_to_tensor=True)

similarity_dict = {}
for i, phraselist_phrase in enumerate(phrase_list):
    similarities = util.cos_sim(phrase_embeddings[i], sent_embeddings)
    matches = [string_to_sent_list[j] for j, sim in enumerate(similarities[0]) if sim &gt; 0.65] 
    similarity_dict[phraselist_phrase] = matches

print(similarity_dict)

similarity_dict.to_csv('similarity dict.csv')
</code></pre>
<p>Are there alternative ways to accomplish this, either thru looping in a different way or using a different library? Open to any ideas. My goal is very simply to save all the corresponding phrase-matches within the corpus for each phrase in my phrase list.</p>
<p>P.S. Anyone know if this method saves as a match multiple version of the same phrase for example, for phrase 'must complete this' would it consider 'we must complete this project', and 'we must' , 'must complete', 'complete this' as matches even-tho they are from the same sentence?</p>
","python, algorithm, nlp, sentence-similarity, sentence-transformers",
Chunking text with bounding box values,"<p>I have used the Azure OCR service to extract text from PDFs. For each page in a PDF, the OCR output contains a list of text lines along with the bounding box values for that line. My original approach to chunk the text in each page was to firstly combine the strings in the list of texts using &quot;\n&quot; and use the RecursiveCharacterTextSplitter() from Langchain. However, I am not able to store the bounding box values for each chunk using this approach.</p>
<p>For each chunk, I want to be able to store the boundingbox value of the first and last line in the chunk. This is to ensure that I can later highlight the particular chunk in the PDF using the chunk's bounding box values.</p>
<p>I am now currently using a different chunking strategy where I iterate through each line in a page and keep track of the character count and once it reaches a particular character limit, I add text from those set of lines into a new chunk and start the process all over again with the next of chunks. This is my code to do the same :</p>
<pre><code>for line in item['lines']:
    character_count += len(line['text'])
    cumulative_lines.append(line)
    if character_count &gt;= character_limit:
        chunk_start_bbox = cumulative_lines[0]['boundingBox']
        chunk_end_bbox = cumulative_lines[-1]['boundingBox']
        chunk_text = &quot;\n&quot;.join(line['text'] for line in cumulative_lines)
        chunk_metadata = metadata_dict.copy()
        chunk_metadata.update({'chunk_start_bbox': chunk_start_bbox, 'chunk_end_bbox': chunk_end_bbox})
        chunk_docs.append(Document(page_content=chunk_text, metadata=metadata_dict))
        character_count = 0
        cumulative_lines = []
</code></pre>
<p>However, I feel this is a very inefficient solution, going through each line in a page. Moreover, I also wouldn't consider the overlap of characters in a chunk, like how Langchain does.</p>
<p>Can someone suggest a better way to achieve this? Also, is there a way I can make my code more performant?</p>
","python, text, nlp, langchain",
Semantic chunking of an audio,"<p>I need to perform semantic chunking of an audio file. So far, I have resampled the audio to 16000 Hz and then used wav2vec2 for getting transcriptions of the audio.</p>
<p>I need to</p>
<ol>
<li>Time-Align Transcript with Audio: Describe the methodology and steps for aligning the transcript with the audio.</li>
<li>Semantic Chunking of Data: Slice the data into audio-text pairs, using both semantic information from the text and voice activity information from the audio, with each audio-chunk being less than 15s in length.</li>
</ol>
<p>Here is the code I have so far</p>
<pre><code>import torch
import librosa
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC

# Load the pre-trained Wav2Vec2 model and processor

processor = Wav2Vec2Processor.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)
model = Wav2Vec2ForCTC.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)
model.eval()  # Set the model to evaluation mode

# Function to process and transcribe audio in chunks

def transcribe_audio(input_file, chunk_duration=30):
\# Load the audio file
speech, sr = librosa.load(input_file, sr=16000, mono=True)

# Calculate the number of samples per chunk
chunk_samples = int(sr * chunk_duration)

# Initialize the transcript result
transcript = &quot;&quot;

# Process audio in chunks
for start in range(0, len(speech), chunk_samples):
    # Get a chunk of speech
    end = start + chunk_samples
    speech_chunk = speech[start:end]

    # Convert the speech chunk to tensor
    input_values = processor(speech_chunk, return_tensors=&quot;pt&quot;, sampling_rate=sr).input_values

    # Perform inference
    with torch.no_grad():
        logits = model(input_values).logits

    # Decode the predicted ids to text
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)

    # Append the chunk transcription to the total transcription
    transcript += transcription[0] + ' '

return transcript

file_path = 'resampled_audio.wav'
final_transcript = transcribe_audio(file_path)
print(&quot;Final transcription:&quot;, final_transcript)
</code></pre>
<p>Any help would be appreciated. Also, what is  the general concept that I should study to learn these?</p>
<p>I want to slice the data in to audio-text pairs. Any help on how to do it would be much appreciated.</p>
","nlp, speech-recognition, speech-to-text",
How to extract text from two column pdf with Python?,"<p>I have :
<a href=""https://i.sstatic.net/R9Dnl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/R9Dnl.png"" alt=""enter image description here""></a></p>

<p>I have a PDF which are in two-column format.Is there a way to read each PDF according to the two-column format without cropping each PDF individually?</p>
","python, nlp",
Not able to install spacy==2.3.5 version,"<p>I tried to install spacy==2.3.5 for a resume analyser program. Encountered with a pip subprocess to install build dependencies did not run successfully error.</p>
<p>Using Python 3.12.3</p>
<p>Also it gives a E053 config file error when running the program regarding pyresparser:
&quot;OSError: [E053] Could not read config file from C:\Smart_Resume_Analyser_App-master.venv\Lib\site-packages\pyresparser\config.cfg&quot;</p>
<pre><code>`(.venv) PS C:\Smart_Resume_Analyser_App-master&gt; pip install spacy==2.3.5
</code></pre>
<p><code> </code></p>
<pre><code>    Getting requirements to build wheel did not run successfully.
    exit code: 1

    [267 lines of output]

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
        len_t* widths
        int i
        int nr_layer
        int batch_size

        __init__(len_t* widths, int nr_layer, int batch_size) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:140:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            this._nr_feat = &lt;len_t*&gt;calloc(batch_size, sizeof(len_t))
            this._is_valid = &lt;int*&gt;calloc(batch_size * widths[nr_layer-1], sizeof(int))
            this._costs = &lt;weight_t*&gt;calloc(batch_size * widths[nr_layer-1], sizeof(weight_t))
            this.signatures = &lt;uint64_t*&gt;calloc(batch_size, sizeof(uint64_t))

        __dealloc__() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:157:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            free(this._nr_feat)
            free(this._is_valid)
            free(this._costs)
            free(this.signatures)

        void reset() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:172:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            for i in range(this.i):
                free(this._feats[i])
                this._feats[i] = NULL
            this.i = 0

        int nr_in() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:189:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            this.i = 0

        int nr_in() nogil:
            return this.widths[0]

        int nr_out() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:192:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this.widths[0]

        int nr_out() nogil:
            return this.widths[this.nr_layer - 1]

        int push_back(const FeatureC* feats, int nr_feat,
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:195:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
                for i in range(this.nr_out()):
                    this.is_valid(this.i)[i] = 1
            this.i += 1
            return this.i &gt;= this.batch_size

        FeatureC* features(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:226:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this.i &gt;= this.batch_size

        FeatureC* features(int i) nogil:
            return this._feats[i]

        int nr_feat(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:229:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._feats[i]

        int nr_feat(int i) nogil:
            return this._nr_feat[i]

        weight_t* fwd(int i, int j) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:232:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._nr_feat[i]

        weight_t* fwd(int i, int j) nogil:
            return this._fwd[i] + (j * this.widths[i])

        weight_t* bwd(int i, int j) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:235:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._fwd[i] + (j * this.widths[i])

        weight_t* bwd(int i, int j) nogil:
            return this._bwd[i] + (j * this.widths[i])

        weight_t* scores(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:238:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._bwd[i] + (j * this.widths[i])

        weight_t* scores(int i) nogil:
            return this.fwd(this.nr_layer-1, i)

        weight_t* losses(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:241:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this.fwd(this.nr_layer-1, i)

        weight_t* losses(int i) nogil:
            return this.bwd(this.nr_layer-1, i)

        weight_t* costs(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:244:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this.bwd(this.nr_layer-1, i)

        weight_t* costs(int i) nogil:
            return this._costs + (i * this.nr_out())

        int* is_valid(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:247:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._costs + (i * this.nr_out())

        int* is_valid(int i) nogil:
            return this._is_valid + (i * this.nr_out())

        int guess(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:250:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._is_valid + (i * this.nr_out())

        int guess(int i) nogil:
            return VecVec.arg_max_if_true(this.scores(i), this.is_valid(i), this.nr_out())

        int best(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:253:4: function definition in pxd file must be declared 'cdef inline'
    warning: thinc\linalg.pxd:14:0: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310
    warning: thinc\linalg.pxd:90:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310
    warning: thinc\linalg.pxd:174:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310
    Compiling thinc/linalg.pyx because it changed.
    Compiling thinc/structs.pyx because it changed.
    Compiling thinc/typedefs.pyx because it changed.
    Compiling thinc/linear/avgtron.pyx because it changed.
    Compiling thinc/linear/features.pyx because it changed.
    Compiling thinc/linear/serialize.pyx because it changed.
    Compiling thinc/linear/sparse.pyx because it changed.
    Compiling thinc/linear/linear.pyx because it changed.
    Compiling thinc/neural/optimizers.pyx because it changed.
    Compiling thinc/neural/ops.pyx because it changed.
    Compiling thinc/neural/_aligned_alloc.pyx because it changed.
    Compiling thinc/extra/eg.pyx because it changed.
    Compiling thinc/extra/mb.pyx because it changed.
    Compiling thinc/extra/search.pyx because it changed.
    Compiling thinc/extra/cache.pyx because it changed.
    [ 1/15] Cythonizing thinc/extra/cache.pyx
    [ 2/15] Cythonizing thinc/extra/eg.pyx
    Traceback (most recent call last):
      File &quot;C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 353, in &lt;module&gt;
        main()
      File &quot;C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 335, in main
        json_out['return_val'] = hook(**hook_input['kwargs'])
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File &quot;C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 118, in get_requires_for_build_wheel      
        return hook(config_settings)
               ^^^^^^^^^^^^^^^^^^^^^
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 325, in get_requires_for_build_wheel
        return self._get_build_requires(config_settings, requirements=['wheel'])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 295, in _get_build_requires
        self.run_setup()
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 311, in run_setup
        exec(code, locals())
      File &quot;&lt;string&gt;&quot;, line 258, in &lt;module&gt;
      File &quot;&lt;string&gt;&quot;, line 195, in setup_package
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py&quot;, line 1154, in cythonize
        cythonize_one(*args)
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py&quot;, line 1321, in cythonize_one
        raise CompileError(None, pyx_file)
    Cython.Compiler.Errors.CompileError: thinc/extra/eg.pyx
    [end of output]

    note: This error originates from a subprocess, and is likely not a problem with pip.
  error: subprocess-exited-with-error

  Getting requirements to build wheel did not run successfully.
  exit code: 1

  See above for output.

  note: This error originates from a subprocess, and is likely not a problem with pip.
  [end of output]
</code></pre>
<p>note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error</p>
<p>× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─&gt; See above for output.</p>
<p>note: This error originates from a subprocess, and is likely not a problem with pip.
`</p>
","python, parsing, pip, nlp, spacy","<p>try:</p>
<pre><code>pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz
</code></pre>
<p>or</p>
<pre><code>pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz
</code></pre>
"
bert-tokenizer to tokenize the sentence,"<p>how can i overcome to remove # signs when I am using ber-tokenizer
​
Tokens length: 200
Tokens: ['[CLS]', 'educational', 'background', 'computer', 'applications', 'masters', 'degree', 'software', 'along', 'strong', 'skills', 'p', '##yt', '##hon', 'sq', '##l', 'j', '##ava', 'l', '##in', '##ux', 'g', '##it', 'certification', 'cloud', 'computing', 'well', '##e', '##qui', '##pped', 'successful', 'career', 'tech', 'industry', 'given', 'interest', 'cloud', 'computing', 'could', 'ex', '##cel', 'roles', 'cloud', 'solutions', 'architect', 'cloud', 'engineer', 'de', '##vo', '##ps', 'engineer', 'leverage', 'skills', 'cloud', 'technologies', 'design', 'implement', 'manage', 'cloud', 'infrastructure', 'organizations', 'recommend', 'continuing', 'enhance', 'expertise', 'cloud', 'computing', 'pursuing', 'advanced', 'certification', '##s', 'like', 'a', '##ws', 'certified', 'solutions', 'architect', 'micro', '##so', '##ft', 'certified', 'a', '##zure', 'solutions', 'architect', 'expert', 'stand', 'competitive', 'tech', 'market', 'additionally', 'gaining', 'experience', 'real', '##world', 'cloud', 'projects', 'internship', '##s', 'freelance', 'opportunities', 'boost', 'profile', 'keep', 'networkin</p>
<p>I am tring this code using bert-tokenize
seq_length=200</p>
<pre><code>tokens= tokenizer(df['answer'].tolist(),
                 max_length=seq_length,
                truncation=True,
                padding='max_length',
                add_special_tokens=True,
                return_tensors='np')
</code></pre>
<p>tokens have input_ids where i have seen some tokens are divided to subtokens and it may effect the whole results,
I want to expect only one token instead of sub tokens</p>
","nlp, tokenize, bert-language-model, huggingface-tokenizers",
"Incorrect similarity by Embeddings, How to fix it","<p>I have one problem with texts similarities.</p>
<p>I have a query</p>
<pre><code>&quot;eight point two&quot;
</code></pre>
<p>and when i make vector search, i get such results</p>
<pre><code>sentences = [test]
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

embeddings = model.encode(sentences)
res = client.search(
    collection_name=&quot;test3&quot;,
    search_params=models.SearchParams(hnsw_ef=128, exact=False),
    query_vector=embeddings[0],
)
res


[ScoredPoint(score=0.74708945,'vector_data': 'eight point two'},
ScoredPoint(score=0.7437991,, 'vector_data': 'two point eight'}),  
ScoredPoint(score=0.70646083,  'vector_data': eight point two p1 '}, vector=None),
</code></pre>
<p>how to take into account word order?
i want to see</p>
<pre><code>'eight point two'
'eight point two p1 '
'two point eight'
</code></pre>
","nlp, large-language-model",
Is there a method to load caseless models to Stanford&#39;s NLP sentiment analysis?,"<p>In the Stanford documentation, <a href=""https://stanfordnlp.github.io/CoreNLP/caseless.html"" rel=""nofollow noreferrer"">the authors mention using caseless models to process case-insensitive text.</a> Namely the ability to load the GATE Twitter POS annotator. It is a POS annotator, but it doesn't mention that sentiment analysis processes this model to predict sentiment. How do I load it to CoreNLP and let the sentiment annotator use the more accurate models, not just for part-of-speech tagging?</p>
","nlp, stanford-nlp",
Fasttext pre-trained model is not producing OOV word vectors when using gensim downloader,"<p>I am having A LOT of trouble when trying to use all the fasttext libraries (in Jupyter with Anaconda3 on Windows 11) that I have found so far but this question is mainly about gensim's implementation. I simply can't get it to work for me.</p>
<p>Python version
3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
Version info.
sys.version_info(major=3, minor=9, micro=12, releaselevel='final', serial=0)</p>
<p>In gensim 4.3.2. the only thing that kind of worked for me was using</p>
<pre><code>import gensim.downloader as api
fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')
</code></pre>
<p>The model at least was loaded, however if I try getting the word vectors of OOV words such as fasttext_model300['nonexistentword'] I get a key error which should not happen with fasttext. This was very disappointing... So is this &quot;model&quot; the same as using the downloadable .vec file?</p>
<p>I also downloaded the bin file for the model and tried:</p>
<pre><code>from gensim.models import FastText as ft
fasttext_model=ft.load_fasttext_format('path/crawl-300d-2M-subword/crawl-300d-2M-subword.bin')
</code></pre>
<p>But I got:</p>
<pre><code>2024-05-27 21:07:10,790 : ERROR : failed to decode invalid unicode bytes b'DeutschHrvatskiEnglishDanskNederlandssuomiFran\xc3\xa7ais\xce\x95\xce\xbb\xce\xbb\xce'; replacing invalid characters, using 'DeutschHrvatskiEnglishDanskNederlandssuomiFrançaisΕλλ\\xce'
2024-05-27 21:07:12,269 : ERROR : failed to decode invalid unicode bytes b'\xe3\x81\x99\xe3\x81\xb9\xe3\x81\xa6\xe3\x81\xae\xe5\x9b\x9e\xe7\xad\x94\xe3\x82\x92\xe9\x9d\x9e\xe8\xa1\xa8\xe7\xa4\xba\xe3\x81\xab\xe3\x81\x99\xe3\x82\x8b\xe8\xb3\xaa\xe5\x95\x8f\xe3\x82\x92\xe5\x89\x8a\xe9\x99\xa4\xe3\x81\x97\xe3'; replacing invalid characters, using 'すべての回答を非表示にする質問を削除し\\xe3'
2024-05-27 21:07:13,692 : ERROR : failed to decode invalid unicode bytes b'00Z\xe9\x83\xa8\xe5\xb1\x8b\xe3\x82\xbf\xe3\x82\xa4\xe3\x83\x97\xe3\x81\xbe\xe3\x82\x8b\xe3\x81\xbe\xe3\x82\x8b\xe8\xb2\xb8\xe5\x88\x87\xe5\xbb\xba\xe7\x89\xa9\xe3\x82\xbf\xe3\x82\xa4\xe3\x83\x97\xe4\xb8\x80\xe8\xbb\x92\xe5'; replacing invalid characters, using '00Z部屋タイプまるまる貸切建物タイプ一軒\\xe5'
2024-05-27 21:07:13,734 : ERROR : failed to decode invalid unicode bytes b'2017\xe6\x88\xbf\xe9\x97\xb4\xe7\xb1\xbb\xe5\x9e\x8b\xe7\x8b\xac\xe7\xab\x8b\xe6\x88\xbf\xe9\x97\xb4\xe6\x88\xbf\xe6\xba\x90\xe7\xb1\xbb\xe5\x9e\x8b\xe7\x8b\xac\xe7\xab\x8b\xe5\xb1\x8b\xe5\x8f\xaf\xe4\xbd\x8f2\xe5\x8d'; replacing invalid characters, using '2017房间类型独立房间房源类型独立屋可住2\\xe5\\x8d'
2024-05-27 21:07:13,901 : ERROR : failed to decode invalid unicode bytes b'2016\xe6\x88\xbf\xe9\x97\xb4\xe7\xb1\xbb\xe5\x9e\x8b\xe7\x8b\xac\xe7\xab\x8b\xe6\x88\xbf\xe9\x97\xb4\xe6\x88\xbf\xe6\xba\x90\xe7\xb1\xbb\xe5\x9e\x8b\xe7\x8b\xac\xe7\xab\x8b\xe5\xb1\x8b\xe5\x8f\xaf\xe4\xbd\x8f2\xe5\x8d'; replacing invalid characters, using '2016房间类型独立房间房源类型独立屋可住2\\xe5\\x8d'
2024-05-27 21:07:15,714 : ERROR : failed to decode invalid unicode bytes b'00Z\xe9\x83\xa8\xe5\xb1\x8b\xe3\x82\xbf\xe3\x82\xa4\xe3\x83\x97\xe3\x81\xbe\xe3\x82\x8b\xe3\x81\xbe\xe3\x82\x8b\xe8\xb2\xb8\xe5\x88\x87\xe5\xbb\xba\xe7\x89\xa9\xe3\x82\xbf\xe3\x82\xa4\xe3\x83\x97\xe5\x88\xa5\xe8\x8d\x98\xe5'; replacing invalid characters, using '00Z部屋タイプまるまる貸切建物タイプ別荘\\xe5'
2024-05-27 21:07:17,849 : ERROR : failed to decode invalid unicode bytes b'\xe6\xb6\x88\xe8\xb2\xbb\xe8\x80\x85\xe7\x9f\xa5\xe9\x81\x93\xe4\xb8\x80\xe5\x80\x8b\xe8\xa3\xbd\xe9\x80\xa0\xe5\x95\x86\xe5\x85\xb6\xe4\xb8\xad\xe4\xb8\x80\xe5\x80\x8b\xe7\x94\xa2\xe5\x93\x81\xe7\x9a\x84\xe4\xb8\x80\xe8\x88\xac\xe5'; replacing invalid characters, using '消費者知道一個製造商其中一個產品的一般\\xe5'
2024-05-27 21:07:18,053 : ERROR : failed to decode invalid unicode bytes b'\xce\xb9\xce\xb4\xce\xb9\xce\xbf\xce\xba\xcf\x84\xce\xb7\xcf\x83\xce\xaf\xce\xb1\xcf\x82\xce\x94\xce\xb9\xce\xb1\xce\xbc\xce\xad\xcf\x81\xce\xb9\xcf\x83\xce\xbc\xce\xb1\xce\x86\xcf\x84\xce\xbf\xce\xbc\xce\xb12\xce\xa5\xcf\x80\xce'; replacing invalid characters, using 'ιδιοκτησίαςΔιαμέρισμαΆτομα2Υπ\\xce'
2024-05-27 21:07:18,202 : ERROR : failed to decode invalid unicode bytes b'\xe6\x88\x96\xe5\x85\xb6\xe4\xbb\x96\xe5\xae\x98\xe6\x96\xb9\xe7\x82\xb9\xe8\xaf\x84\xe6\x94\xb6\xe9\x9b\x86\xe5\x90\x88\xe4\xbd\x9c\xe4\xbc\x99\xe4\xbc\xb4\xe6\x8f\x90\xe4\xbe\x9b\xe7\x9a\x84\xe5\xb7\xa5\xe5\x85\xb7\xe9\xbc\x93\xe5'; replacing invalid characters, using '或其他官方点评收集合作伙伴提供的工具鼓\\xe5'
2024-05-27 21:07:18,245 : ERROR : failed to decode invalid unicode bytes b'00Z\xe6\x88\xbf\xe9\x96\x93\xe9\xa1\x9e\xe5\x9e\x8b\xe7\xa7\x81\xe4\xba\xba\xe6\x88\xbf\xe9\x96\x93\xe6\x88\xbf\xe6\xba\x90\xe9\xa1\x9e\xe5\x9e\x8b\xe5\xae\xb6\xe5\xba\xad\xe5\xbc\x8f\xe6\x97\x85\xe9\xa4\xa8\xe5\x8f\xaf\xe4'; replacing invalid characters, using '00Z房間類型私人房間房源類型家庭式旅館可\\xe4'
</code></pre>
<p>Eventually my laptop got a memory error and crashed which suggests I lack the RAM to do this but in any case this decoding error seems like actual Chinese to me...</p>
<p>Would the above work somehow if I upgraded my laptop's RAM, for example?</p>
<p>Any suggestions would be HUGELY appreciated.</p>
","python, nlp, gensim, word-embedding, fasttext",
Custom Named Entity Recognition (NER) Model with spaCy V3,"<p>This is my first time building a custom model with SPACY NER.</p>
<pre><code># Define a function to create spaCy DocBin objects from the annotated data
def get_spacy_doc(file, data):
  # Create a blank spaCy pipeline
  nlp = spacy.blank('en')
  db = DocBin()

  # Iterate through the data
  for text, annot in tqdm(data):
    doc = nlp.make_doc(text)
    annot = annot['entities']

    ents = []
    entity_indices = []

    # Extract entities from the annotations
    for start, end, label in annot:
      skip_entity = False
      for idx in range(start, end):
        if idx in entity_indices:
          skip_entity = True
          break
      if skip_entity:
        continue

      entity_indices = entity_indices + list(range(start, end))
      try:
        span = doc.char_span(start, end, label=label, alignment_mode='strict')
      except:
        continue

      if span is None:
        # Log errors for annotations that couldn't be processed
        err_data = str([start, end]) + &quot;    &quot; + str(text) + &quot;\n&quot;
        file.write(err_data)
      else:
        ents.append(span)

    try:
      doc.ents = ents
      db.add(doc)
    except:
      pass

  return db
</code></pre>
<pre><code># Split the annotated data into training and testing sets
from sklearn.model_selection import train_test_split
train, test = train_test_split(cv_data, test_size=0.2)

# Display the number of items in the training and testing sets
len(train), len(test)

# Open a file to log errors during annotation processing
file = open('/content/drive/MyDrive/trial_domain_extraction/trained_models/train_file.txt','w')

# Create spaCy DocBin objects for training and testing data
db = get_spacy_doc(file, train)
db.to_disk('/content/drive/MyDrive/trial_domain_extraction/trained_models/train_data.spacy')

db = get_spacy_doc(file, test)
db.to_disk('/content/drive/MyDrive/trial_domain_extraction/trained_models/test_data.spacy')

# Close the error log file
file.close()
</code></pre>
<pre><code># Train a spaCy NER model using the provided configuration and data
!python -m spacy train /content/drive/MyDrive/trial_domain_extraction/config/config.cfg  --output /content/drive/MyDrive/trial_domain_extraction/trained_models/output  --paths.train /content/drive/MyDrive/trial_domain_extraction/trained_models/train_data.spacy  --paths.dev /content/drive/MyDrive/trial_domain_extraction/trained_models/test_data.spacy  --gpu-id 0
</code></pre>
<p>Output pipeline:</p>
<pre><code>============================= Training pipeline =============================
ℹ Pipeline: []
ℹ Initial learn rate: 0.001
E    #       SCORE 
---  ------  ------
  0       0    0.00
100     200    0.00
200     400    0.00
300     600    0.00
400     800    0.00
500    1000    0.00
600    1200    0.00
700    1400    0.00
800    1600    0.00
</code></pre>
<p>I have followed the following article to the T.<a href=""https://medium.com/@mjghadge9007/building-your-own-custom-named-entity-recognition-ner-model-with-spacy-v3-a-step-by-step-guide-15c7dcb1c416"" rel=""nofollow noreferrer"">text</a></p>
<p>My pipeline looks empty (please refer above.) Is it because there was too little training data?
I have used 13 entities. Would you suggest training with more amount of annotated data would resolve this issue? If yes, how much quantitatively?</p>
","nlp, spacy, named-entity-recognition, spacy-3, spacy-transformers",
How do I train gpt 2 from scratch?,"<p>I want to train gpt 2 from scratch but there is only fine-tuning approach based on pretrained models in articles I found.
I've used this <a href=""https://github.com/nshepperd/gpt-2"" rel=""noreferrer"">https://github.com/nshepperd/gpt-2</a> for train with existing model. Should I edit these Python scripts to train from scratch?</p>
","python, machine-learning, nlp, nlg","<p>I found the answer in <a href=""https://github.com/nshepperd/gpt-2/issues/11"" rel=""nofollow noreferrer"">https://github.com/nshepperd/gpt-2/issues/11</a></p>
<blockquote>
<p>If you want to not use the released model at all, for instance because
you want to train a model with incompatible hyperparameters, it should
be sufficient to just skip the restore from the released model
checkpoint (around train.py:164-177) on your first run so the
parameters will all be randomly initialized.</p>
</blockquote>
"
How to automatically choose the &quot;best&quot; K for a STM model based on multiple diagnostic variables,"<p>In structural topic modeling using the <code>stm</code> R package, the function <code>searchK</code> allows the user to run multiple models with different numbers of topics, and then returns the diagnostic properties for each model, so that the user can choose which performed better and therefore choose the ideal number of topics (K) for the data.</p>
<p>Some of these measures are <em>held-out likelihood</em> (higher is better), <em>residuals</em> (lower is better), and <em>semantic coherence</em> (higher is better).</p>
<p>The data looks like this, where each row is a model with a different amount of topics (K):</p>
<pre><code># K   semcoh    heldout  residual
# 10 -55.25328 -5.584059 1.127426
# 15 -54.84985 -5.532205 1.07168
# 17 -57.78122 -5.516972 1.056486
# 20 -56.90894 -5.493265 1.030628
# 22 -58.15011  -5.44756 1.015333
# 25 -59.86378 -5.431342 1.003179
</code></pre>
<p>Using <code>plot()</code>, the package returns the following graphic which allows the user to choose the best model:</p>
<p><a href=""https://i.sstatic.net/AJV0OjS8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AJV0OjS8.png"" alt=""enter image description here"" /></a>
In this case, its cleat that 25 is the best amount of topics, because it has higher held-out likelihood and lower residuals.</p>
<p>But in cases like the following the decision is not so clear, as the higher held-out likelihood is around 17, but the residuals are not lower at K = 17, so it's kind of a tradeoff: after K = 17, the higher K is, the lower the residuals are (good), but the held-out likelihood decreases (bad), but the better measure in residuals is actually a smaller change than the worsening measure in held-out likelihood, therefore in this case it's not worth it to go for lower residuals. Is there a way to automate these kind of decisions?</p>
<p>In other words, is there a way to find the optimal value for a variable by looking at diminishing returns on different variables, i.e., choose considering the next possibility can measure better on one variable, but also worse on other variable, balance the win vs. the loss, and settle on a value?</p>
<p>I tried doing a median-based approach using the <code>which</code> functions, but it doesn't always give the best K:</p>
<pre><code>median(c(
       which.max(results_a$semcoh),
       which.max(results_a$heldout),
       which.min(results_a$residual)
))
</code></pre>
<p><a href=""https://i.sstatic.net/WihPVKow.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WihPVKow.png"" alt=""enter image description here"" /></a></p>
<pre><code>results_a &lt;- structure(list(K = list(10, 15, 17, 20, 22, 25), semcoh = list(
    -55.2532813572245, -54.8498529671037, -57.7812176351009, 
    -56.9089355258198, -58.1501148277006, -59.8637782940342), 
    heldout = list(-5.58405885132492, -5.53220465315627, -5.51697205885505, 
        -5.49326482691615, -5.44755983789593, -5.43134154907285), 
    residual = list(1.12742587858013, 1.07167984608645, 1.05648599573627, 
        1.03062808239133, 1.01533334688186, 1.00317896181321)), row.names = c(NA, 
-6L), class = &quot;data.frame&quot;)

results_b &lt;- structure(list(K = list(10, 15, 17, 20, 22, 25), semcoh = list(
    -65.8929182884279, -69.437105852102, -73.1939358885751, -75.6839741459559, 
    -77.3398101594828, -77.3598106852508), heldout = list(-6.31404845126443, 
    -6.321083522624, -6.28269982323658, -6.30191607990617, -6.32046527332797, 
    -6.32299991122077), residual = list(1.34467069132499, 1.2780706228344, 
    1.23831374392626, 1.21041197190302, 1.19623018255656, 1.17377825556606)), row.names = c(NA, 
-6L), class = &quot;data.frame&quot;)
</code></pre>
","r, nlp, stm",
Information extraction from borderless table,"<p>How can I extract value from the following reports with different report structures? <a href=""https://i.sstatic.net/wjiUcoXY.png"" rel=""nofollow noreferrer"">enter image description here</a> <a href=""https://i.sstatic.net/82ywSyHT.png"" rel=""nofollow noreferrer"">enter image description here</a><a href=""https://i.sstatic.net/kZRqyjyb.png"" rel=""nofollow noreferrer"">enter image description here</a> <a href=""https://i.sstatic.net/Ep0tsZPa.png"" rel=""nofollow noreferrer"">enter image description here</a><a href=""https://i.sstatic.net/Z4uiBpmS.png"" rel=""nofollow noreferrer"">enter image description here</a><a href=""https://i.sstatic.net/Ex8RUuZP.png"" rel=""nofollow noreferrer"">enter image description here</a> <a href=""https://i.sstatic.net/XWRIPWSc.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I have tried converting it to txt first, after that the entities from the report such as assets, current assets, etc. will be predicted using XLM-RoBERTa with NER</p>
",nlp,
"ERROR: Could not build wheels for spacy, which is required to install pyproject.toml-based projects","<p><a href=""https://i.sstatic.net/ipmqI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ipmqI.png"" alt=""enter image description here"" /></a></p>
<p>Hi Guys, I am trying to install spacy model == 2.3.5 but I am getting this error, please help me!</p>
","python, python-3.x, pip, nlp, spacy","<p>I had the similar error while executing <code>pip install -r requirements.txt</code> but for <code>aiohttp</code> module:</p>
<pre><code>socket.c -o build/temp.linux-armv8l-cpython-311/aiohttp/_websocket.o
aiohttp/_websocket.c:198:12: fatal error: 'longintrepr.h' file not found
#include &quot;longintrepr.h&quot;                                   
          ^~~~~~~                        1 error generated.
error: command '/data/data/com.termux/files/usr/bin/arm-linux-androideabi-clang' 
failed with exit code 1
[end of output]
note: This error originates from a subprocess, and is likely not a problem with pip.
ERROR: Failed building wheel for aiohttp
Failed to build aiohttp
ERROR: Could not build wheels for aiohttp, which is required to install
pyproject.toml-based projects
</code></pre>
<p>Just in case I will leave here solution to my error. This error is specific to Python <code>3.11</code> version. On Python with <code>3.10.6</code> version installation went fine.</p>
<p>To solve it I needed to update <code>requirements.txt</code>.</p>
<p>Not working versions of modules with Python <code>3.11</code>:</p>
<pre><code>aiohttp==3.8.1
yarl==1.4.2
frozenlist==1.3.0
</code></pre>
<p>Working versions:</p>
<pre><code>aiohttp==3.8.2
yarl==1.8.1
frozenlist==1.3.1
</code></pre>
<p>Links to the corresponding issues with fixes:</p>
<ul>
<li><a href=""https://github.com/aio-libs/aiohttp/issues/6600"" rel=""nofollow noreferrer"">https://github.com/aio-libs/aiohttp/issues/6600</a></li>
<li><a href=""https://github.com/aio-libs/yarl/issues/706"" rel=""nofollow noreferrer"">https://github.com/aio-libs/yarl/issues/706</a></li>
<li><a href=""https://github.com/aio-libs/frozenlist/issues/305"" rel=""nofollow noreferrer"">https://github.com/aio-libs/frozenlist/issues/305</a></li>
</ul>
"
What does GlobalAveragePooling1D do in keras?,"<p>In the embedding example here:
<a href=""https://www.tensorflow.org/text/guide/word_embeddings"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/guide/word_embeddings</a></p>
<pre><code>result = embedding_layer(tf.constant([[0, 1, 2], [3, 4, 5]]))
result.shape
TensorShape([2, 3, 5])
</code></pre>
<p>Then it explains:</p>
<blockquote>
<p>When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape (samples, sequence_length, embedding_dimensionality). To convert from this sequence of variable length to a fixed representation there are a variety of standard approaches. You could use an RNN, Attention, or pooling layer before passing it to a Dense layer. This tutorial uses pooling because it's the simplest.</p>
<p>The GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.</p>
</blockquote>
<p>Then the code:</p>
<pre><code>embedding_dim=16

model = Sequential([
  vectorize_layer,
  Embedding(vocab_size, embedding_dim, name=&quot;embedding&quot;),
  GlobalAveragePooling1D(),
  Dense(16, activation='relu'),
  Dense(1)
])
</code></pre>
<p>The GlobalAveragePooling1D should calculate a single integer for each word's embedding of dimension = n. I don't understand this part:</p>
<blockquote>
<p>This allows the model to handle input of variable length, in the simplest way possible.</p>
</blockquote>
<p>Similarly:</p>
<blockquote>
<p>To convert from this sequence of variable length to a fixed representation there are a variety of standard approaches.</p>
</blockquote>
<p>In each embedding layer, input length is already fixed by the parameter 'input_length'. Truncation and padding are used to ensure the fixed length of the input. So what does it mean by saying GlobalAveragePooling1D is used to convert from this sequence of variable length to a fixed representation? What does the 'variable length' mean here?</p>
","keras, nlp, embedding",
"ERROR: Could not build wheels for neuralcoref, which is required to install pyproject.toml-based projects","<p>I am running spacy 3.6.1 on google colab. After all requirements installation, I got an error when I tried to install tokenizer</p>
<p>I am trying to use NeuralCoref4.0 to perform coreference resolution on google colab. I am running spacy 3.6.1 and python 3.10.12. However, I got an error when I tried to install neuralcoref. The NeuralCoref documentation say its compatible with spacy 2.1+ and python 3.6+ <a href=""https://github.com/huggingface/neuralcoref"" rel=""nofollow noreferrer"">neuralcoref github</a></p>
<p>I used the command <code>!pip install neuralcoref</code> to install, but I'm getting an error;</p>
<pre><code>ERROR: Could not build wheels for neuralcoref, which is required to install pyproject.toml-based projects
</code></pre>
<p><a href=""https://i.sstatic.net/73XhR.png"" rel=""nofollow noreferrer"">error_screenshot</a></p>
<p>Does anyone have the solution for my problem, either a way to install neuralcoref on google colab or a simple approach to do coreference resolution without it. Thanks for your help.</p>
<p>I expected NeuralCoref to install correctly on google colab, so that it can be used for coreference resolution.</p>
","python, pip, nlp, google-colaboratory, spacy",
chatbot memory problem for langchain using ConversationalRetrievalChain,"<p>i have build a chatbot on custom data using langchain, but the chatbot can not remember my previous questions, I found it difficult with the ConversationalRetrievalChain.</p>
<p>My code is as below:</p>
<pre><code>`class MyBot(ActivityHandler):
    def __init__(self, conversation_state: ConversationState):
        self.conversation_state = conversation_state
        self.session_accessor = self.conversation_state.create_property(&quot;Session&quot;)
       
            
        # Data loader
        loader = CSVLoader(file_path=&quot;data.csv&quot;, encoding=&quot;utf-8&quot;, csv_args={'delimiter': ','})
        data = loader.load()

        # OpenAI API key
        openai_api_key = os.environ.get('OPENAI_API_KEY')

        # Initialize OpenAIEmbeddings
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

        # Initialize FAISS for the vector database
        vectors = FAISS.from_documents(data, embeddings)
        

        # Initialize the ConversationalRetrievalChain
        self.chain = ConversationalRetrievalChain.from_llm(
            llm=ChatOpenAI(temperature=0.5,request_timeout=100,  model_name='gpt-4o', max_tokens= 2048, openai_api_key=openai_api_key),
            retriever=vectors.as_retriever(), combine_docs_chain_kwargs={&quot;prompt&quot;: QA_PROMPT}
        )    

def run_chain(self, chat_history, question):
        return self.chain.run({'chat_history': chat_history, 'question': question})

    
    

    async def get_response(self, user_message, turn_context: TurnContext):
    # Retrieve the session for the current user
        session = await self.session_accessor.get(turn_context, lambda: {&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You work for and M company answer any questions related to the company.&quot;}]})

    # Add user message to the session
        session[&quot;messages&quot;].append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_message})

        response = self.run_chain(&quot;&quot;, user_message)

        ai_response = response  # Use response directly

    # Add AI response to the session
        session[&quot;messages&quot;].append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: ai_response})

        # Save the updated session
        await self.conversation_state.save_changes(turn_context)

        return ai_response`
</code></pre>
<p>I just want my chatbot to remember my previous question like chatgpt.</p>
<p>thanks in advance!</p>
<p>integrate the memory within the chatbot</p>
","nlp, chatbot, openai-api, langchain, conversational-ai",
How can I solve installation error of texthero?,"<p>When I tried to <code>!pip install texthero</code> on Google Colab, below error happen therefore I cannot import texthero.</p>
<ul>
<li>code of pip install</li>
</ul>
<pre><code>!pip install texthero
</code></pre>
<ul>
<li>result message with error after <code>pip install</code></li>
</ul>
<pre><code>Collecting texthero
  Downloading texthero-1.1.0-py3-none-any.whl (24 kB)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Collecting gensim&lt;4.0,&gt;=3.6.0 (from texthero)
  Downloading gensim-3.8.3.tar.gz (23.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.4/23.4 MB 22.2 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Building wheels for collected packages: gensim
  error: subprocess-exited-with-error
  
  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─&gt; See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Building wheel for gensim (setup.py) ... error
  ERROR: Failed building wheel for gensim
  Running setup.py clean for gensim
Failed to build gensim
ERROR: Could not build wheels for gensim, which is required to install pyproject.toml-based projects
</code></pre>
<ul>
<li>error message after <code>import texthero as hero</code></li>
</ul>
<pre><code>ModuleNotFoundError: No module named 'texthero'
</code></pre>
","python, nlp, gensim, data-preprocessing",
Fuzzy sentence search algorithms,"<p>Suppose I have a set of phrases - about 10 000 - of average length - 7-20 words in which I want to find some given phrase. The phrase I am looking for could have some errors - for example miss one or two words, have some words misplaced, or some random words - for example my database contains ""As I was riding my red bike, I saw Christine"", and I want it to much ""As I was riding my blue bike, saw Christine"", or ""I was riding my bike, I saw Christine and Marion"". What could be some good approach to this problem? I know about Levenhstein's distance, and I also suppose that this problem may have no easy, good solution.</p>
","nlp, fuzzy-search","<p>A good text search engine will provide capabilities such as you describe, fsh.  A typical approach would be to create a query that matches if any of the words occurs and orders the results using a weight based on number of terms occurring in proximity to each other and weighted inversely to their probability of occurring, since uncommon words will be less likely to co-occur by chance.  There's a whole theory of this sort of thing called information retrieval, but maybe you know about that.  Furthermore you'd like to make sure that word-level fuzziness gets accounted for by normalizing case, punctuation and the like and applying some basic linguistic transformations (stemming), and in some cases introducing a dictionary of synonyms, especially when there is domain knowledge available to condition it.</p>

<p>If you're interested in messing around with this stuff, try an open-source search engine, <a href=""http://zooie.wordpress.com/2009/07/06/a-comparison-of-open-source-search-engines-and-indexing-twitter/"" rel=""nofollow"">this article by Vik</a> gives a reasonable survey from the perspective of 2009, and <a href=""http://wrg.upf.edu/WRG/dctos/Middleton-Baeza.pdf"" rel=""nofollow"">this one by Middleton and Baeza-Yates</a> gives a good detailed introduction to the topic.</p>
"
Question about sentence generation according to specific patterns with predefined vocabulary list,"<p>I am interested in learning a language and want to generate practice sentences from a predefined vocabulary list according to patterns that I define ie noun(creature) - verb of being - emotion adjective. Chat GPT does a decent job with some of this and I can get it to output what I want some of the time, but is there a better option? Has anyone tried this before?</p>
","nlp, nlg",
Fine tuning T5 not converging,"<p>I am new in this world of transformers and NLP, and I am having a problem when fine tuning T5 for my specific use case.</p>
<p>What I want to achieve, is that the model receives an input text, and outputs a JSON (as a string) of the relevant information in the text.</p>
<p>There are 3 formats that the model can respond, below are some examples:
Input: Hey, can you give one hundred dollars to John?
Expected Output:  '{&quot;action&quot;: &quot;T&quot;, &quot;data&quot;: {&quot;name&quot;: &quot;John&quot;, &quot;amount&quot;: 100, &quot;currency&quot;: &quot;USD&quot;}}'</p>
<p>Input: I want to add Benjamin Franklin to my contacts. He has an account on citibank, with number 412389124.
Expected Output: '{&quot;action&quot;: &quot;A&quot;, &quot;data&quot;: {&quot;name&quot;: &quot;Benjamin Franklin&quot;, &quot;account_no&quot;: 412389124, &quot;entity&quot;: &quot;Citibank&quot;, &quot;id_num&quot;: null}'</p>
<p>Input: Hey, what's the weather gonna be tonight?
Expected Output:  '{&quot;accion&quot;: &quot;N&quot;, &quot;datos&quot;: {}}'</p>
<p>I've built a Python script to generate the inputs and labels as random as possible. With that python script, I generated 20000 data points (I can generate less or more of that).</p>
<p>Using T5 as my base model, I've trained it using the trainer from pytorch.</p>
<p>Below is my code:</p>
<pre><code>model_name_huggingface = &quot;google/t5-base&quot;

tokenizer = T5Tokenizer.from_pretrained(model_name_huggingface)
model = T5ForConditionalGeneration.from_pretrained(model_name_huggingface)
</code></pre>
<p>Then, after I tokenize my dataset.</p>
<pre><code>batch_size = 16

training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;models/chimi-mt5-base&quot;,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=100,
    logging_strategy=&quot;steps&quot;,
    logging_steps=100,
    save_strategy=&quot;steps&quot;,
    save_steps=200,
    # learning_rate=1e-4,
    optim=&quot;adafactor&quot;,
    learning_rate=5e-4,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    predict_with_generate=True,
    weight_decay=0.05,
    save_total_limit=3,
    num_train_epochs=2,
    metric_for_best_model=&quot;exact_match&quot;,
    # greater_is_better=False,
    load_best_model_at_end=True
)
</code></pre>
<pre><code>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=base_model)
cer = evaluate.load(&quot;cer&quot;, module_type=&quot;metric&quot;)
exact_match = evaluate.load(&quot;exact_match&quot;, module_type=&quot;metric&quot;)
</code></pre>
<pre><code>import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    result = {}

    # Compute CER
    result[&quot;cer&quot;] = cer.compute(predictions=decoded_preds, references=decoded_labels)

    # Compute Exact Match
    exact_match_res = exact_match.compute(predictions=decoded_preds, references=decoded_labels, ignore_case=True)
    result[&quot;exact_match&quot;] = exact_match_res[&quot;exact_match&quot;]

    return {k: round(v, 4) for k, v in result.items()}
</code></pre>
<pre><code>trainer = Seq2SeqTrainer(
    model=base_model,
    args=training_args,
    train_dataset=tokenized_chimi_dataset[&quot;train&quot;],
    eval_dataset=tokenized_chimi_dataset[&quot;validation&quot;],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
</code></pre>
<pre><code>result = trainer.train()
</code></pre>
<p>That's the current code I am using to fine tune T5.</p>
<p>The training loss goes down up to 0.054, and never improves.
The validation loss goes down up 0.034, and never improves.
The CER metric goes down up to 0.4875 and never improves after that. But, just to let you know, after the first 100 steps, it already has a CER of 0.583.
The Exact Match Metric goes up to 0.3089, and that already happens after the 600th step.</p>
<p>By testing, I see that it responds in the correct JSON format, and the action is responded correctly normally. But then, the data inside the JSON is not often correct.</p>
<p>What can I do to improve this?
I am stuck on this for a long time, and I am not really sure how to proceed. Any help is appreciated.</p>
<p>Thanks in advance!</p>
<p>I tried balancing my dataset, and tuning the hyperparameters, but it still didn't result in any relevant ups in performance.</p>
","nlp, huggingface-transformers",
Calculating weighted cosine similarity between vectors of words,"<p>I have two word lists, where each word makes up a topic, and has a tf-idf weight for that topic:</p>
<pre><code>topic1 = [('blue',.1), ('red',.05), ('sky',.01)]
topic2 = [('water',.5), ('fire',.1), ('earth',.02)]
</code></pre>
<p>I am trying to calculate the cosine similarity between the vectors, but also account for the tf-idf weighting of each word.</p>
<p>Is there a commonly accepted way to account for individual weightings when doing vector similarity? How would I implement this?</p>
","python, nlp, cosine-similarity",
Filtering stop words out of a multiple text files (using a list of stop words),"<p>I have a folder named <strong>cleaned_texts</strong>. The folder contains text files(a.txt, b.txt, c.txt etc) and each text file contains tokenized words in this format:<strong>['Rise', 'of', 'e-health', 'and', 'its', 'Germany', 'dollar']</strong>.</p>
<p>Example:</p>
<p>a.txt contains <strong>['Rise', 'of', 'e-health', 'and', 'its', 'Thailand', 'YEN', 'India']</strong> and</p>
<p>b.txt contains <strong>['PESO', 'Man', 'development', 'never', 'Japan', 'year', 'date', 'Canada']</strong>.</p>
<p>I also have another folder named <strong>StopWords</strong> which also contains text files and each text file contains a stop word. The text files are named in this format (currency.txt, names.txt, geographic.txt etc).</p>
<p>Example:</p>
<p>currency.txt contains names of currencies <strong>(Eg: BAHT | Thailand, PESO  | Mexico, YEN | Japan etc)</strong>.</p>
<p>geographic.txt contains names of countries <strong>(Eg: Canada, China, India, Germany etc)</strong>.</p>
<p>I want to filter all the stop words contained in the text files inside the StopWords folder, from all the text files in the cleaned_texts folder.</p>
<p>I looped through the stop words folder, Combined all the stop words and converted it to a list. My challenge is how to filter the stop words from my cleaned_texts files. I have been on it for days now but i couldn't figure out how to do it.</p>
<p>Here is my script:</p>
<pre><code>import glob
import codecs
import os

#Cleaned texts
os.getcwd()
clean_texts_folder =  os.path.join(os.getcwd(), 'cleaned_texts')

clean_text_data = []
for root, folders, files in os.walk(clean_texts_folder):
    for file in files:
        path = os.path.join(root, file)
        with codecs.open(path, encoding='utf-8', errors='ignore') as info:
            clean_text_data.append(info.read())


#Stop Words
stopwords_folder_path = &quot;StopWords&quot;
stopwords_files = glob.glob(os.path.join(stopwords_folder_path, '*.txt'))

for file in stopwords_files:
    with open(file, 'r') as w:
        stop_words = w.read()
        
        map_dict = {'|': ''}
        res = ''.join(
            idx if idx not in map_dict else map_dict[idx] for idx in stop_words)
        new_list = res.split()

#new_list Output= ['SMITH', 'Surnames', 'from', '1990', 'Thailand', 'YEN', 'India', 'PESO', 'Japan', 'Canada']


#Trying to save the filtered texts
folder_name = &quot;new_texts&quot;
Path(folder).mkdir(parents=True, exist_ok=True)
filtered_sentence = []
for index, word in enumerate(clean_text_data):
    if word not in new_list:
        #print(filtered_sentence.append(word))
        file_path = Path(folder_name, f&quot;{index}.txt&quot;)
        with pathlib.Path.open(file_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
           f.write(f&quot;{filtered_sentence }&quot;)

</code></pre>
<p><strong>Actual/Resulting Output:</strong>
&quot;None&quot; is printing in all the text files.</p>
<p>a.txt = None</p>
<p>b.txt = None</p>
<p>c.txt = None</p>
<p><strong>Expected Output:</strong></p>
<p>a.txt = ['Rise', 'of', 'e-health', 'and', 'its']</p>
<p>b.txt = ['Man', 'development', 'never','year', 'date']</p>
","python, nlp","<p>You are not correctly combining stopwords from different files. Also, you never assigned any value to <code>filtered_sentence</code> so you end up writing an empty list to your files, resulting in the unexpected output. Try the following instead:</p>
<pre><code>import glob
import codecs
import os
from pathlib import Path

# Cleaned texts
os.getcwd()
clean_texts_folder = os.path.join(os.getcwd(), 'cleaned_texts')

# Stop Words
stopwords_folder_path = &quot;StopWords&quot;
stopwords_files = glob.glob(os.path.join(stopwords_folder_path, '*.txt'))

# Combine all stop words into a single list
stop_words = []
for file in stopwords_files:
    with open(file, 'r', encoding='utf-8') as w:
        stop_words_in_file = [word.strip() for word in w.read().split('|')]
        stop_words.extend(stop_words_in_file)

# Remove duplicates and convert to set for faster lookup
stop_words = set(stop_words)

# Loop through cleaned texts and filter out stop words
folder_name = &quot;new_texts&quot;
Path(folder_name).mkdir(parents=True, exist_ok=True)

for root, folders, files in os.walk(clean_texts_folder):
    for file in files:
        path = os.path.join(root, file)
        with codecs.open(path, encoding='utf-8', errors='ignore') as info:
            content = eval(info.read())  # Convert string to list
            filtered_content = [word for word in content if word not in stop_words]
            file_path = os.path.join(folder_name, os.path.basename(path))
            with open(file_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
                f.write(str(filtered_content))
</code></pre>
"
Mapping word vector to the most similar/closest word using spaCy,"<p>I am using spaCy as part of a topic modelling solution and I have a situation where I need to map a derived word vector to the ""closest"" or ""most similar"" word in a vocabulary of word vectors.</p>

<p>I see gensim has a function (WordEmbeddingsKeyedVectors.similar_by_vector) to calculate this, but I was wondering if spaCy has something like this to map a vector to a word within its vocabulary (nlp.vocab)?</p>
","nlp, spacy, word2vec, word-embedding","<p>After a bit of experimentation, I found a scikit function (cdist in scikit.spatial.distance) that finds a ""close"" vector in a vector space to the input vector. </p>

<pre><code># Imports
from scipy.spatial import distance
import spaCy

# Load the spacy vocabulary
nlp = spacy.load(""en_core_web_lg"")

# Format the input vector for use in the distance function
# In this case we will artificially create a word vector from a real word (""frog"")
# but any derived word vector could be used
input_word = ""frog""
p = np.array([nlp.vocab[input_word].vector])

# Format the vocabulary for use in the distance function
ids = [x for x in nlp.vocab.vectors.keys()]
vectors = [nlp.vocab.vectors[x] for x in ids]
vectors = np.array(vectors)

# *** Find the closest word below ***
closest_index = distance.cdist(p, vectors).argmin()
word_id = ids[closest_index]
output_word = nlp.vocab[word_id].text
# output_word is identical, or very close, to the input word
</code></pre>
"
How to use tflite model coverted from TFMarianMTModel on huggingface,"<p>I am looking for a en-zh translate model that can be used in TFLite, and i found one on huggingface: <a href=""https://huggingface.co/Helsinki-NLP/opus-mt-en-zh"" rel=""nofollow noreferrer"">https://huggingface.co/Helsinki-NLP/opus-mt-en-zh</a></p>
<p>I have coverted the model to .tflite by follow code:</p>
<pre><code>import tensorflow as tf
from transformers import TFMarianMTModel, AutoTokenizer

print(&quot;loading model...&quot;)
model_name = 'Helsinki-NLP/opus-mt-en-zh'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = TFMarianMTModel.from_pretrained(model_name, from_pt=True)

converter = tf.lite.TFLiteConverter.from_keras_model(model);
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert();

with open(&quot;./out/tf_model.tflite&quot;, 'wb') as o_:
    o_.write(tflite_model)
</code></pre>
<p>but when i try to use it for infrence, i meet some problem:</p>
<pre><code>from transformers import AutoTokenizer
import tensorflow as tf
import numpy as np

model_name = 'Helsinki-NLP/opus-mt-en-zh'
tokenizer = AutoTokenizer.from_pretrained(model_name)
result = tokenizer(&quot;&gt;&gt;cmn_Hans&lt;&lt; hello world&quot;, return_tensors=&quot;tf&quot;, padding=True)

print(&quot;tokenize result: &quot;, result, '\n')

interpreter = tf.lite.Interpreter(model_path=&quot;./out/tf_model.tflite&quot;)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_ids = result[&quot;input_ids&quot;]

interpreter.set_tensor(input_details[2]['index'], input_ids)
interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(&quot;output_data:&quot;, output_data)
</code></pre>
<p>the error is:</p>
<pre><code>Traceback (most recent call last):
  File &quot;lite.py&quot;, line 48, in &lt;module&gt;
    interpreter.set_tensor(input_details[2]['index'], input_ids)
  File &quot;/Users/xuanyue/venv/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py&quot;, line 607, in set_tensor
    self._interpreter.SetTensor(tensor_index, value)
ValueError: Cannot set tensor: Dimension mismatch. Got 1 but expected 3 for dimension 0 of input 2.
</code></pre>
<p>I try to resize the shape, but i don't know how to do that, any suggestions?</p>
","nlp, tensorflow-lite, transformer-model",
Hugging face; Problem with custom retriever in transformers library,"<p>I am trying to build a RAG-based Chatbot with Chain of Thought for WordPress Site. I am not very experienced with Hugging face. I am using API to retrieve the data for this.</p>
<p>Problem description:
I have built a custom retriever for the rag model. I know that I can use RagRetriever, I tried and I am having severe problems with version control as it requires me to import datasets.</p>
<p>After running the program, the Error says that, my 'CustomRetriever' object is not callable. Is there a way around this?</p>
<p>I'm using,
Python v3.10.12
Transformers v4.42.0 dev0</p>
<p>My Custom Retriever Class</p>
<pre><code># Custom Retriever Class
class CustomRetriever:
    def __init__(self, index):
        self.index = index

    def retrieve(self, query_embedding, n_docs=5):
        distances, indices = self.index.search(query_embedding, n_docs)
        return indices

# Initialize RAG components
tokenizer = RagTokenizer.from_pretrained(&quot;facebook/rag-sequence-nq&quot;)
rag_model = RagSequenceForGeneration.from_pretrained(&quot;facebook/rag-sequence-nq&quot;)

# Initialize Custom Retriever
custom_retriever = CustomRetriever(index=index)

# Set the retriever for RAG model
rag_model.set_retriever(custom_retriever)
</code></pre>
<p>My implementation(passing input ids and attention mask)</p>
<pre><code># Maintain a conversation history
conversation_history = []

def process_query_with_chain_of_thought(user_query, previous_context=&quot;&quot;):
    # Tokenize the input query
    inputs = tokenizer(user_query, return_tensors=&quot;pt&quot;)

    # Generate embeddings for the query
    query_embedding = model.encode([user_query]).reshape(1, -1)

    # Retrieve relevant document indices
    doc_indices = custom_retriever.retrieve(query_embedding, n_docs=5)

    # Debugging: Print retrieved document indices
    # print(f&quot;Retrieved document indices: {doc_indices}&quot;)

    # Fetch actual documents
    retrieved_docs = []
    for i in doc_indices[0]:
        if 0 &lt;= i &lt; len(posts):
            retrieved_docs.append(posts[i]['content']['rendered'])
        else:
            print(f&quot;Index {i} is out of range.&quot;)

    # Print the retrieved documents in a human-readable format
    # print(&quot;Retrieved documents:&quot;)
    full_content = &quot;&quot;
    for doc in retrieved_docs:
        # Clean HTML and convert to plain text
        cleaned_text = clean_html(doc)
        # print(cleaned_text)  # Print the extracted text
        # print(&quot;=&quot;*50)   # Separate each document with a line of '='
        full_content += cleaned_text + &quot;\n\n&quot;

    # Combine user query and retrieved documents to create context
    context = user_query + &quot;\n\n&quot; + full_content

    if context:
        # Tokenize the retrieved documents
        context_inputs = tokenizer(full_content, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=512)

        # Ensure that context_input_ids are passed correctly
        response_ids = rag_model.generate(
            input_ids=inputs['input_ids'],
            context_input_ids=context_inputs['input_ids'],
            context_attention_mask=context_inputs['attention_mask']
        )

        response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)

        # Append the initial response to the conversation history
        conversation_history.append(f&quot;User: {user_query}&quot;)
        conversation_history.append(f&quot;Bot: {response_text}&quot;)

        # Create a new context by combining the conversation history
        new_context = &quot;\n&quot;.join(conversation_history)

        # # Debug: print the final response
        # print(&quot;Final response:&quot;)
        # print(new_context)

        return response_text, new_context
    else:
        print(&quot;No documents retrieved&quot;)
        return &quot;Sorry, I couldn't find any relevant information.&quot;, previous_context
</code></pre>
<p>Queries:</p>
<pre><code># Example multi-turn conversation
user_queries = [
    &quot;Are there any news on fiction stories?&quot;,
    &quot;Which one is the most populer&quot;,
    &quot;What are the reviews.&quot;
]


context = &quot;&quot;
for query in user_queries:
    response, context = process_query_with_chain_of_thought(query, previous_context=context)
    print(f&quot;Processed response: {response}\n&quot;)
</code></pre>
<p>Error message:</p>
<pre><code>TypeError                                 Traceback (most recent call last)

&lt;ipython-input-22-c7d6867da07f&gt; in &lt;cell line: 2&gt;()
      1 context = &quot;&quot;
      2 for query in user_queries:
----&gt; 3     response, context = process_query_with_chain_of_thought(query, previous_context=context)
      4     print(f&quot;Processed response: {response}\n&quot;)

8 frames

&lt;ipython-input-20-3284408ddf21&gt; in process_query_with_chain_of_thought(user_query, previous_context)
     41 
     42         # Ensure that context_input_ids are passed correctly
---&gt; 43         response_ids = rag_model.generate(
     44             input_ids=inputs['input_ids'],
     45             context_input_ids=context_inputs['input_ids'],

/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py in decorate_context(*args, **kwargs)
    113     def decorate_context(*args, **kwargs):
    114         with ctx_factory():
--&gt; 115             return func(*args, **kwargs)
    116 
    117     return decorate_context

/usr/local/lib/python3.10/dist-packages/transformers/models/rag/modeling_rag.py in generate(self, input_ids, attention_mask, context_input_ids, context_attention_mask, doc_scores, do_deduplication, num_return_sequences, num_beams, n_docs, **model_kwargs)
   1019             if input_ids is not None:
   1020                 new_input_ids = input_ids[index : index + 1].repeat(num_candidates, 1)
-&gt; 1021                 outputs = self(new_input_ids, labels=output_sequences, exclude_bos_score=True)
   1022             else:  # input_ids is None, need context_input_ids/mask and doc_scores
   1023                 assert context_attention_mask is not None, (

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)
   1530             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1531         else:
-&gt; 1532             return self._call_impl(*args, **kwargs)
   1533 
   1534     def _call_impl(self, *args, **kwargs):

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1539                 or _global_backward_pre_hooks or _global_backward_hooks
   1540                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1541             return forward_call(*args, **kwargs)
   1542 
   1543         try:

/usr/local/lib/python3.10/dist-packages/transformers/models/rag/modeling_rag.py in forward(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, exclude_bos_score, reduce_loss, labels, n_docs, **kwargs)
    843             use_cache = False
    844 
--&gt; 845         outputs = self.rag(
    846             input_ids=input_ids,
    847             attention_mask=attention_mask,

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)
   1530             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1531         else:
-&gt; 1532             return self._call_impl(*args, **kwargs)
   1533 
   1534     def _call_impl(self, *args, **kwargs):

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1539                 or _global_backward_pre_hooks or _global_backward_hooks
   1540                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1541             return forward_call(*args, **kwargs)
   1542 
   1543         try:

/usr/local/lib/python3.10/dist-packages/transformers/models/rag/modeling_rag.py in forward(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)
    591                 question_encoder_last_hidden_state = question_enc_outputs[0]  # hidden states of question encoder
    592 
--&gt; 593                 retriever_outputs = self.retriever(
    594                     input_ids,
    595                     question_encoder_last_hidden_state.cpu().detach().to(torch.float32).numpy(),

TypeError: 'CustomRetriever' object is not callable
</code></pre>
<p>If you want any more details, please do let me know. Thanks.</p>
","nlp, chatbot, huggingface-transformers, huggingface, retrieval-augmented-generation",
"ValueError: too many values to unpack (expected 2) or not enough values to unpack (expected 2, got 1)","<p>I am trying to train T5 model binary classification. When I am trying to
define data class, I am getting</p>
<pre><code>batch_size, seq_length = input_shape
1017 # required mask seq length can be calculated via length of past
1018 mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length
</code></pre>
<p>If I am using unsqueeze(0)  for input_id and attention_mask I am getting following error</p>
<pre><code>ValueError: too many values to unpack (expected 2)
</code></pre>
<p>if I am using squeeze(0), flatten() or none of them I am getting</p>
<pre><code>not enough values to unpack (expected 2, got 1)
</code></pre>
<p>The data class is below
......</p>
<pre><code>  def __init__(self,
               df: pd.DataFrame,
               tokenizer:T5Tokenizer,
               source_max_token_length: int=1000,
               target_max_token_length: int=400):
    self.tokenizer=tokenizer
    self.df=df
    self.source_max_token_length=source_max_token_length
    self.target_max_token_length=target_max_token_length

  def __len__(self):
    return len(self.df)
  def __getitem__(self,index: int):
    data_row=self.df.iloc[index]
    source_encoding=tokenizer.batch_encode_plus(
      str(data_row['question']),
      # str(data_row['context']),
      max_length=self.source_max_token_length,
      padding='max_length',
      truncation='only_second',
      return_attention_mask=True,
      add_special_tokens=True,
      return_tensors='pt'
    )
    target_encoding=tokenizer.batch_encode_plus(
      str(data_row['answers']),
      max_length=self.target_max_token_length,
      padding='max_length',
      truncation=True,
      return_attention_mask=True,
      add_special_tokens=True,
      return_tensors='pt'
    )

    labels=target_encoding['input_ids']
    labels[labels==0]=-100
    return dict(
        question=data_row['question'],
        # context=data_row['context'],
        answer_text=data_row['answers'],
        # input_ids=np.array([source_encoding['input_ids']]),#.flatten(),np.array([d['input_ids']])
        # attention_mask=np.array([source_encoding['attention_mask']]),#.flatten(),#.flatten(),
        # labels=labels
        input_ids=source_encoding['input_ids'].unsqueeze(0),#.flatten(),#.flatten(),np.array([d['input_ids']])
        attention_mask=source_encoding['attention_mask'].unsqueeze(0),#.flatten(),#.flatten(),#.flatten(),
        labels=labels.flatten()
    )
</code></pre>
<p>I tried ever debugging like:</p>
<pre><code>example=tokenizer.batch_encode_plus(
      str(data_row['question']),
      # str(data_row['context']),
      max_length=1000,
      padding='max_length',
      truncation='only_second',
      return_attention_mask=True,
      add_special_tokens=True,return_tensors='pt'
      )
</code></pre>
<p>The size of example is example['input_ids'].shape</p>
<pre><code>example['input_ids'].shape
torch.Size([61, 1000])
</code></pre>
<p>I think I do not need unsqueeze(0) or flatten(), but none of them works</p>
<p>sagemaker: 2.221.0
transformers: 4.41.0
pytorch_lightning: 1.1.3</p>
","python, nlp, huggingface-transformers, pytorch-lightning, pytorch-dataloader",
Model prediction differed after one year with same config and dataset,"<p>I tried a TrOCR-Hugging Face model, with a training data (400k) and got around 83% of accuracy with the test data. After a year, I tried to train the same model, with same config and dataset. Now the model predicts only 69% of my test data only.</p>
<p>I used a pretrained model 'microsoft/trocr-base-handwritten' from Hugging Face. And the thing differed was GPU instances. Trained using data_parallel on dual gpus in first attempt (32*2 batch size) and 4 gpus on second attempt (16*4 batch_size).</p>
<p>Is there any other way to reproduce the same results?</p>
","deep-learning, nlp, ocr, huggingface-transformers",
Single LoRA gives different outputs with and without merge_unload for Sequence Classification task. Which one is the correct method?,"<p>I keep everything same for Fine tuned LoRA for Sequence classification task. When I load it like:</p>
<pre><code>def load_model(lora_path, device, num_labels, merge_unload = False):
    
    model = Phi3ForSequenceClassification.from_pretrained(&quot;microsoft/Phi-3-mini-4k-instruct&quot;, trust_remote_code=True, 
                                                          device_map = device, 
                                                          torch_dtype=torch.bfloat16,
                                                          attn_implementation = &quot;flash_attention_2&quot;, 
                                                          num_labels = num_labels)
    if model.config.pad_token_id is None:
        model.config.pad_token_id = model.config.eos_token_id
    
    peft_model = PeftModel.from_pretrained(model,
                                           lora_path, 
                                           device_map = device)
    
    peft_model = peft_model.eval().to(device)
    if merge_unload: peft_model = peft_model.merge_and_unload()
    
    return peft_model

</code></pre>
<p>With and without <code>.merge_and_unload()</code>, it gives different outputs for the same text. How can this be happening? What is the right way to do this?</p>
<p><strong>LoRA Model</strong>:</p>
<pre><code>PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): Phi3ForSequenceClassification(
      (model): Phi3Model(
        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)
        (embed_dropout): Dropout(p=0.0, inplace=False)
        (layers): ModuleList(
          (0-31): 32 x Phi3DecoderLayer(
            (self_attn): Phi3FlashAttention2(
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (qkv_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=9216, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): Phi3RotaryEmbedding()
            )
            (mlp): Phi3MLP(
              (gate_up_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=16384, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=16384, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear(
                (base_layer): Linear(in_features=8192, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (activation_fn): SiLU()
            )
            (input_layernorm): Phi3RMSNorm()
            (resid_attn_dropout): Dropout(p=0.0, inplace=False)
            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
            (post_attention_layernorm): Phi3RMSNorm()
          )
        )
        (norm): Phi3RMSNorm()
      )
      (score): ModulesToSaveWrapper(
        (original_module): Linear(in_features=3072, out_features=2, bias=False)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=3072, out_features=2, bias=False)
        )
      )
    )
  )
)
</code></pre>
<p><strong>MERGED and Loaded Model</strong></p>
<pre><code>Phi3ForSequenceClassification(
  (model): Phi3Model(
    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3FlashAttention2(
          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)
          (rotary_emb): Phi3RotaryEmbedding()
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm()
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
        (post_attention_layernorm): Phi3RMSNorm()
      )
    )
    (norm): Phi3RMSNorm()
  )
  (score): Linear(in_features=3072, out_features=2, bias=False)
)
</code></pre>
","pytorch, nlp, huggingface-transformers, huggingface",
How to convert a AutoModelForCausalLM object to a dspy model object?,"<ol>
<li><p>import dspy</p>
</li>
<li><p>llm = dspy.HFModel(model='model')</p>
</li>
</ol>
<p>This method takes a string as input for the model if i have a quantized model object of the class AutoModelForCausalLM How i can convert the model object to dspy object?</p>
<p>direct assignment gives error on inference</p>
<ol start=""3"">
<li><p>llm = model #previously created as AutoModelForCausalLM class object</p>
</li>
<li><p>llm(&quot;Testing testing, is anyone out there?&quot;)</p>
</li>
</ol>
<p>Error After Code Line 4
File /opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:623, in LlamaModel.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)
621     raise ValueError(&quot;You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time&quot;)
622 elif input_ids is not None:
--&gt; 623     batch_size, seq_length = input_ids.shape
624 elif inputs_embeds is not None:
625     batch_size, seq_length, _ = inputs_embeds.shape</p>
<p>AttributeError: 'str' object has no attribute 'shape'</p>
","nlp, large-language-model, llama, autoclass, dspy",
How to import tensorflow-text correctly,"<p>I have bundle of errors in importing tensorflow-text. I was first trying to import the below versions which were working correctly.</p>
<pre><code>!pip install tensorflow==2.8
</code></pre>
<p>but now it says like this</p>
<pre><code>`import tensorflow as tf
ModuleNotFoundError: No module named 'tensorflow.tsl'`
</code></pre>
<p>with this version of tensorflow i was installing the below version of tensorflow-text which were fine but as know when i use other version of tensorflow that 2.8 it generates error.</p>
<pre><code>!pip install -q -U &quot;tensorflow-text==2.8.*&quot;
</code></pre>
<p>Can any help me with explaining for which version of tensorflow should we use tesnflow-tex.version. As it seems to be a very big trouble for tensorflow handling nlp models.</p>
<p>Thanks</p>
<p>I tried installing many tensorflow versions and then tensorflow text but no match.</p>
","tensorflow, keras, deep-learning, nlp",
ValueError: Expected input batch_size (2) to match target batch_size (4),"<p>Here is the code for a text classification task I am doing. The issue seems to lie here. This is a multi class problem. I have 3 labels. I tried several things. I changed the format of the labels to integers and tried looking into the loss function. I am not sure what parameter needs to be changed.</p>
<pre><code>def evaluate(model, dataloader_val):
    model.eval()
    model.train(False)
    
    loss_val_total = 0
    predictions, true_vals = [], []
    
    for batch in dataloader_val:
        
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }

        with torch.no_grad():        
            outputs = model(**inputs)
            
        loss = outputs[0]
        
        logits = outputs[1]
        loss_val_total += loss.item()

        probs = torch.argmax(logits, dim = 1).detach().cpu().numpy()
        label_ids = inputs['labels'].cpu().numpy()
        predictions.append(probs)
        true_vals.append(label_ids)
        
    loss_val_avg = loss_val_total/len(dataloader_val) 
    
    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)
    
    ### after evaluating we resume model training
    model.train(True)
    
    return loss_val_avg, predictions, true_vals
</code></pre>
<p>And this is the error I get</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-55-a095c6ad8f10&gt; in &lt;module&gt;
     44                      }       
     45 
---&gt; 46             outputs = model(**inputs)
ValueError: Expected input batch_size (2) to match target batch_size (4).
</code></pre>
","python, pytorch, nlp, torch",
Find the most likely word alignment between two strings in Python,"<p>I have 2 similar strings. How can I find the most likely word alignment between these two strings in Python?</p>
<p>Example of input:</p>
<pre class=""lang-py prettyprint-override""><code>string1 = 'my channel is youtube dot com slash example and then I also do live streaming on twitch.'
string2 = 'my channel is youtube.com/example and then I also do livestreaming on twitch.'
</code></pre>
<p>Desired output:</p>
<pre><code>alignment['my']        = 'my'
alignment['channel']   = 'channel'
alignment['is']        = 'is'
alignment['youtube']   = 'youtube.com/example'
alignment['dot']       = 'youtube.com/example'
alignment['com']       = 'youtube.com/example'
alignment['slash']     = 'youtube.com/example'
alignment['example']   = 'youtube.com/example'
alignment['and']       = 'and'
alignment['then']      = 'then'
alignment['I']         = 'I'
alignment['also']      = 'also'
alignment['do']        = 'do'
alignment['live']      = 'livestreaming'
alignment['streaming'] = 'livestreaming'
alignment['on']        = 'on'
alignment['twitch']    = 'twitch'
</code></pre>
","python, text, nlp, alignment, text-alignment",
Longformer get last_hidden_state,"<p>I am trying to follow this example in the huggingface documentation here <a href=""https://huggingface.co/transformers/model_doc/longformer.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/longformer.html</a>:</p>
<pre><code>import torch
from transformers import LongformerModel, LongformerTokenizer
model = LongformerModel.from_pretrained('allenai/longformer-base-4096')
tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')
SAMPLE_TEXT = ' '.join(['Hello world! '] * 1000)  # long input document
input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1
# Attention mask values -- 0: no attention, 1: local attention, 2: global attention
attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to local attention
global_attention_mask = torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to global attention to be deactivated for all tokens
global_attention_mask[:, [1, 4, 21,]] = 1  # Set global attention to random tokens for the sake of this example
                                    # Usually, set global attention based on the task. For example,
                                    # classification: the &lt;s&gt; token
                                    # QA: question tokens
                                    # LM: potentially on the beginning of sentences and paragraphs
outputs = model(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask, output_hidden_states= True)
sequence_output = outputs[0].last_hidden_state
pooled_output = outputs.pooler_output
</code></pre>
<p>I suppose that this would return a document embedding for the sample text.
However, I run into the following error:</p>
<pre><code>AttributeError: 'Tensor' object has no attribute 'last_hidden_state'
</code></pre>
<p>Why isnt it possible to call last_hidden_state?</p>
","python, nlp, pytorch, huggingface-transformers","<p>Do not select via index:</p>
<pre class=""lang-py prettyprint-override""><code>sequence_output = outputs.last_hidden_state
</code></pre>
<p><code>outputs</code> is a LongformerBaseModelOutputWithPooling object with the following properties:</p>
<pre class=""lang-py prettyprint-override""><code>print(outputs.keys())
</code></pre>
<p>Output:</p>
<pre><code>odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])
</code></pre>
<p>Calling <code>outputs[0]</code> or <code>outputs.last_hidden_state</code> will both give you the same tensor, but this tensor does not have a property called <code>last_hidden_state</code>.</p>
"
Hindi accent(&quot;मात्रा&quot;) issue in word cloud using Devnagri font(gargi.ttf),"<p>I am working on a Hindi dataset for a project and did the pre-processing of the data where I am creating a word cloud for the same. I have used &quot;gargi&quot; font to plot the Hindi words on word cloud where I am facing an issue of accent(&quot;ि मात्रा&quot;). In the word cloud, this accent is coming next to the letter on which it is supposed to be, for example, पुलिस is coming as पुलसि. (Kindly refer to this attached where the word किसान has the accent(मात्रा) is on the same letter(वर्ण)).
<img src=""https://i.sstatic.net/sgeUC.png"" alt=""image"" />
There are several other words in this word cloud that reflect a similar issue. I have tried using different fonts as well like &quot;lohit-devnagri&quot;, &quot;samyak-devnagri&quot;.</p>
<pre><code>font = &quot;gargi.ttf&quot;

figure,axis = plt.subplots(2,2,figsize=(16,10))
figure.tight_layout(pad=5.0)

wordcloud_kisaan = WordCloud(width = 1000, height = 700,
                background_color ='white',
                min_font_size = 10, font_path= font).generate_from_frequencies(counter_kisaan)

axis[0][0].imshow(wordcloud_kisaan,interpolation=&quot;bilinear&quot;)
axis[0][0].axis('off')
axis[0][0].set_title('Kisaan Andolan', fontsize=22)
 
plt.axis(&quot;off&quot;)
plt.tight_layout(pad = 5.0)


plt.show()
</code></pre>
","python, machine-learning, nlp, word-cloud, hindi",
Fine-tuning BERT with deterministic masking instead of random masking,"<p>I want to fine-tune BERT on a specific dataset. My problem is that I do not want to mask some tokens of my training dataset randomly, but I already have chosen which tokens I want to mask (for certain reasons).</p>
<p>To do so, I created a dataset that has two columns: <code>text</code> in which some tokens have been replaced with <code>[MASK]</code> (I am aware of the fact that some words could be tokenised with more than one token and I took care of that) and <code>label</code> where I have the whole text.</p>
<p>Now I want to fine-tune a BERT model (say, bert-base-uncased) using Hugging Face's <code>transformers</code> library, but I do not want to use <code> DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.2)</code> where the masking is done randomly and I only can control the probability. What can I do?</p>
","nlp, huggingface-transformers, bert-language-model","<p>This is what I did to solve my problem. I created a custom class and changed the tokenization in a way that I needed (mask one of the numerical spans in the input).</p>
<pre class=""lang-py prettyprint-override""><code>class CustomDataCollator(DataCollatorForLanguageModeling):

    mlm: bool = True
    return_tensors: str = &quot;pt&quot;

    def __post_init__(self):
        if self.mlm and self.tokenizer.mask_token is None:
            raise ValueError(
                &quot;This tokenizer does not have a mask token which is necessary &quot;
                &quot;for masked language modeling. You should pass `mlm=False` to &quot;
                &quot;train on causal language modeling instead.&quot;
            )

    def torch_mask_tokens(self, inputs, special_tokens_mask):
        &quot;&quot;&quot;
        Prepare masked tokens inputs/labels for masked language modeling.
        NOTE: keep `special_tokens_mask` as an argument for avoiding error
        &quot;&quot;&quot;

        # labels is batch_size x length of the sequence tensor
        # with the original token id
        # the length of the sequence includes the special tokens (2)
        labels = inputs.clone()

        batch_size = inputs.size(0)
        # seq_len = inputs.size(1)
        # in each seq, find the indices of the tokens that represent digits
        dig_ids = [1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023]
        dig_idx = torch.zeros_like(labels)
        for dig_id in dig_ids:
            dig_idx += (labels == dig_id)
        dig_idx = dig_idx.bool()
        # in each seq, find the spans of Trues using `find_spans` function
        spans = []
        for i in range(batch_size):
            spans.append(find_spans(dig_idx[i].tolist()))
        masked_indices = torch.zeros_like(labels)
        # spans is a list of lists of tuples
        # in each tuple, the first element is the start index
        # and the second element is the length
        # in each child list, choose a random tuple
        for i in range(batch_size):
            if len(spans[i]) &gt; 0:
                idx = torch.randint(0, len(spans[i]), (1,))
                start, length = spans[i][idx[0]]
                masked_indices[i, start:start + length] = 1
            else:
                print(&quot;No digit found in the sequence!&quot;)
        masked_indices = masked_indices.bool()

        # We only compute loss on masked tokens
        labels[~masked_indices] = -100

        # change the input's masked_indices to self.tokenizer.mask_token
        inputs[masked_indices] = self.tokenizer.mask_token_id

        return inputs, labels

def find_spans(lst):
    spans = []
    for k, g in groupby(enumerate(lst), key=itemgetter(1)):
        if k:
            glist = list(g)
            spans.append((glist[0][0], len(glist)))

    return spans
</code></pre>
"
How do I fix &quot;ValueError: The model did not return a loss from the inputs&quot;?,"<p>I am writing a program in Python. What it does is to train a question-answering model, and it uses a custom-made dataset from a JSON file that I created. However, I ran into the error from the full traceback below:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\chenp\Documents\ML\machineLearning.py&quot;, line 52, in &lt;module&gt;
    trainer.train()
  File &quot;C:\Users\chenp\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py&quot;, line 1859, in train
    return inner_training_loop(
  File &quot;C:\Users\chenp\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py&quot;, line 2203, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File &quot;C:\Users\chenp\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py&quot;, line 3138, in training_step
    loss = self.compute_loss(model, inputs)
  File &quot;C:\Users\chenp\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py&quot;, line 3179, in compute_loss
    raise ValueError(
ValueError: The model did not return a loss from the inputs, only the following keys: start_logits,end_logits. For reference, the inputs it received are input_ids,attention_mask.
  0%|          | 0/9 [00:00&lt;?, ?it/s]
</code></pre>
<p>from the offending line:</p>
<pre><code>trainer.train()
</code></pre>
<p>And this is my full code:</p>
<pre><code># https://www.mlexpert.io/blog/alpaca-fine-tuning
# https://wellsr.com/python/fine-tuning-huggingface-models-in-tensorflow-keras/
# https://learnopencv.com/fine-tuning-bert/
# https://medium.com/@karary/nlp-fine-tune-question-answering-model-%E5%AF%A6%E4%BD%9C-3-model-training-%E5%84%B2%E5%AD%98%E8%88%87-inference-13d2a5bf5c32
# https://medium.com/@anyuanay/fine-tuning-the-pre-trained-bert-model-in-hugging-face-for-question-answering-8edc76890ce0
import transformers as tf
import datasets as ds
import pandas as pd
import numpy as np
import torch
import json
 
############## Check if CUDA is enabled. ################
hasCUDA=torch.cuda.is_available()
print(f&quot;CUDA Enabled? {hasCUDA}&quot;)
device=&quot;cuda&quot; if hasCUDA else &quot;cpu&quot;      
 
############## Loading file and populating data ################
fileName=&quot;qna.json&quot;
sampleDS=ds.load_dataset(&quot;json&quot;, data_files=fileName, split=&quot;train&quot;)


############## Model ##########################################
modelName=&quot;distilbert/distilbert-base-cased&quot;     #or replace the model name with whatever you feel like.
# config=tf.AutoConfig.from_pretrained(modelName+&quot;/config.json&quot;)
model=tf.AutoModelForQuestionAnswering.from_pretrained(modelName)
tokenizer=tf.AutoTokenizer.from_pretrained(modelName)

############## Encoding and Tokenizing #######################################
sampleDS=sampleDS.map(lambda batch: tokenizer(sampleDS[&quot;text&quot;], truncation=True, padding='max_length', max_length=512), batched=True)
sampleDS.set_format(&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;text&quot;])
# sampleDS.rename_column(&quot;answer&quot;, &quot;labels&quot;)
print(sampleDS)
############## Training #######################################
trnArgs=tf.TrainingArguments(
    output_dir=&quot;./&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    num_train_epochs=3,
    remove_unused_columns=True,
    fp16=True
)
 
trainer=tf.Trainer(
    model=model,
    args=trnArgs,
    train_dataset=sampleDS, 
    eval_dataset=sampleDS,
    tokenizer=tokenizer
)
trainer.train()
</code></pre>
<p>Here is a sample of the JSON file that I made:</p>
<pre><code>{&quot;text&quot;: &quot;Who wrote Charlie and the Chocolate Factory?&quot;, &quot;labels&quot;: &quot;Roald Dahl&quot;}
{&quot;text&quot;: &quot;Name a few ways to treat constipation naturally.&quot;, &quot;labels&quot;: &quot;Exercise regularly, eat more fibers, and drink more water.&quot;}
{&quot;text&quot;: &quot;Where is the longest roller coaster located?&quot;, &quot;labels&quot;: &quot;Nagashima, Japan. The name of the coaster is Steel Dragon 2000.&quot;}
{&quot;text&quot;: &quot;Who murdered JFK?&quot;, &quot;labels&quot;: &quot;It is said to be Harvey Oswald.&quot;}
{&quot;text&quot;: &quot;What are the 11 herbs and spices that Colonel Sanders used in KFC?&quot;, &quot;labels&quot;: &quot;Nobody knows, as it's a secret.&quot;}
{&quot;text&quot;: &quot;Who wrote Les Miserables?&quot;, &quot;labels&quot;: &quot;Victor Hugo&quot;}
{&quot;text&quot;: &quot;What is the Watergate Scandal?&quot;, &quot;labels&quot;: &quot;The Watergate scandal was a significant political controversy in the United States during the presidency of Richard Nixon from 1972 to 1974, ultimately resulting in Nixon's resignation. It originated from attempts by the Nixon administration to conceal its involvement in the June 17, 1972, break-in at the Democratic National Committee headquarters located in the Watergate Office Building in Washington, D.C.&quot;}
{&quot;text&quot;: &quot;What is Obama's most famous quote?&quot;, &quot;labels&quot;: &quot;'Yes we can!'&quot;}
{&quot;text&quot;: &quot;Where did the 2008 Olympic take place?&quot;, &quot;labels&quot;: &quot;Beijing&quot;}
{&quot;text&quot;: &quot;Lentils and Chickpeas are what kind of food?&quot;, &quot;labels&quot;: &quot;Beans&quot;}
{&quot;text&quot;: &quot;Who was the disciple that Jesus loved?&quot;, &quot;labels&quot;: &quot;John&quot;}
{&quot;text&quot;: &quot;Why did the Boston Tea Party happen?&quot;, &quot;labels&quot;: &quot;The colonists were unhappy with the tax and restrictions imposed by the British colonists.&quot;}
{&quot;text&quot;: &quot;What was the effect of the Boston Tea Party?&quot;, &quot;labels&quot;: &quot;The British imposed a new Intolerable Act, and tensions between the colonies and the British escalated.&quot;}
{&quot;text&quot;: &quot;Who conquered the Aztec Empire?&quot;, &quot;labels&quot;: &quot;Hernan Cortes&quot;}
{&quot;text&quot;: &quot;What is the longest flight as of 2024?&quot;, &quot;labels&quot;: &quot;Singapore to New York, operated by Singapore Airlines.&quot;}
{&quot;text&quot;: &quot;Name a few infamous Roman dictators.&quot;, &quot;labels&quot;: &quot;Caligula, Nero, and Tiberius.&quot;}
{&quot;text&quot;: &quot;Where did the early Hungarians come from?&quot;, &quot;labels&quot;: &quot;They originated from the Uralic region as nomads, and then migrated to Central Europe's Carpathian basin.&quot;}
{&quot;text&quot;: &quot;Where is the fastest roller coaster located?&quot;, &quot;labels&quot;: &quot;Abu Dhabi, and the coaster is known as F1.&quot;}
{&quot;text&quot;: &quot;What are some popular painting in Uffizi Gallery?&quot;, &quot;labels&quot;: &quot;The Birth of Venus, Madonna of the Goldfinch, and Judith and Holofernes.&quot;}
{&quot;text&quot;: &quot;Who wrote A Christmas Carol and Oliver Twist?&quot;, &quot;labels&quot;: &quot;Charles Dickens&quot;}
</code></pre>
<p>Any suggestions for this fix is welcome.</p>
","machine-learning, nlp, torch, valueerror, fine-tuning",
Text to Openpose and Weird RNN bugs,"<p>I want to create AI that generate openpose from textual description for example if input &quot;a man running&quot; output would be like the image I provided Is there any model architecture recommend for me?</p>
<p>my data condition is</p>
<ul>
<li>canvas_width: 900px</li>
<li>canvas_height: 300px</li>
<li>frames: 5 (5 person)</li>
</ul>
<p><a href=""https://i.sstatic.net/jt6VjR2F.png"" rel=""nofollow noreferrer"">expected output</a></p>
<p>I trying to train RNN for this task and I use sentence transformer for embedding text and then pass to RNN and the loss is look like image below</p>
<pre><code>from sentence_transformers import SentenceTransformer 
sentence_model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)
text = &quot;a man running&quot;
text_input = torch.tensor(sentence_model.encode(text), dtype=torch.float)
</code></pre>
<p><a href=""https://i.sstatic.net/f1sYGb6t.png"" rel=""nofollow noreferrer"">loss image with num_layers=3</a></p>
<p>My RNN setting</p>
<pre><code>embedding_dim = 384
hidden_dim = 512
num_layers = 3
output_dim = 180
num_epochs = 100
learning_rate = 0.001
rnn_model = RNN(embedding_dim, hidden_dim, num_layers, output_dim)
</code></pre>
<p>but the problem is whatever I input the output is the same everytime! but when I try changing num_layers to 1 and keep other setting the same like this</p>
<pre><code>embedding_dim = 384
hidden_dim = 512
num_layers = 1
output_dim = 180
num_epochs = 100
learning_rate = 0.001
rnn_model = RNN(embedding_dim, hidden_dim, num_layers, output_dim)
</code></pre>
<p>the loss now look like this
<a href=""https://i.sstatic.net/ACZ1Ma8J.png"" rel=""nofollow noreferrer"">loss image with num_layers=1</a>
and now the problem is gone !!</p>
<p>Also I try to check the cause of the &quot;output is the same everytime&quot; problem I check dataloader and other code but no problem was found only num_layers=3 that cause the problem num_layers=1 fixed it</p>
<p>This is my training loop</p>
<pre><code>criterion = nn.MSELoss()
optimizer = torch.optim.Adam(rnn_model.parameters(), lr=learning_rate)
</code></pre>
<pre><code>trainingEpoch_loss = []
validationEpoch_loss = []

for epoch in range(num_epochs):
    step_loss = []
    rnn_model.train()
    for idx, train_inputs in enumerate(train_dataloader):
        optimizer.zero_grad()
        outputs = rnn_model(torch.unsqueeze(train_inputs['text'], dim=0))
        training_loss = criterion(outputs, train_inputs['poses'])
        training_loss.backward()
        optimizer.step()
        step_loss.append(training_loss.item())

        if (idx+1) % 1 == 0: print (f'Epoch [{epoch+1}/{num_epochs}], Step [{idx+1}/{len(train_dataloader)}], Loss: {training_loss.item():.4f}')
    trainingEpoch_loss.append(np.array(step_loss).mean())

    rnn_model.eval()
    for idx, val_inputs in enumerate(val_dataloader):
      validationStep_loss = []
      outputs = rnn_model(torch.unsqueeze(val_inputs['text'], dim=0))
      val_loss = criterion(outputs, val_inputs['poses'])
      validationStep_loss.append(val_loss.item())
    validationEpoch_loss.append(np.array(validationStep_loss).mean())
</code></pre>
<p>This is my Inference</p>
<pre><code>text = &quot;a man running&quot;
processed_text = torch.tensor(sentence_model.encode(text), dtype=torch.float)
output_poses = rnn_model(processed_text.unsqueeze(0))
print(output_poses.shape) #shape=(1, 180) 1 person is 36 (original data for 1 person is 54 but I change to 36 because I want only x and y and not z so cut out the z axis) and there's 5 person so 5*36 = 180
</code></pre>
<p>My question is</p>
<ol>
<li>Is there any model architecture recommend for this task other than RNN?</li>
<li>Why whatever I input the output is the same everytime when num_layers=3 I'm very confused because the loss wouldn't go down if the model was giving the same output right? that's mean it give the same output in the Inference phase</li>
</ol>
<p>Expected Answer</p>
<ol>
<li>Model architecture that suit best for my task any papers or github repo related given would be appreciated</li>
<li>Answer why whatever I input the output is the same everytime when num_layers=3</li>
</ol>
","machine-learning, deep-learning, pytorch, nlp",
Looking for the right framework to implement an enterprise document management and analysis system,"<p>I have already spent quite some time with literature reviews and Google searches, but I didn't find anything suitable, yet.</p>
<p>The task is to implement a flexible and scalable enterprise document management and analysis system. I guess that represents a prototypical use case for many businesses.</p>
<p>The perfect framework would allow on premises operation (only Azure would be an option) and provide a low-code platform that allows to receive, tag and register documents (PDFs, Word and Excel files, other text files), indexing and smart search within and across documents and document collections, plus an interface to implement NLP tasks with Python.</p>
<p>Moreover, it would be benefitial, if this framework also would allow to model meta data about documents and about the business processes they are embedded in (for example, to check and verify completeness of a set of necessary documents, before further processing gets triggered).</p>
<p>I thought about a combination of Elastic Search and a NoSql Database like Cassandra, but that would not fit the low-code requirement.</p>
<p>You might call me naive, but I supposed that there ought to be trillions of such frameworks, as this is such a typical use case in terms of business automation. But I did not find the right framework, yet. I hope someone can provide hints.</p>
<p>Summary:</p>
<p>A document management and analysis framework that features:</p>
<ul>
<li>Enterprise-ready (on premises or compatible with Microsoft Azure)</li>
<li>Low-code framework</li>
<li>Large-scale document management and analysis</li>
<li>Modular and extensible via Python and NLP models</li>
<li>Connectable to business logics (i.e. checks for completeness of document collections)</li>
<li>Allowing for meta data and smart search within and across documents</li>
</ul>
","nlp, frameworks, document, analysis, enterprise",
How to use &lt;unk&gt; default token in input sentence in Transformer (OpenNMT) model,"<p>I'm using the OpenNMT library to develop a translation model.
I want to display certain tokens in their original language without translating them, or translate certain tokens to certain tokens only.
If you have any good ideas, please recommend them.</p>
<p>I've been trying to use custom tokens or  tokens to prevent them from being translated during translation, but I'm having trouble with this.</p>
<p>And modifying certain tokens before the attention operation has a negative impact on the attention operation, and I'm trying to figure out how to fix this.</p>
<br/>
What i want
<p>ex) I like apples for breakfast. -&gt; 나는 아침에 먹는 apples를 좋아합니다.</p>
<p>ex) I like <strong>unk</strong> for breackfast. -&gt; 나는 아침에 먹는 <strong>unk</strong>를 좋아합니다.</p>
<p>or</p>
<p>ex) I like <strong>apples</strong> for breakfast. -&gt; 나는 아침에 먹는 <strong>사과</strong>를 좋아합니다.</p>
<p>I want find that position of &quot;apples&quot;  = position of &quot;사과&quot;</p>
","nlp, translate, transformer-model, opennmt",
Calculate coherence for non-gensim topic model,"<p>I've built a topic model, with:</p>
<ul>
<li><strong>Input</strong>: list of tokenized lists</li>
<li><strong>Output</strong>: a <em>m x t</em> matrix (with each cell indicating the probability of word <em>i</em> appearing in topic <em>k</em>).</li>
<li><strong>Output</strong>: a <em>k x n</em> matrix (with each cell indicating the probability of topic <em>k</em> in document <em>j</em>).</li>
</ul>
<p>To find the optimal number of topics, I want to calculate the coherence for a model. However, I am only aware of <code>Gensim</code>'s <code>Coherencemodel</code>, which seems to require a Gensim model as input.</p>
<p>Are there any other packages/implementations that I could use to calculate the coherence of a computed topic model? Or, if it is indeed possible to use the <code>Coherencemodel</code> without inputting a LDAmodel, could someone show me how to do that?</p>
","python-3.x, nlp, package, topic-modeling","<p>Actually, you can do this with the Gensim package.</p>
<p>input_data = list of list with tokenized texts</p>
<p>topics = list with top N words per topic</p>
<pre><code>import gensim.corpora as corpora
from gensim.models.coherencemodel import CoherenceModel

id2word = corpora.Dictionary(input_data)
corpus = [id2word.doc2bow(text) for text in input_data]

cm = CoherenceModel(
    topics=topics,
    texts=input_data,
    corpus=corpus,
    dictionary=id2word,
    coherence='c_v')
coherence = cm.get_coherence()
</code></pre>
"
can i use langchain retrievers such ParentDocumentRetriever&#160;with supabase PGvector enabled table,"<p>I want to know what possible retrievers I can use with SupabaseVectorStore as a vector store. also ParentDocumentRetriever, is it supported by supabase??</p>
<pre><code>vector_store = SupabaseVectorStore(client=get_supabase_client(),
                                   embedding=get_embeddings(),
                                   table_name='documents',
                                   query_name='match_documents')
store = InMemoryStore()
child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)
retriever = ParentDocumentRetriever(vectorstore=vector_store,
                                        docstore=store,
                                        child_splitter=child_splitter)
</code></pre>
<p>this code returns an empty list. in langchain ParentDocumentRetriever documentation Chroma vector store.</p>
","python, nlp, supabase, langchain, embedding",
TF accuracy metric wants a single value but gets a list of probabilities,"<p>I am performing a simple seq-to-seq transformer task. I have tried various loss and metrics but none are working. Currently, in model.compile() I am using these:</p>
<pre><code>loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
metrics=['accuracy'],
</code></pre>
<p>The input and final_layer output of the code is correct (batch_size, output_seq_length, output_vocab_size):</p>
<pre><code>Sample input shape (encoder input): (18, 20)
Sample input shape (decoder input): (18, 100)
Model output shape: (18, 100, 200)
tf.Tensor([ 18 100 200], shape=(3,), dtype=int32)
</code></pre>
<p>Regardless, I get the error that it wants a single value:</p>
<pre><code>InvalidArgumentError: Can not squeeze dim[1], expected a dimension of 1, got 100
 [[{{node metrics/accuracy/Squeeze}}]]
</code></pre>
<p>All code is available in a relatively simple playground:<br />
<a href=""https://colab.research.google.com/drive/1tfk518PwmrJEapxIQlOuiLmiTSfxREk3?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1tfk518PwmrJEapxIQlOuiLmiTSfxREk3?usp=sharing</a><br />
But here is some core code (I've tried all of the metrics and losses):</p>
<pre><code>self.encoder_embedding = tf.keras.layers.Embedding(args.problem_vocab_size, args.dim)
self.encoder_ffn = tf.keras.Sequential([
    tf.keras.layers.Dense(args.dim_ff, activation='relu'),
    tf.keras.layers.Dense(args.dim)
])

self.decoder_embedding = tf.keras.layers.Embedding(args.solution_vocab_size, args.dim)
self.cross_attention = tf.keras.layers.MultiHeadAttention(num_heads=args.num_heads, key_dim=args.dim // args.num_heads)
self.decoder_ffn = tf.keras.Sequential([
    tf.keras.layers.Dense(args.dim_ff, activation='relu'),
    tf.keras.layers.Dense(args.dim)
])

self.final_layer = tf.keras.layers.Dense(args.solution_vocab_size)

model.compile(optimizer=tf.keras.optimizers.legacy.Adam(args.learning_rate),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=[tf.keras.metrics.sparse_categorical_accuracy],
              run_eagerly=False)

model.fit(dataset, epochs=args.epochs, steps_per_epoch=args.num_batches)
</code></pre>
","tensorflow, nlp, tensorflow2.0",
Encode a list of sentences into embeddings using a HuggingFace model not in its hub,"<p>I am trying to encode a list of sentences into a list of embeddings. When I use a model that is in the HuggingFace hub, it works as expected. But when I use a model not in the hub, in this case Facebook's <code>M2M100</code> model, I do not get the expected results.</p>
<p>When using a model within <code>SentenceTransformer()</code>, my results look like this:</p>
<pre><code>from sentence_transformers import SentenceTransformer
dat = ['Meteorite fell on the road ', 'I went in the wrong direction']
model_1 = SentenceTransformer('all-distilroberta-v1')
embeddings_1 = model_1.encode(dat)
embeddings_1.shape
&gt; (2, 768)
</code></pre>
<p>However, when I use the <code>M2M100</code> model, my results do not look right at all, specifically I would expect 2 rows of results:</p>
<pre><code>from transformers import M2M100Tokenizer
model_m2m = M2M100Tokenizer.from_pretrained(&quot;facebook/m2m100_418M&quot;)
model_m2m.src_lang = &quot;en&quot;
embeddings_m2m = model_m2m.encode(dat, return_tensors=&quot;pt&quot;)
embeddings_m2m.shape
&gt; torch.Size([1, 4])
</code></pre>
<p>How should I format this so that it returns an n-dimensional list of embeddings, where each row corresponds to a sentence and the number of columns is equal to the dimensionality of the embedding?</p>
<p>(As a note, eventually I will be doing this for sentences in other languages, which is why I'm using a multi-lingual model.)</p>
","nlp, huggingface-transformers, encode, embedding, huggingface","<p>The code you provided only uses the tokenizer of the model, which maps the text to integer ids that don't represent any kind of (semantical) meaning.</p>
<p>To retrieve sentence embeddings (i.e. a vector that represents the text) from
<a href=""https://huggingface.co/facebook/m2m100_418M"" rel=""nofollow noreferrer"">facebook/m2m100_418M</a>, which is an encoder-decoder model, you need to perform some kind of pooling over the last-hidden-state of the encoder. <a href=""https://arxiv.org/abs/2104.06979"" rel=""nofollow noreferrer"">Common</a> approaches, which are cls and mean pooling are shown in the example below:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import M2M100Tokenizer, M2M100Model

def mean_pooling(last_hidden_state, attention_mask):
  non_pad_tokens = attention_mask.sum(1)
  sum_embeddings =  torch.sum(attention_mask.unsqueeze(-1) * last_hidden_state, 1)
  return sum_embeddings/non_pad_tokens.unsqueeze(-1)

def cls_pooling(last_hidden_state):
  return last_hidden_state[:,0]

dat = ['Meteorite fell on the road ', 'I went in the wrong direction']


model_id = &quot;facebook/m2m100_418M&quot;
t_m2m = M2M100Tokenizer.from_pretrained(model_id)
t_m2m.src_lang = &quot;en&quot;
m_m2m = M2M100Model.from_pretrained(model_id)

tokenized = t_m2m(dat, padding=True, return_tensors='pt')

with torch.inference_mode():
  encoder_o = m_m2m.encoder(**tokenized)

encoder_last_hidden_state = encoder_o.last_hidden_state
print(encoder_last_hidden_state.shape)
mean_pooling_embeddings = mean_pooling(encoder_last_hidden_state, tokenized.attention_mask)
print(mean_pooling_embeddings.shape)
cls_pooling_embeddings = cls_pooling(encoder_last_hidden_state)
print(cls_pooling_embeddings.shape)
</code></pre>
<p>Output:</p>
<pre><code>torch.Size([2, 9, 1024])
torch.Size([2, 1024])
torch.Size([2, 1024])
</code></pre>
<p>Which of the two approaches works better for your downstream task, must be tested with your data. Please also note that even when you have the sentence embeddings now, it doesn't mean they are semantically meaningful (i.e. the embeddings are useless for your downstream task). Refer to this StackOverflow <a href=""https://stackoverflow.com/questions/63461262/bert-sentence-embeddings-from-transformers/64237402#64237402"">answer</a> for further explanation.</p>
"
How to lemmatize text column in pandas dataframes using stanza?,"<p>I read csv file into pandas dataframe.</p>
<p>my text column is df['story'].</p>
<p>how do I lemmatize  this colummn ?</p>
<p>should I tokenize before?</p>
","pandas, nlp, tokenize, lemmatization, stanza","<p>No, you don't necessarily have to tokenize before lemmatizing. You can try the following code:</p>
<pre><code>import stanza
import pandas as pd

nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')

def lemmatize_text(text):
    doc = nlp(text)
    lemmas = [word.lemma for sent in doc.sentences for word in sent.words]
    return ' '.join(lemmas)

df['lemmatized_story'] = df['story'].apply(lemmatize_text)
</code></pre>
"
How to Reduce Snippet Length and Minimize Token Usage in Cohere API Webhook Responses?,"<p><strong>Problem Description:</strong>
I am integrating the Cohere API with Zapier to perform web searches and generate concise summaries. However, I am encountering an issue where the snippets in the webhook responses are extremely long, leading to high token consumption and inefficient processing. This significantly impacts the efficiency and cost-effectiveness of the integration.</p>
<p><strong>1. Adjusting Parameters:</strong></p>
<ul>
<li>I have tried setting max_tokens to a lower value.</li>
<li>Experimented with different values for top_p and top_k.</li>
<li>Used return_snippets: false in the webhook configuration.</li>
</ul>
<p><strong>2. Configuration Example:</strong>**</p>
<pre><code>{
  &quot;model&quot;: &quot;command-r-plus&quot;,
  &quot;message&quot;: &quot;Please provide a concise summary of what was happening in New York on 15 May 2024 using web search.&quot;,
  &quot;temperature&quot;: 0.1,
  &quot;max_tokens&quot;: 150,
  &quot;top_p&quot;: 0.3,
  &quot;top_k&quot;: 3,
  &quot;frequency_penalty&quot;: 0.5,
  &quot;connectors&quot;: [
    {
      &quot;id&quot;: &quot;web-search&quot;,
      &quot;options&quot;: {
        &quot;num_results&quot;: 1,
        &quot;return_snippets&quot;: false
      }
    }
  ],
  &quot;prompt_truncation&quot;: &quot;AUTO&quot;,
  &quot;citation_quality&quot;: &quot;fast&quot;
}

</code></pre>
<p>Despite these attempts, the snippets remain long, and the token usage is still high. This results in responses that are not as concise as needed and lead to excessive costs.</p>
<p><strong>Expected Outcome:</strong>
I am looking for a way to completely exclude or significantly reduce the length of snippets in the Cohere API webhook responses to ensure concise summaries and efficient token usage.</p>
<p><strong>Endpoint:</strong>
I am using the <a href=""https://api.cohere.com/v1/chat"" rel=""nofollow noreferrer"">https://api.cohere.com/v1/chat</a> endpoint for these requests.</p>
","nlp, webhooks, tokenize, zapier",
Why does TensorFlow graph execution require different shapes than eager execution?,"<p>I have 10-100 token inputs and ~1000 token outputs, so I have separate embeddings for them so I can have computational efficiency.  I'm pretty sure the answer is that there's no way to have differently lengthed inputs and outputs in graph execution.  But I know this isn't the case because at multiple steps throughout my project I've had the code work fine.  So maybe it's a new version or something?</p>
<p>Running in eager execution works just fine:</p>
<pre><code>((TensorSpec(shape=(None, 20), dtype=tf.int32, name=None), TensorSpec(shape=(None, 100), dtype=tf.int32, name=None)), TensorSpec(shape=(None, 100), dtype=tf.int32, name=None))
</code></pre>
<p>Running in graph mode it won't create the dataset:</p>
<pre><code>---&gt; 39     dataset = tf.data.Dataset.from_tensor_slices((list(problems), list(solutions), list(solutions)))
ValueError: Expected values [array([27, ..., 16], dtype=int32)] to be a dense tensor with shape [20], but got shape [20, 20].
</code></pre>
<p>I've created a minimal version in Colab:</p>
<p><a href=""https://colab.research.google.com/drive/1FTUqr22gmLNOjOwNAykTmd91o0DlVlIR?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1FTUqr22gmLNOjOwNAykTmd91o0DlVlIR?usp=sharing</a></p>
<p>Caveat: It could be that graph execution skips a step it deems unnecessary - and maybe it really does want the same sized inputs?</p>
","tensorflow, nlp, tensorflow2.0",
Error.__init__() got an unexpected keyword argument &#39;trainable&#39; when try to load a .keras model,"<p>I've trained a NER model and save it as .keras file, when I try to load the model, it pops up this error</p>
<pre><code>TypeError: &lt;class 'modeling.NERModel'&gt; could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned 
by `get_config()` are explicitly deserialized in the model's `from_config()` method.

config={'module': 'modeling', 'class_name': 'NERModel', 'config': {'trainable': True, 'dtype': 'float32'}, 'registered_name': 'NERModel', 'build_config': {'input_shape': 
[None, None]}, 'compile_config': {'optimizer': 'adam', 'loss': {'module': 'modeling', 'class_name': 'CustomNonPaddingTokenLoss', 'config': {'name': 'custom_ner_loss', 'reduction': 'sum'}, 'registered_name': 'CustomNonPaddingTokenLoss'}, 'loss_weights': None, 'metrics': None, 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False}}.

Exception encountered: Unable to revive model from config. When overriding the `get_config()` method, make sure that the returned config contains all items used as arguments in the  constructor to &lt;class 'modeling.NERModel'&gt;, which is the default behavior. You can override this default behavior by defining a `from_config(cls, config)` class method to specify how to create an instance of NERModel from its config.

Received config={'trainable': True, 'dtype': 'float32'}

Error encountered during deserialization: NERModel.__init__() got an unexpected keyword argument 'trainable'
</code></pre>
<p>Below is my NER model code and I face no problem when training.</p>
<pre><code>import os
from tensorflow import keras
import tensorflow as tf
from keras import layers

class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super().__init__()
        self.att = keras.layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.ffn = keras.Sequential(
            [
                keras.layers.Dense(ff_dim, activation=&quot;relu&quot;),
                keras.layers.Dense(embed_dim),
            ]
        )
        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = keras.layers.Dropout(rate)
        self.dropout2 = keras.layers.Dropout(rate)

    def call(self, inputs, training=False):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim):
        super().__init__()
        self.token_emb = keras.layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

    def call(self, inputs):
        maxlen = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        position_embeddings = self.pos_emb(positions)
        token_embeddings = self.token_emb(inputs)
        return token_embeddings + position_embeddings
    
class NERModel(keras.Model):
    def __init__(
        self, num_tags, vocab_size, maxlen=1000, embed_dim=32, num_heads=2, ff_dim=32
    ):
        super().__init__()
        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)
        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
        self.dropout1 = layers.Dropout(0.1)
        self.ff = layers.Dense(ff_dim, activation=&quot;relu&quot;)
        self.dropout2 = layers.Dropout(0.1)
        self.ff_final = layers.Dense(num_tags, activation=&quot;softmax&quot;)

    def call(self, inputs, training=False):
        x = self.embedding_layer(inputs)
        x = self.transformer_block(x)
        x = self.dropout1(x, training=training)
        x = self.ff(x)
        x = self.dropout2(x, training=training)
        x = self.ff_final(x)
        return x

class CustomNonPaddingTokenLoss(keras.losses.Loss):
    def __init__(self, reduction='sum', name=&quot;custom_ner_loss&quot;):
        super().__init__(reduction=reduction, name=name)

    def call(self, y_true, y_pred):
        loss_fn = keras.losses.SparseCategoricalCrossentropy(
            from_logits=False, reduction=self.reduction  # Pass the reduction argument here
        )
        loss = loss_fn(y_true, y_pred)
        mask = tf.cast((y_true &gt; 0), dtype=tf.float32)
        loss = loss * mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)
    
def save_model(model, filepath):
    if os.path.exists(filepath):
        filepath = filepath[:-6] + &quot;1&quot; + filepath[-6:]
    model.save(filepath)
</code></pre>
<p>When I try to run predict and load the trained model, the error above shows up. I tried to change the save type (h5) but there are a different error, therefore I change back to .keras file and try to solve this first.</p>
<pre><code>import re
import pickle
import keras
import tensorflow as tf
import numpy as np
from data_preprocess import map_record_to_training_data
from modeling import CustomNonPaddingTokenLoss
    
def lookup(tokens):
    # Load the list from the file
    with open('./resources/vocabulary.pkl', 'rb') as f:
        loaded_list = pickle.load(f)
    # The StringLookup class will convert tokens to token IDs
    lookup_layer = keras.layers.StringLookup(vocabulary=loaded_list)

    return lookup_layer(tokens)

def format_datatype(data):
    tokens =  [re.sub(r'[;,]', '', d) for d in data.split(' ')]
    #default is 0, since is for prediction
    ner_tags = [0 for d in data.split(' ')]

    #tab to separate
    string_input = str(len(tokens))+ &quot;\t&quot;+ &quot;\t&quot;.join(tokens)+ &quot;\t&quot;+ &quot;\t&quot;.join(map(str, ner_tags))
    string_input = tf.data.Dataset.from_tensor_slices([string_input])


    finalize_input = (string_input.map(map_record_to_training_data)
                      .map(lambda x, y: (lookup(x),  y))
                      .padded_batch(1)
                      )

    return finalize_input

def prediction(data):
    # Load model
    # Load the model
    loaded_model = tf.keras.models.load_model(&quot;./resources/trained_model/ner_model.keras&quot;, 
                                        custom_objects={'CustomNonPaddingTokenLoss': CustomNonPaddingTokenLoss})

    print(loaded_model.summary())
    all_predicted_tag_ids = []

    for x, _ in data:
        print(&quot;Input Tensor Info:&quot;)
        print(&quot;Data Type:&quot;, x.dtype)
        print(&quot;Shape:&quot;, x.shape)
        output = loaded_model(x, training=False)
        predictions = np.argmax(output, axis=-1)
        predictions = np.reshape(predictions, [-1])
        all_predicted_tag_ids.append(predictions)

    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)

    ner_labels = [&quot;[PAD]&quot;, &quot;N&quot;, &quot;M&quot;, &quot;other&quot;]
    mapping =  dict(zip(range(len(ner_labels)), ner_labels))
    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]

    return predicted_tags


sample_input = &quot;Hi, my name is David&quot;
print(result)
print(sample_input.split(' '))
print(len(result))
</code></pre>
","python, tensorflow, keras, nlp, named-entity-recognition",
Interpreting the rows and columns of the attention Heatmap,"<p>I have a simple question to which I didn't find an answer:
<strong>How to read the attention heatmap ? Rows attend to Columns or Columns attend to Rows</strong></p>
<p>Since it isn't symmetric sometimes, like in this plot, we see the diagonal is shifted by one down, do we here say that the tokens are attending most to the precedent token or to the next token ? (without considering the exceptions)</p>
<p>If the answer is precedent token, it means that the rows are attending to the columns
e.g. CTTCTG (2nd token) is attending the most to CTGATGA (1st token).</p>
<p>If the answer is next token, it means that the columns are attending to the rows
e.g. CTGATGA (1st token) is attending the most to CTTCTG (2nd token).</p>
<p>If you have any article where I can read about the interpretations of the attention heatmaps, please refer to it, I'd like to understand more the role of different heads, and how visualizing different layers can be useful, if it is.</p>
<p>Thank you</p>
<p><a href=""https://i.sstatic.net/cQDmiBgY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cQDmiBgY.png"" alt=""DNA Attention"" /></a></p>
","nlp, heatmap, attention-model, self-attention, multihead-attention",
No Attention returned even when output_attentions= True,"<p>I'm using a pretrained model based BERT (github link:<a href=""https://github.com/MAGICS-LAB/DNABERT_2"" rel=""nofollow noreferrer"">DNABERT-2</a>)</p>
<p>It uses AutoModelForSequenceClassification and mosaicml/mosaic-bert-base.</p>
<p>I'm having the problem that I cannot extract the attention. I have read many posts which show ways of dealing with that by activating output_attentions=True in the model, but none of the posts solved the problem.
<code>output</code> is of length 2 and each element is of shape: <code>torch.Size([1, 7, 768])</code> and
<code>torch.Size([1, 768])</code>. When trying to get <code>output.attentions</code> I get <code>None</code>.</p>
<p>I'm not sure where to search and what a solution would be.</p>
<p>I'm providing my whole code:</p>
<p>Defining model, trainer, data, tokenizer:</p>
<pre><code>from copy import deepcopy

from sklearn.metrics import precision_recall_fscore_support import wandb from transformers import TrainerCallback
# END NEW import os import csv import json import logging from dataclasses import dataclass, field from typing import Optional, Dict, Sequence, Tuple, List

import torch import transformers import sklearn import numpy as np from torch.utils.data import Dataset

@dataclass class ModelArguments:
    model_name_or_path: Optional[str] = field(default=&quot;facebook/opt-125m&quot;)
    use_lora: bool = field(default=False, metadata={&quot;help&quot;: &quot;whether to use LoRA&quot;})
    lora_r: int = field(default=8, metadata={&quot;help&quot;: &quot;hidden dimension for LoRA&quot;})
    lora_alpha: int = field(default=32, metadata={&quot;help&quot;: &quot;alpha for LoRA&quot;})
    lora_dropout: float = field(default=0.05, metadata={&quot;help&quot;: &quot;dropout rate for LoRA&quot;})
    lora_target_modules: str = field(default=&quot;query,value&quot;, metadata={&quot;help&quot;: &quot;where to perform LoRA&quot;})


@dataclass class DataArguments:
    data_path: str = field(default=None, metadata={&quot;help&quot;: &quot;Path to the training data.&quot;})
    kmer: int = field(default=-1, metadata={&quot;help&quot;: &quot;k-mer for input sequence. -1 means not using k-mer.&quot;})


@dataclass class TrainingArguments(transformers.TrainingArguments):
    cache_dir: Optional[str] = field(default=None)
    run_name: str = field(default=&quot;run&quot;)
    optim: str = field(default=&quot;adamw_torch&quot;)
    model_max_length: int = field(default=512, metadata={&quot;help&quot;: &quot;Maximum sequence length.&quot;})
    gradient_accumulation_steps: int = field(default=1)
    per_device_train_batch_size: int = field(default=1)
    per_device_eval_batch_size: int = field(default=1)
    num_train_epochs: int = field(default=1)
    logging_steps: int = field(default=100)
    save_steps: int = field(default=100)
    fp16: bool = field(default=False)
    # START NEW
    # eval_steps: int = field(default=100)
    eval_steps: int = field(default=0.1)
    # END NEW
    evaluation_strategy: str = field(default=&quot;steps&quot;)
    warmup_steps: int = field(default=50)
    weight_decay: float = field(default=0.01)
    learning_rate: float = field(default=1e-4)
    save_total_limit: int = field(default=3)
    load_best_model_at_end: bool = field(default=True)
    output_dir: str = field(default=&quot;output&quot;)
    find_unused_parameters: bool = field(default=False)
    checkpointing: bool = field(default=False)
    dataloader_pin_memory: bool = field(default=False)
    eval_and_save_results: bool = field(default=True)
    save_model: bool = field(default=False)
    seed: int = field(default=42)


def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    &quot;&quot;&quot;Collects the state dict and dump to disk.&quot;&quot;&quot;
    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa


&quot;&quot;&quot; Get the reversed complement of the original DNA sequence. &quot;&quot;&quot;


def get_alter_of_dna_sequence(sequence: str):
    MAP = {&quot;A&quot;: &quot;T&quot;, &quot;T&quot;: &quot;A&quot;, &quot;C&quot;: &quot;G&quot;, &quot;G&quot;: &quot;C&quot;}
    # return &quot;&quot;.join([MAP[c] for c in reversed(sequence)])
    return &quot;&quot;.join([MAP[c] for c in sequence])


&quot;&quot;&quot; Transform a dna sequence to k-mer string &quot;&quot;&quot;


def generate_kmer_str(sequence: str, k: int) -&gt; str:
    &quot;&quot;&quot;Generate k-mer string from DNA sequence.&quot;&quot;&quot;
    return &quot; &quot;.join([sequence[i:i + k] for i in range(len(sequence) - k + 1)])


&quot;&quot;&quot; Load or generate k-mer string for each DNA sequence. The generated k-mer string will be saved  to the same directory as the original data with the same name but with a suffix of &quot;_{k}mer&quot;. &quot;&quot;&quot;


def load_or_generate_kmer(data_path: str, texts: List[str], k: int) -&gt; List[str]:
    &quot;&quot;&quot;Load or generate k-mer string for each DNA sequence.&quot;&quot;&quot;
    kmer_path = data_path.tokenizerreplace(&quot;.csv&quot;, f&quot;_{k}mer.json&quot;)
    if os.path.exists(kmer_path):
        logging.warning(f&quot;Loading k-mer from {kmer_path}...&quot;)
        with open(kmer_path, &quot;r&quot;) as f:
            kmer = json.load(f)
    else:
        logging.warning(f&quot;Generating k-mer...&quot;)
        kmer = [generate_kmer_str(text, k) for text in texts]
        with open(kmer_path, &quot;w&quot;) as f:
            logging.warning(f&quot;Saving k-mer to {kmer_path}...&quot;)
            json.dump(kmer, f)

    return kmer


class SupervisedDataset(Dataset):
    &quot;&quot;&quot;Dataset for supervised fine-tuning.&quot;&quot;&quot;

    def __init__(self,
                 data_path: str,
                 tokenizer: transformers.PreTrainedTokenizer,
                 kmer: int = -1):

        super(SupervisedDataset, self).__init__()

        # load data from the disk
        with open(data_path, &quot;r&quot;) as f:
            data = list(csv.reader(f))[1:]
        if len(data[0]) == 2:
            # data is in the format of [text, label]
            logging.warning(&quot;Perform single sequence classification...&quot;)
            texts = [d[0] for d in data]
            labels = [int(d[1]) for d in data]
        # All genes sequences are concat: we don't work with the sequence-pair,
        # But we are tricking the model to think it is single sequence.
        elif len(data[0]) == 3:
            # data is in the format of [text1, text2, label]
            logging.warning(&quot;Perform sequence-pair classification...&quot;)
            texts = [[d[0], d[1]] for d in data]
            labels = [int(d[2]) for d in data]
        else:
            raise ValueError(&quot;Data format not supported.&quot;)

        if kmer != -1:
            # only write file on the first process
            if torch.distributed.get_rank() not in [0, -1]:
                torch.distributed.barrier()

            logging.warning(f&quot;Using {kmer}-mer as input...&quot;)
            texts = load_or_generate_kmer(data_path, texts, kmer)

            if torch.distributed.get_rank() == 0:
                torch.distributed.barrier()

        output = tokenizer(
            texts,
            return_tensors=&quot;pt&quot;,
            padding=&quot;longest&quot;,
            max_length=tokenizer.model_max_length,
            truncation=True,
        )

        self.input_ids = output[&quot;input_ids&quot;]
        # CHANGE
        self.input_ids[0][self.input_ids[0] == 0] = 2
        # Change to which tokens we want to attend and to which we don't
        self.attention_mask = output[&quot;attention_mask&quot;]
        self.labels = labels
        self.num_labels = len(set(labels))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, i) -&gt; Dict[str, torch.Tensor]:
        return dict(input_ids=self.input_ids[i], labels=self.labels[i])


@dataclass class DataCollatorForSupervisedDataset(object):
    &quot;&quot;&quot;Collate examples for supervised fine-tuning.&quot;&quot;&quot;

    tokenizer: transformers.PreTrainedTokenizer

    def __call__(self, instances: Sequence[Dict]) -&gt; Dict[str, torch.Tensor]:
        input_ids, labels = tuple([instance[key] for instance in instances] for key in (&quot;input_ids&quot;, &quot;labels&quot;))
        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = torch.Tensor(labels).long()
        return dict(
            input_ids=input_ids,
            labels=labels,
            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),
        )


&quot;&quot;&quot; Manually calculate the accuracy, f1, matthews_correlation, precision, recall with sklearn. &quot;&quot;&quot;

def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):
    if logits.ndim == 3:
        # Reshape logits to 2D if needed
        logits = logits.reshape(-1, logits.shape[-1])
    predictions = np.argmax(logits, axis=-1)
    valid_mask = labels != -100  # Exclude padding tokens (assuming -100 is the padding token ID)
    valid_predictions = predictions[valid_mask]
    valid_labels = labels[valid_mask]
    return {
        # START NEW
        &quot;sum prediction&quot;: f'{sum(valid_predictions)}/{len(valid_predictions)}',
        # END NEW
        &quot;accuracy&quot;: sklearn.metrics.accuracy_score(valid_labels, valid_predictions),
        &quot;f1&quot;: sklearn.metrics.f1_score(
            valid_labels, valid_predictions, average=&quot;macro&quot;, zero_division=0
        ),
        &quot;matthews_correlation&quot;: sklearn.metrics.matthews_corrcoef(
            valid_labels, valid_predictions
        ),
        &quot;precision&quot;: sklearn.metrics.precision_score(
            valid_labels, valid_predictions, average=&quot;macro&quot;, zero_division=0
        ),
        &quot;recall&quot;: sklearn.metrics.recall_score(
            valid_labels, valid_predictions, average=&quot;macro&quot;, zero_division=0
        ),
    }

&quot;&quot;&quot; Compute metrics used for huggingface trainer. &quot;&quot;&quot; def compute_metrics(eval_pred):
    logits, labels = eval_pred
    if isinstance(logits, tuple):  # Unpack logits if it's a tuple
        logits = logits[0]
    return calculate_metric_with_sklearn(logits, labels)


class CustomTrainer(transformers.Trainer):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.epoch_predictions = []
        self.epoch_labels = []
        self.epoch_loss = []

    def compute_loss(self, model, inputs, return_outputs=False):
        &quot;&quot;&quot;
        MAX: Subclassed to compute training accuracy.

        How the loss is computed by Trainer. By default, all models return the loss in
        the first element.

        Subclass and override for custom behavior.
        &quot;&quot;&quot;
        if self.label_smoother is not None and &quot;labels&quot; in inputs:
            labels = inputs.pop(&quot;labels&quot;)
        else:
            labels = None
        outputs = model(**inputs, output_attentions=True)
        # TEST
        try:
            print(f&quot;Attention: {outputs.attentions}&quot;)
        except Exception:
            print(&quot;No Attention returned&quot;)

        if &quot;labels&quot; in inputs:
            preds = outputs.logits.detach()

            # Log accuracy
            acc = (
                (preds.argmax(axis=1) == inputs[&quot;labels&quot;])
                .type(torch.float)
                .mean()
                .item()
            )
            # Uncomment it if you want to plot the batch accuracy
            # wandb.log({&quot;batch_accuracy&quot;: acc})  # Log accuracy

            # Store predictions and labels for epoch-level metrics
            self.epoch_predictions.append(preds.cpu().numpy())
            self.epoch_labels.append(inputs[&quot;labels&quot;].cpu().numpy())

        # Save past state if it exists
        if self.args.past_index &gt;= 0:
            self._past = outputs[self.args.past_index]

        if labels is not None:
            loss = self.label_smoother(outputs, labels)
        else:
            loss = outputs[&quot;loss&quot;] if isinstance(outputs, dict) else outputs[0]
            # Uncomment it if you want to plot the batch loss
            # wandb.log({&quot;batch_loss&quot;: loss})
            self.epoch_loss.append(loss.item())  # Store loss for epoch-level metrics

        return (loss, outputs) if return_outputs else loss

# Define a custom callback to calculate metrics at the end of each epoch class CustomCallback(TrainerCallback):

    def __init__(self, trainer) -&gt; None:
        super().__init__()
        self._trainer = trainer

    def on_epoch_end(self, args, state, control, **kwargs):
        # Aggregate predictions and labels for the entire epoch
        epoch_predictions = np.concatenate(self._trainer.epoch_predictions)
        epoch_labels = np.concatenate(self._trainer.epoch_labels)

        # Compute accuracy
        accuracy = np.mean(epoch_predictions.argmax(axis=1) == epoch_labels)

        # Compute mean loss
        mean_loss = np.mean(self._trainer.epoch_loss)

        # Compute precision, recall, and F1-score
        precision, recall, f1, _ = precision_recall_fscore_support(
            epoch_labels, epoch_predictions.argmax(axis=1), average=&quot;weighted&quot;
        )

        # Log epoch-level metrics
        wandb.log({&quot;epoch_accuracy&quot;: accuracy, &quot;epoch_loss&quot;: mean_loss})
        wandb.log({&quot;precision&quot;: precision, &quot;recall&quot;: recall, &quot;f1&quot;: f1})

        # Clear stored predictions, labels, and loss for the next epoch
        self._trainer.epoch_predictions = []
        self._trainer.epoch_labels = []
        self._trainer.epoch_loss = []
        return None

        # TODO: use this function to gather the prediction and labels and get the metrics
#%%
</code></pre>
<p>Instantiating and training:</p>
<pre><code>from transformer_model import SupervisedDataset, DataCollatorForSupervisedDataset, ModelArguments, \
    TrainingArguments, DataArguments, safe_save_model_for_hf_trainer, CustomTrainer, CustomCallback, \
    compute_metrics


from copy import deepcopy
from transformers import TrainerCallback
# END NEW
import os
import json
import torch
import transformers

from peft import (
    LoraConfig,
    get_peft_model,
    get_peft_model_state_dict,
)

import wandb
run = wandb.init()
assert run is wandb.run

def train(device):
    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    # load tokenizer
    tokenizer = transformers.AutoTokenizer.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
        model_max_length=training_args.model_max_length,
        padding_side=&quot;right&quot;,
        use_fast=True,
        trust_remote_code=True,
    )

    if &quot;InstaDeepAI&quot; in model_args.model_name_or_path:
        tokenizer.eos_token = tokenizer.pad_token

    # define datasets and data collator
    train_dataset = SupervisedDataset(tokenizer=tokenizer,
                                      data_path=os.path.join(data_args.data_path, &quot;train.csv&quot;),
                                      kmer=data_args.kmer)
    val_dataset = SupervisedDataset(tokenizer=tokenizer,
                                    data_path=os.path.join(data_args.data_path, &quot;dev.csv&quot;),
                                    kmer=data_args.kmer)
    test_dataset = SupervisedDataset(tokenizer=tokenizer,
                                     data_path=os.path.join(data_args.data_path, &quot;test.csv&quot;),
                                     kmer=data_args.kmer)
    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)

    # load model
    model = transformers.AutoModelForSequenceClassification.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
        num_labels=train_dataset.num_labels,
        trust_remote_code=True,
        output_attentions = True
    ).to(device)

    # configure LoRA
    if model_args.use_lora:
        lora_config = LoraConfig(
            r=model_args.lora_r,
            lora_alpha=model_args.lora_alpha,
            target_modules=list(model_args.lora_target_modules.split(&quot;,&quot;)),
            lora_dropout=model_args.lora_dropout,
            bias=&quot;none&quot;,
            task_type=&quot;SEQ_CLS&quot;,
            inference_mode=False,
        )
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()

    trainer = CustomTrainer(model=model,
                            tokenizer=tokenizer,
                            args=training_args,
                            compute_metrics=compute_metrics,
                            train_dataset=train_dataset,
                            eval_dataset=val_dataset,
                            data_collator=data_collator
                            )

    trainer.add_callback(CustomCallback(trainer))
    trainer.train()

    # train_result = trainer.train()
    # loss = train_result[&quot;loss&quot;]
    # print(f&quot;loss issss: {loss}&quot;)
    # print(f&quot;Train reusults: {train_result}&quot;) # NEW: result: only returns metrics at the end of training

    if training_args.save_model:
        trainer.save_state()
        safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)

    # get the evaluation results from trainer
    if training_args.eval_and_save_results:
        results_path = os.path.join(training_args.output_dir, &quot;results&quot;, training_args.run_name)
        results = trainer.evaluate(eval_dataset=test_dataset)
        os.makedirs(results_path, exist_ok=True)
        with open(os.path.join(results_path, &quot;eval_results.json&quot;), &quot;w&quot;) as f:
            json.dump(results, f)


if __name__ == &quot;__main__&quot;:
    # Define device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print('Using device:', device)
    # Call the train function with the device
    train(device)
</code></pre>
<p>After training, I try to run it on an example:</p>
<pre><code>model_path = './finetune/output/dnabert2'
tokenizer = AutoTokenizer.from_pretrained(model_path)
# Load the model with output_attention=True
model = AutoModel.from_pretrained(model_path, trust_remote_code=True, output_attentions=True)
model_input = tokenizer(&quot;ACTGACGGGTAGTGACTG&quot;, return_tensors=&quot;pt&quot;)

with torch.inference_mode():
  output = model(**model_input, output_attentions=True)
</code></pre>
<p>My code might have some tests and prints. Let me know if anything is missing. Thank you very much for the help.</p>
","nlp, huggingface-transformers, bert-language-model, transformer-model, attention-model","<p>The problem was deep in the structure: attention was discarded early in the model, I had therefore to go through the code to understand what is happening, and change it.</p>
<p><a href=""https://huggingface.co/jaandoui/DNABERT2-AttentionExtracted"" rel=""nofollow noreferrer"">https://huggingface.co/jaandoui/DNABERT2-AttentionExtracted</a></p>
<p>I had to extract attention_probs.
Here are the changes I have done.</p>
"
"How to extract specific content, like name or DOB, from a document using NLP and python?","<p>I want to extract very specific content like name, address and dob from a document (say for example, a resume). Assuming I have 1000 of such documents, I want to automate it using machine learning and natural language processing. And preferably python.<br>
How can I do that? or Where do I start?</p>

<p>Update: I am aware of NER but I am looking to extract very specific information from a document which can be loaded into an excel or something.</p>

<p>Example: From a project report, I would like to extract the topic, team member names and tenure of the project.</p>
","python, machine-learning, nlp",
save and load keras transformer model,"<p>i am still new to deep learning right now i follow this keras tutorial to make an translation model using transformer <a href=""https://keras.io/examples/nlp/neural_machine_translation_with_transformer/"" rel=""nofollow noreferrer"">here the link</a>. Everything works just fine but i have no idea how to save the model, currently i have this implementation</p>
<pre><code>transformer.save(&quot;models/transformer.h5&quot;)
</code></pre>
<p>and load the model with this</p>
<pre><code>new_model = tf.keras.models.load_model(
    &quot;models/transformer.h5&quot;,
    custom_objects={
        &quot;PositionalEmbedding&quot;: PositionalEmbedding,
        &quot;TransformerEncoder&quot;: TransformerEncoder,
        &quot;TransformerDecoder&quot;: TransformerDecoder,
    },
)
new_model.summary()
</code></pre>
<p>i also attempt to save the vectorization and load it with pickle following <a href=""https://stackoverflow.com/questions/77774499/how-to-save-keras-textvectorization-layer-configuration-with-custom-standardizat"">this</a>
but when i use the model outside the notebook the model is worsen, when i load the model in the same notebook it just fine (without reloading the vectorization)</p>
<p>can someone guide me how to correctly how to save this model?</p>
","python, tensorflow, keras, deep-learning, nlp",
AttributeError: module &#39;chromadb&#39; has no attribute &#39;config&#39;,"<p>so i recently started to work on chromabd and i am facing this error:
&quot;module 'chromadb' has no attribute 'config'&quot;</p>
<p>here is my code:</p>
<pre><code>    from langchain.vectorstores import Chroma 
    from sentence_transformers import SentenceTransformer

    model = SentenceTransformer('all-MiniLM-L6-v2')

    #Sentences are encoded by calling model.encode()
    embeddings = [model.encode(text[i].page_content) for i in range(len(text))]

    presist_directory = 'db'
    vectordb =       Chroma.from_documents(documents=text,embedding=embeddings,persist_directory=presist_directory)
</code></pre>
<ol>
<li>i have already tried down versioning chromadb</li>
<li>tried this solution:</li>
</ol>
<pre><code>    from langchain.indexes import VectorstoreIndexCreator
    from langchain.vectorstores import Chroma

    presist_directory = 'db'
    vectordb = VectorstoreIndexCreator().from_documents(
        documents=text,
        embedding=embeddings,
        persist_directory=presist_directory,
        vectorstore_cls=Chroma,
    ) 
</code></pre>
<ol start=""3"">
<li>and tried this solution too:</li>
</ol>
<pre><code>    import chromadb

    # Get the version of ChromaDB
    chroma_version = chromadb.__version__

    # Check if the version is 0.4 or later
    if float(chroma_version[:3]) &gt;= 0.4:
        # Use the new configuration
        _client_settings = chromadb.config.Settings(
            chroma_db_impl=&quot;new_configuration&quot;,
            persist_directory=persist_directory,
        )
    else:
        # Use the old configuration
        _client_settings = chromadb.config.Settings(
            chroma_db_impl=&quot;duckdb+parquet&quot;,
            persist_directory=persist_directory,
        )

</code></pre>
","python, nlp, chatbot, langchain, chromadb",
Attention Tensor Shape meaning,"<p>While I'm trying to extract Attention from a model, I see that there is a part where the attention changes its shape after <code>matmul()</code> with <code>v</code> (value).</p>
<p>The shape goes from:
attention_probs 2 shape: <code>torch.Size([1, 12, 464, 464])</code></p>
<p>To:
attention shape: <code>torch.Size([1, 464, 12, 64])</code></p>
<p>And then rearranged to: <code>torch.Size([464, 768])</code></p>
<p><strong>The most important question, is what part should I use to visualize the attention of each token to another ?</strong></p>
<p>If you can tell me why, I'd be happy. Especially, if I need to use the part before the <code>matmul()</code>, then I wonder why isn't <code>v</code> included in the attention.</p>
<pre><code>qkv = self.Wqkv(hidden_states)
        qkv = pad_input(qkv, indices, cu_seqlens.shape[0] - 1,
                        max_seqlen_in_batch)  # batch, max_seqlen_in_batch, thd
        qkv = rearrange(qkv,
                        'b s (t h d) -&gt; b s t h d',
                        t=3,
                        h=self.num_attention_heads)
        if self.p_dropout or flash_attn_qkvpacked_func is None:
            # if we have nonzero attention dropout (e.g. during fine-tuning) or no Triton, compute attention in PyTorch
            q = qkv[:, :, 0, :, :].permute(0, 2, 1, 3)  # b h s d
            k = qkv[:, :, 1, :, :].permute(0, 2, 3, 1)  # b h d s
            v = qkv[:, :, 2, :, :].permute(0, 2, 1, 3)  # b h s d
            attention_scores = torch.matmul(q, k) / math.sqrt(
                self.attention_head_size)
            attention_scores = attention_scores + bias
            attention_probs = nn.functional.softmax(attention_scores, dim=-1)
            attention_probs = self.dropout(attention_probs)

            # Before matmul(): Torch.Size([1, 12, 464, 464])
            print(f'BUSA: attention_probs 2 shape: {attention_probs.shape}')

            attention = torch.matmul(attention_probs, v).permute(0, 2, 1,
                                                                 3)  # b s h d
            # After matmul() torch.Size([1, 464, 12, 64])
            print(f'BUSA: attention shape: {attention.shape}')

        else:
            # Triton implementation only supports 0 attention dropout
            convert_dtype = qkv.dtype not in [torch.float16, torch.bfloat16]
            if convert_dtype:
                # Triton implementation only supports fp16 and bf16
            ...
            else:
                attention = flash_attn_qkvpacked_func(qkv, bias)
                print(f'BUSA Triton: attention 2 shape: {attention_probs.shape}')

        # attn_mask is 1 for attend and 0 for don't
        attention = unpad_input_only(attention, torch.squeeze(attn_mask) == 1)

        # Still the same: torch.Size([1, 12, 464, 464])
        print(f'BUSA unpadded final attention shape: {attention_probs.shape}')

        rearranged_attention = rearrange(attention, 'nnz h d -&gt; nnz (h d)')

        # torch.Size([464, 768]) which is [464,12,64]
        print(f'REARRANGED ATTENTION: {rearranged_attention.shape}')

        return rearrange(attention, 'nnz h d -&gt; nnz (h d)')
</code></pre>
","python, nlp, transformer-model, attention-model, self-attention",
TypeError: Exception encountered when calling layer &#39;embeddings&#39; (type TFBertEmbeddings),"<p>My model was wholly workable two weeks back, but now it's showing the following error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-23-a3e5a45f06c9&gt; in &lt;cell line: 14&gt;()
     12 
     13 # Encode input using BERT model
---&gt; 14 bert_output = bert_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
     15 
     16 # Get pooled output and pass through dropout layer

8 frames
/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---&gt; 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

TypeError: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings).

Could not build a TypeSpec for name: &quot;tf.debugging.assert_less/assert_less/Assert/Assert&quot;
op: &quot;Assert&quot;
input: &quot;tf.debugging.assert_less/assert_less/All&quot;
input: &quot;tf.debugging.assert_less/assert_less/Assert/Assert/data_0&quot;
input: &quot;tf.debugging.assert_less/assert_less/Assert/Assert/data_1&quot;
input: &quot;tf.debugging.assert_less/assert_less/Assert/Assert/data_2&quot;
input: &quot;Placeholder&quot;
input: &quot;tf.debugging.assert_less/assert_less/Assert/Assert/data_4&quot;
input: &quot;tf.debugging.assert_less/assert_less/y&quot;
attr {
  key: &quot;T&quot;
  value {
    list {
      type: DT_STRING
      type: DT_STRING
      type: DT_STRING
      type: DT_INT32
      type: DT_STRING
      type: DT_INT32
    }
  }
}
attr {
  key: &quot;summarize&quot;
  value {
    i: 3
  }
}
 of unsupported type &lt;class 'tensorflow.python.framework.ops.Operation'&gt;.

Call arguments received by layer 'embeddings' (type TFBertEmbeddings):
  • input_ids=&lt;KerasTensor: shape=(None, 50) dtype=int32 (created by layer 'input_ids')&gt;
  • position_ids=None
  • token_type_ids=&lt;KerasTensor: shape=(None, 50) dtype=int32 (created by layer 'token_type_ids')&gt;
  • inputs_embeds=None
  • past_key_values_length=0
  • training=False
</code></pre>
<p>I think this error occurs when the input layers are sent to the corresponding BERT layer. If I use old versions of TensorFlow instead of 2.15.0, the error is resolved. However, with those old versions, I did not get GPU and faced a Graph Execution Error.</p>
<p>Can anyone help me with this</p>
","tensorflow, deep-learning, nlp, bert-language-model, transformer-model","<p>it's a problem with the transformers library, I had the same problem and solved it using version 4.31.0</p>
"
Cardinality issue with graph execution in TensorFlow,"<p>I was using a TFRecord and couldn't get past a cardinality issue to get it to work with graph execution.  I switched back to a regular dataset loading method and now I have another cardinality issue.</p>
<pre><code>  File &quot;/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/tf_dataset_adapter.py&quot;, line 57, in num_batches
    cardinality = int(self._dataset.cardinality())
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'SymbolicTensor'
</code></pre>
<p>I have done all of the suggested methods for fixing it, and have correctly set the cardinality of the dataset.  I have verified my cardinality in many ways.  In eager execution I can simply print</p>
<pre><code>print(tf.data.experimental.cardinality(dataset).numpy())
</code></pre>
<p>But in graph execution the changed code is much harder to understand</p>
<pre><code>dataset = dataset.apply(tf.data.experimental.assert_cardinality(self.num_batches))
cardinality = self.get_cardinality(dataset)
print(cardinality)
print(cardinality == tf.data.experimental.INFINITE_CARDINALITY)
print(cardinality == tf.data.experimental.UNKNOWN_CARDINALITY)
</code></pre>
<p>Which gives Tensor(&quot;PartitionedCall:0&quot;, shape=(), dtype=int64) and False for both beforehand. (the cardinality was set to '3')</p>
<p>After setting the cardinality, it gives Tensor(&quot;PartitionedCall_1:0&quot;, shape=(), dtype=int64) and False for both.  If I remove the @tf_function from my get_cardinality method (I read using a return object makes it possible to get it in graph execution which is why I'm using a function), the output becomes the same but of type DatasetCardinality: Tensor(&quot;DatasetCardinality_1:0&quot;, shape=(), dtype=int64).</p>
<p>I have dug, for hundreds of hours, trying to fix my code over months now and have read all of the documentation down to the source code.
This is what is getting called and wants the cardinality:
<a href=""https://github.com/keras-team/keras/blob/v3.3.3/keras/src/trainers/data_adapters/tf_dataset_adapter.py"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras/blob/v3.3.3/keras/src/trainers/data_adapters/tf_dataset_adapter.py</a></p>
<p>I think there needs to be some page I've missed which goes over why graph execution is doing what it's doing and why it wants cardinality etc.  The wierdest part is over the past couple months this exact code has worked and I've gone back to my github and copied various portions of it.  So I think the issue could lie in tensorflow.python.framework.ops import disable_eager_execution or something which changes how graph execution is set up.  I also have tf.config.run_functions_eagerly(False) and run_eagerly=False in my compile().</p>
","tensorflow, nlp, tensorflow2.0, tensorflow-datasets",
Assertion with no scription in vllm with DeepSeekMath 7b model,"<p>I'm working with the DeepSeekMath 7b model to generate synthetic data using Python, but I'm encountering an <code>AssertionError</code> with no description alongside warnings related to token length exceeding the model's limit. The relevant portion of my code triggers this issue during a batch operation with <code>llm.generate(...)</code>. Here’s the code snippet and the error logs:</p>
<pre class=""lang-py prettyprint-override""><code>request_outputs: list[RequestOutput] = gen.llm.generate(batch_prompts, gen.sampling_params)
</code></pre>
<p>This is part of a larger function that processes datasets, initiated by:</p>
<pre class=""lang-py prettyprint-override""><code>lst: list[dict] = gen_data_set_from_ans_str_2_jsonl(gen, path_2_src_dataset, output_path, prompt_template, num_data_gens_per_txt_files=num_data_gens_per_txt_files)
</code></pre>
<p>Here are the logs indicating the error and warnings:</p>
<pre><code>AssertionError: 
len(all_raw_data)=274
len(all_raw_data)/batch_size = 1/9223372036854775807 = 1
num_batches=1
Processed prompts:   1%|██▌ | 2/274 [00:10&lt;19:40,  4.34s/it]
WARNING 05-15 19:44:53 scheduler.py:619] Input prompt (4557 tokens) is too long and exceeds limit of 4096
Processed prompts:  24%|███████████ | 66/274 [01:41&lt;03:38,  1.05s/it]
</code></pre>
<p>The process always halts at 66 out of 274. I suspect the issue relates to handling input sizes, but I’m unclear why the assertion fails without any description or how to handle these warnings properly. Has anyone faced similar issues, or can anyone suggest how to debug or fix this situation?</p>
<hr />
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>import math
import os
from pathlib import Path
from tqdm import tqdm

import torch

import json
from typing import Optional
import sys
from pprint import pprint
from vllm import LLM, SamplingParams, CompletionOutput, RequestOutput

import tenacity


# -- Mathematica gen prompt

# HELM_MATH_PROMPT_8SHOT_COT2_GEN_SYN_METHEMATICA_TEMPLATE: str = (
# &quot;&quot;&quot;Given a mathematics problem, its answer, math category, and problem type, generate a solution to the problem, with step by step reasoning that justifies the answer. 
# Simplify your answer as much as possible. Always give the final answer inside a \\boxed{answer}.### 
# This is the format:
# Problem: Let $r=3^s-s$ and $s=2^n+1$. What is the value of $r$ when $n=2$?
# Solution: Let's think step by step. First substitute $n=2$ into the expression for $s$ to find $s=2^2+1=5$. Then substitute $s=5$ into the expression for $r$ to find $r=3^5-5=243-5=\\boxed{238}. The final answer is: \\boxed{238}.###
# Problem: If $x^{2y}= 4$ and $x = 4$, what is the value of $y$? Express your answer as a common fraction.
# Solution: Let's think step by step. Plugging $x = 4$ into the first equation, we get $4^{2y} = 4^1 \\Rightarrow 2y = 1 \\Rightarrow y = \\boxed{\\frac{1}{2}}. The final answer is: \\boxed{\\frac{1}{2}}.###
# Problem: If $y = \\displaystyle\\frac{1}{3x+1}$, what is the value of $x$ when $y = 1$?
# Solution: Let's think step by step.Since $y=1$, we have $1 =\\displaystyle\\frac{1}{3x+1}$. Multiplying both sides by $3x+1$, we have $$3x+1=1$$ $$\\Rightarrow \\qquad 3x=0$$ $$\\Rightarrow \\qquad x=\\boxed{0}$$. The final answer is: \\boxed{0}.###
# Problem: A scale drawing of a park shows that one inch represents 800 feet. A line segment in the drawing that is 4.75 inches long represents how many feet?
# Solution: Let's think step by step. Each inch of the 4.75-inch line segment represents 800 feet, so the whole line segment represents $4.75\\times800=\\frac{19}{4}\\cdot800=19\\cdot200=\\boxed{3800}$ feet. The final answer is: \\boxed{3800}###
# Problem: If $(x + y)^2 = 25$ and $xy = 6$, what is the value of $x^2 + y^2$?
# Solution: Let's think step by step. We know that $(x + y)^2 = (x^2 + y^2) + 2xy = 25$. We are given that $xy = 6$. So, by substitution, $x^2 + y^2 + 2xy = x^2 + y^2 + 2(6) = 25$. It follows that $x^2 + y^2 = 25 - 12 = \\boxed{13}$. The final answer is: \\boxed{13}###
# Problem: On a hot day, Megan likes to eat a Popsicle every 15 minutes. Assuming she keeps up that rate of consumption, how many Popsicles can Megan finish in 4 hours and 30 minutes?
# Solution: Let's think step by step. Let $p$ be the number of Popsicles Megan can finish in 4 hours and 30 minutes. If we convert that period of time into minutes, we find that 4 hours and 30 minutes is equal to $(4)(60)+30=270$ minutes. From here, we can set up the proportion \\begin{align*} \\frac{x}{270}&amp; =\\frac{1}{15}\\\\\\Rightarrow \\qquad x&amp; =\\left(\\frac{1}{15}\\right)(270)\\\\\\Rightarrow \\qquad x&amp; =\\boxed{18}\\end{align*}. The final answer is: \\boxed{18}###
# Problem: Compute $95^2$ in your head.
# Solution: Let's think step by step. We have $(90 + 5)^2 = 90^2 + 2(90)(5) + 5^2 = 8100 + 900 + 25 = \\boxed{9025}$. The final answer is: \\boxed{9025}.###
# Problem: If $2^8=16^x$, find $x$.
# Solution: Let's think step by step. We can write $16$ as $2^4$. Therefore, we can write our equation as $2^8 = 2^{4 \\cdot x}$. Solving, we get that $x = \\boxed{2}$. The final answer is: \\boxed{2}.###
# This is the new problem to generate a solution string from the answer, problem, category, and problem type:
# Problem: {problem}
# The answer is {answer}, the math category is {category}, the problem type is {prob_type}.
# Solution: Let's think step by step.&quot;&quot;&quot;)

# HELM_MATH_PROMPT_8SHOT_COT2_USE_SOLN_CODE_GEN_SYN_METHEMATICA_TEMPLATE: str = (
# &quot;&quot;&quot;Given a mathematics problem, its answer, math category, problem type, and mathematica code that describes the solution to the answer, generate a solution to the problem, with step by step reasoning followed by the answer. 
# Simplify your answer as much as possible. Always give the final answer inside a \\boxed{answer}.### 
# This is the format:
# Problem: Let $r=3^s-s$ and $s=2^n+1$. What is the value of $r$ when $n=2$?
# Solution: Let's think step by step. First substitute $n=2$ into the expression for $s$ to find $s=2^2+1=5$. Then substitute $s=5$ into the expression for $r$ to find $r=3^5-5=243-5=\\boxed{238}. The final answer is: \\boxed{238}.###
# Problem: If $x^{2y}= 4$ and $x = 4$, what is the value of $y$? Express your answer as a common fraction.
# Solution: Let's think step by step. Plugging $x = 4$ into the first equation, we get $4^{2y} = 4^1 \\Rightarrow 2y = 1 \\Rightarrow y = \\boxed{\\frac{1}{2}}. The final answer is: \\boxed{\\frac{1}{2}}.###
# Problem: If $y = \\displaystyle\\frac{1}{3x+1}$, what is the value of $x$ when $y = 1$?
# Solution: Let's think step by step.Since $y=1$, we have $1 =\\displaystyle\\frac{1}{3x+1}$. Multiplying both sides by $3x+1$, we have $$3x+1=1$$ $$\\Rightarrow \\qquad 3x=0$$ $$\\Rightarrow \\qquad x=\\boxed{0}$$. The final answer is: \\boxed{0}.###
# This is the new problem to generate a solution string from the answer, problem, category, and problem type:
# Problem: {problem}
# The answer is {answer}, the math category is {category}, the problem type is {prob_type}, and the mathematica code is {mathematica_code}. 
# Generate a solution using that information:
# Solution: Let's think step by step.&quot;&quot;&quot;)

# # Goal of prompt is to generate (missing) solution strings from a the mathematica code, given 1 few shot example. ref: https://chatgpt.com/c/fb17c070-bd1e-4aad-9078-88c44abc4ae9
# mathematica_code1: str = open(os.path.expanduser('~/gold-ai-olympiad/py_src/training/algebraic_manipulation.nb')).read()
# MATHEMATICA_SYNTH_GEN_PROMPT_TEMPLATE: str = (
# f&quot;&quot;&quot;
# Given a mathematics problem, its answer, math category, problem type, and mathematica code that describes the solution to the answer, generate a solution to the problem, with step by step reasoning followed by the answer. 
# Simplify your answer as much as possible. Always give the final answer inside a \\boxed{{answer}}.###
# Problem: Let $r=3^s-s$ and $s=2^n+1$. What is the value of $r$ when $n=2$?
# Answer: 238
# Math Category: Algebra
# Problem Type: Algebraic Manipulation
# Mathematica Code: {mathematica_code1}
# Solution: Let's think step by step. First substitute $n=2$ into the expression for $s$ to find $s=2^2+1=5$. Then substitute $s=5$ into the expression for $r$ to find $r=3^5-5=243-5=\\boxed{238}. The final answer is: \\boxed{238}.###
# Problem: {{problem}}
# Answer: {{answer}}
# Math Category: {{category}}
# Problem Type: {{prob_type}}
# Mathematica Code: {{mathematica_code}}
# Solution: Let's think step by step.&quot;&quot;&quot;
# )
# Goal of prompt is to generate (missing) solution strings from a the mathematica code, given 1 few shot example. ref: https://chatgpt.com/c/fb17c070-bd1e-4aad-9078-88c44abc4ae9
MATHEMATICA_SYNTH_GEN_PROMPT_TEMPLATE: str = (
f&quot;&quot;&quot;
Given a mathematics problem, its answer, math category, and problem type that describes the solution to the answer, generate a solution to the problem, with step by step reasoning followed by the answer. 
Simplify your answer as much as possible. Always give the final answer inside a \\boxed{{answer}}.###
Problem: Let $r=3^s-s$ and $s=2^n+1$. What is the value of $r$ when $n=2$?
Answer: 238
Math Category: Algebra
Problem Type: Algebraic Manipulation
Solution: Let's think step by step. First substitute $n=2$ into the expression for $s$ to find $s=2^2+1=5$. Then substitute $s=5$ into the expression for $r$ to find $r=3^5-5=243-5=\\boxed{238}. The final answer is: \\boxed{238}.###
Problem: {{problem}}
Answer: {{answer}}
Math Category: {{category}}
Problem Type: {{prob_type}}
Solution: Let's think step by step.&quot;&quot;&quot;
)

# --

@tenacity.retry(stop=tenacity.stop_after_attempt(10), wait=tenacity.wait_exponential(multiplier=2, max=64))
def gen_synthetic_solution(gen, prompt_template: str, problem, answer, category, prob_type = None) -&gt; str:
    &quot;&quot;&quot;
    Generates a synthetic solution to a math problem using a language model.

    Args:
        gen (GPT): The language model used to generate the solution.
        problem (str): The math problem.
        answer (str): The correct answer to the problem.
        category (str): The category of the problem.
        prob_type (str): The type of problem.

    Returns:
        str: The synthetic solution to the problem.
    &quot;&quot;&quot;
    prompt: str = prompt_template.replace('{problem}', problem)
    # solution = generate_completion(problem, answer, category, prob_type)
    response: dict = gen.llm.chat.completions.create(
        model=gen.model,
        messages=[
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: gen.system_prompt},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt},
        ],
        temperature=gen.sampling_params.temperature,
        top_p=gen.sampling_params.top_p,
        n=gen.sampling_params.n,
        stop=gen.sampling_params.stop[:3],
        )
    solution: str = response.choices[0].message.content
    return solution

def extract_problem_data(file_path: str) -&gt; dict[str, str]:
    &quot;&quot;&quot;
    Extracts the problem and answer from a .txt file.

    Args:
        file_path (str): The path to the .txt file containing the problem.

    Returns:
        Dict[str, str]: A dictionary with keys 'problem' and 'answer'.
    &quot;&quot;&quot;
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read().split(&quot;\nAnswer:\n&quot;)
        problem = content[0].strip()
        answer = content[1].strip()
    return {'problem': problem, 'solution': '', 'answer': answer}

def gen_data_set_from_ans_str_2_jsonl(
                                        gen, 
                                        source_path: str, 
                                        output_path: str,
                                        prompt_template: str, 
                                        num_data_gens_per_txt_files: int = 100,
                                        batch_size: Optional[int] = sys.maxsize,
                                        batched: Optional[bool] =True,
                                        use_mathematica_code: bool = True,
                                ) -&gt; list[dict]:
    &quot;&quot;&quot;
    Creates a JSON Lines file from .txt files containing math problems across directories.

    Args:
        source_path (str): Path to the source directory containing folders of .txt files.
        output_path (str): Path to save the .jsonl output file.

    Returns:
        List[Dict[str, str]]: A list of dictionaries representing the json lines.
    &quot;&quot;&quot;
    source_path: Path = Path(source_path).expanduser()
    output_path: Path = Path(output_path).expanduser()
    print(f'{source_path=} {output_path=}')

    # -- Collect all the raw data
    print()
    all_raw_data: list[dict] = []
    for root, dirs, files in os.walk(source_path):
        # print(f'\n{root=} \n{dirs[:3]=} \n{files[:3]=}')
        for f in files[:num_data_gens_per_txt_files]:  # Select up to 50 .txt files per directory
            if f.endswith('.txt'):
                # print(f'\n{root=} \n{dirs[:3]=} \n{files[:3]=}')
                # selected_files = [os.path.join(root, f) for f in files if f.endswith('.txt')][:num_data_gens_per_txt_files]  
                file_path = os.path.join(root, f)
                category: str = root.split('/')[-2]
                problem_type: str = root.split('/')[-1]
                mathematica_script_name: str = f'{problem_type}.nb'
                # extract problem, answer, category, prob_type, (empty) solution
                problem_data: dict = extract_problem_data(file_path)
                all_raw_data.append(problem_data)
                problem_data['problem_type'] = ' '.join(problem_type.split('_'))  # &quot;variance_and_std&quot; -&gt; &quot;variance and std&quot;
                problem_data['category'] = ' '.join(category.split('_'))  # &quot;counting_and_statics&quot; -&gt; &quot;counting and statics&quot;
                problem_data['mathematica_script_name'] = mathematica_script_name
                # get everything up to the last but last is not included
                path_2_mathematica_scripts: str = '/'.join(root.split('/')[:-1])  # &lt;path&gt;/&lt;cat&gt; since cat has the nb files
                mathematica_script: str = open(f'{path_2_mathematica_scripts}/{mathematica_script_name}', 'r').read()
                print(f'{problem_data=}')
                problem_data['mathematica_script'] = mathematica_script
                print(f'{problem_data=}')
                print()
    print(f'{len(all_raw_data)=}')
    
    # TODO -- Batch the data
    if batched:
        from evals.utils import batch_data
        assert batch_size &gt; 0, f'batch_size should be greater than 0 but got: {batch_size=}'
        all_raw_data: list[dict] = batch_data(all_raw_data, batch_size=batch_size)
        num_batches: int = len(all_raw_data)
        print(f'len(all_raw_data)/batch_size = {len(all_raw_data)}/{batch_size} = {math.ceil(len(all_raw_data)/batch_size)}')
        print(f'{num_batches=}')

    # -- Gen synthetic data
    from evals.inference_eval import VllmGenerator, OpenAIGenerator
    print(f'{type(all_raw_data)=} {type(all_raw_data[0])=}')  # to see which type of prompts we will make
    if isinstance(gen, VllmGenerator) and isinstance(all_raw_data[0], dict):  # list of single string prompt 
        all_raw_data: list[dict]
        # - Generate synthetic solution string for each problem and answer (using meta-data too)
        all_data_with_gen_synth_soln: list[dict] = []
        for raw_data in tqdm(all_raw_data, total=len(all_raw_data)):
            # Make prompt to generate synthetic solution
            problem: str = raw_data['problem']
            answer: str = raw_data['answer']
            category: str = raw_data['category']
            prob_type: str = raw_data['problem_type']
            mathematica_code: str = raw_data['mathematica_script']
            # prompt: str = prompt_template.replace('{problem}', problem).replace('{answer}', answer).replace('{category}', category).replace('{prob_type}', prob_type).replace('{mathematica_code}', mathematica_code)
            prompt: str = prompt_template.replace('{problem}', problem).replace('{answer}', answer).replace('{category}', category).replace('{prob_type}', prob_type)
            # Generate synthetic solutions (completions), n&gt;1 means same reasoning per solution but phrased differently
            request_outputs_per_batch_prompts: list[RequestOutput] = gen.llm.generate(prompt, gen.sampling_params) 
            completions_per_prompt: list[CompletionOutput] = request_outputs_per_batch_prompts[0].outputs
            synthetic_solutions: list[str] = [completion.text for completion in completions_per_prompt]
            # Create dict data point per problem answer pair with its synthetic solutions
            print(f'First solution: {synthetic_solutions[0]=}')
            print(f'Real answer: {answer=}')
            if len(synthetic_solutions) == 1:
                raw_data['solution'] = synthetic_solutions[0]
                all_data_with_gen_synth_soln.append(raw_data)
            else:
                data_points_per_ans: list[dict] = [{**raw_data, 'solution': solution} for solution in synthetic_solutions]
                all_data_with_gen_synth_soln.extend(data_points_per_ans)
                assert len(synthetic_solutions) == gen.sampling_params.n, f'{len(synthetic_solutions)=} != {gen.sampling_params.n=}'
        assert len(all_data_with_gen_synth_soln) == len(all_raw_data) * len(synthetic_solutions), f'{len(all_data_with_gen_synth_soln)=} != {len(all_raw_data) * len(synthetic_solutions)=}'
    elif isinstance(gen, VllmGenerator) and isinstance(all_raw_data[0], list):  # list of batch of prompts
        all_raw_data: list[list[dict]]
        # - Generate synthetic solution string for each problem and answer (using meta-data too)
        all_data_with_gen_synth_soln: list[dict] = []
        for batch_probs_ans in all_raw_data:
            batch_prompts: list[str] = [prompt_template.replace('{problem}', prob_ans['problem']).replace('{answer}', prob_ans['answer']).replace('{category}', prob_ans['category']).replace('{prob_type}', prob_ans['problem_type']) for prob_ans in batch_probs_ans]
            request_outputs: list[RequestOutput] = gen.llm.generate(batch_prompts, gen.sampling_params)
            for request_output_per_prompt in request_outputs:
                completions_per_prompt: list[CompletionOutput] = request_output_per_prompt.outputs
                synthetic_solutions: list[str] = [completion.text for completion in completions_per_prompt]
                # Create dict data point per problem answer pair with its synthetic solutions
                for i, prob_ans in enumerate(batch_probs_ans):
                    prob_ans['solution'] = synthetic_solutions[i]
                    all_data_with_gen_synth_soln.append(prob_ans)
        assert len(synthetic_solutions) == gen.sampling_params.n, f'{len(synthetic_solutions)=} != {gen.sampling_params.n=}'
        assert len(all_data_with_gen_synth_soln) == len(all_raw_data) * len(synthetic_solutions), f'{len(all_data_with_gen_synth_soln)=} != {len(all_raw_data) * len(synthetic_solutions)=}'
        raise NotImplemented
    elif isinstance(gen, OpenAIGenerator):
        raise ValueError(f'Invalid value for {gen=}.')
    else:
        raise ValueError(f'Invalid value for {gen=}.')
    print(f'{len(all_data_with_gen_synth_soln)=} {len(all_raw_data)=}')
    assert isinstance(all_data_with_gen_synth_soln, list) and isinstance(all_data_with_gen_synth_soln[0], dict), f'{type(all_data_with_gen_synth_soln)=} {type(all_data_with_gen_synth_soln[0])=}'
    
    # -- Save all data to a jsonlines file
    output_path: Path = output_path.expanduser()
    output_path.parent.mkdir(parents=True, exist_ok=True) if output_path.is_dir() else output_path.parent.mkdir(parents=True, exist_ok=True)
    print(f'{output_path=}')
    with open(output_path, 'w', encoding='utf-8') as file:
        for synth_data in all_data_with_gen_synth_soln:
            json.dump(synth_data, file)
            file.write('\n')
    return all_data_with_gen_synth_soln

# -- Main

def main_gen_synth_data(
        # model: str = 'DEFAULT',  # e.g., Mistral-7B-Instrcut-v0.2 on http://120.77.8.29:12345 
        # model: str = 'deepseek-ai/deepseek-math-7b-instruct', 
        model: str = 'mistralai/Mistral-7B-Instruct-v0.1', 
        # model: str = 'gpt2', 
        n: int = 4, # num seqs to return for given prompt
        max_tokens: int = 2048,
        # max_tokens: int = 4096,
        top_p: float = 0.95, 
        temperature: float = 0.8,
        num_beams: int = None,
        best_of: int = None,
        use_beam_search: bool = False,
        num_similar_data_pts_txt_files: int = 30,  # number of problems to use (that were generate from the same mathamatica script)
    ):
    &quot;&quot;&quot; Gen synthetic data from math problems. &quot;&quot;&quot;
    from evals.prompts_evals import STOP_TOKENS
    from evals.inference_eval import VllmGenerator, OpenAIGenerator
    from evals.utils import get_dtype_for_vllm
    from evals.inference_eval import VllmGenerator
    # -- Print
    print(f'{model=}')
    print()

    # -- Get vllm generator
    prompt_template: str = MATHEMATICA_SYNTH_GEN_PROMPT_TEMPLATE
    stop: list[str] = STOP_TOKENS
    sampling_params: SamplingParams = SamplingParams(n=n, max_tokens=max_tokens, top_p=top_p, temperature=temperature, stop=stop, use_beam_search=use_beam_search, best_of=best_of)
    print(f'{model=}, \n{sampling_params=} \n{num_similar_data_pts_txt_files=}')
    # gen: OpenAIGenerator = OpenAIGenerator(model=None, sampling_params=sampling_params)
    dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16'
    llm: LLM = LLM(model=model, dtype=dtype)
    gen: VllmGenerator = VllmGenerator(llm, sampling_params)
    print(f'{sampling_params} \n {sampling_params=}')

    # -- Generate synthetic data &amp; save it as a jsonlines file
    path_2_src_dataset: str = '~/data/amps/mathematica/'
    output_path: str = '~/gold-ai-olympiad/data/amps/mathematica/train.jsonl'
    mathematica, num_data_gens_per_txt_files = False, 2
    lst: list[dict] = gen_data_set_from_ans_str_2_jsonl(gen, path_2_src_dataset, output_path, prompt_template, num_data_gens_per_txt_files=num_data_gens_per_txt_files)
    print(f'Number of jsonlines: {len(lst)=}, written to {output_path=}, from {path_2_src_dataset=}')

if __name__ == '__main__':
    import time
    start = time.time()
    # _test_batch_data()
    # main_math_jsonlines_file()
    main_gen_synth_data()
    print(f&quot;Done!\a Time: {time.time()-start:.2f} sec, {(time.time()-start)/60:.2f} min, {(time.time()-start)/3600:.2f} hr\a&quot;)

</code></pre>
<p>ref: <a href=""https://github.com/vllm-project/vllm/issues/4849"" rel=""nofollow noreferrer"">https://github.com/vllm-project/vllm/issues/4849</a></p>
","machine-learning, deep-learning, nlp, huggingface-transformers, vllm",
How can i make a transformer output a translation relative to a specific context,"<p>I am working a machine-translation-like project, where i have a transformer with the encoder-decoder structure, which is supposed to generate SQL queries from natural language commands, example:</p>
<p><strong>Input:</strong> Count the number of aircrafts produced by the company XYZ</p>
<p><strong>Output:</strong> SELECT COUNT(*) FROM aircrafts WHERE manufacturer='XYZ';</p>
<p>Now i have kind of achieved my goal and my model can generate decent looking queries in most of cases, but there's still one problem to solve, which is that the queries it could generate are all SYNTACTICALLY correct but it doesn't nail the attributes/tables names, for example:</p>
<p><strong>Input:</strong> Count the number of aircrafts produced by the company XYZ</p>
<p><strong>Output:</strong> SELECT COUNT(*) FROM aircrafts WHERE produced='XYZ';</p>
<p>or:</p>
<p><strong>Input:</strong> Show all the employees older than 40</p>
<p><strong>Output:</strong> SELECT * FROM employee WHERE country &gt; 40;</p>
<p>So like i said, syntactically-speaking the query has no errors but it sure cannot be executed correctly on the database itself.</p>
<p>My model is trained on a json dataset that contains a list of samples who follows the following form:</p>
<pre><code>[
  [
    &quot;Count the number of aircraft produced by company XYZ&quot;,
    &quot;SELECT COUNT(*) FROM aircraft WHERE manufacturer = 'XYZ';&quot;
  ],
  [
    &quot;How many marine species are found in the Atlantic Ocean?&quot;,
    &quot;SELECT COUNT(*) FROM marine_species WHERE location = 'Atlantic Ocean';&quot;
  ]
]
</code></pre>
<p>Although i found some datasets that contains the schema of the database as a set of CREATE queries like this one:</p>
<pre><code>{&quot;instruction&quot;: &quot;CREATE TABLE table_72445 (
    \&quot;County\&quot; text,
    \&quot;Population\&quot; real,
    \&quot;Per capita income\&quot; text,
    \&quot;Median household income\&quot; text,
    \&quot;Median family income\&quot; text
)
-- Name the median family income for riverside&quot;,
&quot;output&quot;: &quot;SELECT \&quot;Median family income\&quot; FROM table_72445 WHERE \&quot;County\&quot; = 'Riverside'&quot;}
</code></pre>
<p>So is there anyway i can leverage this kind of dataset so i can give my transformer the database context it should work on, or is there any other way to achieve this task goal?</p>
<p>NOTE: i cannot make the previous corpus as a whole my input, because it will result in a huge performance-overhead. Imagine defining the very SAME set that contains tens of tables every single time that i want to execute, so this is no choice to take.</p>
","machine-learning, nlp, transformer-model, machine-translation",
Hugging Face - Trainer object returning constant values for metrics every epoch,"<p>This is my first time playing around around with the Hugging Face library: my goal is a simple binary text classification task. Since my text data is in several different languages my aim is to fine-tune the XLM-RoBERTa model. My code is as follows:</p>
<pre><code>import transformers
from transformers import (
    AutoModel,
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from datasets import Dataset, DatasetDict
import torch

# setting seed
SEED = 42
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
transformers.set_seed(SEED)

# tokenize
model_ckpt = 'xlm-roberta-base'
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

def tokenize(batch):
 return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)

dataset_encoded = joint_dataset.map(tokenize, batched=True, batch_size=None)

# FYI dataset_encoded -&gt;
# DatasetDict({
#     train: Dataset({
#         features: ['text', 'label', 'input_ids', 'attention_mask'],
#         num_rows: 3395
#     })
#     validation: Dataset({
#         features: ['text', 'label', 'input_ids', 'attention_mask'],
#         num_rows: 425
#     })
#     test: Dataset({
#         features: ['text', 'label', 'input_ids', 'attention_mask'],
#         num_rows: 425
#     })
# })

# train model
num_labels = 2
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    recall = recall_score(labels, preds, average='macro')
    precision = precision_score(labels, preds, average='macro')
    f1 = f1_score(labels, preds, average='macro')
    return {'accuracy': acc, 'macro_f1': f1, 'macro_recall': recall, 'macro_precision': precision}

batch_size = 16
num_train_epochs = 3
logging_steps = len(dataset_encoded['train']) // batch_size
output_dir = './results'
logging_dir = './logs'
os.makedirs(output_dir, exist_ok=True)
os.makedirs(logging_dir, exist_ok=True)
training_args = TrainingArguments(output_dir=output_dir,
                                  num_train_epochs=num_train_epochs,
                                  learning_rate=1e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy=&quot;epoch&quot;,
                                  disable_tqdm=False,
                                  logging_dir=logging_dir,
                                  logging_steps=logging_steps,
                                  push_to_hub=False,
                                  log_level=&quot;error&quot;)

trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=dataset_encoded['train'],
                  eval_dataset=dataset_encoded['validation'],
                  tokenizer=tokenizer)

trainer.train()
</code></pre>
<p>The code runs smoothly however the 'computed' metrics remain the same after the first epoch. At first I thought the issue might be related to the learning rate but after changing it I keep getting the same results regardless of the batch size and epoch number.</p>
<p><a href=""https://i.sstatic.net/yrDjafY0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yrDjafY0.png"" alt=""Progress Callback"" /></a></p>
<p>It is also worthy of note that I also get the same metric values when making predictions on the test set:</p>
<pre><code>preds_output = trainer.predict(dataset_encoded['test'])
print(preds_output.metrics)
</code></pre>
<p><a href=""https://i.sstatic.net/JpO9Se92.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JpO9Se92.png"" alt=""Test set Predictions"" /></a></p>
","nlp, huggingface-transformers, huggingface-trainer",
How to add label in config for ViLT model fine tunning,"<p>I am trying to fine-tune ViLT model for visual question and answering using this <a href=""https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/Fine_tuning_ViLT_for_VQA.ipynb#scrollTo=g1eAPG32CduY"" rel=""nofollow noreferrer"">notebook</a>. In my dataset, I have introduced some new labels and was wondering how can I add new label.</p>
<p>Referring code</p>
<pre><code>for answer in answer_count:
    if answer not in list(config.label2id.keys()):
        continue
    labels.append(config.label2id[answer])
    score = get_score(answer_count[answer])
    scores.append(score)
</code></pre>
<p>In this code if answer is not found it just skip the or otherwise I face &quot;KeyError&quot;. What is the right way to introduce label in the model config. Please advise.</p>
","nlp, huggingface-transformers, fine-tuning, vqa",
"Unexpected Attention dimension [nbr_layers, seq_length, hidden_layer_dim]","<p>I'm working on extracting Attention from a modified Bert model that originally did NOT output any attention. When I extracted it by getting it from the BertEncoder (and through all the intermediate classes: ModelLayer, SelfUnpaddedAttention...), <strong>it seems to have  [nbr_layers, seq_length, hidden_layer_dim] as shape.</strong></p>
<p><strong>But if I understand well, I should somewhere have [seq_length, seq_length] matrix through which I can visualize the attention map.</strong></p>
<p>Here is the repository where I try to extract the attention : <a href=""https://huggingface.co/jaandoui/DNABERT2-AttentionExtracted/blob/main/bert_layers.py"" rel=""nofollow noreferrer"">HuggingFace Model</a>
And you can find the code where I extract the attention below.</p>
<p><strong>I'm not sure if I have done a mistake if how to extract the attention, or should I just change something to get that expected shape ?</strong></p>
<pre><code>class BertEncoder(nn.Module):
    &quot;&quot;&quot;A stack of BERT layers providing the backbone of Mosaic BERT.
    This module is modeled after the Hugging Face BERT's :class:`~transformers.model.bert.modeling_bert.BertEncoder`,
    but with substantial modifications to implement unpadding and ALiBi.
    Compared to the analogous Hugging Face BERT module, this module handles unpadding to reduce unnecessary computation
    at padded tokens, and pre-computes attention biases to implement ALiBi.
    &quot;&quot;&quot;

...
...
# OTHER CODE HERE (SEE SOURCE LINK)
...
...
        # PART WHERE I EXTRACT ATTENTION
        all_encoder_layers = []
        all_attention_weights = []  # List to store attention weights
    
        if subset_mask is None:
            for layer_module in self.layer:
                # Since we get now attention too, we need to unpack 2 elements instead of 1.
                hidden_states, attention_weights = layer_module(hidden_states,
                                                                cu_seqlens,
                                                                seqlen,
                                                                None,
                                                                indices,
                                                                attn_mask=attention_mask,
                                                                bias=alibi_attn_mask)
                
                all_attention_weights.append(attention_weights)  # Store attention weights
                if output_all_encoded_layers:
                    all_encoder_layers.append(hidden_states)
            # Pad inputs and mask. It will insert back zero-padded tokens.
            # Assume ntokens is total number of tokens (padded and non-padded)
            # and ntokens_unpad is total number of non-padded tokens.
            # Then padding performs the following de-compression:
            #     hidden_states[ntokens_unpad,hidden] -&gt; hidden_states[ntokens,hidden]
            hidden_states = pad_input(hidden_states, indices, batch, seqlen)
        else:
            for i in range(len(self.layer) - 1):
                layer_module = self.layer[i]
                # Since we get now attention too, we need to unpack 2 elements instead of 1.
                hidden_states, attention_weights = layer_module(hidden_states,
                                                                cu_seqlens,
                                                                seqlen,
                                                                None,
                                                                indices,
                                                                attn_mask=attention_mask,
                                                                bias=alibi_attn_mask)
                all_attention_weights.append(attention_weights)  # Store attention weights
                if output_all_encoded_layers:
                    all_encoder_layers.append(hidden_states)
            subset_idx = torch.nonzero(subset_mask[attention_mask_bool],
                                       as_tuple=False).flatten()
            # Since we get now attention too, we need to unpack 2 elements instead of 1.
            hidden_states, attention_weights = self.layer[-1](hidden_states,
                                                              cu_seqlens,
                                                              seqlen,
                                                              subset_idx=subset_idx,
                                                              indices=indices,
                                                              attn_mask=attention_mask,
                                                              bias=alibi_attn_mask)
            all_attention_weights.append(attention_weights)  # appending the attention of different layers together.
        if not output_all_encoded_layers:
            all_encoder_layers.append(hidden_states)

        # Since we now return both, we need to handle them wherever BertEncoder forward is called.
        return all_encoder_layers, all_attention_weights  # Return both hidden states and attention weights
        # return all_encoder_layers  # original return.
</code></pre>
","pytorch, nlp, bert-language-model, transformer-model, attention-model",
"TypeError: cross_entropy_loss(): argument &#39;target&#39; (position 2) must be Tensor, not NoneType","<p><strong>Purpose:</strong></p>
<p>I tried to build a text classification pipeline using PyTorch and the Huggingface transformers library. The plan was to tokenize text data and combine it with numerical features for training a neural network model. I used custom datasets and a data collator to handle the data, and a Trainer class to manage the training loop.</p>
<p>I expected the model to train successfully, with the Trainer handling the loss computation and training loop seamlessly. However, I encountered an TypeError: cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not NoneType error during training. The error message indicated that the labels key was missing in the input batch, which was unexpected since the dataset and data collator were supposed to include it.</p>
<p><strong>Objective:</strong></p>
<p>The pipeline aims to:</p>
<ul>
<li>Tokenize text data using a pre-trained tokenizer.</li>
<li>Feed tokenized text and numerical features to a neural network model.</li>
<li>Train and evaluate the model using the Huggingface Trainer class.</li>
</ul>
<p><strong>Implementation:</strong></p>
<p>Dataset:</p>
<ul>
<li>A custom dataset class (MinimalTextDataset) is created, which returns tokenized text data along with numerical features and labels.</li>
</ul>
<p>Data Collator:</p>
<ul>
<li>A custom data collator (CustomDataCollator) is used to stack tensors and collate batches of data. It ensures that batches contain input_ids, attention_mask, features, and labels.</li>
</ul>
<p>Model:</p>
<ul>
<li>The model (MinimalModel) is a simple neural network that combines the tokenized text features and numerical features to make predictions.</li>
</ul>
<p>Trainer:</p>
<ul>
<li>A custom trainer (MinimalTrainer) is used to compute the loss and handle the training loop.</li>
</ul>
<p><strong>Problem:</strong></p>
<p>When running the training pipeline, an TypeError: cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not NoneType. This error occurs in the compute_loss function, where the labels key is expected but not found in the input batch. The error suggests that:</p>
<p>The dataset might not be returning the expected 'labels' key.
The data collator might not be correctly handling the data.
There might be an unintended modification to the data before reaching the compute_loss function.</p>
<p><strong>Code:</strong></p>
<p>Here's the code that reproduces the error:</p>
<pre><code>import torch  
import torch.nn as nn  
from torch.utils.data import Dataset, DataLoader  
from transformers import AutoTokenizer, TrainingArguments  
from transformers import Trainer  
  
# Minimal Dataset  
class MinimalTextDataset(Dataset):  
    def __init__(self, texts, features, labels, tokenizer):  
        self.texts = texts  
        self.features = features  
        self.labels = labels  
        self.tokenizer = tokenizer  
  
    def __getitem__(self, idx):  
        text = self.texts[idx]  
        features = torch.tensor(self.features[idx], dtype=torch.float32)  
        labels = torch.tensor(self.labels[idx], dtype=torch.int64)  
  
        tokens = self.tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')  
        tokens = {key: value.squeeze(0) for key, value in tokens.items()}  
        tokens['features'] = features  
        tokens['labels'] = labels  
        return tokens  
  
    def __len__(self):  
        return len(self.labels)  
  
# Custom Data Collator  
class CustomDataCollator:  
    def __init__(self, tokenizer):  
        self.tokenizer = tokenizer  
  
    def __call__(self, batch):  
        features = torch.stack([item['features'] for item in batch])  
        labels = torch.tensor([item['labels'] for item in batch], dtype=torch.int64)  
        input_ids = torch.stack([item['input_ids'] for item in batch])  
        attention_mask = torch.stack([item['attention_mask'] for item in batch])  
  
        return {  
            'features': features,  
            'labels': labels,  
            'input_ids': input_ids,  
            'attention_mask': attention_mask  
        }  
  
# Minimal Model  
class MinimalModel(nn.Module):  
    def __init__(self):  
        super(MinimalModel, self).__init__()  
        self.fc1 = nn.Linear(130, 64)  
        self.fc2 = nn.Linear(64, 2)  
  
    def forward(self, input_ids, attention_mask, features):  
        x = torch.cat((input_ids.float(), features.float()), dim=1)  
        x = nn.ReLU()(self.fc1(x))  
        x = self.fc2(x)  
        return x  
  
# Minimal Trainer  
class MinimalTrainer(Trainer):  
    def compute_loss(self, model, inputs):  
        print(f&quot;Debug Inputs: {inputs.keys()}&quot;)  
        labels = inputs.get('labels')  
        assert labels is not None, &quot;Labels missing from inputs&quot;  
        input_ids = inputs['input_ids']  
        attention_mask = inputs['attention_mask']  
        features = inputs['features']  
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, features=features)  
        logits = outputs  
        loss_fct = nn.CrossEntropyLoss()  
        loss = loss_fct(logits, labels)  
        return loss  
  
# Prepare data  
train_texts = [&quot;This is a sample text&quot;, &quot;Another sample text&quot;]  
train_labels = [0, 1]  
train_features = [[0.5, 0.3], [0.2, 0.7]]  
  
val_texts = [&quot;Validation text&quot;]  
val_labels = [1]  
val_features = [[0.6, 0.4]]  
  
# Initialize tokenizer, dataset, and dataloader  
tokenizer = AutoTokenizer.from_pretrained(&quot;dbmdz/bert-base-turkish-cased&quot;)  
train_dataset = MinimalTextDataset(train_texts, train_features, train_labels, tokenizer)  
val_dataset = MinimalTextDataset(val_texts, val_features, val_labels, tokenizer)  
  
train_loader = DataLoader(train_dataset, batch_size=2, collate_fn=CustomDataCollator(tokenizer))  
val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=CustomDataCollator(tokenizer))  
  
# Train the model  
model = MinimalModel()  
trainer = MinimalTrainer(  
    model=model,  
    args=TrainingArguments(output_dir='./results', per_device_train_batch_size=2, per_device_eval_batch_size=1),  
    train_dataset=train_loader.dataset,  
    eval_dataset=val_loader.dataset  
)  
trainer.train()  

</code></pre>
<p><strong>Error Message:</strong></p>
<p>When I run the above code, the below error appears, and I could not figure out what is wrong:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[281], line 100
     93 model = MinimalModel()  
     94 trainer = MinimalTrainer(  
     95     model=model,  
     96     args=TrainingArguments(output_dir='./results', per_device_train_batch_size=2, per_device_eval_batch_size=1),  
     97     train_dataset=train_loader.dataset,  
     98     eval_dataset=val_loader.dataset  
     99 )  
--&gt; 100 trainer.train()  

File c:\Users\bsedef\Desktop\simay_hanim_v2\.env\lib\site-packages\transformers\trainer.py:1859, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1857         hf_hub_utils.enable_progress_bars()
   1858 else:
-&gt; 1859     return inner_training_loop(
   1860         args=args,
   1861         resume_from_checkpoint=resume_from_checkpoint,
   1862         trial=trial,
   1863         ignore_keys_for_eval=ignore_keys_for_eval,
   1864     )

File c:\Users\bsedef\Desktop\simay_hanim_v2\.env\lib\site-packages\transformers\trainer.py:2203, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   2200     self.control = self.callback_handler.on_step_begin(args, self.state, self.control)
   2202 with self.accelerator.accumulate(model):
-&gt; 2203     tr_loss_step = self.training_step(model, inputs)
   2205 if (
   2206     args.logging_nan_inf_filter
   2207     and not is_torch_xla_available()
   2208     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   2209 ):
   2210     # if loss is nan or inf simply add the average of previous logged losses
   2211     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File c:\Users\bsedef\Desktop\simay_hanim_v2\.env\lib\site-packages\transformers\trainer.py:3138, in Trainer.training_step(self, model, inputs)
   3135     return loss_mb.reduce_mean().detach().to(self.args.device)
   3137 with self.compute_loss_context_manager():
-&gt; 3138     loss = self.compute_loss(model, inputs)
   3140 if self.args.n_gpu &gt; 1:
   3141     loss = loss.mean()  # mean() to average on multi-gpu parallel training

Cell In[281], line 72
     70 logits = outputs  
     71 loss_fct = nn.CrossEntropyLoss()  
---&gt; 72 loss = loss_fct(logits, labels)  
     73 return loss

File c:\Users\bsedef\Desktop\simay_hanim_v2\.env\lib\site-packages\torch\nn\modules\module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)
   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1531 else:
-&gt; 1532     return self._call_impl(*args, **kwargs)

File c:\Users\bsedef\Desktop\simay_hanim_v2\.env\lib\site-packages\torch\nn\modules\module.py:1541, in Module._call_impl(self, *args, **kwargs)
   1536 # If we don't have any hooks, we want to skip the rest of the logic in
   1537 # this function, and just call forward.
   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1539         or _global_backward_pre_hooks or _global_backward_hooks
   1540         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1541     return forward_call(*args, **kwargs)
   1543 try:
   1544     result = None

File c:\Users\bsedef\Desktop\simay_hanim_v2\.env\lib\site-packages\torch\nn\modules\loss.py:1185, in CrossEntropyLoss.forward(self, input, target)
   1184 def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:
-&gt; 1185     return F.cross_entropy(input, target, weight=self.weight,
   1186                            ignore_index=self.ignore_index, reduction=self.reduction,
   1187                            label_smoothing=self.label_smoothing)

File c:\Users\bsedef\Desktop\simay_hanim_v2\.env\lib\site-packages\torch\nn\functional.py:3086, in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   3084 if size_average is not None or reduce is not None:
   3085     reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 3086 return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)

TypeError: cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not NoneType
</code></pre>
<p>Where should I fix in this code for it to work? I need your help.</p>
<p>Despite my efforts to debug by printing the inputs in the compute_loss function, I couldn't identify the exact root cause of the issue. The labels seemed to be correctly included when testing the dataset and data collator separately, but they were missing during training.</p>
","python, nlp, neural-network, huggingface-transformers, bert-language-model",
ImportError: cannot import name &#39;_LazyModule&#39; from &#39;transformers.utils&#39;,"<p>I am trying to run a text summarization &quot;t5-base&quot; model. The code used to work when I first ran it but after installing/reinstalling some packages, it no longer works. Can anyone please tell me how to resolve this issue? 😭</p>
<p>Here is my code:</p>
<pre><code>import torch
from transformers import AutoModel, AutoTokenizer 

tokenizer = AutoTokenizer.from_pretrained('t5-base')
model = AutoModelWithLMHead.from_pretrained('t5-base', return_dict=True)

inputs = tokenizer.encode(&quot;summarize: &quot; + text,
                          return_tensors='pt',
                          max_length=512,
                          truncation=True)
summary_ids = model.generate(inputs, max_length=150, min_length=80, length_penalty=5., num_beams=2)
text = tokenizer.decode(summary_ids[0])
text = text.replace(&quot;&lt;pad&gt;&quot;,&quot;&quot;).replace(&quot;&lt;/s&gt;&quot;,&quot;&quot;)
text
</code></pre>
<p>Below is the error message I get:</p>
<pre><code>    ---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-46-2c9eeafa599f&gt; in &lt;module&gt;
      1 import torch
----&gt; 2 from transformers import AutoModel, AutoTokenizer

~/opt/anaconda3/lib/python3.7/site-packages/transformers/__init__.py in &lt;module&gt;
     29 # Check the dependencies satisfy the minimal versions required.
     30 from . import dependency_versions_check
---&gt; 31 from .utils import (
     32     _LazyModule,
     33     is_flax_available,

ImportError: cannot import name '_LazyModule' from 'transformers.utils' (/Users/sangjinlee/opt/anaconda3/lib/python3.7/site-packages/transformers/utils/__init__.py)
</code></pre>
","python, nlp, pytorch, huggingface-transformers",
Got memory error while performing cosine similarity on a huge vector,"<p>I was trying to build a content based recommendation system using bag of words model. The tutorial which I am following uses cosine similarity  from sklearn library on a vector of size (4000,5000) where 4000 is the number of rows in dataset and 5000 is the number of features.</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=5000, stop_words='english')
vectors = cv.fit_transform(new_df['tags']).toarray() 
// here new_df is the dataframe and new_df[tags] contain all the tags (eg: location, genre) based on which recommendation will be performed
</code></pre>
<p>But when I try to implement cosine similarity on another datset with 94955 rows, which results in a vector of size (94955, 5000), I get the following error</p>
<pre><code>MemoryError: Unable to allocate 67.2 GiB for an array with shape (94955, 94955) and data type float64
</code></pre>
<p>on line
<code>similarity = cosine_similarity(vectors, dense_output=False)</code></p>
<p>Is there a way to implement batching on cosine similarity so that I can overcome this issue or should I change the algorithm?</p>
","python, machine-learning, scikit-learn, nlp, cosine-similarity",
`AcceleratorState` object has no attribute `distributed_type`,"<p>I am trying to use an Accelerator with a Trainer using the code bellow:</p>
<pre><code>    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)

    config = AutoConfig.from_pretrained(model_args.model_name_or_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        model_args.model_name_or_path, config=config)
    collator = DataCollatorForSeq2Seq(tokenizer, model=model)

    train_set = CorefDataset(tokenizer, data_args, training_args, 'train')
    tb_callback = TensorBoardCallback()

    accelerator = Accelerator()
    trainer = accelerator.prepare(CorefTrainer(
        tokenizer=tokenizer,
        model=model,
        args=training_args,
        train_dataset=train_set,
        #        eval_dataset=dev_set,
        data_collator=collator,
        callbacks=[tb_callback]
    ))
    trainer.train()
</code></pre>
<p>Then following the instructions in this <a href=""https://stackoverflow.com/a/76886877/2729348"">post</a>, I ran this code with the command in the Google Colab:</p>
<pre><code>!accelerate launch --config_file /root/.cache/huggingface/accelerate/default_config.yaml Seq2seqCoref/main.py
</code></pre>
<p>Then I got the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/content/Seq2seqCoref/main.py&quot;, line 41, in &lt;module&gt;
    trainer = accelerator.prepare(CorefTrainer(
  File &quot;/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py&quot;, line 1248, in prepare
    if self.distributed_type == DistributedType.DEEPSPEED:
  File &quot;/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py&quot;, line 529, in distributed_type
    return self.state.distributed_type
  File &quot;/usr/local/lib/python3.10/dist-packages/accelerate/state.py&quot;, line 1076, in __getattr__
    raise AttributeError(
AttributeError: `AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized.
</code></pre>
<p>The versions of transformers and accelerate libraries are  4.40.2 and 0.30.0, respectively.</p>
<p>Before, I tried the code directly in the Google colab instead of using the main.py. However, the same error appears.</p>
","nlp, huggingface-transformers, accelerate, huggingface-trainer",
How to Load an Already Instantiated Hugging Face Model into vLLM for Inference?,"<p>I am working on a project where I need to utilize a model that has already been loaded and instantiated on the GPU using Hugging Face's Transformers library. The goal is to pass this loaded model into the vLLM framework for further processing and inference without reloading it from disk or a model hub.</p>
<p>Here's what I have done so far using Hugging Face to load the model:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2LMHeadModel

# Load GPT-2 model already fine-tuned and available in memory
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.to('cuda')  # Assuming the model is moved to GPU
</code></pre>
<p>I am looking for a way to pass this model instance directly to vLLM's LLM class or a similar interface within vLLM that can accept an already instantiated model. The vLLM documentation primarily discusses initializing its LLM class with a model identifier, which it then loads internally.</p>
<p>Is there a way to integrate an in-memory model from Hugging Face into vLLM without reloading it? Specifically, I want to leverage vLLM's efficient serving capabilities with a model instance that is already in use and configured in my existing pipeline.</p>
<p>Any insights or suggestions on how to achieve this would be greatly appreciated!</p>
<hr />
<p>This doesn't work:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Step 1: Load and Save the Model using Hugging Face
model_name = 'gpt2'
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Save the model and tokenizer
model_save_path = './model/gpt2'
tokenizer.save_pretrained(model_save_path)
model.save_pretrained(model_save_path)

# Step 2: Load the model in vLLM using the saved model path
from vllm import LLM, SamplingParams

# Assuming vLLM can load models from a path
vllm_model = LLM(model=model)

# Define sampling parameters and prompts
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
prompts = [&quot;Hello, my name is&quot;, &quot;The future of AI is&quot;]

# Generate text using the specified model and parameters in vLLM
outputs = vllm_model.generate(prompts, sampling_params)

# Print the generated texts
for output in outputs:
    print(f&quot;Prompt: {output.prompt}, Generated text: {output.outputs[0].text}&quot;)
</code></pre>
<p>Error:</p>
<pre><code>Exception has occurred: OSError
Incorrect path_or_model_id: 'GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)'.

The above exception was the direct cause of the following exception:

  File &quot;/lfs/ampere1/0/brando9/gold-ai-olympiad/py_src/training/maf_self_improv_train.py&quot;, line 35, in &lt;module&gt;
    vllm_model = LLM(model=model)
                 ^^^^^^^^^^^^^^^^
OSError: Incorrect path_or_model_id: 'GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
</code></pre>
<p>ref: <a href=""https://github.com/vllm-project/vllm/issues/4843"" rel=""nofollow noreferrer"">https://github.com/vllm-project/vllm/issues/4843</a></p>
","nlp, huggingface-transformers, huggingface, huggingface-hub, vllm",
Is it valid to separate text generation and logits computation in reinforcement learning?,"<p>I'm working with a reinforcement learning setup using the REINFORCE algorithm for a text summarization task in NLP. Originally, I developed a setup by extending Huggingface's <code>.generate()</code> function, where I collected log probabilities directly during text generation by setting the model to training mode before calling <code>.generate()</code> like below [the code is simplified for the sake of understandability]:</p>
<pre><code>model.train()
# Sampling a text and collecting the logits/logprobs...
# ... via the extended .generate() function:
output = model.generate(
    do_sample=True,
    **inputs
)
# The output has sequence and collected logits
</code></pre>
<p>However, this method was computationally intensive, so I'm considering a new approach that involves two distinct phases: generating text samples (through sampling) without gradient tracking and then computing the logits for these samples with gradient tracking. Here's the new approach I'm considering:</p>
<pre><code>model.eval()
with torch.no_grad():
    output_sequence = model.generate(do_sample=True, **inputs)  
    # Sampling happens here with Huggingface's .generate() function

model.train()
inputs[&quot;decoder_input_ids&quot;] = output_sequence
output = model(**inputs)  # Logits are computed here, and gradients are tracked
</code></pre>
<p>I am doing this because I want to optimize the computational efficiency during the sampling phase while still being able to compute and backpropagate based on the logits of the generated text.</p>
<p>Questions:</p>
<ol>
<li>Is this new approach that I'm considering valid in the context of reinforcement learning? as with logits, the logprobs will be calculated, playing a key role in the REINFORCE algorithm.</li>
<li>What are the implications of not tracking gradients during the text generation phase in terms of the model's learning capability, especially concerning the policy gradient methods like REINFORCE?</li>
<li>Compared to my original method that I had (the method #1), will this new approach lead to inefficiencies or missed learning opportunities in the RL context, and how might it be modified for better integration of sampling and learning?</li>
</ol>
","deep-learning, pytorch, nlp, huggingface-transformers, reinforcement-learning",
past key values from hidden states,"<p>I'm trying to extract past key, value pair using attention_layers and hidden_state for a particular layer</p>
<pre><code>import torch
import torch.nn.functional as F
from transformers import LlamaConfig
from transformers import LlamaModel, LlamaTokenizer, LlamaForCausalLM

tokenizer = LlamaTokenizer.from_pretrained(path_to_llama2)

# Load the configuration and enable required outputs
config = LlamaConfig.from_pretrained(path_to_llama2)
config.output_hidden_states = True
config.output_attentions = True  # To get self_attn_weights and biases if needed
config.use_cache = True  # To get past_key_values

model = LlamaForCausalLM.from_pretrained(path_to_llama2, config=config)

model.eval()

input_text = &quot;Once upon a time&quot;
inputs = tokenizer(input_text, return_tensors='pt')
outputs = model(**inputs)
hidden_states = outputs.hidden_states  # List of hidden states from each layer
state_dict = model.state_dict()

# Function to compute past_key_values for a single layer
def compute_past_key_values_for_layer(layer_idx, hidden_state):
    attention_layers = [layer.self_attn for layer in model.model.layers]
    
    W_q = state_dict[f'model.layers.{layer_idx}.self_attn.q_proj.weight']
    W_k = state_dict[f'model.layers.{layer_idx}.self_attn.k_proj.weight']
    W_v = state_dict[f'model.layers.{layer_idx}.self_attn.v_proj.weight']
    
    queries = torch.matmul(hidden_state, W_q.T)
    keys = torch.matmul(hidden_state, W_k.T)
    values = torch.matmul(hidden_state, W_v.T)

    batch_size, seq_length, hidden_dim = hidden_state.size()
    num_attention_heads = attention_layers[layer_idx].num_heads
    head_dim = hidden_dim // num_attention_heads

    keys = keys.view(batch_size, seq_length, num_attention_heads, head_dim)
    keys = keys.permute(0, 2, 1, 3)
    
    values = values.view(batch_size, seq_length, num_attention_heads, head_dim)
    values = values.permute(0, 2, 1, 3)
    
    return keys, values

past_key_values = []
for i, hidden_state in enumerate(hidden_states[1:]):  # Skip the embedding layer
    keys, values = compute_past_key_values_for_layer(i, hidden_state)
    past_key_values.append((keys, values))

past_key_values = tuple(past_key_values)
</code></pre>
<p>but these past_key_values  don't match with the values I get from outputs.past_key_values for the particular layer.
why's it happening? are there any suggestions?</p>
","python, nlp, huggingface-transformers, large-language-model",
Docker build taking too long and failing with requirements.txt containing TensorFlow and other packages,"<p>I'm trying to build my Docker image, but it's taking forever. Even when I leave it to continue building, it fails after a long time. Here is my Dockerfile:</p>
<pre><code>FROM ubuntu:24.04

# Install Python and pip
RUN apt-get update &amp;&amp; apt-get install -y python3 python3-pip

WORKDIR /ner

COPY requirements.txt .

# Install required packages
RUN pip install -r requirements.txt --break-system-packages

COPY . .
</code></pre>
<p>I think the problem may be in the requirements file. Here is my <strong><code>requirements.txt</code></strong>:</p>
<pre><code>tensorflow==2.16.1
pydantic-settings==2.2.1
transformers==4.40.1
fastapi==0.111.0
torchvision==0.18.0
torch==2.3.0
tqdm==4.66.2
numpy==1.26.4
scikit-learn==1.4.2
pandas==2.2.1
</code></pre>
<p>I tried to build it using this command:</p>
<pre><code>docker build -t train .
</code></pre>
<p>The process is too slow and eventually fails. Could the issue be related to the size of these packages or their dependencies? Are there any optimizations I can apply to speed up the build process or resolve this issue?</p>
","docker, tensorflow, pytorch, nlp, dockerfile",
Removing Narrow &#39;No-Break Space&#39; Unicode Characters (U+00A0) in python nlp,"<p>Non-breaking spaces are printed as whitespace, but handled internally as <code>\xa0</code>. How do I remove all these characters at once?</p>

<p>So far I've replaced it directly:</p>

<pre><code>text = text.replace('\u202f','')  
text = text.replace('\u200d','') 
text = text.replace('\xa0','')
</code></pre>

<p>But each time I scrape the text sentences from external source, These characters are different. How do I remove it all at once?</p>
","python-3.x, string, nlp, python-unicode, unicode-string",
Text classification. TFIDF and Naive Bayes?,"<p>I'm attempting a text classification task, where I have training data of around 500 restaurant reviews that are labelled across 12 categories. I spent longer than I should have implementing TF.IDF and cosine similarity for the classification of test data, only to get some very poor results (0.4 F-measure). With time not on my side now, I need to implement something significantly more effective that doesn't have a steep learning curve. I am considering using the TF.IDF values in conjunction with Naive Bayes. Does this sound sensible? I know if I can get my data in the right format, I can do this with Scikit learn. Is there anything else you recommend I consider?</p>
","python, machine-learning, scikit-learn, nlp","<p>You should try to use fasttext: <a href=""https://pypi.python.org/pypi/fasttext"" rel=""nofollow noreferrer"">https://pypi.python.org/pypi/fasttext</a> . It can be used to classify text like this:</p>

<p>(don't forget to download a pretrained model here <a href=""https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip"" rel=""nofollow noreferrer"">https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip</a> by changing the language if it's not english)</p>

<pre><code>import fasttext

model = fasttext.load_model('wiki.en.bin')  # the name of the pretrained model

classifier = fasttext.supervised('train.txt', 'model', label_prefix='__label__')

result = classifier.test('test.txt')
print ('P@1:', result.precision)
print ('R@1:', result.recall)
print ('Number of examples:', result.nexamples)
</code></pre>

<p>Every line in your training and test sets should be like this:</p>

<blockquote>
  <p>__label__classname Your restaurant review blah blah blah</p>
</blockquote>
"
How can I build an accurate dataset in a short span of time?,"<p>We are developing an iOS app that lets users send customizable digital cards. Users can choose from various card templates, enter their own text, and make edits to the card as they like. We also have a feature where users can provide a short message, like &quot;happy birthday mom,&quot; and receive an expanded version of the text, such as &quot;happy birthday to my special mother! I love you and hope you have an amazing day.&quot;</p>
<p>I'm conducting research to figure out how to accomplish this and planning to create a model using Natural Language Processing (NLP) and CoreML. However, I've encountered an issue in finding a suitable dataset for this particular task. As a result, I'm interested in building an accurate dataset specifically tailored to this purpose. However, I'm unsure where I can obtain the necessary data or if there are alternative data sources available for quick use.</p>
<p>If you have any insights or alternative methods to achieve this feature, please share them.</p>
","machine-learning, nlp, dataset, coreml",
How to use NLTK to generate sentences from an induced grammar?,"<p>I have a (large) list of parsed sentences (which were parsed using the Stanford parser), for example, the sentence ""Now you can be entertained"" has the following tree:</p>

<pre><code>(ROOT
  (S
    (ADVP (RB Now))
    (, ,)
    (NP (PRP you))
    (VP (MD can)
      (VP (VB be)
        (VP (VBN entertained))))
    (. .)))
</code></pre>

<p>I am using the set of sentence trees to induce a grammar using nltk:</p>

<pre><code>import nltk

# ... for each sentence tree t, add its production to allProductions
allProductions += t.productions()

# Induce the grammar
S = nltk.Nonterminal('S')
grammar = nltk.induce_pcfg(S, allProductions)
</code></pre>

<p>Now I would like to use <code>grammar</code> to generate new, random sentences. My hope is that since the grammar was learned from a specific set of input examples, then the generated sentences will be semantically similar. Can I do this in nltk?</p>

<p>If I can't use nltk to do this, do any other tools exist that can take the (possibly reformatted) <code>grammar</code> and generate sentences?</p>
","python, nlp, nltk",
Key matrix redundant in Transformer language models?,"<p>Simple implementations of Transformer language models such as <a href=""https://www.youtube.com/watch?v=kCc8FmEb1nY"" rel=""nofollow noreferrer"">this one</a> define 3 matrices K,Q,V to compute keys, queries and values. However matrices K and Q are never used separately: all Transformer computations form their product <code>Q^t K</code>. So I wonder why not learn this product matrix directly instead of splitting it into 2 matrices K and Q.</p>
<p>Part of the answer may come from the size of K and Q, which is <code>d -&gt; n</code>, where d is the dimension of the token embeddings and n is the dimension of keys and queries. The size of <code>Q^t K</code> is <code>d -&gt; d</code>. So learning K and Q separately means optimizing <code>2*n*d</code> parameters, whereas learning the product <code>Q^t K</code> is <code>d*d</code> parameters. The only useful splitting I see is when <code>n &lt;= d/2</code>, because that's less parameters to optimize. But at the limit case <code>n = d/2</code>, the rank of the product matrix <code>Q^t K</code> is <code>d/2</code>, which is very degenerate. With the same number of parameters <code>d^2</code>, we could learn an unconstrained square matrix. That might learn more flexible and subtle patterns in the training data.</p>
<p>In the <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention is all you need</a> paper, base model page 9, we see d = 512 and n = 64, so the product matrix <code>Q^t K</code> does have degenerate rank. Is reducing the number of parameters the true and unique intent here? Is there a theoretical justification that these degenerate ranks help natural language processing?</p>
","nlp, transformer-model","<p>The <code>2 * n * d</code> vs. <code>d * d</code> structure is very similar to how a LORA works, or indeed a WALS model. So as long as <code>2 * n * d &lt; d * d</code>, this might well be a good way of saving parameters.</p>
"
How do I specify that different Huggingface Trainers in the same process use different GPUs?,"<p>I have instantiated multiple Huggingface Trainers in the same process and each of them train different models. Now I need to count the GPU cost of each Trainer, so I need to make different Trainers exclusive to one GPU for training to get accurate statistics.</p>
<p>I have tried to set <code>CUDA_VISIBLE_DEVICES</code> as</p>
<pre><code>import os
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;1&quot;
</code></pre>
<p>but this would cause all Trainers to use GPU1.</p>
","nlp, huggingface-trainer",
process_tweet and build_freqs,"<p>I am unable to import process_tweet and build_freqs into JupyterLab notebook for use in natural language processing. I have checked previous recommendations on stackoverflow for the resolution of this issue. None seems to work. Kindly assist.</p>
","nlp, regression",
in R converting a text file into a data frame,"<p>in R have a .txt file that i would like to extract data from as a character string. my .txt file is formatted like the following with a list separated by numbers. 1. [text1] 2. [text2] 3. [text3] and so on to 400. i want each observation to be extracted so that the first observation is [text1] and second is [text2] and so on. how would i do this? see below for actual example.</p>
<pre><code>1. 
Readability and Quality of Online Information on Sickle Cell Retinopathy for 
Patients [embase.com]
Gbedemah Z.E.E., Fuseini M.-S.N., Fordjuor S.K.E.J., Baisie-Nkrumah E.J., Beecham R.-
M.E.M., Amissah-Arthur K.N. 
[In Process] Am. J. Ophthalmol. 2024 259: (45-52) 
Embase, MEDLINE
Go to publisher for the full text [dx.doi.org]
Embase Open URL: redirect to full text [embase.com]
Abstract
PURPOSE: This study aims to evaluate the readability and quality of Internet-based health 
information on sickle cell retinopathy. DESIGN: Retrospective cross-sectional website analysis. 
METHODS: To simulate a patient's online search, the terms “sickle cell retinopathy” and “sickle 
cell disease in the eye” were entered into the top 3 search engines (Google, Bing and Yahoo). The 
first 20 results of each search were retrieved and screened for analysis. The DISCERN 
questionnaire, the Journal of the American Medical Association (JAMA) standards, and the Health 
on the Net (HON) criteria were used to evaluate the quality of the information. The Flesch–Kincaid 
Grade Level (FKGL), the Flesch Reading Ease (FRES), and the Automated Readability Index 
(ARI) were used to assess the readability of each website. RESULTS: Of 16 online sources, 12 
(75%) scored moderately on the DISCERN tool. The mean DISCERN score was 40.91 (SD, 
10.39; maximum possible, 80). None of the sites met all of the JAMA benchmarks, and only 3 
(18.75%) of the websites had HONcode certification. All of the websites had scores above the 
target American Medical Association grade level of 6 on both the FKGL and ARI. The mean FRES 
was 57.76 (±4.61), below the recommended FRES of 80 to 90. CONCLUSION: There is limited 
online information available on sickle cell retinopathy. Most included websites were fairly difficult to 
read and of substandard quality. The quality and readability of Internet-based, patient-focused 
information on sickle cell retinopathy needs to be improved.


 
2. 
Multicomponent Strategy Improves Human Papillomavirus Vaccination Rates 
Among Adolescents with Sickle Cell Disease [embase.com]
Aurora T., Cole A., Rai P., Lavoie P., McIvor C., Klesges L.M., Kang G., Liyanage J.S.S., 
Brandt H.M., Hankins J.S. 
J. Pediatr. 2024 265: 
Embase, MEDLINE, NURSING
Go to publisher for the full text [dx.doi.org]
Embase Open URL: redirect to full text [embase.com]
Abstract
Objective: To evaluate the effectiveness of a vaccine strategy bundle to increase human 
papillomavirus (HPV) vaccine initiation and completion in a specialty clinic setting. Study design: 
Our Hematology clinic utilized an implementation framework from October 1, 2018, to December 
31, 2019, involving nurses, nursing coordinators, and clinicians in administering the HPV 
vaccination series to our adolescent sickle cell sample of nearly 500 patients. The bundle included 
education for staff on the need for HPV vaccine administration, provider incentives, vaccines 
offered to patients in SCD clinics, and verification of patients' charts of vaccine completion. 
Results: Following the implementation of the bundle, the cumulative incidence of HPV vaccination 
initiation and completion improved from 28% to 46% and 7% to 49%, respectively. Both rates 
remained higher postimplementation as well. HPV vaccination series completion was associated 
with a decreased distance to the health care facility, lower state deprivation rank, and increased 
hospitalizations. Conclusion: Our clinic's implementation strategy successfully improved vaccine 
completion rates among adolescents with sickle cell disease (SCD) while continuing to educate 
staff, patients, and families on the importance of cancer prevention among people living with SCD.
</code></pre>
<pre><code># Sample character string
cleaned_text &lt;- &quot;1. Readability and Quality of Online Information on Sickle Cell Retinopathy for Patients [embase.com] Gbedemah 2. Another text here 3. Yet another text 4. More text 5. Even more text&quot;

# Split the text based on the numbering pattern
text_split &lt;- strsplit(cleaned_text, &quot;\\d+\\.\\s+&quot;)[[1]]

# Remove the first empty element
text_split &lt;- text_split[-1]

# Convert the result into a data frame
df &lt;- data.frame(Text = text_split)
</code></pre>
","r, pdf, text, nlp",
How can I process raw text format job posts from LinkedIn or similar sites into a key-value format?,"<p>Ive collected some job posts from linked for research purpose. I would like to get specific datas from these job posts and save inside my sql database. So how can I process the job posts txt files and get specific fields such as title, description, skills, requirements list, notes (if there is any), role, location, benefits etc. And also every job posts have their own formats. I guess I would need to use some NLP techniques like NER but im clueless on how to actually approach this. Im not an ML guy so some suggestions, references would be great.</p>
","machine-learning, web-scraping, nlp, text-processing, named-entity-recognition",
How to decide which chunking technique to use for implementing Retrieval-Augmented Generation(RAG),"<p>I want to automatically generate testcases using generative AI. For this purpose I will be using open source LLM (Llama 3, will try others as well). Since the LLM is trained only on publically available data, it needs more information regarding the application being developed (for which I wish to generate testcases) and the requirements which contain detailed information regarding the expected behaviour.</p>
<p>This additional information can be provided through RAG. The Vector Database being used is ChromaDB.</p>
<p>As of now, I want this additional information to be provided as a PDF file.
According to my researched, there are many ways of dividing this PDF file into smaller chunks:</p>
<ul>
<li>Recursive Character Splitter</li>
<li>Sentence splitter</li>
<li>Semantic splitting</li>
<li>LLM based chunking</li>
<li>Document specific splitting</li>
</ul>
<p>Please do let me know if I missed some other useful method.</p>
<p>So there are 2 questions here:</p>
<ol>
<li>How to decide which chunking method to choose?</li>
<li>How can I evaluate the performance of the chosen chunking technique?</li>
</ol>
","nlp, large-language-model, retrieval-augmented-generation, text-chunking",
"Some problem in tensorflow_hub, tensorflow_text, and transformers when working on BERT and RoBerta","<p>I am working on RoBerta, but I am unable to import TFAutoModel from transformers. Initially it was possible but from a few weeks there is some problem in tensorflow_text and transformers. Please help.</p>
<p>from transformers import RobertaTokenizer, TFRobertaModel
tokenizer = RobertaTokenizer.from_pretrained(&quot;roberta-base&quot;)
model = TFRobertaModel.from_pretrained(&quot;roberta-base&quot;)</p>
<p>RuntimeError: Failed to import transformers.models.roberta.modeling_tf_roberta because of the following error (look up to see its traceback):
module 'tensorflow._api.v2.compat.v2.<strong>internal</strong>' has no attribute 'register_load_context_function'</p>
","nlp, bert-language-model, roberta-language-model, roberta",
Failed to import transformers.integrations.peft,"<p><a href=""https://i.sstatic.net/2CoI6pM6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2CoI6pM6.png"" alt=""enter image description here"" /></a></p>
<pre><code>RuntimeError: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):
Failed to import transformers.integrations.peft because of the following error (look up to see its traceback):
/usr/local/lib/python3.10/dist-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops5zeros4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEE
</code></pre>
<p>This is the Error , i am using jupyter notebook on portrainer.
Any Help would be appreciated. I am completely newbie working in field of LLM and RAG</p>
","nlp, huggingface-transformers, large-language-model",
Fine-tuned openAI API is giving NotFoundError: Error code: 404,"<p>this is my first openAI fine-tuning job so I don't know much. I am trying to fine-tune OpenAI API to generate product ID based on the description. The code runs successfully and also generates model ID, I also got billed for it. However, when I want to use to the model to generate output it gives <strong>NotFoundError: Error code: 404 - {'error': {'message': 'The model `ftjob-</strong>****<strong>' does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}</strong></p>
<p>My complete code is given below.</p>
<pre><code>import pandas as pd
df = pd.read_csv(&quot;/content/machine screws.csv&quot;)
df.head()

def convert_to_gpt35_format(dataset):
    fine_tuning_data = []
    for _, row in dataset.iterrows():
        fine_tuning_data.append({
            &quot;messages&quot;: [
                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: row['Input']},
                {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: row['Output']} #json_response}
            ]
        })
    return fine_tuning_data

converted_data = convert_to_gpt35_format(df)

from sklearn.model_selection import train_test_split


train_data, val_data = train_test_split(
    converted_data,
    test_size=0.2,
    random_state=42  # for reproducibility
)


import json
def write_to_jsonl(data, file_path):
    with open(file_path, 'w') as file:
        for entry in data:
            json.dump(entry, file)
            file.write('\n')

training_file_name = &quot;train_a.jsonl&quot;
validation_file_name = &quot;val_a.jsonl&quot;

write_to_jsonl(train_data, training_file_name)
write_to_jsonl(val_data, validation_file_name)


from openai import OpenAI
client = OpenAI(api_key=&quot;OPENAI_KEY&quot;)


training_file = client.files.create(
    file=open(training_file_name, &quot;rb&quot;), purpose=&quot;fine-tune&quot;
)
validation_file = client.files.create(
    file=open(validation_file_name, &quot;rb&quot;), purpose=&quot;fine-tune&quot;
)


response = client.fine_tuning.jobs.create(
    training_file=training_file.id,
    validation_file=validation_file.id,
    model=&quot;gpt-3.5-turbo&quot;,
)

print(&quot;Fine-Tuning Job ID:&quot;, response.id)
print(&quot;Status:&quot;, response.status)
print(&quot;Created at:&quot;, response.created_at)

job_id = response.id


prompt = &quot; 1-8 X 15 FLAT SLTD MACH SCREW 3 THRD HDG &quot;


response = client.chat.completions.create(
        model=job_id, messages=prompt, temperature=0, max_tokens=50
    )

prediction_text = response.choices[0].text.strip()
print(f&quot;Predicted code: {prediction_text}&quot;)
</code></pre>
","nlp, openai-api, fine-tuning",
Applying Representation Model on Non-Leaf Nodes in Hierarchical Topics with BERTopic,"<p>I am currently using BERTopic for topic modeling on a set of documents and have integrated an OpenAI model as the representation layer, as outlined in the <a href=""https://maartengr.github.io/BERTopic/getting_started/representation/representation.html"" rel=""nofollow noreferrer"">BERTopic documentation</a>. I am also interested in hierarchical topic modeling, which is detailed <a href=""https://maartengr.github.io/BERTopic/getting_started/hierarchicaltopics/hierarchicaltopics.html"" rel=""nofollow noreferrer"">here</a>.</p>
<p>In the process of hierarchical modeling, BERTopic provides descriptive labels for the non-leaf topics using the representation model. My question is whether there is a possibility within BERTopic to apply the representation model explicitly to these non-leaf nodes in the hierarchical topics? Specifically, I am looking to generate or refine labels for these higher-level topics based on their aggregated content.</p>
<p>Is there a built-in method to achieve this, or would it require a custom implementation? Any pointers or examples would be greatly appreciated.</p>
<p>I have applied hierarchical topic modeling using the hierarchical_topics method of the fitted topic model. I examined the options available in the hierarchical_topics method to see if there was a way to specify a representation model for non-leaf nodes. However, I could not find any options that would allow me to explicitly apply or adjust the representation model for these nodes within the hierarchical structure.</p>
","python, nlp, openai-api, hierarchical-clustering, topic-modeling",
word vectors using spacy module of python showing error,"<p>I'm trying to get the word vectors using the spacy module of python and using en_core_web_lg for creating the word vectors.</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf=TfidfVectorizer(lowercase=False)

df1['question1']=df1['question1'].apply(lambda x : str(x))
df1['question2']=df1['question2'].apply(lambda x : str(x))

tot_ques=list(df1['question1']) + list(df1['question2'])

tfidf.fit(tot_ques)

idfscore=dict(zip(tfidf.get_feature_names_out(),tfidf.idf_))
print(idfscore)

from tqdm import tqdm
import spacy
nlp=spacy.load('en_core_web_lg')

vec1 = []
# Iterate through each question1
for qu1 in tqdm(list(df1['question1'])):
doc1 = nlp(qu1)

# Initialize sum of vectors and total IDF score
sum_vec = np.zeros(len(doc1[0].vector))  # Vector dimension from the first word
total_idf = 0.0  # Initialize the total IDF score

# Iterate through each word in the sentence
for word in doc1:
    vec = word.vector
    
    # Calculate IDF score for the word
    try:
        idf = idfscore(str(word))
    except:
        idf = 0.0
    
    # Accumulate weighted sum of word vectors
    sum_vec += vec * idf
    
    # Accumulate the total IDF score
    total_idf += idf

# Calculate the mean vector if total IDF score is not zero
if total_idf != 0:
    mean_vec = sum_vec / total_idf
else:
    mean_vec = sum_vec  # Fall back to sum_vec if total IDF is zero

# Append the mean vector to vec1
vec1.append(mean_vec)

# Assign the calculated vectors to the new column 'q1' in the dataframe
df1['q1'] = vec1`
</code></pre>
<p>But when I am looking at the values of column q1 it is showing 0 for every row.</p>
","python, machine-learning, nlp, spacy, tfidfvectorizer",
How do I use a model that can be loaded with AutoModelForSequenceClassification with AutoModelForTokenClassification for Ner fine tuning?,"<p>While I can fine tune the BERT model with the custom data in the specific domain by loading it with AutoModelForTokenClassification, I can load another BERT model trained in that domain only with AutoModelForSequenceClassification and as a result I get a tensor shape error. If this is the case, how can I perform fine tuning using AutoModelForSequenceClassification?</p>
<p>The size of tensor a (716) must match the size of tensor b (512) at non-singleton dimension 1</p>
","nlp, bert-language-model, named-entity-recognition, fine-tuning",
Fine-tuning CodeBert for classification with more than 512 tokens,"<p>I have a dataset of SmartContracts source code written in Solidity labeled with one ore more vulnerabilities they contains.</p>
<p>I used the CodeBert model to classify them in a multilabel classification problem obtaining a 83% accuracy on test set.
The problem is that the CodeBert takes in input a maximum of 512 tokens so just a small initial portions of contracts, how can I input longer sequences to improve the results of my model?
I think that splitting a contract in substrings of lenght 512 is not the best solution since I'm training the model on wrong label for that subtring.</p>
","nlp, huggingface-transformers",
Is BertForSequenceClassification using the CLS vector?,"<p>In the hugging face <a href=""https://github.com/huggingface/transformers/blob/536ea2aca234fb48c5c69769431d643b0d93b233/src/transformers/models/bert/modeling_bert.py#L1552"" rel=""nofollow noreferrer"">source code</a>, <code>pooled_output = outputs[1]</code> is used.</p>
<pre class=""lang-py prettyprint-override""><code>        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        pooled_output = outputs[1]
</code></pre>
<p>Shouldn't it be <code>pooled_output = outputs[0]</code>? (This <a href=""https://stackoverflow.com/a/60883003/20898396"">answer</a> mentioning BertPooler seems to be outdated)</p>
<p>Based on <a href=""https://stackoverflow.com/a/62981360/20898396"">this</a> answer, it seems that the CLS token learns a sentence level representation. I am confused as to why/how masked language modelling would lead to the start token learning a sentence level representation. (I am thinking that <code>BertForSequenceClassification</code> freezes the Bert model and only trains the classification head, but maybe that's not the case)</p>
<p>Would a sentence embedding be equivalent or even better than the [CLS] token embedding?</p>
","nlp, huggingface-transformers, bert-language-model","<blockquote>
<p>Would a sentence embedding be equivalent or even better than the [CLS] token embedding?</p>
</blockquote>
<p>A sentence embedding is everything that represents the input sequence as a numerical vector. The question is whether this embedding is semantical meaningful (e.g. can we use it with similarity metrics). This is for example not the case for the pretrained Bert weights released by google (refer to this <a href=""https://stackoverflow.com/a/64237402/6664872"">answer</a> for more information).</p>
<p>Is the CLS token a sentence embedding? Yes.
Is some kind of pooling a sentence embedding? Yes.
Are they semantically meaningful with the Bert weights release by google? No.</p>
<blockquote>
<p>Shouldn't it be pooled_output = outputs[0]?</p>
</blockquote>
<p>No, because when you check the <a href=""https://github.com/huggingface/transformers/blob/536ea2aca234fb48c5c69769431d643b0d93b233/src/transformers/models/bert/modeling_bert.py#L1005"" rel=""nofollow noreferrer"">code</a>, you will see that the first element of the tuple is the last_hidden_state</p>
<pre class=""lang-py prettyprint-override""><code>sequence_output = encoder_outputs[0]
pooled_output = self.pooler(sequence_output) if self.pooler is not None else None

if not return_dict:
    return (sequence_output, pooled_output) + encoder_outputs[1:]
</code></pre>
<blockquote>
<p>I am confused as to why/how masked language modeling would lead to the start token learning a sentence level representation.</p>
</blockquote>
<p>Because it is included in every training sequence and the <code>[CLS]</code> &quot;absorbs&quot; the other tokens. You can also see this in the attention mechanism (compare Revealing the Dark Secrets of BERT <a href=""https://arxiv.org/abs/1908.08593"" rel=""nofollow noreferrer"">paper</a>). As mentioned above, the questions is if they are semantically meaningful without any further finetuning. No (compare this StackOverflow <a href=""https://stackoverflow.com/a/64237402/6664872"">answer</a>).</p>
"
OSError: [Errno -9996] Invalid input device (no default output device) using Pyaudio in google collab,"<p>I am working on a Realtime speech emotion recognition using LSTM. I have used pydub library to capture the live  input speech . I am working with google collab . I am not sure that whether i am using the right runtime for this process or not .</p>
<p>Currently I am using &quot;T4 GPU&quot; hardware accelerator. I am encountering a issue with the pyaudio whenever  I am trying to open the input stream to capture the audio.</p>
<pre><code>import pyaudio
import wave
from array import array
import struct
import time

# Initialize variables
RATE = 24414
CHUNK = 512
RECORD_SECONDS = 7.1

FORMAT = pyaudio.paInt32
CHANNELS = 1
WAVE_OUTPUT_FILE = &quot;/content/drive/My Drive/Colab Notebooks/output.wav&quot;

# Open an input channel
p = pyaudio.PyAudio()
stream = p.open(format=FORMAT,
                channels=CHANNELS,
                rate=RATE,
                input=True,
                frames_per_buffer=CHUNK)


# Initialize a non-silent signals array to state &quot;True&quot; in the first 'while' iteration.
data = array('h', np.random.randint(size = 512, low = 0, high = 500))

# SESSION START
print(&quot;** session started&quot;)
total_predictions = [] # A list for all predictions in the session.
tic = time.perf_counter()

while is_silent(data) == False:
    print(&quot;* recording...&quot;)
    frames = [] 
    data = np.nan # Reset 'data' variable.

    timesteps = int(RATE / CHUNK * RECORD_SECONDS) # =&gt; 339

    # Insert frames to 'output.wav'.
    for i in range(0, timesteps):
        data = array('l', stream.read(CHUNK)) 
        frames.append(data)

        wf = wave.open(WAVE_OUTPUT_FILE, 'wb')
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(p.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))

    print(&quot;* done recording&quot;)

    x = preprocess(WAVE_OUTPUT_FILE) # 'output.wav' file preprocessing.
    # Model's prediction =&gt; an 8 emotion probabilities array.
    predictions = model.predict(x, use_multiprocessing=True)
    pred_list = list(predictions)
    pred_np = np.squeeze(np.array(pred_list).tolist(), axis=0) # Get rid of 'array' &amp; 'dtype' statments.
    total_predictions.append(pred_np)
    
    # Present emotion distribution for a sequence (7.1 secs).
    fig = plt.figure(figsize = (10, 2))
    plt.bar(emo_list, pred_np, color = 'darkturquoise')
    plt.ylabel(&quot;Probabilty (%)&quot;)
    plt.show()
    
    max_emo = np.argmax(predictions)
    print('max emotion:', emotions.get(max_emo,-1))
    
    print(100*'-')
    
    # Define the last 2 seconds sequence.
    last_frames = np.array(struct.unpack(str(96 * CHUNK) + 'B' , np.stack(( frames[-1], frames[-2], frames[-3], frames[-4],
                                                                            frames[-5], frames[-6], frames[-7], frames[-8],
                                                                            frames[-9], frames[-10], frames[-11], frames[-12],
                                                                            frames[-13], frames[-14], frames[-15], frames[-16],
                                                                            frames[-17], frames[-18], frames[-19], frames[-20],
                                                                            frames[-21], frames[-22], frames[-23], frames[-24]),
                                                                            axis =0)) , dtype = 'b')
    if is_silent(last_frames): # If the last 2 seconds are silent, end the session.
        break

# SESSION END        
toc = time.perf_counter()
stream.stop_stream()
stream.close()
p.terminate()
wf.close()
print('** session ended')

# Present emotion distribution for the whole session.
total_predictions_np =  np.mean(np.array(total_predictions).tolist(), axis=0)
fig = plt.figure(figsize = (10, 5))
plt.bar(emo_list, total_predictions_np, color = 'indigo')
plt.ylabel(&quot;Mean probabilty (%)&quot;)
plt.title(&quot;Session Summary&quot;)
plt.show()

print(f&quot;Emotions analyzed for: {(toc - tic):0.4f} seconds&quot;)
</code></pre>
<pre><code>Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
python3-pyaudio is already the newest version (0.2.11-1.3ubuntu1).
0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
&lt;ipython-input-8-97261d8a231f&gt; in &lt;cell line: 19&gt;()
     17 # Open an input channel
     18 p = pyaudio.PyAudio()
---&gt; 19 stream = p.open(format=FORMAT,
     20                 channels=CHANNELS,
     21                 rate=RATE,

1 frames
/usr/lib/python3/dist-packages/pyaudio.py in __init__(self, PA_manager, rate, channels, format, input, output, input_device_index, output_device_index, frames_per_buffer, start, input_host_api_specific_stream_info, output_host_api_specific_stream_info, stream_callback)
    439 
    440         # calling pa.open returns a stream object
--&gt; 441         self._stream = pa.open(**arguments)
    442 
    443         self._input_latency = self._stream.inputLatency

OSError: [Errno -9996] Invalid input device (no default output device)
</code></pre>
<p>I have tried  alternative approaches to check the audio devices connected to my device .</p>
<pre><code>import pyaudio
p = pyaudio.PyAudio()
for i in range(p.get_device_count()):
    dev = p.get_device_info_by_index(i)
    if dev.get('maxInputChannels') &gt; 0:
        print(f&quot;Device {i}: {dev.get('name')}&quot;)

After running the above chunk of code my output window is blank.
</code></pre>
","deep-learning, nlp, speech-recognition, pydub, collaborative",
How to make LLMs answer in certain pattern,"<p>How can I design the LLM program so that the LLM will answer in some preset pattern?For example, if I want to use the LLM to promote some products or collect some information from the user, what adjustment should I try on the LLM program? Thx~</p>
<p>I tried modifying the prompts but it doesn't work well when the user says something deviated from the conversation pattern, like when the user does not answer LLM's question.</p>
","nlp, artificial-intelligence, openai-api, large-language-model",
spaCy Custom NER model only works on its own,"<p>I have created a custom NER model for spaCy and can load and run it and it works as expected.</p>
<pre><code>
nlp = spacy.load('/content/drive/MyDrive/Custom_NER/trained_models/output/model-best')

text = &quot;I want to book a ticket for 2 adults and 2 children travelling to Norwich from London Liverpool Street departing at 09:00 and arriving by 14:00 leaving Tomorrow and returning on 14/05/24&quot;

doc = nlp(text)

for ent in doc.ents:
  print(ent.text, &quot;  -&gt;&gt;&gt;&gt;  &quot;, ent.label_)
</code></pre>
<p>Outputs:</p>
<pre><code>```2   -&gt;&gt;&gt;&gt;   STATION
to   -&gt;&gt;&gt;&gt;   TO
Norwich   -&gt;&gt;&gt;&gt;   STATION
from   -&gt;&gt;&gt;&gt;   FROM
London   -&gt;&gt;&gt;&gt;   STATION
Liverpool   -&gt;&gt;&gt;&gt;   STATION
Street   -&gt;&gt;&gt;&gt;   STATION
departing at   -&gt;&gt;&gt;&gt;   TIME_KEY
09:00   -&gt;&gt;&gt;&gt;   TICKET_TIME
arriving by   -&gt;&gt;&gt;&gt;   TIME_KEY
14:00   -&gt;&gt;&gt;&gt;   TICKET_TIME
leaving Tomorrow   -&gt;&gt;&gt;&gt;   TIME_KEY
returning   -&gt;&gt;&gt;&gt;   TO
14/05/24   -&gt;&gt;&gt;&gt;   DATE```
</code></pre>
<p>However, if I try to combine my NER model with a built-in pipeline so I can use other features using</p>
<pre><code>
    nlp = spacy.load('en_core_web_sm', exclude=[&quot;ner&quot;])
    
    ner_nlp = spacy.load('/content/drive/MyDrive/Custom_NER/trained_models/output/model-best')
    nlp.add_pipe(&quot;ner&quot;, source=ner_nlp)

</code></pre>
<p>My output is suddenly:</p>
<pre><code>I want   -&gt;&gt;&gt;&gt;   STATION
to book   -&gt;&gt;&gt;&gt;   STATION
a ticket   -&gt;&gt;&gt;&gt;   STATION
for 2   -&gt;&gt;&gt;&gt;   STATION
adults and   -&gt;&gt;&gt;&gt;   STATION
2 children   -&gt;&gt;&gt;&gt;   STATION
travelling to   -&gt;&gt;&gt;&gt;   STATION
Norwich from   -&gt;&gt;&gt;&gt;   STATION
London Liverpool   -&gt;&gt;&gt;&gt;   STATION
Street departing   -&gt;&gt;&gt;&gt;   STATION
at 09:00   -&gt;&gt;&gt;&gt;   STATION
and arriving   -&gt;&gt;&gt;&gt;   STATION
by 14:00   -&gt;&gt;&gt;&gt;   STATION
leaving Tomorrow   -&gt;&gt;&gt;&gt;   STATION
and returning   -&gt;&gt;&gt;&gt;   STATION
on 14/05/24   -&gt;&gt;&gt;&gt;   STATION
</code></pre>
<p>So I am unsure why this would be happening.
I believe it maybe something to do with the new model using a transformer and the built in model using words2vec</p>
<p>Any help would be gladly appreciated</p>
","nlp, spacy, named-entity-recognition",
Use Spacy to find Lemma of Russian (Those langs which don&#39;t have model),"<p>I have downloaded Spacy English model and finding lemma using this code.</p>

<pre><code>import spacy
nlp = spacy.load('en')
doc = nlp(u'Two apples')
for token in doc:
    print(token, token.lemma, token.lemma_)
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>Two 11711838292424000352 two
apples 8566208034543834098 apple
</code></pre>

<p>Now I wanted to do same thing for <strong>Russian</strong> language. But Spacy don't have models for Russian language. But I am seeing their <a href=""https://github.com/explosion/spaCy/tree/master/spacy/lang/ru"" rel=""noreferrer"">GitHub code for Russian language</a> and I think that code could be used to find lemma.</p>

<p>I am new to Spacy. Will needed a starting point for those languages which don't have models. Also I have noted that for some languages let say for URDU they have provided a <a href=""https://github.com/explosion/spaCy/blob/master/spacy/lang/ur/lemmatizer.py"" rel=""noreferrer"">look up dictionary</a> for lemmatization.</p>

<p>I want to expand this thing to all those languages which don't have models.</p>

<p>Note: In above code I believe that it could be further improved as in my case I needed lemma only so what are the things which I can turn off and how?</p>
","nlp, spacy, lemmatization",
pydantic has been upgraded from 1.10.13 to version 2.7.1. How to modify the following code?,"<pre><code>from pydantic.schema import model_schema

@classmethod
    def create_prompt(
            cls,
            tools: Sequence[BaseTool],
            prompt: str = None,
            input_variables: Optional[List[str]] = None,
            memory_prompts: Optional[List[BasePromptTemplate]] = None,
    ) -&gt; BasePromptTemplate:
        tools_json = []
        tool_names = []
        for tool in tools:
            tool_schema = model_schema(tool.args_schema) if tool.args_schema else {}
            simplified_config_langchain = {
                &quot;name&quot;: tool.name,
                &quot;description&quot;: tool.description,
                &quot;parameters&quot;: tool_schema.get(&quot;properties&quot;, {})
            }
            tools_json.append(simplified_config_langchain)
            tool_names.append(tool.name)
        formatted_tools = &quot;\n&quot;.join([
            f&quot;{tool['name']}: {tool['description']}, args: {tool['parameters']}&quot;
            for tool in tools_json
        ])
        formatted_tools = formatted_tools.replace(&quot;'&quot;, &quot;\\'&quot;).replace(&quot;{&quot;, &quot;{{&quot;).replace(&quot;}&quot;, &quot;}}&quot;)
        template = prompt.format(tool_names=tool_names,
                                 tools=formatted_tools,
                                 history=&quot;None&quot;,
                                 input=&quot;{input}&quot;,
                                 agent_scratchpad=&quot;{agent_scratchpad}&quot;)

        if input_variables is None:
            input_variables = [&quot;input&quot;, &quot;agent_scratchpad&quot;]
        _memory_prompts = memory_prompts or []
        messages = [
            SystemMessagePromptTemplate.from_template(template),
            *_memory_prompts,
        ]
        return ChatPromptTemplate(input_variables=input_variables, messages=messages)
</code></pre>
<p>Code after modification</p>
<p>model_schema can be used normally in pydantic1.0, but it is deprecated in pydantic2.0. Is there any replacement plan?</p>
<p>Error message:</p>
<p>/home/xqxls/git/Langchain-Chatchat/venv/lib/python3.11/site-packages/pydantic/_internal/_config.py:334: UserWarning: Valid config keys have changed in V2:</p>
<ul>
<li>'schema_extra' has been renamed to 'json_schema_extra'
warnings.warn(message, UserWarning)
/home/xqxls/git/Langchain-Chatchat/venv/lib/python3.11/site-packages/pydantic/_internal/<em>fields.py:160: UserWarning: Field &quot;model_name&quot; has conflict with protected namespace &quot;model</em>&quot;.</li>
</ul>
<p>You may be able to resolve this warning by setting <code>model_config['protected_namespaces'] = ()</code>.
warnings.warn(
Process API Server:
Traceback (most recent call last):
File &quot;/home/xqxls/anaconda3/lib/python3.11/multiprocessing/process.py&quot;, line 314, in _bootstrap
self.run()
File &quot;/home/xqxls/anaconda3/lib/python3.11/multiprocessing/process.py&quot;, line 108, in run
self._target(*self._args, **self._kwargs)
File &quot;/home/xqxls/git/Langchain-Chatchat/startup.py&quot;, line 456, in run_api_server
app = create_app(run_mode=run_mode)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/home/xqxls/git/Langchain-Chatchat/server/api.py&quot;, line 51, in create_app
mount_app_routes(app, run_mode=run_mode)
File &quot;/home/xqxls/git/Langchain-Chatchat/server/api.py&quot;, line 77, in mount_app_routes
mount_knowledge_routes(app)
File &quot;/home/xqxls/git/Langchain-Chatchat/server/api.py&quot;, line 142, in mount_knowledge_routes
from server.chat.agent_chat import agent_chat
File &quot;/home/xqxls/git/Langchain-Chatchat/server/chat/agent_chat.py&quot;, line 15, in 
from server.agent.custom_agent.ChatGLM3Agent import initialize_glm3_agent
File &quot;/home/xqxls/git/Langchain-Chatchat/server/agent/custom_agent/ChatGLM3Agent.py&quot;, line 9, in 
from pydantic.schema import model_schema
ImportError: cannot import name 'model_schema' from 'pydantic.schema' (/home/xqxls/git/Langchain-Chatchat/venv/lib/python3.11/site-packages/pydantic/schema.py)</p>
<p>origianl code:</p>
<pre><code>&quot;&quot;&quot;
This file is a modified version for ChatGLM3-6B the original glm3_agent.py file from the langchain repo.
&quot;&quot;&quot;
from __future__ import annotations

import json
import logging
from typing import Any, List, Sequence, Tuple, Optional, Union
from pydantic.schema import model_schema


from langchain.agents.structured_chat.output_parser import StructuredChatOutputParser
from langchain.memory import ConversationBufferWindowMemory
from langchain.agents.agent import Agent
from langchain.chains.llm import LLMChain
from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate
from langchain.agents.agent import AgentOutputParser
from langchain.output_parsers import OutputFixingParser
from langchain.pydantic_v1 import Field
from langchain.schema import AgentAction, AgentFinish, OutputParserException, BasePromptTemplate
from langchain.agents.agent import AgentExecutor
from langchain.callbacks.base import BaseCallbackManager
from langchain.schema.language_model import BaseLanguageModel
from langchain.tools.base import BaseTool

HUMAN_MESSAGE_TEMPLATE = &quot;{input}\n\n{agent_scratchpad}&quot;
logger = logging.getLogger(__name__)


class StructuredChatOutputParserWithRetries(AgentOutputParser):
    &quot;&quot;&quot;Output parser with retries for the structured chat agent.&quot;&quot;&quot;

    base_parser: AgentOutputParser = Field(default_factory=StructuredChatOutputParser)
    &quot;&quot;&quot;The base parser to use.&quot;&quot;&quot;
    output_fixing_parser: Optional[OutputFixingParser] = None
    &quot;&quot;&quot;The output fixing parser to use.&quot;&quot;&quot;

    def parse(self, text: str) -&gt; Union[AgentAction, AgentFinish]:
        special_tokens = [&quot;Action:&quot;, &quot;&lt;|observation|&gt;&quot;]
        first_index = min([text.find(token) if token in text else len(text) for token in special_tokens])
        text = text[:first_index]
        if &quot;tool_call&quot; in text:
            action_end = text.find(&quot;```&quot;)
            action = text[:action_end].strip()
            params_str_start = text.find(&quot;(&quot;) + 1
            params_str_end = text.rfind(&quot;)&quot;)
            params_str = text[params_str_start:params_str_end]

            params_pairs = [param.split(&quot;=&quot;) for param in params_str.split(&quot;,&quot;) if &quot;=&quot; in param]
            params = {pair[0].strip(): pair[1].strip().strip(&quot;'\&quot;&quot;) for pair in params_pairs}

            action_json = {
                &quot;action&quot;: action,
                &quot;action_input&quot;: params
            }
        else:
            action_json = {
                &quot;action&quot;: &quot;Final Answer&quot;,
                &quot;action_input&quot;: text
            }
        action_str = f&quot;&quot;&quot;
Action:
</code></pre>
<p>{json.dumps(action_json, ensure_ascii=False)}</p>
<pre><code>        try:
            if self.output_fixing_parser is not None:
                parsed_obj: Union[
                    AgentAction, AgentFinish
                ] = self.output_fixing_parser.parse(action_str)
            else:
                parsed_obj = self.base_parser.parse(action_str)
            return parsed_obj
        except Exception as e:
            raise OutputParserException(f&quot;Could not parse LLM output: {text}&quot;) from e

    @property
    def _type(self) -&gt; str:
        return &quot;structured_chat_ChatGLM3_6b_with_retries&quot;


class StructuredGLM3ChatAgent(Agent):
    &quot;&quot;&quot;Structured Chat Agent.&quot;&quot;&quot;

    output_parser: AgentOutputParser = Field(
        default_factory=StructuredChatOutputParserWithRetries
    )
    &quot;&quot;&quot;Output parser for the agent.&quot;&quot;&quot;

    @property
    def observation_prefix(self) -&gt; str:
        &quot;&quot;&quot;Prefix to append the ChatGLM3-6B observation with.&quot;&quot;&quot;
        return &quot;Observation:&quot;

    @property
    def llm_prefix(self) -&gt; str:
        &quot;&quot;&quot;Prefix to append the llm call with.&quot;&quot;&quot;
        return &quot;Thought:&quot;

    def _construct_scratchpad(
            self, intermediate_steps: List[Tuple[AgentAction, str]]
    ) -&gt; str:
        agent_scratchpad = super()._construct_scratchpad(intermediate_steps)
        if not isinstance(agent_scratchpad, str):
            raise ValueError(&quot;agent_scratchpad should be of type string.&quot;)
        if agent_scratchpad:
            return (
                f&quot;This was your previous work &quot;
                f&quot;(but I haven't seen any of it! I only see what &quot;
                f&quot;you return as final answer):\n{agent_scratchpad}&quot;
            )
        else:
            return agent_scratchpad

    @classmethod
    def _get_default_output_parser(
            cls, llm: Optional[BaseLanguageModel] = None, **kwargs: Any
    ) -&gt; AgentOutputParser:
        return StructuredChatOutputParserWithRetries(llm=llm)

    @property
    def _stop(self) -&gt; List[str]:
        return [&quot;&lt;|observation|&gt;&quot;]

    @classmethod
    def create_prompt(
            cls,
            tools: Sequence[BaseTool],
            prompt: str = None,
            input_variables: Optional[List[str]] = None,
            memory_prompts: Optional[List[BasePromptTemplate]] = None,
    ) -&gt; BasePromptTemplate:
        tools_json = []
        tool_names = []
        for tool in tools:
            tool_schema = model_schema(tool.args_schema) if tool.args_schema else {}
            simplified_config_langchain = {
                &quot;name&quot;: tool.name,
                &quot;description&quot;: tool.description,
                &quot;parameters&quot;: tool_schema.get(&quot;properties&quot;, {})
            }
            tools_json.append(simplified_config_langchain)
            tool_names.append(tool.name)
        formatted_tools = &quot;\n&quot;.join([
            f&quot;{tool['name']}: {tool['description']}, args: {tool['parameters']}&quot;
            for tool in tools_json
        ])
        formatted_tools = formatted_tools.replace(&quot;'&quot;, &quot;\\'&quot;).replace(&quot;{&quot;, &quot;{{&quot;).replace(&quot;}&quot;, &quot;}}&quot;)
        template = prompt.format(tool_names=tool_names,
                                 tools=formatted_tools,
                                 history=&quot;None&quot;,
                                 input=&quot;{input}&quot;,
                                 agent_scratchpad=&quot;{agent_scratchpad}&quot;)

        if input_variables is None:
            input_variables = [&quot;input&quot;, &quot;agent_scratchpad&quot;]
        _memory_prompts = memory_prompts or []
        messages = [
            SystemMessagePromptTemplate.from_template(template),
            *_memory_prompts,
        ]
        return ChatPromptTemplate(input_variables=input_variables, messages=messages)

    @classmethod
    def from_llm_and_tools(
            cls,
            llm: BaseLanguageModel,
            tools: Sequence[BaseTool],
            prompt: str = None,
            callback_manager: Optional[BaseCallbackManager] = None,
            output_parser: Optional[AgentOutputParser] = None,
            human_message_template: str = HUMAN_MESSAGE_TEMPLATE,
            input_variables: Optional[List[str]] = None,
            memory_prompts: Optional[List[BasePromptTemplate]] = None,
            **kwargs: Any,
    ) -&gt; Agent:
        &quot;&quot;&quot;Construct an agent from an LLM and tools.&quot;&quot;&quot;
        cls._validate_tools(tools)
        prompt = cls.create_prompt(
            tools,
            prompt=prompt,
            input_variables=input_variables,
            memory_prompts=memory_prompts,
        )
        llm_chain = LLMChain(
            llm=llm,
            prompt=prompt,
            callback_manager=callback_manager,
        )
        tool_names = [tool.name for tool in tools]
        _output_parser = output_parser or cls._get_default_output_parser(llm=llm)
        return cls(
            llm_chain=llm_chain,
            allowed_tools=tool_names,
            output_parser=_output_parser,
            **kwargs,
        )

    @property
    def _agent_type(self) -&gt; str:
        raise ValueError

def initialize_glm3_agent(
        tools: Sequence[BaseTool],
        llm: BaseLanguageModel,
        prompt: str = None,
        memory: Optional[ConversationBufferWindowMemory] = None,
        agent_kwargs: Optional[dict] = None,
        *,
        tags: Optional[Sequence[str]] = None,
        **kwargs: Any,
) -&gt; AgentExecutor:
    tags_ = list(tags) if tags else []
    agent_kwargs = agent_kwargs or {}
    agent_obj = StructuredGLM3ChatAgent.from_llm_and_tools(
        llm=llm,
        tools=tools,
        prompt=prompt,
        **agent_kwargs
    )
    return AgentExecutor.from_agent_and_tools(
        agent=agent_obj,
        tools=tools,
        memory=memory,
        tags=tags_,
        **kwargs,
    )

</code></pre>
","python-3.x, nlp, pydantic, chatgpt-api, vllm",
Generating outputs from last layer&#39;s hidden state values,"<p>I manipulated the hidden state values obtained from the llama-2 model after feeding it a certain input, let's call it Input_1. Now, I want to examine the output (causal output) it produces from this. My hypothesis is that it should correspond to a different input, let's call it Input_2, which would yield a distinct output from the initial input.</p>
<p>I got last layer's hidden state values in the following manner :</p>
<pre><code>from transformers import LlamaModel, LlamaTokenizer, LlamaForCausalLM
tokenizer = LlamaTokenizer.from_pretrained(path_to_llama2)
model = LlamaModel.from_pretrained(path_to_llama2)
model_ = LlamaForCausalLM.from_pretrained(path_to_llama2)

tokenizer.pad_token = tokenizer.eos_token
inputs = tokenizer(prompt, return_tensors='pt')    

with torch.no_grad():
  outputs = model(**inputs, output_attentions=True, output_hidden_states=True)
  hidden_states = outputs.hidden_states[-1]  # Last layer hidden states
</code></pre>
<p>As shown above, I was trying to change hidden_states values which I got from model but now I want to generate a causal output. How can I do it? Are there any suggestions?</p>
","python, nlp, huggingface-transformers, large-language-model",
Passing multiple sentences to BERT?,"<p>I have a dataset with paragraphs that I need to classify into two classes. These paragraphs are usually 3-5 sentences long. The overwhelming majority of them are less than 500 words long. I would like to make use of BERT to tackle this problem.</p>
<p>I am wondering how I should use BERT to generate vector representations of these paragraphs and especially, whether it is fine to just pass the whole paragraph into BERT?</p>
<p>There have been informative discussions of related problems <a href=""https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification/63413589#63413589"">here</a> and <a href=""https://stackoverflow.com/questions/63671085/how-to-use-bert-for-long-sentences"">here</a>. These discussions focus on how to use BERT for representing whole documents. In my case the paragraphs are not that long, and indeed could be passed to BERT without exceeding its maximum length of 512. However, BERT was trained on sentences. Sentences are relatively self-contained units of meaning. I wonder if feeding multiple sentences into BERT doesn't conflict fundamentally with what the model was designed to do (although this appears to be done regularly).</p>
","nlp, text-classification, bert-language-model, huggingface-transformers",
Python spacy 2.3.5 installation error within the subprocesses,"<p><a href=""https://i.sstatic.net/ThvtZ.png"" rel=""nofollow noreferrer"">The error i got when installing the spacy 2.3.5 version</a></p>
<p>I ran the command pip install spacy==2.3.5</p>
<p>i got error for multiple lines and here is the end of the error message</p>
<p>Cython.Compiler.Errors.CompileError: thinc/extra/eg.pyx
[end of output]</p>
<pre><code>    note: This error originates from a subprocess, and is likely not a problem with pip.
  error: subprocess-exited-with-error

  Getting requirements to build wheel did not run successfully.
  exit code: 1

  See above for output.

  note: This error originates from a subprocess, and is likely not a problem with pip.
  [end of output]
</code></pre>
<p>note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error</p>
<p>× pip subprocess to install build dependencies did not run successfully.</p>
<p>I was trying to use pyrespaser it cannot access the config.cfg error indicating the missing of this cfg file it was due to the version of the spacy I had the 3.7.1 version but pyresparser requires the spacy 2.3.5 version which also produces errors</p>
","python, pip, nlp, spacy",
pre-trained model embeddings from Universal Sentence Encoder,"<p>I am trying to load pretrained embeddings from Universal Sentence Encoder on TF-Hub. It seems to work only on keras v. 2.15.0. I am seeking help with its implementation on keras v 3.0.5</p>
<p>FYI - I did implement a class wrapper as suggested in the error below, but it still fails. This time it complained about <code>type string</code> being unknown.</p>
<pre><code>import os
os.environ[&quot;KERAS_BACKEND&quot;] = &quot;jax&quot;  # &quot;jax&quot; or &quot;tensorflow&quot; or &quot;torch&quot; 

import keras_nlp
import keras
import keras.backend as K

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras.layers import TextVectorization
from tensorflow.keras.layers import Embedding


USE_model_url = &quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;
embedding_layer = hub.KerasLayer(USE_model_url,
                                 input_shape=[],
                                 dtype=tf.string,
                                 name=&quot;Universal_sentence_encoder&quot;
)
</code></pre>
<h4>In the following model definition the call to <code>embedding_layer()</code></h4>
<p><em><strong>Fails</strong></em> for keras v. <strong>3.0.5</strong> <br />
<em><strong>Works</strong></em> for keras v. <strong>2.15.0</strong></p>
<pre><code>inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)
pretrained_embeddings = embedding_layer(inputs) &lt;--- Error on this line

</code></pre>
<h3>Error:</h3>
<p>TypeError: Exception encountered when calling layer 'Universal_sentence_encoder' (type KerasLayer).</p>
<p>Binding inputs to <code>tf.function</code> failed due to A <code>KerasTensor</code> cannot be used as input to a <code>TensorFlow</code> function. A <code>KerasTensor</code> is a symbolic placeholder for a shape and <code>dtype</code>, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces <code>keras.layers</code> and <code>keras.operations</code>). You are likely doing something like:</p>
<pre><code>x = Input(...)
...
tf_fn(x)  # Invalid.
</code></pre>
<p>What you should do instead is wrap <code>tf_fn</code> in a layer:</p>
<pre><code>class MyLayer(Layer):
    def call(self, x):
        return tf_fn(x)

x = MyLayer()(x)
</code></pre>
<p>. Received args: <code>(&lt;KerasTensor shape=(None,), dtype=string, sparse=None, name=keras_tensor&gt;,)</code> and <code>kwargs: {}</code> for signature: <code>(inputs: TensorSpec(shape=&lt;unknown&gt;, dtype=tf.string, name=None))</code>.</p>
<p>Call arguments received by layer 'Universal_sentence_encoder' (type <code>KerasLayer</code>): <br />
• <code>inputs=&lt;KerasTensor shape=(None,), dtype=string, sparse=None, name=keras_tensor&gt;</code> <br />
• <code>training=None</code></p>
<pre><code></code></pre>
","nlp, keras-layer, encoder, sentence-transformers",
How to integrate Power BI&#39;s Q&amp;A feature inside a Nextjs application,"<p>I want to integrate the feature of power BI of asking questions to the Database then returning visualizations into a Nextjs app. Is there a way to do that?</p>
<p>I know of the <a href=""https://github.com/microsoft/powerbi-client-react"" rel=""nofollow noreferrer"">https://github.com/microsoft/powerbi-client-react</a> library in react, but can't seem to find a real example of the Q&amp;A feature working inside a Nextjs app.</p>
<p>I want to be able to pass a Database with its schema, then ask questions based on the provided info. From my research, this is called a NLIDB (Natural Language Interface for Databases). According to some further research, this is exactly what power BI Q&amp;A does. However, I have found it troublesome to find examples of codes that integrate this Q&amp;A feature into a Nextjs app.</p>
<p>If there exist another tool better for creating this sort of application with Nextjs, please do point it out.</p>
","database, next.js, nlp, powerbi",
Why doesn&#39;t fuzzywuzzy&#39;s process.extractBests give a 100% score when the tested string 100% contains the query string?,"<p>I'm testing <code>fuzzywuzzy</code>'s <code>process.extractBests()</code> as follows:</p>
<pre><code>from fuzzywuzzy import process

# Define the query string
query = &quot;Apple&quot;

# Define the list of choices
choices = [&quot;Apple&quot;, &quot;Apple Inc.&quot;, &quot;Apple Computer&quot;, &quot;Apple Records&quot;, &quot;Apple TV&quot;]

# Call the process.extractBests function
results = process.extractBests(query, choices)

# Print the results
for result in results:
    print(result)
</code></pre>
<p>It outputs:</p>
<pre><code>('Apple', 100)
('Apple Inc.', 90)
('Apple Computer', 90)
('Apple Records', 90)
('Apple TV', 90)
</code></pre>
<p>Why didn't the scorer give 100 to all strings since they all 100% contain the query string (&quot;Apple&quot;)?</p>
<p>I use fuzzywuzzy==0.18.0 with Python 3.11.7.</p>
","python, nlp, string-matching, fuzzywuzzy","<p>The <code>fuzzywuzzy</code>'s <code>extractBests()</code> function does not give 100% because it does not check for a match, it checks for similarity, such as length of string, contents of string compared to the query, positions of the query string, and a few other factors.    In your case, it does not output 100% because &quot;Apple Inc.&quot; is not an exact match of your query, &quot;Apple&quot;.  This is why only the &quot;Apple&quot; choice outputs 100%, because it 100% matches with the query, &quot;Apple&quot;.  I hoped this helped!</p>
"
NLTK agreement with distance metric,"<p>I have a task to calculate <a href=""https://en.wikipedia.org/wiki/Inter-rater_reliability"" rel=""nofollow noreferrer"">inter-annotator agreement</a> in <a href=""https://en.wikipedia.org/wiki/Multi-label_classification"" rel=""nofollow noreferrer"">multi-label classification</a>, where for each example more than one label can be assigned. I found that <a href=""http://www.nltk.org/api/nltk.metrics.html"" rel=""nofollow noreferrer"">NLTK</a> can measure agreement based on a distance metric.</p>
<p>I am looking for an example of calculating krippendorff alpha with MASI distance.</p>
<p>This is what I have.</p>
<pre><code>import nltk
from nltk.metrics import masi_distance


toy_data = [['1', 5723, [1,2]],['2', 5723, [2,3]]]

task = nltk.metrics.agreement.AnnotationTask(data=toy_data, distance=masi_distance)
print task.alpha()
</code></pre>
<p>This code fails with</p>
<pre><code>TypeError: unhashable type: 'list'
</code></pre>
<p>The following doesn't work either:</p>
<pre><code>toy_data = [['1', 5723, set([1,2])],['2', 5723, set([2,3])]]
</code></pre>
<p>Do you have a working example?
Thank you!</p>
","python, machine-learning, nlp, nltk","<p>To be more precise, what needs to be a frozenset (as @alexis has pointed out) is just the third member of the triple, this is the labels assigned to the item.</p>
<pre><code>toy_data = [['1', 5723, frozenset([1,2])],['2', 5723, frozenset([2,3])]]
</code></pre>
"
AttributeError: &#39;Tensor&#39; object has no attribute &#39;embed_documents&#39;,"<p>I have uploaded PDF file and split file into chunks, then apply tokenizer to each chunck and created embeddings. But when i try to store my embedding in FIASS, it give me AttributeError: 'Tensor' object has no attribute 'embed_documents' error. My embeddings are in tensors. I also tried to converts it 2D Numpy array, but still problem sustained. Pl guide.</p>
<p>it give me AttributeError: 'Tensor' object has no attribute 'embed_documents' error</p>
","python, nlp, chatbot, langchain, vectorstore",
why gradient scaling used in mix-precision training could lead to bigger Learning Rate in float32 model?,"<p>Background:</p>
<p>Gradient Scaling original is used in mix-precision training like (part of model weights are float16 and part of them are float 32), which its purpose is to reduce underflow of small gradients stored with float16.
Recently, I was doing some experiments and found some result confussing me. In the case of whole model with float32 weights, if I update the model with Gradient Scaling, I could use much bigger LR (Learning Rate) for convergence compared to don't use it.</p>
<p>Scaled version updating code excerpt:</p>
<pre><code>loss_scaler = torch.cuda.amp.GradScaler()
loss_scaler.scale(loss).backward()
loss_scaler.unscale_(optimizer)
loss_scaler.step(optimizer)
loss_scaler.update()
</code></pre>
<p>unscaled version updating code excerpt:</p>
<pre><code>loss.backward()
optimizer.step()
</code></pre>
<p>Problem:
why would this happen? Should the scaled gradients be cancelled out tranformed to original gradients? What I expect is that thier LR should be the same for model training?</p>
<p>Here is the ChatGPT's explaination:
Even float32 could underflow, and the scale make these underflow gradients make contribute to the weights updating which make training more stable, so the lr could be bigger.</p>
<p>I wonder if this explaination is reasonable, cause the LR is much different. (1.5e-4 vs 1.5e-8)</p>
","machine-learning, nlp, artificial-intelligence, large-language-model",
AttributeError: module &#39;click.utils&#39; has no attribute &#39;_expand_args&#39;,"<p>I am following a nlp tutorial this is the video I am on: <a href=""https://www.youtube.com/watch?v=h2kBNEShsiE&amp;list=PLeo1K3hjS3uuvuAXhYjV2lMEShq2UYSwX&amp;index=7"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=h2kBNEShsiE&amp;list=PLeo1K3hjS3uuvuAXhYjV2lMEShq2UYSwX&amp;index=7</a></p>
<p>In the video the instructor asks us to run the command python -m spacy download en but when I try to run it I get this:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;frozen runpy&gt;&quot;, line 198, in _run_module_as_main
  File &quot;&lt;frozen runpy&gt;&quot;, line 88, in _run_code
  File &quot;C:\Users\alokk\AppData\Local\Programs\Python\Python311\Lib\site-packages\spacy\__main__.py&quot;, line 4, in &lt;module&gt;
    setup_cli()
  File &quot;C:\Users\alokk\AppData\Local\Programs\Python\Python311\Lib\site-packages\spacy\cli\_util.py&quot;, line 87, in setup_cli
    command(prog_name=COMMAND)
  File &quot;C:\Users\alokk\AppData\Local\Programs\Python\Python311\Lib\site-packages\click\core.py&quot;, line 829, in __call__
    return self.main(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\alokk\AppData\Local\Programs\Python\Python311\Lib\site-packages\typer\core.py&quot;, line 783, in main
    return _main(
           ^^^^^^
  File &quot;C:\Users\alokk\AppData\Local\Programs\Python\Python311\Lib\site-packages\typer\core.py&quot;, line 199, in _main
    args = click.utils._expand_args(args)
           ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'click.utils' has no attribute '_expand_args'
</code></pre>
<p>I have tried running it in vscode command prompt, command prompt, administrator command prompt, powershell, and administrator powershell.</p>
<p>Can anyone help me?</p>
<p>I tried to install the en libraries for spacy but it didn't work and returned an error.</p>
","python, pip, nlp, spacy",
FastText language_identification in R returns too many arguments - how to match to texts?,"<p>FastText language_identification returns multiple predictions per original text, and also fails to indicate which belong to which original document.</p>
<p>There are differing numbers of predictions per original document too -- their GitHub forums are closed now, but does anyone know how to match the output to the original texts?</p>
<p>Code:</p>
<pre><code>DF = data.frame(doc_id = seq(1, 5),
speechtext = c(&quot;Hello. Fake text entry 1.&quot;, &quot;Fake text entry 2&quot;, &quot;more text&quot;, &quot;Text in a
different language&quot;, &quot;Hola&quot;))

library(fastText)
# download .ftz pretrained model from https://fasttext.cc/docs/en/language-identification.html
file_ftz = system.file(&quot;language_identification/lid.176.ftz&quot;, package = &quot;fastText&quot;)
lang1 = language_identification(DF$speechtext,
                                pre_trained_language_model_path = file_ftz,
                                verbose = T)
</code></pre>
<p>I was expecting one prediction per original text, or at least a consistent number, or some way of marking which document the predictions align with.</p>
<p>Really I could guess based on the largest number per series of a few elements outputted, but this doesn't seem optimal -- it does seem like a bug.</p>
<p>(I tried adding intern = T as an argument per <a href=""https://stackoverflow.com/questions/65130621/r-fasttext-how-to-load-output-into-a-dataframe-from-command-line"">R - fasttext how to load output into a dataframe from command line</a> -- this is not recognized as an argument).</p>
","r, nlp, fasttext, language-detection","<p>The first argument to <a href=""https://cran.r-project.org/web/packages/fastText/fastText.pdf"" rel=""nofollow noreferrer""><code>fastText::language_identification()</code></a> is defined as:</p>
<blockquote>
<p>either a valid character string to a valid path <em>where each line represents a different text extract or a vector of text extracts</em> (emphasis mine)</p>
</blockquote>
<p>You have line breaks in your input data:</p>
<pre class=""lang-r prettyprint-override""><code>DF$speechtext[4]
[1] &quot;Text in a\ndifferent language&quot;
</code></pre>
<p>As one prediction is generated per line, you'll get two predictions from this element. You have two options:</p>
<ol>
<li>Remove new lines in your input data. This makes sense in this case.</li>
<li>Keep new lines and map document IDs to each line. This makes sense if new lines might actually be in different languages.</li>
</ol>
<h3>Remove new lines</h3>
<p>If you replace new lines with spaces you will get the same number of predictions returned as input rows.</p>
<p>In the regex below, I have used the <a href=""https://www.pcre.org/"" rel=""nofollow noreferrer"">PCRE</a> <code>\v</code> which matches newlines and any character considered vertical whitespace. This now produces five rows, one relating to each input row.</p>
<pre class=""lang-r prettyprint-override""><code>language_identification(gsub(&quot;\\v&quot;, &quot; &quot;, DF$speechtext, perl = TRUE), file_ftz)
#    iso_lang_1   prob_1
#        &lt;char&gt;    &lt;num&gt;
# 1:         en 0.220767
# 2:         en 0.388695
# 3:         en 0.613707
# 4:         en 0.757671
# 5:         es 0.721487
</code></pre>
<p><code>\v</code> includes several vertical space characters (such as form feed and line separator), so should cover all possible types of new line. For full details see the table <a href=""https://perldoc.perl.org/perlrecharclass#Whitespace"" rel=""nofollow noreferrer"">here</a>.</p>
<h3>Keep new lines and map document ID to each line</h3>
<p>Alternatively, if different lines of each input document might be in different languages, you may not want to remove new lines. In this case, you can predict each line separately and then map the document IDs to each line:</p>
<pre class=""lang-r prettyprint-override""><code># As before
lang1 &lt;- language_identification(DF$speechtext, file_ftz)

# Add document IDs
lang1$doc_id &lt;- rep(
    DF$doc_id,
    lengths(strsplit(DF$speechtext, &quot;\\v&quot;, perl = TRUE))
)

lang1
#    iso_lang_1   prob_1 doc_id
#        &lt;char&gt;    &lt;num&gt;  &lt;int&gt;
# 1:         en 0.220767      1
# 2:         en 0.388695      2
# 3:         en 0.613707      3
# 4:         en 0.932691      4
# 5:         en 0.571937      4
# 6:         es 0.721487      5
</code></pre>
"
Obtaining the name of the executed chain in the output during runtime,"<p>I was going through a <a href=""https://www.analyticsvidhya.com/blog/2023/10/a-comprehensive-guide-to-using-chains-in-langchain/"" rel=""nofollow noreferrer"">tutorial</a> about router chains. Is there a method to fetch the name of the executed chain in the output, such as &quot;math&quot;. How can I retrieve the name of the executed chain? Also, is there a way to store the output shown below when the verbosity is set to True, into a variable?</p>
<p><a href=""https://github.com/jyotiyadav94/RouterChains/blob/main/Router_Chains.ipynb"" rel=""nofollow noreferrer"">Complete code</a></p>
<p><a href=""https://i.sstatic.net/3Ki6tpkl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3Ki6tpkl.png"" alt=""enter image description here"" /></a></p>
","python, nlp, langchain, large-language-model",
403 Forbidden 453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints only,"<p>I had recently registered for the free level Twitter API, and I would like to use Tweepy to help me extract tweets from users.</p>
<pre class=""lang-py prettyprint-override""><code>api_key = config['twitter']['api_key']
api_secret = config['twitter']['api_key_secret']

access_token = config['twitter']['access_token']
access_token_secret = config['twitter']['access_token_secret']

auth = tweepy.OAuthHandler(api_key, api_secret)
auth.set_access_token(access_token, access_token_secret)

api = tweepy.API(auth)
tweets = api.home_timeline()
</code></pre>
<p>But after I ran it, the below error showed up:</p>
<blockquote>
<p>Forbidden: 403 Forbidden 453 - You currently have access to a subset
of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media
post, oauth) only. If you need access to this endpoint, you may need a
different access level. You can learn more here:
<a href=""https://developer.twitter.com/en/portal/product"" rel=""noreferrer"">https://developer.twitter.com/en/portal/product</a>.</p>
</blockquote>
<p>If I understand correctly, the Twitter API document did allow for free-level access. However, I am not sure why it does not work for me.</p>
<p>Does any professional know how to solve this issue?</p>
","python, twitter, nlp, tweepy",
Determining contents of decoder_hidden_states from T5ForConditionalGeneration,"<p>I'm using the Huggingface <code>T5ForConditionalGeneration</code> model without modification.</p>
<p>I want to compute mean pooling over the last hidden state of the T5 decoder, but I can't determine which part of the <code>decoder_hidden_states</code> contains what I'm looking for.</p>
<p>I want to do something like this:</p>
<pre><code># Prepare batch data
sources = batch_df['Source'].tolist()
tokenized_input = self.tokenizer(sources, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length).to('cuda')
input_ids = tokenized_input['input_ids'].to('cuda')
attention_mask = tokenized_input['attention_mask'].to('cuda')

input_batch = {
    'input_ids': input_ids, 
    'attention_mask': attention_mask,
    'do_sample': False,
    'num_beams': 1,
    'eos_token_id': self.tokenizer.eos_token_id,
    'pad_token_id': self.tokenizer.pad_token_id,
    'max_length': self.max_output_length,
    'output_scores': True,
    'return_dict_in_generate': True,
    'output_hidden_states': True,
}
outputs = self.model.generate(**input_batch)

# Retrieve the decoder hidden states
decoder_last_hidden_state = outputs.decoder_hidden_states[-1]  # Last layer's hidden states

# Compute the mean of the hidden states across the sequence length dimension
mean_pooled_output = torch.mean(decoder_last_hidden_state, dim=1, keepdim=False)
</code></pre>
<p>This approach works for the encoder, but for the decoder, <code>decoder_hidden_states[-1]</code> is a tuple of tensors, not a tensor.</p>
<p>When I first inspected the tuples, there were 10 tuples, and each tuple contained 7 tensors.</p>
<p>When I inspected the dimensions, like this:</p>
<pre><code>for tuple_number in range(n):  # Checking the tuples
    print(f&quot;Tuple {layer_number}:&quot;)
    for i, tensor in enumerate(outputs.decoder_hidden_states[layer_number]):
        print(f&quot;  Tuple {i} in Layer {layer_number}: shape {tensor.shape}&quot;)
</code></pre>
<p>the outputs were all like this:</p>
<pre><code>Tuple 0:
  Tensor 0 in Tuple 0: shape torch.Size([2, 1, 512])
  Tensor 1 in Tuple 0: shape torch.Size([2, 1, 512])
  Tensor 2 in Tuple 0: shape torch.Size([2, 1, 512])
  Tensor 3 in Tuple 0: shape torch.Size([2, 1, 512])
  Tensor 4 in Tuple 0: shape torch.Size([2, 1, 512])
  Tensor 5 in Tuple 0: shape torch.Size([2, 1, 512])
  Tensor 6 in Tuple 0: shape torch.Size([2, 1, 512])
Tuple 1:
  Tensor 0 in Tuple 1: shape torch.Size([2, 1, 512])
  Tensor 1 in Tuple 1: shape torch.Size([2, 1, 512])
  Tensor 2 in Tuple 1: shape torch.Size([2, 1, 512])
  Tensor 3 in Tuple 1: shape torch.Size([2, 1, 512])
  Tensor 4 in Tuple 1: shape torch.Size([2, 1, 512])
  Tensor 5 in Tuple 1: shape torch.Size([2, 1, 512])
  Tensor 6 in Tuple 1: shape torch.Size([2, 1, 512])
. . .
</code></pre>
<p>512 is the max_length of my tokenizer, and 2 is my batch size.
(I verified that 2 is the batch size because that number changed when I modified my batch size.)</p>
<p>Then, when I trimmed the length of my input strings to 10 characters, to my surprise, the number of tuples went from 10 to 39. When I trimmed the strings further to only 2 chars per string, the number of tuples didn't increase beyond 39.
Then, when I doubled my input string length instead, the number of tuples went down to 7. So, it appears like the number of tuples corresponds to iterations of the decoder over some chunk size up to some limits.</p>
<p>So, if I wanted to compute mean pooling over the first token, it seems like I'd compute the mean over the last tensor of the first tuple.
However, I don't understand exactly how the token length corresponds to the number of tuples.</p>
<p>How do I determine what exactly is represented by each of these tuples and tensors? I have not been successful in finding this information by going through the T5 source code.</p>
","pytorch, nlp, huggingface-transformers","<p>I think whats happening is that T5 returns the hidden state per step of decoding. Therefore, the number of tuples should correspond to the longest generated sequence. You are most likely interested in the last decoding step and could take the last tuple.</p>
<p>In that tuple you have a tuple of size num_layers + 1 (+1 for the final LayerNorm). The output of the last layer should be the last tuple entry.</p>
"
ValueError: The model did not return a loss from the inputs: During Further Pretraining ARBERT Model,"<p>I am applying further pretraining for the ARBERT model for both the Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) tasks. However, I am receiving the below error. I know this error arises if I don't name my dataset columns as &quot;text&quot; and &quot;labels&quot;. I am already naming my dataset column as &quot;text&quot;. I don't have a &quot;labels&quot; one since it is an unsupervised task. Also, as far as I know, the &quot;labels&quot; here should be the Tokens of MLM and NSP ([MASK], [CLS], [NSP]) which should be handled automatically by the DataCollator function. So, can anyone advice me on this?</p>
<p><strong>ValueError: The model did not return a loss from the inputs, only the following keys: prediction_logits,seq_relationship_logits. For reference, the inputs it received are input_ids,attention_mask,labels</strong>.</p>
<p><em><strong>Note:</strong></em> I receive this error upon initializing the &quot;Trainer&quot;:<br />
<code>trainer_mlm = Trainer(model=ARBERT_model, args=training_args, data_collator=data_collator_mlm, train_dataset=train_dataset_for_mlm) </code></p>
","nlp, huggingface-transformers, arabic, language-model",
LDA over time in R,"<p>I'm trying to apply LDA over a large sample of documents. My dataset is the speeches given by Yellen and Powell taken from <a href=""https://www.bis.org/cbspeeches/index.htm"" rel=""nofollow noreferrer"">this website</a></p>
<p>My code looks like this</p>
<pre><code>library(topicmodels)
library(tidytext)
library(dplyr)
library(ggplot2)
library(quanteda)
library(tm)
library(lsa)
library(lubridate)
bis_speeches=read.csv(&quot;central_bank_speeches.csv&quot;)
bis_speeches=bis_speeches[,c(&quot;date&quot;, &quot;description&quot;,&quot;text&quot;, &quot;author&quot;)]
authors=c(&quot;Janet L Yellen&quot;, &quot;Jerome H Powell&quot;)
PY_speeches=bis_speeches[bis_speeches$author %in% authors,]
PY_speeches$date=as.Date(PY_speeches$date)
positions=c(&quot;Vice Chair&quot;, &quot;CEO&quot;, &quot;President&quot;, &quot;Member&quot;)
PY_speeches &lt;- PY_speeches[!grepl(paste(positions, collapse = &quot;|&quot;), PY_speeches$description), ]
PY_speeches$date_month=substr(PY_speeches$date, 1,7)

PY_speeches_list=split(PY_speeches$text, PY_speeches$date_month)

PY_speeches_list=as.list(PY_speeches_list)

speeches_monthly_corpus &lt;- Corpus(VectorSource(PY_speeches_list))
speeches_monthly_corpus &lt;- tm_map(speeches_monthly_corpus, tolower)
speeches_monthly_corpus &lt;- tm_map(speeches_monthly_corpus, removePunctuation)
speeches_monthly_corpus &lt;- tm_map(speeches_monthly_corpus, function(x) removeWords(x, stopwords(&quot;english&quot;)))
speeches_monthly_corpus &lt;- tm_map(speeches_monthly_corpus, stemDocument, language = &quot;english&quot;)
speeches_monthly_matrix=DocumentTermMatrix(speeches_monthly_corpus, control = list(minWordLength=5, maxWordLength=12))

speeches_lda_results=LDA(speeches_monthly_matrix, k=50, method=&quot;Gibbs&quot;,control=list( seed=141096)) 
</code></pre>
<p>Now the thing is that these speeches have a time component to them, and I'd to know if there's a way for me to get time variation in the topics without having to run LDA over each speech (or a subset of speeches)</p>
<p>Also, please note that the csv I read is the one I download from the link posted</p>
","r, nlp, lda",
"Error with llamaindex NL2SQL, agent gets query 100% right but engine returns no data:","<p>Working on an OpenAI (gpt-3.5-turbo-0613) RAG chat agent and the agent is taking the prompt, accurately translating to the correct SQL query and running it, but I get this response:</p>
<h1>=== Calling Function === Calling function: AG-Database-Query with args: {&quot;input&quot;:&quot;SELECT CLIENT FROM CLIENT_DELIVERY&quot;} Got output: Error: None.H</h1>
<p>Yet, if I copy/paste that query into SSMS I get the proper result. The code uses SQLAlchemy to connect to the database, so I hard-coded the same query against the SQLAlchemy connection and I get a valid result. Somewhere in the libraries of the llama_index agent, to the llama_index query_engine it is losing the ability to connect to get a query response. I'm happy to provide additional code, just wondering if anyone has seen this?</p>
<p>The primary code chunks are:</p>
<pre><code>sql_query_engine = NLSQLTableQueryEngine(
        sql_database = sql_database,
        tables=table_name,
)

sql_tool = QueryEngineTool.from_defaults(
        query_engine=sql_query_engine,
        name=&quot;AG-Database-Query&quot;,
        description=DESCRIPTION,
)
agent = OpenAIAgent.from_tools(
        tools=[sql_tool],
        context=context,
        verbose=True
)
</code></pre>
","sql-server, sqlalchemy, nlp, artificial-intelligence, llama-index",
OpenAI API: What would be a good strategy to handle 80+ function calling?,"<p>My business handles a variety of entities (job, invoice, quote, resource, vehicle, contact, person, message, alert, etc.).</p>
<p>My goal is to use OpenAI function calling to allow my users to ask &quot;anything&quot; about their data. If you roughly estimate 20 entities and 4 potential actions each, that's approximately 80 API calls I need to define in my prompt so GPT can select the one best matching the user's question.</p>
<p>Not only would this consume a significant number of tokens, but it might also confuse the engine.</p>
<p>What are some effective strategies to work around this issue?</p>
<p>Initially, I considered presenting users with a route so they can first select which area of my system they want to inquire about, in order to narrow it down. However, this defeats the purpose of being able to &quot;ask for anything.&quot;</p>
<p>I'm also considering making an initial call to the OpenAI API to identify the entity from a predefined list, followed by a second call with a list of my APIs matching that entity. However, if the user's language doesn't match my predefined list, I may direct the logic down the wrong path. For example, &quot;<em>What is Ryan's phone number?</em>&quot; Is Ryan a resource? A person? A web user? What if the context needs switching because the user asks a follow-up question?</p>
<p><strong>-- Edit --</strong></p>
<p>After Rok's answer I think I need to precise I have over 2,000 customers who will be be asking questions about their own data. Each of these customers could have 100k CRM contacts, 100k jobs completed for these contacts, 100k invoices raised for these jobs etc...</p>
","nlp, tokenize, openai-api, chatgpt-api, gpt-4",
How to get all words from spacy vocab?,"<p>I need all the words from Spacy vocab. Suppose, I initialize my spacy model as </p>

<pre><code>nlp = spacy.load('en')
</code></pre>

<p>How do I get the text of words from <code>nlp.vocab</code>?</p>
","python-3.x, nlp, spacy","<p>You can get it as a list like this:</p>

<pre><code>list(nlp.vocab.strings)
</code></pre>
"
Trouble Implementing Text-to-Sign Language Translation: Need Guidance,"<p>i was working on text to sign language translator using text and video dataset. i was trying to use normal language translation seq2seq model with pytorch. i faced a problem on output dimension of my decoder model.</p>
<p>I was trying the encoder decoder model with attention and getting a dimentionality issues.</p>
","tensorflow, pytorch, nlp, lstm, seq2seq",
How to calculate the frequency of bigrams on fixed size windows,"<p>I am computing the <strong>frequency of bigrams</strong> given a list of token files <code>tokenized_corpus = ['tokens_A.pickle', 'tokens_B.pickle', ...]</code> where every <code>tokens_X</code> file unpickles as <code>['x', 'a', 'b', 'a', 'b', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'ñ']</code>. A list of bigrams as a list of tuples as <code>input_bigrams=[('o', 'q'), ('x', 'q'), ('x', 'b'), ('a', 'b')]</code> and a window size as <code>window_size = 5</code>.</p>
<p>The expected output for windows of size 5 are:
<code>['x', 'a', 'b', 'a', 'b'], ['a', 'b', 'a', 'b', 'd']...</code> and so on. Then for this case the bigrams <code>('x', 'b')</code> and <code>('a', 'b')</code> count's to <code>1</code>.</p>
<p><strong>My goal</strong> is to <strong>optimize</strong> the following function in order avoid RAM overload because I will be running several pickle files (1GB each). <strong>My approach</strong> is the following:</p>
<pre class=""lang-py prettyprint-override""><code>def generate_word_windows(tokens, window_size):
    for i in range(len(tokens) - window_size + 1):
        yield tokens[i:i+window_size]

def bigram_frequencies(input_bigrams, tokenized_corpus, window_size):
    bigram_frequency = defaultdict(int)
    for token_file in tokenized_corpus:
        with open(token_file, 'rb') as file:
            while True:
                try:
                    tokens = pickle.load(file)
                    for window in generate_word_windows(tokens, window_size):
                        for bigram in input_bigrams:
                            if all(word in window for word in bigram):
                                bigram_frequency[bigram] += 1
                except EOFError:
                    break
    return bigram_frequency

input_bigrams = [('o', 'q'), ('x', 'q'), ('x', 'b'), ('a', 'b')]
tokenized_corpus = ['tokens_A.pickle', 'tokens_B.pickle']
window_size = 5

bigram_distributions = bigram_frequencies(input_bigrams, tokenized_corpus, window_size)

print('Output:')
print(*[f'{word}\n' for word in bigram_distributions.items()])

</code></pre>
<p>What I'm doing is to iterate over the pickle files, create a window of size 5 and iterate over it. Then, for all windows I iterate over every bigram and check if the words in every bigram exist in the window, if it does then I count it.</p>
<p>How can I improve this program?</p>
","python, nlp, n-gram",
Feedback on spaCy NER Model Annotations for Recipe Ingredient Extraction,"<p>I'm training a spaCy NER model to specifically identify and categorize lines of ingredients in recipes. The goal is to accurately extract ingredients along with their quantities, units, and preparation instructions from various recipes. Below is an outline of how I'm annotating the data:</p>
<h3>Spanning Labels</h3>
<ul>
<li><strong>Quantity:</strong> Digits related to a unit (e.g., &quot;2&quot;, &quot;3/4&quot;)</li>
<li><strong>Ingredient:</strong> Actual ingredient name (e.g., &quot;sugar&quot;, &quot;milk&quot;)</li>
<li><strong>Measurement Unit:</strong> The unit of measurement (e.g., &quot;cup&quot;, &quot;tbsp&quot;)</li>
<li><strong>Instruction:</strong> Preparation instructions (e.g., &quot;finely diced&quot;, &quot;divided&quot;)</li>
</ul>
<h3>Relationship Labels</h3>
<ul>
<li><strong>quantity_of:</strong> Relates the span label quantity to the ingredient</li>
<li><strong>action_to:</strong> Relates the span label instruction to the ingredient</li>
<li><strong>unit_of:</strong> Relates the span label measurement unit to the quantity</li>
</ul>
<p>I've provided two examples: one simple ingredient line and one complex one.</p>
<p><a href=""https://i.sstatic.net/08yFkwCY.png"" rel=""nofollow noreferrer"">Simple Line</a></p>
<p><a href=""https://i.sstatic.net/fj2kSO6t.png"" rel=""nofollow noreferrer"">Complex Line</a></p>
<p>I'm seeking feedback on a couple of points:</p>
<ol>
<li><strong>Annotation Overkill:</strong> Are there too many categories or overly detailed annotations for effective learning and practical application? Should some categories be merged or omitted?</li>
<li><strong>Training Data Volume:</strong> How much annotated data is typically recommended to achieve a robust model in this type of NER task? I want to ensure the model is reliable across various recipe formats.</li>
</ol>
<p>What I've tried:
I trained a model using a similar labeling set-up, minus the relationship labels and the &quot;Measurement Unit&quot; spanning label. I trained the model on about 700 samples from random recipes. The results were subpar; the model had a hard time identifying certain aspects, specifically in situations where the quantities and their units were not next to each other (e.g. 3 garlic cloves)</p>
","machine-learning, nlp, spacy, named-entity-recognition",
How do I test whether an nltk resource is already installed on the machine running my code?,"<p>I just started my first NLTK project and am confused about the proper setup. I need several resources like the Punkt Tokenizer and the maxent pos tagger. I myself downloaded them using the GUI <code>nltk.download()</code>. For my collaborators I of course want that this things get downloaded automatically. I haven't found any idiomatic code for that in the docu. </p>

<p>Am I supposed to just put <code>nltk.data.load('tokenizers/punkt/english.pickle')</code> and their like into the code? Is this going to download the resources every time the script is run? Am I to provide feedback to the  user (i.e. my co-developers) of what is being downloaded and why this is taking so long? There MUST be gear out there that does the job, right? :)</p>

<p>//Edit To explify my question: <br>
<strong>How do I test whether an nltk resource (like the Punkt Tokenizer) is already installed on the machine running my code, and install it if it is not?</strong> </p>
","python, nlp, nltk","<p>You can use the <code>nltk.data.find()</code> function, see <a href=""https://github.com/nltk/nltk/blob/develop/nltk/data.py"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/data.py</a>:</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.data.find('tokenizers/punkt.zip')
ZipFilePathPointer(u'/home/alvas/nltk_data/tokenizers/punkt.zip', u'')
</code></pre>

<p>When the resource is not available you'll find the error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/nltk-3.0a3-py2.7.egg/nltk/data.py"", line 615, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource u'punkt.zip' not found.  Please use the NLTK Downloader
  to obtain the resource:  &gt;&gt;&gt; nltk.download()
  Searched in:
    - '/home/alvas/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************
</code></pre>

<p>Most probably, you would like to do something like this to ensure that your collaborators have the package:</p>

<pre><code>&gt;&gt;&gt; try:
...     nltk.data.find('tokenizers/punkt')
... except LookupError:
...     nltk.download('punkt')
... 
[nltk_data] Downloading package punkt to /home/alvas/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
True
</code></pre>
"
Transformer: cannot import name &#39;AutoModelWithLMHead&#39; from &#39;transformers&#39;,"<p>I was referring to this answer from stackoverflow but I can't get any leads regarding my problem: [https://stackoverflow.com/questions/63141267/importerror-cannot-import-name-automodelwithlmhead-from-transformers][1]</p>
<p>This is the code that I ran:</p>
<pre><code>import transformers
from transformers import AutoModelWithLMHead
</code></pre>
<p>Results:</p>
<pre><code>cannot import name 'AutoModelWithLMHead' from 'transformers' (/Users/xev/opt/anaconda3/lib/python3.7/site-packages/transformers/__init__.py)
</code></pre>
<p>My transformer version is '3.0.2'.
My import for AutoTokenizer is fine.</p>
<p>Would appreciate if there's anyone who can help regarding the Transformers package!
[1]: <a href=""https://stackoverflow.com/questions/63141267/importerror-cannot-import-name-automodelwithlmhead-from-transformers"">ImportError: cannot import name &#39;AutoModelWithLMHead&#39; from &#39;transformers&#39;</a></p>
","python, nlp, huggingface-transformers",
How to remove duplicate sentences from paragraph using NLTK?,"<p>I had a huge document with many repeated sentences such as (footer text, hyperlinks with alphanumeric chars), I need to get rid of those repeated hyperlinks or Footer text. I have tried with the below code but unfortunately couldn't succeed. Please review and help.</p>

<pre><code>corpus = ""We use file handling methods in python to remove duplicate lines in python text file or function. The text file or function has to be in the same directory as the python program file. Following code is one way of removing duplicates in a text file bar.txt and the output is stored in foo.txt. These files should be in the same directory as the python script file, else it won’t work.Now, we should crop our big image to extract small images with amounts.In terms of topic modelling, the composites are documents and the parts are words and/or phrases (phrases n words in length are referred to as n-grams).We use file handling methods in python to remove duplicate lines in python text file or function.As an example I will use some image of a bill, saved in the pdf format. From this bill I want to extract some amounts.All our wrappers, except of textract, can’t work with the pdf format, so we should transform our pdf file to the image (jpg). We will use wand for this.Now, we should crop our big image to extract small images with amounts.""

from nltk.tokenize import sent_tokenize
sentences_with_dups = []
for sentence in corpus:
    words = sentence.sent_tokenize(corpus)
    if len(set(words)) != len(words):
        sentences_with_dups.append(sentence)
        print(sentences_with_dups)
    else:
        print('No duplciates found')
</code></pre>

<p>Error message for the above code :</p>

<pre><code>AttributeError: 'str' object has no attribute 'sent_tokenize'
</code></pre>

<p>Desired Output :</p>

<pre><code>Duplicates = ['We use file handling methods in python to remove duplicate lines in python text file or function.','Now, we should crop our big image to extract small images with amounts.']

Cleaned_corpus = {removed duplicates from corpus}
</code></pre>
","python-3.x, nlp, nltk",
partially initialized module &#39;keras&#39; has no attribute &#39;__version__&#39;,"<p>what should i do
Traceback (most recent call last):
File &quot;E:\pyfile\bert_for_ner\build_model.py&quot;, line 4, in 
from keras import backend as K
File &quot;D:\python3.11.7\Lib\site-packages\keras_<em>init</em>_.py&quot;, line 3, in 
from . import utils
File &quot;D:\python3.11.7\Lib\site-packages\keras\utils_<em>init</em>_.py&quot;, line 26, in 
from .vis_utils import model_to_dot
File &quot;D:\python3.11.7\Lib\site-packages\keras\utils\vis_utils.py&quot;, line 7, in 
from ..models import Model
File &quot;D:\python3.11.7\Lib\site-packages\keras\models.py&quot;, line 10, in 
from .engine.input_layer import Input
File &quot;D:\python3.11.7\Lib\site-packages\keras\engine_<em>init</em>_.py&quot;, line 8, in 
from .training import Model
File &quot;D:\python3.11.7\Lib\site-packages\keras\engine\training.py&quot;, line 14, in 
from . import training_utils
File &quot;D:\python3.11.7\Lib\site-packages\keras\engine\training_utils.py&quot;, line 17, in 
from .. import metrics as metrics_module
File &quot;D:\python3.11.7\Lib\site-packages\keras\metrics.py&quot;, line 1850, in 
BaseMeanIoU = tf.keras.metrics.MeanIoU
^^^^^^^^^^^^^^^^
File &quot;D:\python3.11.7\Lib\site-packages\tensorflow\python\util\lazy_loader.py&quot;, line 181, in <strong>getattr</strong>
self._initialize()
File &quot;D:\python3.11.7\Lib\site-packages\tensorflow\python\util\lazy_loader.py&quot;, line 149, in _initialize
if keras.<strong>version</strong>.startswith(&quot;3.&quot;):
^^^^^^^^^^^^^^^^^
AttributeError: partially initialized module 'keras' has no attribute '<strong>version</strong>' (most likely due to a circular import)</p>
<p>the whole code:
#! -<em>- coding: utf-8 -</em>-</p>
<p>from keras import backend as K</p>
<p>class SetLearningRate:
&quot;&quot;&quot;
层的一个包装，用来设置当前层的学习率
&quot;&quot;&quot;
def <strong>init</strong>(self, layer, lamb, is_ada=False):
self.layer = layer
self.lamb = lamb # 学习率比例
self.is_ada = is_ada # 是否自适应学习率优化器</p>
<pre><code>def __call__(self, inputs):
    with K.name_scope(self.layer.name):
        if not self.layer.built:
            input_shape = K.int_shape(inputs)
            self.layer.build(input_shape)
            self.layer.built = True
            if self.layer._initial_weights is not None:
                self.layer.set_weights(self.layer._initial_weights)
    for key in ['kernel', 'bias', 'embeddings', 'depthwise_kernel', 'pointwise_kernel', 'recurrent_kernel', 'gamma', 'beta']:
        if hasattr(self.layer, key):
            weight = getattr(self.layer, key)
            if self.is_ada:
                lamb = self.lamb # 自适应学习率优化器直接保持lamb比例
            else:
                lamb = self.lamb**0.5 # SGD（包括动量加速），lamb要开平方
            K.set_value(weight, K.eval(weight) / lamb) # 更改初始化
            setattr(self.layer, key, weight * lamb) # 按比例替换
    return self.layer(inputs)
</code></pre>
<p>def bert_bilstm_crf(config_path,checkpoint_path,num_labels,lstm_units,drop_rate,leraning_rate):
import keras
from bert4keras.models import build_transformer_model
from bert4keras.optimizers import Adam
from bert4keras.layers import ConditionalRandomField
bert = build_transformer_model(
config_path = config_path,
checkpoint_path = checkpoint_path,
model = 'albert',
return_keras_model = False # True，x = bert.output
)
x = bert.model.output # [batch_size, seq_length, 768]
lstm = SetLearningRate(
keras.layers.Bidirectional(
keras.layers.LSTM(
lstm_units,
kernel_initializer='he_normal',
return_sequences=True
)
),
500,
True
)(x) # [batch_size, seq_length, lstm_units * 2]</p>
<pre><code>x = keras.layers.concatenate(
        [lstm,x],
        axis=-1
    ) # [batch_size, seq_length, lstm_units * 2 + 768]

x = keras.layers.TimeDistributed(
        keras.layers.Dropout(drop_rate)
    )(x) # [batch_size, seq_length, lstm_units * 2 + 768]

x = SetLearningRate(
        keras.layers.TimeDistributed(
            keras.layers.Dense(
                    num_labels,
                    activation='relu',
                    kernel_initializer='he_normal',
                )
        ),
        500,
        True
    )(x) # [batch_size, seq_length, num_labels]

crf = ConditionalRandomField(lr_multiplier=500)
output = crf(x)

model = keras.models.Model(bert.input, output)
# model.summary()
model.compile(
        loss=crf.sparse_loss,
        optimizer=Adam(leraning_rate),
        metrics=[crf.sparse_accuracy]
    )

return model,crf
</code></pre>
<p>if <strong>name</strong> == '<strong>main</strong>':
config_path = 'E:/pyfile/bert_for_ner/chinese_L-12_H-768_A-12/bert_config.json'
checkpoint_path = 'E:/pyfile/bert_for_ner/chinese_L-12_H-768_A-12/bert_model.ckpt'
num_labels = 21
lstm_units = 128
drop_rate = 0.1
leraning_rate = 5e-5
model , crf = bert_bilstm_crf(
config_path,checkpoint_path,num_labels,lstm_units,drop_rate,leraning_rate)
print(model.summary())</p>
","nlp, lstm, bert-language-model, crf",
Tensorflow.js tokenizer,"<p>I'm new to Machine Learning and Tensorflow, since I don't know python so I decide to use there javascript version (maybe more like a wrapper). </p>

<p>The <strong>problem</strong> is I tried to build a model that process the Natural Language. So the first step is tokenizer the text in order to feed the data to model. I did a lot research, but most of them are using python version of tensorflow that use method like: <code>tf.keras.preprocessing.text.Tokenizer</code> which I can't find similar in tensorflow.js. I'm stuck in this step and don't know how can I transfer text to vector that can feed to model. Please help :)</p>
","javascript, machine-learning, tensorflow.js, nlp","<p>To transform text to vectors, there are lots of ways to do it, all depending on the use case. The most intuitive one, is the one using the term frequency, i.e , given the vocabulary of the corpus (all the words possible), all text document will be represented as a vector where each entry represents the occurrence of the word in text document.</p>

<p>With this vocabulary :</p>

<pre><code>[""machine"", ""learning"", ""is"", ""a"", ""new"", ""field"", ""in"", ""computer"", ""science""]
</code></pre>

<p>the following text: </p>

<pre><code>[""machine"", ""is"", ""a"", ""field"", ""machine"", ""is"", ""is""] 
</code></pre>

<p>will be transformed as this vector: </p>

<pre><code>[2, 0, 3, 1, 0, 1, 0, 0, 0] 
</code></pre>

<p>One of the disadvantage of this technique is that there might be lots of 0 in the vector which has the same size as the vocabulary of the corpus. That is why there are others techniques. However the <a href=""https://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""noreferrer""><strong>bag of words</strong></a> is often referred to. And there is a slight different version of it using <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""noreferrer""><strong>tf.idf</strong></a></p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const vocabulary = [""machine"", ""learning"", ""is"", ""a"", ""new"", ""field"", ""in"", ""computer"", ""science""]
const text = [""machine"", ""is"", ""a"", ""field"", ""machine"", ""is"", ""is""] 
const parse = (t) =&gt; vocabulary.map((w, i) =&gt; t.reduce((a, b) =&gt; b === w ? ++a : a , 0))
console.log(parse(text))</code></pre>
</div>
</div>
</p>

<p>There is also the following <a href=""https://github.com/NaturalNode/natural#tokenizers"" rel=""noreferrer"">module</a> that might help to achieve what you want</p>
"
Machine Learning Libraries For Android,"<p>I am trying build a small text mining tool for my android app. I am checking for a machine learning library that will allow me to cluster, classify etc.</p>

<p>Are there any machine learning libraries for android? I came across tensorflow but I need a bit more access to common ML functions.</p>
","android, machine-learning, nlp, artificial-intelligence, text-mining","<p>You can try these ports of weka for android (they do not use the latest weka version, but it may be sufficient for your needs):</p>

<p><a href=""https://github.com/rjmarsan/Weka-for-Android"" rel=""nofollow noreferrer"">https://github.com/rjmarsan/Weka-for-Android</a></p>

<p><a href=""https://github.com/andrecamara/weka-android"" rel=""nofollow noreferrer"">https://github.com/andrecamara/weka-android</a></p>
"
Flan-T5 params clarification,"<p>this is a general guidance question. I want a clear explanation of 2 parameters of Flan-T5 :</p>
<ul>
<li>max_length</li>
<li>num_return_sequences</li>
</ul>
<p>Also what is the input limit for flan-t5 ?</p>
","nlp, large-language-model",
"When I try to perform testing in my knowledge base, the model does not load in amazon bedrock","<p>When I try to perform testing using my knowledge base for RAG in amazon bedrock, when I have to select the model, it keeps loading forever</p>
<p>I tried activating the claude model by antrophic, because I know that is the only model available in amazon bedrock</p>
","amazon-web-services, nlp, large-language-model, amazon-bedrock",
Performance of textSimilarity() from R&#39;s text library,"<p>I have a large <code>data.frame</code> with about 4 million rows and 2 columns.</p>
<p>The two columns contain long character strings, texts representing recipes.</p>
<p>For each row, I am comparing the similarity of the recipes in column A and column B, using textSimilarity() from the <code>text</code> library in R.</p>
<p>I like the textSimilarity() function, because it uses text embeddings that &quot;comprises values that represent the latent meaning of a word&quot;.</p>
<p>Yet, performance is very slow. Are there ways of speeding this up? Or am I coding this wrong? Can/should I set this up in parallel? Can I use my GPU?</p>
<p>Example data - with way shorter texts:</p>
<pre><code>df &lt;- data.frame(
columnA= c(&quot;tomato sauce is very tasty to use&quot;, &quot;without garlic, this dish is not chinese&quot;, &quot;British food is as tasteless as it can get&quot;), 
columnB= c(&quot;pizza is the source of life&quot;, &quot;a nice xiaolongbao is steamed until it is soft&quot;, &quot;braised pork can be very healthy if prepared well&quot;)
)
</code></pre>
<pre><code>&gt; df
                                     columnA                                           columnB
1          tomato sauce is very tasty to use                       pizza is the source of life
2   without garlic, this dish is not chinese    a nice xiaolongbao is steamed until it is soft
3 British food is as tasteless as it can get braised pork can be very healthy if prepared will
</code></pre>
<p>To get the similarity, I use:</p>
<pre><code>df$sim &lt;- textSimilarity(textEmbed(df$columnA)$texts$texts , textEmbed(df$columnB)$texts$texts)
</code></pre>
<p>In the current set-up, this process takes days rather than hours. How to speed this up? Parallelization? GPU? Or are there alternatives?</p>
","r, performance, nlp, huggingface-transformers","<h3>Contextual embedding models are computationally expensive</h3>
<p>The default model used by the R <code>text</code> package is <a href=""https://huggingface.co/blog/bert-101"" rel=""nofollow noreferrer""><code>bert-base-uncased</code></a>, which has 110m parameters, including a 12 layer feed-forward neural network.</p>
<p>To compare similarity of sentences, each sentence is split into tokens (which are words or parts of words) and each token is represented as a 768-dimensional vector. Then the token vectors from each sentence are aggregated to create a single, 768-dimensional vector to represent that sentence in vector space. The idea is that with decent token (or word) embeddings and a sensible way of aggregating them this representation captures meaning, so similar sentences appear closer together in vector space. You can then calculate the distance between sentence vectors as a measure of their semantic similarity.</p>
<p>The expensive step is creating the token vectors. Unlike older models such as Word2Vec, with BERT the vector representation of each token depends on the context. A <a href=""https://ubiai.tools/from-words-to-vectors-a-dive-into-spacy-transformers-for-embeddings/"" rel=""nofollow noreferrer"">canonical example</a> of this is that the representation of the first word in <em>Apple Inc. was founded by Steve Jobs</em> will not be the same as the first word in <em>Apple is my favourite fruit</em>. This is a strength of this family of models. These sentences should not appear close together in vector space. But calculating the representation of each token based on the others around it, and working out which tokens affect the meaning of other tokens, is computationally expensive.</p>
<h3>Use a more lightweight context-dependent model</h3>
<p>A good way to speed up things a lot without sacrificing too much accuracy is by using a more lightweight model, e.g. <a href=""https://huggingface.co/docs/transformers/en/model_doc/distilbert"" rel=""nofollow noreferrer""><code>distilbert</code></a> which has 66m parameters and 6 rather than 12 feed-forward neural network layers.</p>
<p>This is still a transformer model. You still get context-specific embeddings with an attention mechanism to appropriately weight each token based on the surrounding ones. With distilled models trained on larger models like BERT, the results tend to be fairly close to the model from which they're derived, and they're much faster:</p>
<blockquote>
<p>DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than google-bert/bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.</p>
</blockquote>
<h3>Benchmarking the default model</h3>
<p>We need a little more text to measure the performance. I'll use the <a href=""https://paperswithcode.com/dataset/biosses"" rel=""nofollow noreferrer"">Biomedical Semantic Similarity Estimation System</a> (BIOSSES) dataset of 100 sentence pairs.</p>
<pre class=""lang-r prettyprint-override""><code># Read in the data
biosses  &lt;- jsonlite::read_json(&quot;https://datasets-server.huggingface.co/rows?dataset=biosses&amp;config=default&amp;split=train&amp;offset=0&amp;length=100&quot;)
df2  &lt;- data.frame(
    sentence1 = sapply(biosses$rows, \(row) row$row$sentence1),
    sentence2 = sapply(biosses$rows, \(row) row$row$sentence2)
)

# Define function to calculate similarity
get_similarity &lt;- function(model, dat = df2, col1 = &quot;sentence1&quot;, col2 = &quot;sentence2&quot;) {
    embeds_a &lt;- text::textEmbed(dat[[col1]], model = model)
    embeds_b &lt;- text::textEmbed(dat[[col2]], model = model)

    text::textSimilarity(
        embeds_a$texts$texts,
        embeds_b$texts$texts
    )
}
</code></pre>
<p>No need to microbenchmark here as it takes so long (about 16 minutes):</p>
<pre class=""lang-r prettyprint-override""><code>system.time(
    base_result &lt;- get_similarity(model = &quot;bert-base-uncased&quot;)
) # 956.136 seconds
</code></pre>
<h3>Speed of smaller BERT models</h3>
<p>Let's try three other BERT derivatives:</p>
<ol>
<li><a href=""https://huggingface.co/distilbert/distilbert-base-uncased"" rel=""nofollow noreferrer"">DistilBERT</a>.</li>
<li><a href=""https://huggingface.co/muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu"" rel=""nofollow noreferrer"">Bert tiny intel CPU optimized</a>.</li>
<li><a href=""https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad"" rel=""nofollow noreferrer"">DistilBERT distilled</a>.</li>
</ol>
<pre class=""lang-r prettyprint-override""><code>models &lt;- c(
    &quot;distilbert&quot; = &quot;distilbert-base-uncased&quot;,
    &quot;tiny_distilbert&quot; = &quot;muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu&quot;,
    &quot;distilled_distilbert&quot; = &quot;distilbert/distilbert-base-uncased-distilled-squad&quot;
)

l &lt;- lapply(models, \(model)
    list(
        time = system.time(x &lt;- get_similarity(model))[&quot;elapsed&quot;],
        result = x,
        diff_from_base = abs(base_result - x)
    )
)
</code></pre>
<p>To compare the results:</p>
<pre class=""lang-r prettyprint-override""><code>sapply(l, \(x) x$time)
# distilbert.elapsed      tiny_distilbert.elapsed distilled_distilbert.elapsed 
#             622.071                       61.956                      569.000 
</code></pre>
<p>So the tiny model is by far the fastest at just over a minute to run, with the other two much slower at around 9-10 mins. All are substantially faster than the base model.</p>
<h3>Accuracy of smaller BERT models</h3>
<p>Of course, speed is not everything. Let's compare the distribution of similarity scores to the <code>bert-base-uncased</code> scores:</p>
<pre class=""lang-r prettyprint-override""><code>res  &lt;- cbind(sentence = seq(nrow(df2)), data.frame(l)) |&gt;
    tidyr::pivot_longer(
        cols = !sentence,
        names_pattern = &quot;(.+)\\.(.+)&quot;,
        names_to = c(&quot;model&quot;, &quot;var&quot;)
    ) |&gt;
    tidyr::pivot_wider(
        names_from = var
    )

library(ggplot2)
ggplot(res) +
    geom_density(aes(x = diff_from_base, color = model, fill = model), alpha = 0.2) +
    theme(legend.position = &quot;bottom&quot;) +
    labs(x = &quot;Absolute difference&quot;, title = &quot;Distribution of differences from bert-base-uncased similarity score&quot;)
</code></pre>
<p><a href=""https://i.sstatic.net/A2GIOgq8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A2GIOgq8.png"" alt=""enter image description here"" /></a></p>
<p>Interestingly, the fastest model also appears to have the closest results to the base model, at least on this task. You may want to do more robust checks with a subset of your actual data to find the optimal trade-off between speed and similarity to BERT (or whatever your gold standard of similarity is).</p>
<h3>Other approaches to speeding up this task</h3>
<h3>Parallel processing</h3>
<p>You suggested using parallel processing. This should not speed things up, as under the hood, <code>text::textEmbed()</code> wraps the Python Hugging Face <code>transformers</code> library, which in turn calls the Python <code>torch</code> library, which already multi-threaded by default. When I run this code, CPU utilisation spikes for all cores. You might want to check the same happens on your machine but I suspect it will.</p>
<h4>GPU</h4>
<p>You suggested using a GPU. If you have one you should use it. This may speed things up significantly, though how much depends on the model. It looks like the way to do this is to call <code>text::textEmbed(..., device = &quot;gpu&quot;)</code>. I would not feel completely confident this will work out of the box though. Getting <code>torch</code> to recognise a GPU generally requires <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">specifying</a> the Cuda version during installation. This cannot be done for you from R when you run <code>text::textrpp_initialize()</code>, as it will not know which hardware or drivers you have installed. You might want to enter the Python <code>conda</code> virtual environment created by the R <code>text</code> package and check whether <code>torch.cuda.is_available() == True</code>. If not, see <a href=""https://stackoverflow.com/questions/60987997/why-torch-cuda-is-available-returns-false-even-after-installing-pytorch-with"">this question</a> for how to find out whether your GPU is compatible with Cuda and if so how to find the compatible <code>torch</code> version.</p>
<h3>A note on using R</h3>
<p>There is a cost to using R here. I replicated these results in Python and it was about twice as fast again. Each sentence is represented as a 768 dimensional vector. There's some work going on to take the Python list of <code>Torch.tensor</code>s representing each sentence, coerce to a <code>numpy</code> array, then pass this to R where ultimately it's stored as a row of a data frame with 768 columns. I suspect that as the size of the data increases, the time spent on translating the vectors from Python to R reduces as a proportion of the total time, but it would certainly be somewhat faster still to do all this in Python.</p>
<p>Nevertheless, if <a href=""https://huggingface.co/muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu"" rel=""nofollow noreferrer""><code>muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu</code></a> will do the job and there are benefits to keeping everything in R, hopefully a sixteen-fold speed up is enough to be able to do this.</p>
"
AttributeError: &#39;Namespace&#39; object has no attribute &#39;inference_server_address&#39; in Python script,"<p>Hello Stack Overflow community,</p>
<p>I am encountering an error while trying to run the 'lighteval' framework to evaluate &quot;meta-llama/Llama-2-7b-chat-hf&quot; on the &quot;GSM8K&quot; dataset.</p>
<p>I use accelarate as an entry point (run_evals_accelerate.py). This is the relevant part of the code, that gives me the error:</p>
<pre><code>--model_args &quot;meta-llama/Llama-2-7b-chat-hf&quot; \
--model_dtype &quot;torch.float16&quot; \
--tasks &quot;lighteval|gsm8k|0|0&quot; \
--override_batch_size 1 \
--output_dir '/content/drive/MyDrive/thesis'
</code></pre>
<p>I'm following the instructions from the huggingface\lighteval repository step by step.</p>
<p>The error I get is this: Traceback (most recent call last):</p>
<pre><code>  File &quot;/content/lighteval/run_evals_accelerate.py&quot;, line 82, in &lt;module&gt;
    main(args)
  File &quot;/usr/local/lib/python3.10/dist-packages/lighteval/logging/hierarchical_logger.py&quot;, line 166, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/lib/python3.10/dist-packages/lighteval/main_accelerate.py&quot;, line 73, in main
    model_config = create_model_config(args=args, accelerator=accelerator)
  File &quot;/usr/local/lib/python3.10/dist-packages/lighteval/models/model_config.py&quot;, line 273, in create_model_config
    if args.inference_server_address is not None and args.model_args is not None:
AttributeError: 'Namespace' object has no attribute 'inference_server_address'
Traceback (most recent call last):
  File &quot;/usr/local/bin/accelerate&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
  File &quot;/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py&quot;, line 46, in main
    args.func(args)
  File &quot;/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py&quot;, line 1075, in launch_command
    simple_launcher(args)
  File &quot;/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py&quot;, line 681, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
</code></pre>
<p>I have checked the relevant code around line 73 in main_accelerate.py, but I'm not sure how to address this issue. Has anybody encountered a similar problem or has ideas on what might be causing it? I would be grateful for your input.</p>
<p>Thank you in advance for your help!</p>
","python, nlp, accelerate",
(TraceBack (most recent call last)):Error When Trying to Download en_core_web_md from spacy in CommandPrompt,"<p>I recently started a tutorial on nlp and we i reached word vectors, the spacy library was required specifically en_core_web_md the , in this part i keep getting this error (when i try to download en_core_web_md from spacy it always returns this error)
I use the command <code>python -m spacy download en_core_web_md</code>in command Prompt</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connectionpool.py&quot;, line 403, in _make_request
    self._validate_conn(conn)
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connectionpool.py&quot;, line 1053, in _validate_conn
    conn.connect()
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connection.py&quot;, line 419, in connect
    self.sock = ssl_wrap_socket(
                ^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\util\ssl_.py&quot;, line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
               ^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\util\ssl_.py&quot;, line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\ssl.py&quot;, line 517, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\ssl.py&quot;, line 1075, in _create
    self.do_handshake()
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\ssl.py&quot;, line 1346, in do_handshake
    self._sslobj.do_handshake()
TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\adapters.py&quot;, line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connectionpool.py&quot;, line 798, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\util\retry.py&quot;, line 550, in increment
    raise six.reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\packages\six.py&quot;, line 770, in reraise
    raise value
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connectionpool.py&quot;, line 714, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connectionpool.py&quot;, line 406, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\urllib3\connectionpool.py&quot;, line 357, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;&lt;frozen runpy&gt;&quot;, line 198, in _run_module_as_main
  File &quot;&lt;frozen runpy&gt;&quot;, line 88, in _run_code
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\spacy\__main__.py&quot;, line 4, in &lt;module&gt;
    setup_cli()
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\spacy\cli\_util.py&quot;, line 87, in setup_cli
    command(prog_name=COMMAND)
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\click\core.py&quot;, line 1157, in __call__
    return self.main(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\typer\core.py&quot;, line 783, in main
    return _main(
           ^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\typer\core.py&quot;, line 225, in _main
    rv = self.invoke(ctx)
         ^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\click\core.py&quot;, line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\click\core.py&quot;, line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\click\core.py&quot;, line 783, in invoke
    return __callback(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\typer\main.py&quot;, line 683, in wrapper
    return callback(**use_params)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\spacy\cli\download.py&quot;, line 43, in download_cli
    download(model, direct, sdist, *ctx.args)
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\spacy\cli\download.py&quot;, line 77, in download
    compatibility = get_compatibility()
                    ^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\spacy\cli\download.py&quot;, line 122, in get_compatibility
    r = requests.get(about.__compatibility__)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\api.py&quot;, line 73, in get
    return request(&quot;get&quot;, url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\api.py&quot;, line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\sessions.py&quot;, line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\sessions.py&quot;, line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\adapters.py&quot;, line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=None)
</code></pre>
<p>I have not tried any thing since i cant find any solution online
other than asking AI but it says i should check  my internet connection and there is nothing wrong with my network. the other option was to download it directly from the website
can some one help me with another solution</p>
","python, nlp, command-prompt, spacy",
Custom spaCy NLP model inside Rasa SpacyNLP pipeline,"<p>I am trying to integrate a custom NER model for my Rasa chatbot but I am having a hard time understanding how the SpacyTokenizer and SpacyFeaturizer in the SpacyNLP pipeline in Rasa are related to my custom NER component (en_CustomNer). I am asking this because my trained model only has &quot;tok2vec&quot; and &quot;ner&quot; components in its pipeline and I am not sure how this affects the initialization of SpacyTokenizer and SpacyFeaturizer in my chatbot pipeline. Here is my config.yml in Rasa (I am using Rasa version 3.6.18 and spaCy version 3.7.4):</p>
<pre><code>language: en

pipeline:
    - name: SpacyNLP
      model: en_CustomNer  ## my custom NER model
    - name: SpacyTokenizer
    - name: SpacyFeaturizer
      pooling: mean
    - name: LexicalSyntacticFeaturizer
    - name: CountVectorsFeaturizer
    - name: CountVectorsFeaturizer
      analyzer: char_wb
      min_ngram: 2
      max_ngram: 4
    - name: DIETClassifier
      epochs: 150
      constrain_similarities: true
    - name: SpacyEntityExtractor
    - name: FallbackClassifier
      threshold: 0.1
      ambiguity_threshold: 0.1
</code></pre>
<p>I have trained my NER model using spaCy by creating a base-config.cfg file using the basic tutorial at <a href=""https://spacy.io/usage/training"" rel=""nofollow noreferrer"">here</a>. I then followed a this  tutorial on Rasa Blog <a href=""https://stackoverflow.com"">rasa-spacy-integration</a> and managed to make it work inside my Rasa pipeline. However, the Rasa blog post I followed takes a different approach and uses a pre-trained pipeline, only replacing the &quot;ner&quot; component for the custom spaCy model. This result in a larger model that has component like &quot;tagger&quot;, &quot;parser&quot;, &quot;lemmatizer&quot; on top of the 2 components (&quot;tok2vec&quot; and &quot;ner&quot;) my model has. I am wondering if there is a reason for this (maybe the Rasa pipeline makes use of the other pre-trained components?) and if there is a problem with my custom NER model only having these 2 components (tok2vec and ner)? Here is the full config.cfg I used for training my model:</p>
<pre><code>[paths]
train = &quot;./ner/train.spacy&quot;
dev = &quot;./ner/test.spacy&quot;
vectors = &quot;en_core_web_lg&quot;
init_tok2vec = null

[system]
gpu_allocator = null
seed = 0

[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;]
batch_size = 1000
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}
vectors = {&quot;@vectors&quot;:&quot;spacy.Vectors.v1&quot;}

[components]

[components.ner]
factory = &quot;ner&quot;
incorrect_spans_key = null
moves = null
scorer = {&quot;@scorers&quot;:&quot;spacy.ner_scorer.v1&quot;}
update_with_oracle_cut_size = 100

[components.ner.model]
@architectures = &quot;spacy.TransitionBasedParser.v2&quot;
state_type = &quot;ner&quot;
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = true
nO = null

[components.ner.model.tok2vec]
@architectures = &quot;spacy.Tok2VecListener.v1&quot;
width = ${components.tok2vec.model.encode.width}
upstream = &quot;*&quot;

[components.tok2vec]
factory = &quot;tok2vec&quot;

[components.tok2vec.model]
@architectures = &quot;spacy.Tok2Vec.v2&quot;

[components.tok2vec.model.embed]
@architectures = &quot;spacy.MultiHashEmbed.v2&quot;
width = ${components.tok2vec.model.encode.width}
attrs = [&quot;NORM&quot;,&quot;PREFIX&quot;,&quot;SUFFIX&quot;,&quot;SHAPE&quot;]
rows = [5000,1000,2500,2500]
include_static_vectors = true

[components.tok2vec.model.encode]
@architectures = &quot;spacy.MaxoutWindowEncoder.v2&quot;
width = 256
depth = 8
window_size = 1
maxout_pieces = 3

[corpora]

[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[corpora.train]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.train}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[training]
dev_corpus = &quot;corpora.dev&quot;
train_corpus = &quot;corpora.train&quot;
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 0
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null
before_update = null

[training.batcher]
@batchers = &quot;spacy.batch_by_words.v1&quot;
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = &quot;compounding.v1&quot;
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.logger]
@loggers = &quot;spacy.WandbLogger.v3&quot;
project_name = &quot;new_custom_ner&quot;
remove_config_values = []
log_dataset_dir = &quot;./output&quot;
model_log_interval = 1000
entity = null
run_name = null

[training.optimizer]
@optimizers = &quot;Adam.v1&quot;
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.001

[training.score_weights]
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0
ents_per_type = null

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]
</code></pre>
<p>Should I replace my model or is it ok the way it is right now? I am worried that my custom model affects the way the SpacyTokenizer and SpacyFeaturizer work.</p>
","nlp, spacy, rasa, spacy-3",
Determining whether a word is a noun or not,"<p>Given an input word, I want to determine whether it is a noun or not (in case of ambiguity, for instance <code>cook</code> can be a noun or a verb, the word must be identified as a noun).</p>

<p>Actually I use the POS tagger from the Stanford Parser (i give it a single word as input, and i extract only the POS tag from the result). The results are quite good but it takes a very long time.</p>

<p>Is there a way (in python, please :) to perform this task quicker than what I do actually?</p>
","python, nlp, stanford-nlp",
"Named entity recognition (NER) task on a large dataset from a data frame column using chunking, and append to results to the original data frame","<p>I want to perform a NER task on a column of a dataframe. The shape of the dataframe is:</p>
<pre><code>import pandas
    df.shape()
    (1312, 12)
</code></pre>
<p>Now the column I wanted to use is called the <code>TEXT</code> column for the NER task. So I have attempted to first convert the column and all the row values to a single string. The code snippet:</p>
<pre><code># Convert to string type 
df['TEXT']=df['TEXT'].astype('string')
non_empty_texts = [text.strip() for text in df['TEXT'] if text.strip()]
# Join the rows into a single string
joined_text =  '\n'.join(non_empty_texts)
</code></pre>
<p>After that, define the chunk size, split the <code>joined_text</code> into chunks, and process each chunk, and predict the entities using the given model.</p>
<pre><code># Define the chunk size 
chunk_size = 5000  

# Split the joined_text into chunks
chunks = [joined_text[i:i+chunk_size] for i in range(0, len(joined_text), chunk_size)]

# Initialize an empty list to store entities
all_entities = []

# Process each chunk and extract entities
for chunk in chunks:
    entities = model.predict_entities(chunk, labels, threshold=0.5)
    all_entities.extend(entities)
</code></pre>
<p>Finally convert the list of dictionary into a dataframe:</p>
<pre><code># Convert the list of dictionaries to a DataFrame
output= pd.DataFrame(all_entities)
</code></pre>
<p>The issue is the shape of the <code>output</code> is <code>(700,4)</code> and the shape of the original data frame <code>df</code> is <code>13212</code>. Therefore, merging makes no sense. The aim is, I need to have ID column in the original dataframe <code>df</code>, merged to the <code>output</code> dataframe</p>
<p>Any suggestions are much appreciated! Please let me know if you need more information!</p>
","python, pandas, dataframe, nlp, named-entity-recognition",
Optimal Learning Rate and Batch Size for LLM Training,"<p>What are the best practices for optimizing batch size and learning rate in training Large Language Models (LLMs)?</p>
<p>How should these hyperparameters be adjusted relative to each other for efficient convergence and improved performance?</p>
<p>Additionally, could you provide a concise example illustrating the interplay between batch size and learning rate adjustments in training an LLM on a text generation task?</p>
","nlp, large-language-model, batchsize, learning-rate",
TypeError: Transformer._load_model() got an unexpected keyword argument &#39;add_pooling_layer&#39;,"<p>I got this error when i was using snowflake artic embedding model with langchain.TypeError: Transformer._load_model() got an unexpected keyword argument 'add_pooling_layer'</p>
<p>I tried to use the snowflake artic embedding model</p>
","nlp, large-language-model",
Error trying to unpack tuple output into two dataframe column,"<p>I have a function, <code>tokenize</code>:</p>
<pre><code>def tokenize(text,max_len=MAX_LEN):

    encoded = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length = max_len,
            padding='max_length',
            return_attention_mask=True
        )
    
    return encoded['input_ids'], encoded['attention_mask']
</code></pre>
<p>And a block of code that applies it to some training data:</p>
<pre><code>df_train['input_ids'], df_train['attention_masks'] = df_train['text'].progress_apply(tokenize)
</code></pre>
<p>The returns for the <code>tokenize</code>, <code>encoded['input_ids']</code> and <code>encoded['attention_mask']</code> are both lists, and when called on a column of text data (i.e. <code>df_train['text'].progress_apply(tokenize)</code>), it outputs a tuple of two lists, <code>(encoded['input_ids'], encoded['attention_mask'])</code></p>
<p>Despite changes, the following error persists:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[13], line 1
----&gt; 1 df_train['input_ids'], df_train['attention_masks'] = df_train['text'].progress_apply(tokenize)
      2 df_val['input_ids'], df_val['attention_masks'] = df_val['text'].progress_apply(tokenize)
      3 df_test['input_ids'], df_test['attention_masks'] = df_test['text'].progress_apply(tokenize)

ValueError: too many values to unpack (expected 2)
</code></pre>
<p>My understanding is this - the list inside of the tuple is being unpacked for some reason, and instead of each list being saved to <code>df_train['input_ids']</code> and <code>df_train['attention_masks']</code>, it is trying to assign the contents of the first list in the double assignment, and failing because it exceeds two.</p>
<p>The solution in: <a href=""https://stackoverflow.com/questions/40179013/unpack-two-variables-into-two-columns-in-dataframe"">Unpack two variables into two columns in DataFrame</a> was tried, unfortunately, this causes the return tuple to contain series as opposed to lists which causes problems downstream.</p>
<p>Is there any way to have <code>df_train['input_ids']</code> and <code>df_train['attention_masks']</code> be populated by the output of <code>encoded['input_ids']</code> and <code>encoded['attention_mask']</code> in list form, for the relevant training example passed in by <code>df_train['text'].progress_apply(tokenize)</code></p>
<p>Using a temp column and splitting after the fact as in <a href=""https://stackoverflow.com/questions/61216280/too-many-values-to-unpack-trying-to-call-function-for-two-dataframe-columns-us"">&#39;Too many values to unpack&#39; trying to call function for two dataframe columns using apply and lambda</a> seems clunky, is there a more elegant solution?</p>
<p>Possibly .explode() method?</p>
<p>EDIT: Add following code block for working example</p>
<pre><code>tokenizer = DistilBertTokenizer.from_pretrained(&quot;distilbert-base-uncased&quot;)

train_dict = {'text': ['text1', 'text2', 'text3']}
df_train = pd.DataFrame.from_dict(train_dict)
</code></pre>
","pandas, dataframe, nlp",
Disable Vertex AI Gemini API safety attributes,"<p>I am currently developing an application using <code>vertexai</code> API where I am uploading research paper and use prompt engineering to get the major content of the paper.</p>
<p>While I am triggering basic prompt like describe the context of the paper, I receive error <code>HARM_CATEGORY_UNSPECIFIED</code>. How do I disable it?</p>
<p>Provides the context of the paper or a summery.</p>
","python, nlp, google-cloud-vertex-ai, google-gemini, google-generativeai",
usage of vllm for extracting embeddings,"<p>Following is a little piece of code to extract embeddings from a certain layer of LLM:</p>
<pre class=""lang-py prettyprint-override""><code>def process_row(prompt: str, model, tokenizer, layers_to_use: list, remove_period: bool):
    &quot;&quot;&quot;
    Processes a row of data and returns the embeddings.
    &quot;&quot;&quot;
    if remove_period:
        prompt = prompt.rstrip(&quot;. &quot;)
    inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)
    with torch.no_grad():
        outputs = model.generate(inputs.input_ids, output_hidden_states=True, return_dict_in_generate=True, max_new_tokens=1, min_new_tokens=1)
    embeddings = {}
    for layer in layers_to_use:
        last_hidden_state = outputs.hidden_states[0][layer][0][-1]
        embeddings[layer] = [last_hidden_state.numpy().tolist()]
    return embeddings
</code></pre>
<p>It's pretty standard way, but it's pretty slow. Is there any way to use vllm to make it faster without needing to call generate function everytime? I've tried batching, but it's slow too. Any help is appreciated!</p>
<p>One way to get last hidden state values using vllm is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from vllm import LLM, SamplingParams
from vllm.sequence import (SamplerOutput, Sequence, SequenceGroup, SequenceData, 
                           SequenceGroupMetadata, SequenceStatus)
from transformers import LlamaModel, LlamaTokenizer
from vllm import EngineArgs, LLMEngine, SamplingParams, RequestOutput
from vllm.sequence import SamplerOutput, SequenceData, SequenceGroupMetadata


llm = LLM(model=path_to_llama2)


# Enable top-k sampling to reflect the accurate memory usage.
vocab_size = llm.llm_engine.workers[0].model.config.vocab_size
sampling_params = SamplingParams(top_p=0.99, top_k=vocab_size - 1)
max_num_batched_tokens = llm.llm_engine.workers[0].scheduler_config.max_num_batched_tokens
max_num_seqs = llm.llm_engine.workers[0].scheduler_config.max_num_seqs
</code></pre>
<pre class=""lang-py prettyprint-override""><code>prompt = train[0]
prompt_token_ids = llm.llm_engine.tokenizer.encode(prompt) #[2, 100, 524, 10]
seqs = []
    
group_id = 1
seq_data = SequenceData(prompt_token_ids)
seq = SequenceGroupMetadata(
    request_id=str(group_id),
    is_prompt=True,
    seq_data={group_id: seq_data},
    sampling_params=sampling_params,
    block_tables=None,
)
seqs.append(seq)
input_tokens, input_positions, input_metadata = llm.llm_engine.workers[0]._prepare_inputs(
    seqs)
prompt_len = len(seq_data.prompt_token_ids)
input_tokens = input_tokens[:prompt_len]
input_positions = input_positions[:prompt_len]
# Execute the model.
num_layers = llm.llm_engine.workers[0].model_config.get_num_layers(llm.llm_engine.workers[0].parallel_config)
tempOut = llm.llm_engine.workers[0].model.model(
    input_ids=input_tokens,
    positions=input_positions,
    kv_caches=[(None, None)] * num_layers,
    input_metadata=input_metadata,
    cache_events=None,
)
print(tempOut.size())
</code></pre>
<p>but this doesn't get me with all the hidden state embeddings (of all layers). Is there any other way to get such values in a faster manner?</p>
","python, nlp, huggingface-transformers, large-language-model",
Understanding examples of Custom NER using spaCy,"<p>I am wanting to create my own custom Named Entity Recognition using spaCy. Whilst I understand the principles of the process and that I need 50-100 examples to train the model.</p>
<p>What I am struggling to understand is if the number of examples needs to be just be done for each new entity or exponentially for each value the new entity could be?</p>
<p>For example say I had the phrase
&quot;The LA Rams play football at SoFi stadium&quot;
Do I need 50-100 versions of this statement to give named entities to the football team and their stadium?</p>
<p>Or do I need to do 50-100 sentences for all NFL teams and their stadiums individually? i.e.
&quot;The Arizonal Cardinals play ball at State Farm stadium&quot;, &quot;The 49ers play their home games at Levi's stadium&quot;</p>
<p>In American football this is made harder due to sponsorship and these name actually being organizations, but in UK football this is not so common with Wrexham Football Club playing at the Racecourse stadium.</p>
<p>Thank you</p>
","python, nlp, chatbot, spacy, named-entity-recognition",
Get document ID from LDA output R,"<p>I'm trying to do LDA over two very large corpus of documents.
I need to compare the LDA output (planning to use the Kullback-Leibler similarity measure) across time for each pair of documents. Therefore, at some point, I need to &quot;merge&quot; my documents by time.</p>
<pre><code>output_directory= &quot;fed documents/&quot;
names_files=list.files(output_directory)

## FIRST GO WITH MINUTES

## MINUTES
# 5) MINUTES
minutes_names &lt;- names_files[grepl(&quot;(minutes|histmin|moa|ropa)&quot;, names_files, ignore.case = TRUE)]

minutes=list()

for (this_gb in minutes_names) {
  this_books_path=paste(output_directory,this_gb, sep=&quot;/&quot;)
  this_book=pdf_text(this_books_path)
  
  minutes[[this_gb]]=this_book
  remove(this_book)
  
}


mins_corpus &lt;- Corpus(VectorSource(minutes))
mins_corpus &lt;- tm_map(mins_corpus, tolower)
mins_corpus &lt;- tm_map(mins_corpus, removePunctuation)
mins_corpus &lt;- tm_map(mins_corpus, function(x) removeWords(x, stopwords(&quot;english&quot;)))
mins_corpus &lt;- tm_map(mins_corpus, stemDocument, language = &quot;english&quot;)
mins_corp_matrix=DocumentTermMatrix(mins_corpus)

# GET THE LDA MEASURE
mins_lda=LDA(mins_corp_matrix, k=30, control=list( seed=141096)) # ACOSTA USES 30
mins_topics=tidy(mins_lda, matrix=&quot;gamma&quot;) # GET THE PROBABILITIES BY DOCUMENT AND TOPIC


</code></pre>
<p>This is my first corpus. It's basically the set of minutes, minutes of policy actions and record of policy actions from the Fed's historical archive.</p>
<p>The second set is similarly set up, and consists of speeches given by the chair of the Fed.</p>
<pre><code>output_directory= &quot;fed chair speeches/&quot;
speeches_files &lt;- list.files(output_directory)
speeches_chairs_fed=list()

for(this_speech in speeches_files){
  this_speech_path=paste(output_directory,this_speech, sep=&quot;/&quot;)
  this_speech=pdf_text(this_speech_path)
  
  if (grepl(&quot;\\b(high school|opening|closing|education|summit|school|club)\\b&quot;, this_speech, ignore.case = TRUE)) {
    # Skip this speech and move to the next
    next
  }
  
  speeches_chairs_fed[[this_speech]]
  
}



speech_corpus &lt;- Corpus(VectorSource(speeches_chairs_fed))
speech_corpus &lt;- tm_map(speech_corpus, tolower)
speech_corpus &lt;- tm_map(speech_corpus, removePunctuation)
speech_corpus &lt;- tm_map(speech_corpus, function(x) removeWords(x, stopwords(&quot;english&quot;)))
speech_corpus &lt;- tm_map(speech_corpus, stemDocument, language = &quot;english&quot;)
speech_corp_matrix=DocumentTermMatrix(speech_corpus)

# GET THE LDA MEASURE
speech_lda=LDA(speech_corp_matrix, k=30, control=list( seed=141096)) # ACOSTA USES 30
speeches_topics=tidy(speech_lda, matrix=&quot;gamma&quot;) # GET THE PROBABILITIES BY DOCUMENT AND TOPIC
</code></pre>
<p>I have both sets now, and I need to get a way to merge (either the output of LDA, or the lists) by a common date.
That is, I have two desired outputs:</p>
<ol>
<li>LDA output (the gamma and beta matrices, using LDA notation) for a pair of speech-minutes in a given date</li>
<li>a list that merges, by date, the speeches and the minutes, and then applies LDA. I know I could merge the two lists rather easily, but the problem I see in this approach is that I need to apply LDA to each speech and minute separately and compute the gamma and beta matrices for each, not jointly.</li>
</ol>
","r, nlp, lda, tm",
I keep getting Wrong API key error 403 from pinecone,"<p>it’s my first time using VDB and I  wrote some basic scraping and embedding code for the test. I can’t understand what’s wrong with API, but I keep getting the same error. Here’s my code and the output. Additionally, if you could recommend VDB for newcomers it would be great. Thanks in advance.</p>
<pre><code>import os
import requests
from bs4 import BeautifulSoup
import re
from fake_useragent import UserAgent
import openai
import pinecone

# Initialize Pinecone
pc = Pinecone(api_key='')
index_name = &quot;web&quot;

# Check if the index exists, otherwise create one
if index_name not in pc.list_indexes().names():
    pinecone.create_index(name=index_name, dimension=1536)  # Ensure dimension matches your embedding dimension
index = pinecone.Index(index_name, host='https://web-mcilcxv.svc.aped-4627-b74a.pinecone.io')

# Securely fetch your OpenAI API key
openai.api_key = ''

# List of website URLs
website_urls = [
    &quot;https://www.accel.com&quot;, &quot;https://www.a16z.com&quot;, &quot;https://www.greylock.com&quot;,
    &quot;https://www.benchmark.com&quot;, &quot;https://www.sequoiacap.com&quot;, &quot;https://www.indexventures.com&quot;,
    &quot;https://www.kpcb.com&quot;, &quot;https://www.lsvp.com&quot;, &quot;https://www.matrixpartners.com&quot;,
    &quot;https://www.500.co&quot;, &quot;https://www.sparkcapital.com&quot;, &quot;https://www.insightpartners.com&quot;
]

def scrape_home_page_and_save_text(url):
    try:
        ua = UserAgent()
        headers = {'User-Agent': ua.random}
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            text_content = re.sub(r'\s+', ' ', soup.get_text()).strip()
            print(f&quot;Scraped {url} successfully.&quot;)
            return text_content
        else:
            print(f&quot;Failed to scrape {url}. Status code: {response.status_code}&quot;)
            return None
    except Exception as e:
        print(f&quot;An error occurred while scraping {url}: {str(e)}&quot;)
        return None

def embed_text_content(text_content):
    try:
        response = openai.Embedding.create(input=text_content, model=&quot;text-embedding-ada-002&quot;)
        return response['data'][0]['embedding']  # Accessing embedding data correctly
    except openai.Error as e:  # Catching OpenAI specific errors
        print(f&quot;An error occurred while generating embedding: {str(e)}&quot;)
        return None


# Scrape, embed, and store data
embeddings = []
ids = []
for url in website_urls:
    print(f&quot;Scraping {url}...&quot;)
    text_content = scrape_home_page_and_save_text(url)
    if text_content:
        print(f&quot;Embedding content from {url}...&quot;)
        embedding = embed_text_content(text_content)
        if embedding:
            embeddings.append(list(embedding))
            ids.append(url)

# Batch upsert to Pinecone
if embeddings and ids:
    index.upsert(vectors=list(zip(ids, embeddings)))
    print(f&quot;Data for all sites uploaded to Pinecone.&quot;)

print(&quot;Scraping, embedding, and uploading finished.&quot;)
</code></pre>
<p><code>Reason: Forbidden HTTP response headers: HTTPHeaderDict({'Date': 'Wed, 01 May 2024 10:37:34 GMT', 'Content-Type': 'text/plain', 'Content-Length': '9', 'Connection': 'keep-alive', 'x-pinecone-auth-rejected-reason': 'Wrong API key', 'www-authenticate': 'Wrong API key', 'server': 'envoy'}) HTTP response body: Forbidden</code></p>
<p>I double checked the api and tried with env variables.</p>
","nlp, openai-api, vector-database, pinecone",
Matching strings containing &#39;and&#39; in different languages and ampersands,"<p>Suppose that in 2 different data frames <code>df1</code>, <code>df2</code> I have 2 columns</p>
<pre><code>df1['film'] = pd.Series(['Beavis &amp; Butthead', 'Bonnie e Clyde', 'Adam &amp; Eve'])
df2['film'] = pd.Series(['Beavis und Butthead', 'Bonnie &amp; Clyde', 'Adam et Eve'])

</code></pre>
<p>of film titles in multiple different languages. (In reality my columns are much larger than this and don't exclusively contain titles with ampersands or different languages' equivalents of the word 'the'. Hence I cannot use something like stopwords for my problem, as it would amends these titles in too 'destructive' a way.)</p>
<p>None of these records result in matches when merging the data frames on these 2 columns, and they would still not all match if I did a simple <code>str.replace</code> since there is no way of knowing from any of the titles whether the ampersand should be 'e', 'et', 'und', 'and', etc.</p>
<p>Since I do not have a list of all the languages that the film titles are written in, how do I approach this problem?</p>
","python, pandas, string, join, nlp",
Why is my word lemmatization not working as expected?,"<p>Hi stackoverflow community!
Long-time reader but first-time poster. I'm currently trying my hand at NLP and after reading a few forum posts touching upon this topic, I can't seem to get the lemmatizer to work properly (function pasted below). Comparing my original text vs preprocessed text, all the cleaning steps work as expected, except the lemmatization. I've even tried specifying the part of speech : 'v' to not default the word as noun, and still get the base form of the verb (ex: turned -&gt; turn , are -&gt; be, reading -&gt; read) ... however this doesn't seem to be working.</p>
<p>Appreciate another set of eyes and feedback - thanks!</p>
<pre><code># key imports

import pandas as pd
import numpy as np
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from string import punctuation
from nltk.stem import WordNetLemmatizer
import contractions


# cleaning functions

def to_lower(text):
    '''
    Convert text to lowercase
    '''
    return text.lower()

def remove_punct(text):
    return ''.join(c for c in text if c not in punctuation)

def remove_stopwords(text):
    '''
    Removes stop words which don't have meaning (ex: is, the, a, etc.)
    '''
    additional_stopwords = ['app']

    stop_words = set(stopwords.words('english')) - set(['not','out','in']) 
    stop_words = stop_words.union(additional_stopwords)
    return ' '.join([w for w in nltk.word_tokenize(text) if not w in stop_words])

def fix_contractions(text):
    '''
    Expands contractions
    '''
    return contractions.fix(text)



# preprocessing pipeline

def preprocess(text):
    # convert to lower case
    lower_text = to_lower(text)
    sentence_tokens = sent_tokenize(lower_text)
    word_list = []      
            
    for each_sent in sentence_tokens:
        # fix contractions
        clean_text = fix_contractions(each_sent)
        # remove punctuation
        clean_text = remove_punct(clean_text)
        # filter out stop words
        clean_text = remove_stopwords(clean_text)
        # get base form of word
        wnl = WordNetLemmatizer()
        for part_of_speech in ['v']:
            lemmatized_word = wnl.lemmatize(clean_text, part_of_speech)
        # split the sentence into word tokens
        word_tokens = word_tokenize(lemmatized_word)
        for i in word_tokens:
            word_list.append(i)                     
    return word_list

# lemmatize not properly working to get base form of word
# ex: 'turned' still remains 'turned' without returning base form 'turn'
# ex: 'running' still remains 'running' without getting base form 'run'



sample_data = posts_with_text['post_text'].head(5)
print(sample_data)
sample_data.apply(preprocess)
</code></pre>
","python, nlp, lemmatization",
Training Genomic-ULMFIT model but I am encountering an error,"<p>I am trying to train an ULMFiT model found <a href=""https://github.com/kheyer/Genomic-ULMFiT"" rel=""nofollow noreferrer"">https://github.com/kheyer/Genomic-ULMFiT</a></p>
<p>I am using custom data and following one of the notebook examples, specifically Mammals LM 3 3m1s. I use the same exact code but just altered to use my own data. When I run a specific line of code</p>
<pre><code>data = GenomicTextLMDataBunch.from_df(path, df[:100000], df_valid, bs=800, tokenizer=tok, 
                          chunksize=50000, text_cols=0, label_cols=1, max_vocab=80000)
</code></pre>
<p>I receive an error</p>
<pre><code>ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (47613,) + inhomogeneous part.
</code></pre>
<p>Does anyone have any experience with this model or can try to help?</p>
<p>Much appreciated and thanks in advance!</p>
","python, github, nlp, valueerror",
Trying to cluster short survey answers (1 to 10 words). Am I on the right track?,"<p>Here's the explanation of what i want to fully make (its a project for school).</p>
<ol>
<li>A user just puts in a file with just the answers to whatever question was asked in the survey.</li>
</ol>
<p>2.The machine finds similar answers and groups them under one label or cluster that is unnamed (thinking of using MeanShift, GMM, KMeans)</p>
<ol start=""3"">
<li>If possible I would also like it to generate a label for the cluster.</li>
</ol>
<p>4.Write the clustered and labeled answers back into a file to be checked and used for whatever purpose.</p>
<p>Some context on the data: lots of short (with some long, 10+ words) answers like &quot;i dont know&quot;, &quot;??&quot;, &quot;helpful&quot;, &quot;red&quot;, etc. and each has anywhere from 200 to 2000 answers.The answers are in dutch or french, would it be recomanded I translate them to english for better performance? Usually there are around 7 to 20 (high amounts are rare) clusters. I also have the right labels for the answers so I can check if the algorithm has clustered correctly.</p>
<p>I have tried looking into it and I need to vectorize my texts first, for this I tried TF-IDF and Count vectorizer in scikit. I also found their cheat sheet and It recommands i use MeanShift.</p>
<p>I havent tried looking for the best parameters yet, but the performance seem very poor (close to random). I used Adjusted Rand Index, Normalized Mutual Information and Silhouette Score to evbaluate.</p>
<p>Am I on the right track or are there better things out there? Vectorization methods, embeddings, clustering algorithms?</p>
<p>Edit: I just realized something that may overthrow the algorithm. In each survey there is a category named ´others´. It is a category with actual answers that are just irrelevant.</p>
<p>Possible workaroud i think way work: The gategory is ´small´ with around 8%. My clustering algorithm always makes way too many classes, but i could try to figure out a balance between combining classes of 1 or 2 answers into ´others´.</p>
","machine-learning, text, nlp, cluster-analysis",
NefTune Receiving 0 Training Loss on Transformers,"<p>I'm basically trying to fine-tune my model with Neftune. Model is based on Turkish Language. But there I'm receiving zero training lose. I've tried to another model like <a href=""https://huggingface.co/ytu-ce-cosmos/turkish-gpt2-large"" rel=""nofollow noreferrer"">Turkish-GPT2</a> there is no issue everything is okay. I think probably there is problem with model. I don't know how to handle with this issue.</p>
<p>Loading Model:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(&quot;asafaya/kanarya-750m&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;asafaya/kanarya-750m&quot;)

v3_prompt = &quot;&quot;&quot;Aşağıda, daha fazla bağlam sağlayan bir girdiyle eşleştirilmiş, bir görevi açıklayan bir talimat bulunmaktadır. İsteği uygun şekilde tamamlayan bir yanıt yazın.

### Input:
{}

### Instructions:
{}

### Response:
{}
&quot;&quot;&quot;
</code></pre>
<p>Changing format prompts:</p>
<pre><code>EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    inputs       = examples[&quot;input&quot;]
    instructions = examples['instructions']
    outputs      = examples[&quot;response&quot;]
    texts = []
    for input, instructions, output in zip(inputs, instructions, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = v3_prompt.format(input, instructions, output) + EOS_TOKEN
        texts.append(text)
    return { &quot;text&quot; : texts, }
pass

from datasets import Dataset

dataset = Dataset.from_pandas(data[:40000])
dataset = dataset.map(formatting_prompts_func, batched=True)
</code></pre>
<p>Neftune:</p>
<pre><code>from trl import SFTTrainer
from transformers import TrainingArguments

trainer_2 = SFTTrainer(
    model=model,
    train_dataset=dataset,
    dataset_text_field=&quot;text&quot;,
    max_seq_length=512,
    neftune_noise_alpha=5,
    packing=False,
    args = TrainingArguments(
        per_device_train_batch_size = 1, #  batch size
        gradient_accumulation_steps = 2, #  gradient accumulation steps
        warmup_steps = 5,
        max_steps = 80,
        learning_rate = 2e-4,
        fp16 = False,
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 1,
        optim = &quot;adamw_8bit&quot;,
        weight_decay = 0.01,
        lr_scheduler_type = &quot;linear&quot;,
        seed = 3407,
        output_dir = &quot;outputs&quot;,
    ),
)
trainer_2.train()
</code></pre>
<p>Output:</p>
<pre><code> [80/80 00:25, Epoch 0/1]
Step    Training Loss
1   0.000000
2   0.000000
3   0.000000
4   0.000000
5   0.000000
6   0.000000
7   0.000000
8   0.000000
9   0.000000
10  0.000000
11  0.000000
12  0.000000
13  0.000000
14  0.000000
15  0.000000
16  0.000000
17  0.000000
18  0.000000
19  0.000000
20  0.000000
21  0.000000
22  0.000000
23  0.000000
24  0.000000
25  0.000000
26  0.000000
27  0.000000
28  0.000000
29  0.000000
30  0.000000
31  0.000000
32  0.000000
33  0.000000
34  0.000000
35  0.000000
36  0.000000
37  0.000000
38  0.000000
39  0.000000
40  0.000000
41  0.000000
42  0.000000
43  0.000000
44  0.000000
45  0.000000
46  0.000000
47  0.000000
48  0.000000
49  0.000000
50  0.000000
51  0.000000
52  0.000000
53  0.000000
54  0.000000
55  0.000000
56  0.000000
57  0.000000
58  0.000000
59  0.000000
60  0.000000
61  0.000000
62  0.000000
63  0.000000
64  0.000000
65  0.000000
66  0.000000
67  0.000000
68  0.000000
69  0.000000
70  0.000000
71  0.000000
72  0.000000
73  0.000000
74  0.000000
75  0.000000
76  0.000000
77  0.000000
78  0.000000
79  0.000000
80  0.000000
TrainOutput(global_step=80, training_loss=0.0, metrics={'train_runtime': 25.3789, 'train_samples_per_second': 6.304, 'train_steps_per_second': 3.152, 'total_flos': 117937578909696.0, 'train_loss': 0.0, 'epoch': 0.004})
</code></pre>
","machine-learning, deep-learning, nlp, huggingface-transformers",
`index out of range in self` when I ran Dynamic Bernoulli Embeddings,"<p>I'm working on Dynamic Bernoulli Embeddings using Python, but I'm encountering an <code>IndexError: index out of range in self</code> when I attempt to train the model. I'm a beginner in NLP, so I struggle to understand and fix this problem.</p>
<p>Here's the relevant portion of my code:</p>
<pre class=""lang-py prettyprint-override""><code>import pickle
import re
import numpy as np
import pandas as pd
from dynamic_bernoulli_embeddings.analysis import DynamicEmbeddingAnalysis
from dynamic_bernoulli_embeddings.training import train_model
from nltk import word_tokenize as nltk_word_tokenize
from gensim.corpora import Dictionary
from tqdm.notebook import tqdm
tqdm.pandas()

# bow contains tokenized words
dataset = df2[['bow', 'time', 'year']]

# Create a Gensim dictionary
dictionary = Dictionary(dataset['bow'])
dictionary.filter_extremes(no_below=2, no_above=1.0)
dictionary.compactify()

# Training the Dynamic Bernoulli Embeddings model
model, loss_history = train_model(
    dataset, dictionary.token2id, validation=.1, num_epochs=5, k=50)

</code></pre>
<p>The following error occurs during the model training:</p>
<pre class=""lang-py prettyprint-override""><code>IndexError: index out of range in self

Traceback:
  File &quot;&lt;ipython-input-44-1d8d4321a42a&gt;&quot;, in &lt;cell line: 2&gt;
    model, loss_history = train_model(
      dataset, dictionary.token2id, validation=.1, num_epochs=5, k=50)

7 frames
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py&quot;, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)

</code></pre>
<ul>
<li>I'm using the Dynamic Bernoulli Embeddings library from this repository: <a href=""https://www.kaggle.com/code/llefebure/dynamic-bernoulli-embeddings#Dynamic-Bernoulli-Embeddings"" rel=""nofollow noreferrer"">Dynamic Bernoulli Embeddings</a></li>
</ul>
<p>What steps can I take to resolve it?</p>
","python, nlp, word-embedding",
Extracting only technical keywords from a text using RAKE library in Python,"<p>I want to use rake to extract technical keywords from a job description that I've found on Linkedin, which looks like this:</p>
<pre><code>input = &quot;In-depth understanding of the Python software development stacks, ecosystems, frameworks and tools such as Numpy, Scipy, Pandas, Dask, spaCy, NLTK, sci-kit-learn and PyTorch.Experience with front-end development using HTML, CSS, and JavaScript.
Familiarity with database technologies such as SQL and NoSQL.Excellent problem-solving ability with solid communication and collaboration skills.
Preferred Skills And QualificationsExperience with popular Python frameworks such as Django, Flask or Pyramid.&quot;
</code></pre>
<p>I run this code, as it's supposed to return the keywords.</p>
<pre><code>from rake_nltk import Rake

r = Rake()
r.extract_keywords_from_text(input)
keywords = r.get_ranked_phrases_with_scores()

for score, keyword in keywords:
    if len(keyword.split()) == 1:  # Check if the keyword is one word
        print(f&quot;{keyword}: {score}&quot;)
</code></pre>
<p>But the output is this:</p>
<pre><code>frameworks: 2.0
tools: 1.0
sql: 1.0
spacy: 1.0
scipy: 1.0
sci: 1.0
qualificationsexperience: 1.0
pytorch: 1.0
pyramid: 1.0
pandas: 1.0
numpy: 1.0
nosql: 1.0
nltk: 1.0
learn: 1.0
kit: 1.0
javascript: 1.0
front: 1.0
flask: 1.0
familiarity: 1.0
experience: 1.0
ecosystems: 1.0
django: 1.0
dask: 1.0
css: 1.0
</code></pre>
<p>Simply I just want the explicit name of tools, skills and frameworks. Such as &quot;Numpy&quot;, &quot;Scipy&quot;, &quot;HTML&quot;, etc That are used in the text and NOT every single word that's found in it (such as &quot;experience&quot; or &quot;tools&quot;).</p>
<p>Is there any way to do so? Or should I just provide a list of all possible python frameworks and related skill and then filter the output of rake?
If the latter one is the solution, How can I find/make a thorough list?</p>
<p>Any help is appreciated.</p>
","python, python-3.x, nlp, nltk, rake","<p>You can utilize skill and knowledge token classification from Hugging Face's library</p>
<pre><code>from transformers import pipeline

token_skill_classifier = pipeline(model=&quot;jjzha/jobbert_skill_extraction&quot;, aggregation_strategy=&quot;first&quot;)
token_knowledge_classifier = pipeline(model=&quot;jjzha/jobbert_knowledge_extraction&quot;, aggregation_strategy=&quot;first&quot;)

def aggregate_span(results):
    new_results = []
    current_result = results[0]

    for result in results[1:]:
        if result[&quot;start&quot;] == current_result[&quot;end&quot;] + 1:
            current_result[&quot;word&quot;] += &quot; &quot; + result[&quot;word&quot;]
            current_result[&quot;end&quot;] = result[&quot;end&quot;]
        else:
            new_results.append(current_result)
            current_result = result

    new_results.append(current_result)

    return new_results

def ner(text):
    output_skills = token_skill_classifier(text)
    for result in output_skills:
        if result.get(&quot;entity_group&quot;):
            result[&quot;entity&quot;] = &quot;Skill&quot;
            del result[&quot;entity_group&quot;]

    output_knowledge = token_knowledge_classifier(text)
    for result in output_knowledge:
        if result.get(&quot;entity_group&quot;):
            result[&quot;entity&quot;] = &quot;Knowledge&quot;
            del result[&quot;entity_group&quot;]

    if len(output_skills) &gt; 0:
        output_skills = aggregate_span(output_skills)
    if len(output_knowledge) &gt; 0:
        output_knowledge = aggregate_span(output_knowledge)

    return {&quot;text&quot;: text, &quot;entities&quot;: output_skills}, {&quot;text&quot;: text, &quot;entities&quot;: output_knowledge}
</code></pre>
"
Transformer-based Yor&#249;b&#225; to English language processor : requirements issues,"<p>Following the successful implementation of the first transduction model that leveraged attention mechanism, I want to implement similar model applying it to a Nigerian language e.g. Yorùbá. However, I do not have a corpus that can help actualize this experiment. Is there a way of getting it considering the idea in Vaswani et al (2017) in their article 'Attention Is All You Need'? It is expected that the model achieves high Bilingual Evaluation Understudy metric. Help is needed here regarding the corpus as mentioned.</p>
<p>#Yorùbá-to-English translation task</p>
<p>Attempts had been made to browse the internet for it but all to no avail. Equally, I tried to check the WMT 2014 site, all to  no avail.</p>
","nlp, artificial-intelligence, translation, transformer-model, non-english",
Why am I encountering a PocketSphinx Python Initialization Error on google colab,"<p>I am trying to use PocketSphinx to extract Phonemes from WAV audio files in google colab.
I am initizialising PockerSphinx through a directory in my google drive.</p>
<p>This is the code:</p>
<pre><code>sys.path.append('/content/drive/MyDrive/pocketsphinx- 
master/lib/python3.10/site-packages')
import os
from pocketsphinx import Pocketsphinx, Decoder, Config
import sys
import soundfile as sf
from pydub import AudioSegment
# Define directories for models and data
MODELDIR = &quot;/content/drive/MyDrive/pocketsphinx-master/model&quot;
ACOUSTIC_MODEL_DIR = os.path.join(MODELDIR, 'en-us')
PHONETIC_MODEL_FILE = os.path.join(MODELDIR, 'en-us-phone.lm.bin')

DATADIR = &quot;/content/drive/MyDrive/WAV&quot;

# Create a decoder with the correct configuration
config = Config()
config.set_string('-hmm', ACOUSTIC_MODEL_DIR)
config.set_string('-allphone', PHONETIC_MODEL_FILE)
config.set_float('-lw', 2.0)
config.set_float('-beam', 1e-60) 
config.set_float('-pbeam', 1e-50)
config.set_string('-logfn', '/content/drive/MyDrive/pocketsphinx-master/pocketsphinx.log')

# Initialize the decoder using the configuration
try:
    decoder = Decoder(config)
    print(&quot;Decoder initialized successfully.&quot;)
except RuntimeError as e:
    print(f&quot;Failed to initialize decoder: {e}&quot;)

# Check if the decoder is initialized and then process the sample audio file
if 'decoder' in locals():
    decoder.start_utt()
    with open(os.path.join(DATADIR, 'Depression_English.wav'), 'rb') as stream:
        while True:
            buf = stream.read(1024)
            if buf:
                decoder.process_raw(buf, False, False)
            else:
                break
    decoder.end_utt()

    # Print the hypothesis (recognized phonemes)
    hypothesis = decoder.hyp()
    if hypothesis is not None:
        print('Best hypothesis:', hypothesis.hypstr)
        print('Phonemes:', [seg.word for seg in decoder.seg()])
    else:
        print('No hypothesis formed.')
else:
    print(&quot;Decoder could not be initialized.&quot;)
</code></pre>
<p><a href=""https://i.sstatic.net/W3DFJLwX.png"" rel=""nofollow noreferrer"">This is the directory for the pocketsphinx master</a></p>
<p><a href=""https://i.sstatic.net/XI5whL7c.png"" rel=""nofollow noreferrer"">This is whats in the model folder</a></p>
<p><a href=""https://i.sstatic.net/Y4QJoax7.png"" rel=""nofollow noreferrer"">This is whats in the en-us folder</a></p>
<p>I tried modifying the file paths and Im not sure the file paths are the issue. I think it might be an installation error or a version mismatch but I cant find what the error is in the PocketSphinx Directory. I also tried installing pocketsphinx in my terminal.</p>
","python-3.x, audio, nlp, artificial-intelligence, pocketsphinx",
CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling cublasCreate(handle),"<p>I got the following error when I ran my PyTorch deep learning model in Google Colab</p>
<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in linear(input, weight, bias)
   1370         ret = torch.addmm(bias, input, weight.t())
   1371     else:
-&gt; 1372         output = input.matmul(weight.t())
   1373         if bias is not None:
   1374             output += bias

RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
</code></pre>
<p>I even reduced batch size from 128 to 64 i.e., reduced to half,  but still, I got this error. Earlier, I ran the same code with a batch size of 128 but didn't get any error like this.</p>
","python, pytorch, nlp, cuda, bert-language-model",
"fix error &quot; With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.&quot;","<p>I wrote a program in python and I want to implement NLP by BERT algorithm.I have a dataset and the below code but when I ran the program at colab I encounter below error</p>
<pre><code>    import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score
from sklearn.utils import shuffle
from bs4 import BeautifulSoup
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
import hazm
from hazm import Lemmatizer
from cleantext import clean
import nltk
import plotly.graph_objects as go
from tqdm.notebook import tqdm
import os
import re
import json
import copy
import collections

from transformers import BertConfig, BertTokenizer
from transformers import TFBertModel, TFBertForSequenceClassification
from transformers import glue_convert_examples_to_features

import tensorflow as tf
MAX_LEN = 128
TRAIN_BATCH_SIZE = 16
VALID_BATCH_SIZE = 16
TEST_BATCH_SIZE = 16

EPOCHS = 3
EEVERY_EPOCH = 1000
LEARNING_RATE = 2e-5
CLIP = 0.0

MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'
OUTPUT_PATH = '/content/bert-fa-base-uncased-sentiment-taaghceh/pytorch_model.bin'

os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
from google.colab import files
uploaded = files.upload()
data = pd.read_csv('Education.csv',header=None)
data.drop(0,inplace=True,axis=1)
data
data[1].replace('ترس','ترس ',inplace=True)
data[1].unique()
with open('stopwords.txt') as stopwords_file:
   stopwords = stopwords_file.readlines()
nltk_stopwords = [str(line).replace('\n', '') for line in stopwords]
data['comment_len_by_words'] = data[1].apply(lambda t: len(hazm.word_tokenize(t)))
data
min_max_len = data[&quot;comment_len_by_words&quot;].min(), data[&quot;comment_len_by_words&quot;].max()
print(f'Min: {min_max_len[0]} \tMax: {min_max_len[1]}')
def data_gl_than(data, less_than=100.0, greater_than=0.0, col='comment_len_by_words'):
    data_length = data[col].values

    data_glt = sum([1 for length in data_length if greater_than &lt; length &lt;= less_than])

    data_glt_rate = (data_glt / len(data_length)) * 100

    print(f'Texts with word length of greater than {greater_than} and less than {less_than} includes {data_glt_rate:.2f}% of the whole!')
data_gl_than(data, 256, 3)

# remove comments with the length of fewer than three words
minlim, maxlim = 3, 256

data['comment_len_by_words'] = data['comment_len_by_words'].apply(lambda len_t: len_t if minlim &lt; len_t &lt;= maxlim else None)
data = data.dropna(subset=['comment_len_by_words'])
data = data.reset_index(drop=True)

def cleanhtml(raw_html):
    cleanr = re.compile('&lt;.*?&gt;')
    cleantext = re.sub(cleanr, '', raw_html)
    return cleantext

def cleaning(text):
    text = text.strip()
    
    # regular cleaning
    text = clean(text,fix_unicode=True,to_ascii=False,lower=True,no_line_breaks=True,no_urls=True,no_emails=True,no_phone_numbers=True,
        no_numbers=False,no_digits=False,no_currency_symbols=True,no_punct=False,replace_with_url=&quot;&quot;,
        replace_with_email=&quot;&quot;,
        replace_with_phone_number=&quot;&quot;,
        replace_with_number=&quot;&quot;,
        replace_with_digit=&quot;0&quot;,
        replace_with_currency_symbol=&quot;&quot;,
    )

    # cleaning htmls
    text = cleanhtml(text)
    
    # normalizing
    normalizer = hazm.Normalizer()
    text = normalizer.normalize(text)
    # removing wierd patterns
    wierd_pattern = re.compile(&quot;[&quot;
        u&quot;\U0001F600-\U0001F64F&quot;  # emoticons
        u&quot;\U0001F300-\U0001F5FF&quot;  # symbols &amp; pictographs
        u&quot;\U0001F680-\U0001F6FF&quot;  # transport &amp; map symbols
        u&quot;\U0001F1E0-\U0001F1FF&quot;  # flags (iOS)
        u&quot;\U00002702-\U000027B0&quot;
        u&quot;\U000024C2-\U0001F251&quot;
        u&quot;\U0001f926-\U0001f937&quot;
        u'\U00010000-\U0010ffff'
        u&quot;\u200d&quot;
        u&quot;\u2640-\u2642&quot;
        u&quot;\u2600-\u2B55&quot;
        u&quot;\u23cf&quot;
        u&quot;\u23e9&quot;
        u&quot;\u231a&quot;
        u&quot;\u3030&quot;
        u&quot;\ufe0f&quot;
        u&quot;\u2069&quot;
        u&quot;\u2066&quot;
        # u&quot;\u200c&quot;
        u&quot;\u2068&quot;
        u&quot;\u2067&quot;
        &quot;]+&quot;, flags=re.UNICODE)
    
    text = wierd_pattern.sub(r'', text)
    
    # removing extra spaces, hashtags
    text = re.sub(&quot;#&quot;, &quot;&quot;, text)
    text = re.sub(&quot;\s+&quot;, &quot; &quot;, text)
    
    return text
    data
data['cleaned_comment'] = data[1].apply(cleaning)

# # calculate the length of comments based on their words
data['cleaned_comment_len_by_words'] = data['cleaned_comment'].apply(lambda t: len(hazm.word_tokenize(t)))

# # # remove comments with the length of fewer than three words
data['cleaned_comment_len_by_words'] = data['cleaned_comment_len_by_words'].apply(lambda len_t: len_t if minlim &lt; len_t &lt;= maxlim else len_t)
data = data.dropna(subset=['cleaned_comment_len_by_words'])
data = data.reset_index(drop=True)
data.head()
data['tweet_without_stopwords'] = data['cleaned_comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))
data = data[[1,'cleaned_comment']]
data.columns = ['label','comment']
data.head()

fig = go.Figure()

groupby_label = data.groupby('label')['label'].count()

fig.add_trace(go.Bar(
    x=list(sorted(groupby_label.index)),
    y=groupby_label.tolist(),
    text=groupby_label.tolist(),
    textposition='auto'
))

fig.update_layout(
    title_text='Distribution of label within comments [DATA]',
    xaxis_title_text='Label',
    yaxis_title_text='Frequency',
    bargap=0.2,
    bargroupgap=0.2)

fig.show()


labels = list(sorted(data['label'].unique()))
label2id = {label: i for i, label in enumerate(labels)}
id2label = {v: k for k, v in label2id.items()}
print(f'label2id: {label2id}')
print(f'id2label: {id2label}')

data['label_id'] = data['label'].apply(lambda t: labels.index(t))
train, test = train_test_split(data, test_size=0.1, random_state=1, stratify=data['label'])
train, valid = train_test_split(train, test_size=0.1, random_state=1, stratify=train['label'])

train = train.reset_index(drop=True)
valid = valid.reset_index(drop=True)
test = test.reset_index(drop=True)

x_train, y_train = train['comment'].values.tolist(), train['label_id'].values.tolist()
x_valid, y_valid = valid['comment'].values.tolist(), valid['label_id'].values.tolist()
x_test, y_test = test['comment'].values.tolist(), test['label_id'].values.tolist()

print(train.shape)
print(valid.shape)
print(test.shape)

tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)
config = BertConfig.from_pretrained(
    MODEL_NAME_OR_PATH, **{
        'label2id': label2id,
        'id2label': id2label,
    })
print(config.to_json_string())
data
idx = np.random.randint(0, len(train))
sample_comment = train.iloc[idx]['comment']
sample_label = train.iloc[idx]['label']

print(f'Sample: \n{sample_comment}\n{sample_label}')

tokens = tokenizer.tokenize(sample_comment)
token_ids = tokenizer.convert_tokens_to_ids(tokens)

print(f'  Comment: {sample_comment}')
print(f'Token IDs: {token_ids}')
encoding = tokenizer.encode_plus(
    sample_comment,
    max_length=32,
    truncation=True,
    add_special_tokens=True, # Add '[CLS]' and '[SEP]'
    return_token_type_ids=True,
    return_attention_mask=True,
    padding='max_length',
    return_tensors='pt',  # Return tensors
)

print(f'Keys: {encoding.keys()}\n')
for k in encoding.keys():
    print(f'{k}:\n{encoding[k]}')

class InputExample:
    &quot;&quot;&quot; A single example for simple sequence classification. &quot;&quot;&quot;

    def __init__(self, guid, text_a, text_b=None, label=None):
        &quot;&quot;&quot; Constructs a InputExample. &quot;&quot;&quot;
        self.guid = guid
        self.text_a = text_a
        self.text_b = text_b
        self.label = label


def make_examples(tokenizer, x, y=None, maxlen=128, output_mode=&quot;classification&quot;, is_tf_dataset=True):
    examples = []
    y = y if isinstance(y, list) or isinstance(y, np.ndarray) else [None] * len(x)

    for i, (_x, _y) in tqdm(enumerate(zip(x, y)), position=0, total=len(x)):
        guid = &quot;%s&quot; % i
        label = int(_y)
        
        if isinstance(_x, str):
            text_a = _x
            text_b = None
        else:
            assert len(_x) == 2
            text_a = _x[0]
            text_b = _x[1]
        
        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
    
    features = glue_convert_examples_to_features(
        examples, 
        tokenizer, 
        maxlen, 
        output_mode=output_mode, 
        label_list=list(np.unique(y)))

    all_input_ids = []
    all_attention_masks = []
    all_token_type_ids = []
    all_labels = []

    for f in tqdm(features, position=0, total=len(examples)):
        if is_tf_dataset:
            all_input_ids.append(tf.constant(f.input_ids))
            all_attention_masks.append(tf.constant(f.attention_mask))
            all_token_type_ids.append(tf.constant(f.token_type_ids))
            all_labels.append(tf.constant(f.label))
        else:
            all_input_ids.append(f.input_ids)
            all_attention_masks.append(f.attention_mask)
            all_token_type_ids.append(f.token_type_ids)
            all_labels.append(f.label)

    if is_tf_dataset:
        dataset = tf.data.Dataset.from_tensor_slices(({
            'input_ids': all_input_ids,
            'attention_mask': all_attention_masks,
            'token_type_ids': all_token_type_ids
        }, all_labels))

        return dataset, features
    
    xdata = [np.array(all_input_ids), np.array(all_attention_masks), np.array(all_token_type_ids)]
    ydata = all_labels

    return [xdata, ydata], features
    train_dataset_base, train_examples = make_examples(tokenizer, x_train, y_train, maxlen=128)
valid_dataset_base, valid_examples = make_examples(tokenizer, x_valid, y_valid, maxlen=128)

test_dataset_base, test_examples = make_examples(tokenizer, x_test, y_test, maxlen=128)
[xtest, ytest], test_examples = make_examples(tokenizer, x_test, y_test, maxlen=128, is_tf_dataset=False)
for value in train_dataset_base.take(1):
    print(f'     input_ids: {value[0][&quot;input_ids&quot;]}')
    print(f'        target: {value[1]}')
    def get_training_dataset(dataset, batch_size):
       dataset = dataset.repeat()
       dataset = dataset.shuffle(2048)
       dataset = dataset.batch(batch_size)

    return dataset

def get_validation_dataset(dataset, batch_size):
    dataset = dataset.batch(batch_size)

    return dataset

train_dataset = get_training_dataset(train_dataset_base, TRAIN_BATCH_SIZE)
valid_dataset = get_training_dataset(valid_dataset_base, VALID_BATCH_SIZE)

train_steps = len(train_examples) // TRAIN_BATCH_SIZE
valid_steps = len(valid_examples) // VALID_BATCH_SIZE

train_steps, valid_steps

def build_model(model_name, config, learning_rate=3e-5):
    model = TFBertForSequenceClassification.from_pretrained(model_name, config=config)

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

    return model
    r = model.fit(
    train_dataset,
    validation_data=valid_dataset,
    steps_per_epoch=train_steps,
    validation_steps=valid_steps,
    epochs=EPOCHS,
    verbose=1)

final_accuracy = r.history['val_accuracy']
print('FINAL ACCURACY MEAN: ', np.mean(final_accuracy))
model.save_pretrained(os.path.dirname(OUTPUT_PATH))
ev = model.evaluate(test_dataset_base.batch(TEST_BATCH_SIZE))
print()
print(f'Evaluation: {ev}')
print()

predictions = model.predict(xtest)
ypred = predictions[0].argmax(axis=-1).tolist()

print()
print(classification_report(ytest, ypred, target_names=labels))
print()

print(f'F1: {f1_score(ytest, ypred, average=&quot;weighted&quot;)}')
</code></pre>
<blockquote>
<pre><code>ValueError                                Traceback (most recent call last)

&lt;ipython-input-13-1b0a029a88ab&gt; in &lt;cell line: 175&gt;()
    173 
    174 data['label_id'] = data['label'].apply(lambda t: labels.index(t))
--&gt; 175 train, test = train_test_split(data, test_size=0.1, random_state=1, stratify=data['label'])
    176 train, valid = train_test_split(train, test_size=0.1, random_state=1, stratify=train['label'])
    177 

1 frames

/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py
</code></pre>
<p>in _validate_shuffle_split(n_samples, test_size, train_size,
default_test_size)
2234
2235     if n_train == 0:
-&gt; 2236         raise ValueError(
2237             &quot;With n_samples={}, test_size={} and train_size={}, the &quot;
2238             &quot;resulting train set will be empty. Adjust any of the &quot;</p>
<pre><code>ValueError: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the
</code></pre>
<p>aforementioned parameters.</p>
</blockquote>
<p>please help me to solve a error</p>
<p>dataset:</p>
<blockquote>
<p>614484565059596288,&quot;Dorian Gray with Rainbow Scarf&quot;,happy</p>
</blockquote>
","python, nlp, runtime-error, bert-language-model, transformer-model",
server hardware dataset for training SpaCy text annotator,"<p>I was looking for a server hardware dataset to train my NER model specifically my SpaCy annotator to detect different components from unstructured data and give me properties associated with the component</p>
<ol>
<li>First option was to manually do it from an open source NER annotator( <a href=""https://tecoholic.github.io/ner-annotator/"" rel=""nofollow noreferrer"">https://tecoholic.github.io/ner-annotator/</a>)</li>
<li>Train it on existing data</li>
<li>create my own dataset</li>
</ol>
<p>if i were to choose any of these routes an advice or suggestions ?or is there any other way i am not looking into</p>
<p><a href=""https://tecoholic.github.io/ner-annotator/"" rel=""nofollow noreferrer"">https://tecoholic.github.io/ner-annotator/</a></p>
","nlp, dataset, spacy, named-entity-recognition",
Langchain sql agent with context,"<p>I am working on a langchain based SQL chat application and wanted my agent to understand context w.r.t the user session. For e.g.
User - What is highest order placed in last placed?
Bot - Order id : XYZ
User - When was this placed?</p>
<p>Here, bot should be able to deduce that 'this' refers to 'order id XYZ' from previous question. How can I incorporate this in my code?</p>
<p>I am tried using ChatHistory but getting context from session history is where I am stuck.</p>
","text, nlp, langchain","<p>I had a similar issue and I solved by “Contextualizing” the questions. Here is an example (not for SQL) : <a href=""https://python.langchain.com/docs/use_cases/question_answering/chat_history/"" rel=""nofollow noreferrer"">https://python.langchain.com/docs/use_cases/question_answering/chat_history/</a></p>
<p>Read the part on Contextualizing the question to reformulate your question based on the history.</p>
<p>In the context of a RAG system that uses a SQL agent, contextualizing questions involves modifying or framing these questions based on the specific data structure and contents of the SQL database. Here, the questions are crafted to match the structure of the database to efficiently retrieve relevant information. This means incorporating the correct table names, field names, and specific terms used within the database into the query. By doing so, the RAG system leverages the SQL agent to execute precise database queries that fetch data relevant to the ongoing conversation or task, thus enhancing the response accuracy and relevance.</p>
<p>Hope this helps!</p>
"
converting json data to vector for better langchain chatbot results,"<p>im creating a chatbot for my university website as a project.
for the last 3 days i've been searching all over the internet how to use <code>Langchain</code> with json data such that my chatbot is fast. i came up with this:</p>
<pre><code>from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import create_json_agent
from langchain.agents.agent_toolkits import JsonToolkit
from langchain.tools.json.tool import JsonSpec
import json
with open(r'...formation_initial.json','r') as f:
    data = json.load(f)
    f.close()
spec = JsonSpec(dict_=data,max_value_length = 4000)
toolkit = JsonToolkit(spec = spec)
agent = create_json_agent(llm=llm,toolkit=toolkit,verbose=True)
print(agent.run('quelle sont les modules de master intelligence artificielle et sciences de donnees semestre 1'))
</code></pre>
<p>it's working and it gives correct answers but the only problem its not fast because it has to explore every level of the json file each time.
upon futher research i found that in order to get the desired speed answer i need to have a vector database, but there is no clear method to turn a json into vectorDB espicially if its a complicated json.
here is a snippet of the json file which basically represent the diplomats you can get in my university:</p>
<pre><code>        {&quot;DEUST&quot;: {
    &quot;Analytique des donn\u00e9es&quot;: {
    &quot;objectif&quot;: &quot;La Licence Science et Techniques en analytique des donn\u00e9es permet aux \u00e9tudiants de doter de comp\u00e9tences en mati\u00e8re d'outils informatiques, destechniques et des m\u00e9thodes statistiques pour permettre d\u2019organiser, de synth\u00e9tiser et de traduire efficacement les donn\u00e9es m\u00e9tier d\u2019uneorganisation. L'\u00e9tudiant doit \u00eatre en mesure d'apporter un appui analytique \u00e0 la conduite d'exploration et \u00e0 l'analyse complexe de donn\u00e9es. &quot;, 
    &quot;COMPETENCES VISEES ET DEBOUCHES&quot;: &quot;Masters en sciences de donn\u00e9es: fouille de donn\u00e9es, business analytiques, blockchain,Masters orient\u00e9s e-Technologies: e-Business, e-Administration et e-LogistiqueFormations d\u2019Ing\u00e9nieurs dans une \u00e9cole d\u2019ing\u00e9nieurs \u00e0 l\u2019issue de la deuxi\u00e8me ou de la troisi\u00e8me ann\u00e9e de licenceData scientistTechnicien sup\u00e9rieur en SGBD R : installation, configuration et administration des SGBDWebMaster et d\u00e9veloppeur de sites web dynamiquesInt\u00e9gration du monde du travail dans les entreprises et les bureaux d\u2019\u00e9tudes &quot;, &quot;coordinateur&quot;: {&quot;nom&quot;: &quot;Pr.BAIDA Ouafae&quot;, &quot;email&quot;: &quot;wbaida@uae.ac.ma&quot;},
     &quot;semesters&quot;: [
    {&quot;Semestre 5&quot;: 
    [&quot; Math\u00e9matiques pour la science des donn\u00e9es&quot;, &quot; Structures des donn\u00e9es avanc\u00e9es et th\u00e9orie des graphes&quot;, &quot; Fondamentaux des bases de donn\u00e9es&quot;, &quot; Algorithmique avanc\u00e9e et programmation&quot;, &quot; D\u00e9veloppement WEB&quot;, &quot; D\u00e9veloppement personnel et intelligence \u00e9motionnelle (Soft skills)&quot;]}, 

{&quot;Semestre 6&quot;: 
    [&quot; Analyse et fouille de donn\u00e9es&quot;, &quot; Syst\u00e8mes et r\u00e9seaux&quot;, &quot; Ing\u00e9nierie des donn\u00e9es &quot;, &quot; PFE&quot;]}]}
</code></pre>
","python, json, nlp, langchain, large-language-model",
How to specify the schema per table for query with llama_index,"<p>When running below script I get this error:</p>
<blockquote>
<p>It appears there was an error in the SQL query provided. The correct query should be:
SELECT e.employee_id, e.employee_name, w.worked_hours
FROM employees e
JOIN worked_hours w ON e.employee_id = w.employee_id;</p>
</blockquote>
<p>When I run this query in e.g. DBVisualizer, it runs perfectly fine, see screenshot. <a href=""https://i.sstatic.net/EDTxA80Z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EDTxA80Z.png"" alt=""enter image description here"" /></a>
The problem seems to be that both table occur in different schemas. I set the schema in <strong>SQLDatabase</strong> but this is not sufficient. Is it possible to define the tables on schema level, like below?</p>
<p><code>include_tables=[(&quot;employees&quot;, schema_1), (&quot;worked_hours&quot;, schema_2)]</code></p>
<p>This is the code i used.</p>
<pre class=""lang-py prettyprint-override""><code>from llama_index.core import SQLDatabase
from llama_index.core.query_engine import NLSQLTableQueryEngine
import logging
logging.basicConfig(level=logging.DEBUG)


engine = get_engine()
include_tables=[&quot;employees&quot;, &quot;worked_hours&quot;]
schema = &quot;sch_employee&quot;
sql_database = SQLDatabase(engine=engine, include_tables=include_tables, schema=schema)
query_engine = NLSQLTableQueryEngine(sql_database=sql_database, tables = include_tables, verbose=True)

query = &quot;Give ma a table of all employees and worked hours.&quot;
r = query_engine.query(query)
print(r)
</code></pre>
","python, nlp, llama-index",
How to Send a Streaming Response via LlamaIndex to a FastAPI Endpoint?,"<p>I need to send a streaming response using LlamaIndex to my FastAPI endpoint. Below is the code I've written so far:</p>
<pre class=""lang-py prettyprint-override""><code>@bot_router.post(&quot;/bot/pdf_convo&quot;)
async def pdf_convo(query: QuestionInput):
    chat_engine = cache[&quot;chat_engine&quot;]
    user_question = query.content
    streaming_response = chat_engine.stream_chat(user_question)
    for token in streaming_response.response_gen:
        print(token, end=&quot;&quot;)
</code></pre>
<p>I'd appreciate any guidance on how to properly implement the streaming response with LlamaIndex. Thank you!</p>
","python, nlp, openai-api, large-language-model, llama-index",
RNN invalidArgumentError,"<p>I am training RNN model for speech to text purpose.
While training RNN, I am getting this weird error called invalid argument error indices[28,0] = -1 is not in [0,169)</p>
<pre><code>model = keras.Sequential()

# Add an embeddings layers
model.add(layers.Embedding(input_dim = 13*13, output_dim = 32, input_length = 13*13))
#model.add(layers.Flatten())
# add an RNN layer
model.add(layers.SimpleRNN(64,activation = 'relu', return_sequences = True))
# add dense layers
model.add(layers.Dense(11))

model.summary()

</code></pre>
<p>My input is of shape (100,13,13) and target shape is (100,11) for training.
What does this error exactly means? Is this points out error in data or model architecture?</p>
<p>I am trying to resolve this error by adjusting dimension in RNN model. But I am facing this error first time and have no idea about it</p>
","deep-learning, nlp, artificial-intelligence, speech-to-text",
Issue with Phi-3 model in Jupyter Notebook,"<p>I am facing an issue while working with the Phi-3 model in Jupyter Notebook. I am getting an error related to the Phi3Config object not having the attribute _attn_implementation.</p>
<p>Here is the relevant code snippet:</p>
<pre><code># Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/Phi-3-mini-128k-instruct&quot;, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(&quot;microsoft/Phi-3-mini-128k-instruct&quot;, trust_remote_code=True)
</code></pre>
<p>This should've loaded model.
The code successfully loads the tokenizer and the model, but when I try to use the model, I encounter the following error:
<code>AttributeError: 'Phi3Config' object has no attribute '_attn_implementation'</code>
I have already loaded the necessary files, including tokenizer_config.json, tokenizer.model, tokenizer.json, added_tokens.json, and special_tokens_map.json.</p>
<p>Here is the complete output for reference:
<a href=""https://i.sstatic.net/AJaYkJo8.png"" rel=""nofollow noreferrer"">Fils</a></p>
","python, jupyter-notebook, nlp, huggingface-transformers",
Urdu dataset Required,"<p>I want to do research on text prediction but dataset which I am using is only 140000 words,I want larger dataset for model training.</p>
<p>Anyone please help to find out Urdu dataset which is larger in size and can be used in text prediction task.</p>
<p>I have tried to search lots of dataset but didn't find any suitable for my project</p>
","csv, nlp, dataset, lstm, urdu",
How does one use vllm with pytorch 2.2.2 and python 3.11?,"<h1>Title: How does one use vllm with pytorch 2.2.2 and python 3.11?</h1>
<p>I'm trying to use the vllm library with pytorch 2.2.2 and python 3.11. Based on the GitHub issues, it seems vllm 0.4.1 supports python 3.11.</p>
<p>However, I'm running into issues with incompatible pytorch versions when installing vllm. The github issue mentions needing to build from source to use pytorch 2.2, but the pip installed version still uses an older pytorch.</p>
<p>I tried creating a fresh conda environment with python 3.11 and installing vllm:</p>
<pre class=""lang-bash prettyprint-override""><code>$ conda create -n vllm_test python=3.11
$ conda activate vllm_test
(vllm_test) $ pip install vllm
...
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
vllm 0.4.1 requires torch==2.1.2, but you have torch 2.2.2 which is incompatible.
</code></pre>
<p>I also tried installing pytorch 2.2.2 first and then vllm:</p>
<pre class=""lang-bash prettyprint-override""><code>(vllm_test) $ pip install torch==2.2.2
(vllm_test) $ pip install vllm
...
Building wheels for collected packages: vllm
  Building wheel for vllm (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  × Building wheel for vllm (pyproject.toml) did not run successfully.
  │ exit code: 1
</code></pre>
<p>Can someone clarify what versions of vllm, pytorch and python work together currently? Is there a recommended clean setup to use vllm with the latest pytorch 2.2.2 and python 3.11?</p>
<p>I've tried creating fresh conda environments, but still run into version conflicts. Any guidance on the right installation steps would be much appreciated. Thanks!</p>
<p>ref: <a href=""https://github.com/vllm-project/vllm/issues/2747"" rel=""nofollow noreferrer"">https://github.com/vllm-project/vllm/issues/2747</a></p>
","python, pytorch, nlp, huggingface-transformers, vllm",
DFS search sibling node in specific condition else search children node,"<p>I have to do a DFS to search a tree, the DFS will first generate a result by the value of the node. The function check_for_three_kind_of_response is a function that checks if a question is related to a sentence. The result would be three responses, -1 means the sentence is not related to the question, 0 means the sentence is related to the question but the sentence can't help to answer the question, and last, return the answer(text) if the sentence can answer the question. If the result is -1 the DFS should search the sibling node, when the result is 0, the DFS should search the children node. Also, when the result is -1 but doesn't have a sibling node, it should also search its children node. At last, if the result is text, it should just return the text.</p>
<p>This is what I have tried, however, I was stuck in the condition when the result equals -1. Also, my tree is not a binary tree, its children are listed.</p>
<pre><code>#                 Head
#                  |
#                Body
#                /  \
#      paragraph1       paragraph2
#       /   \             /  \  
# Sentence Sentence Sentence Sentence
</code></pre>
<p>This is what the tree looks like, the DFS first searches the head and returns -1 if it's not related to the question. Then, since the head doesn't have a siblings node, so it goes to its children node, which is the body. If it still returns -1, it would search its children node paragraph 1 since it didn't have a siblings node. When paragraph1 returns -1 in the function, it would jump to paragraph2. If it returns 0 it will go down to sentence. In a sentence, if the first one returns 0 it will search the next sentence while -1 will do the same thing. If all sentences return -1, the DFS should stop or otherwise go back to paragraph 2 to do the same routine again.</p>
<pre><code>class TreeNode:
    def __init__(self, value, parent = None):
        self.value = value
        self.children = []

def dfs(node, question):
    result = check_for_three_kind_of_response(question, node.value)
    # Check if the search function returns 0, -1, or text
    if result == &quot;-1&quot;:
        return &quot;-1&quot;
    elif result != &quot;0&quot;:
        return result
    for c in node.children:
        result = dfs(c, question)
        if result != &quot;-1&quot;:
            return result
    return &quot;-1&quot;
</code></pre>
","python-3.x, if-statement, nlp, tree, depth-first-search",
Pandarallel is failing with SSLContext error with Openai upgrade,"<p>I have upgraded openai from 0.28.0 to openai==1.23.5.</p>
<p>My parallel calls to openai with Pandarallel was working well with openai==0.28.0 version.</p>
<p>But failing with the below error after upgrading to openai==1.23.5</p>
<pre><code>File &quot;/app/imssumm/Summ_parallel.py&quot;, line 239, in call_iterative_summ_logic

prompt_df_1[&quot;result&quot;] = prompt_df_1.parallel_apply(lambda x: 
self.summarize(x[&quot;to_be_summarized&quot;],x[&quot;token_len&quot;]), axis=1)

File &quot;/usr/local/lib/python3.8/site-packages/pandarallel/core.py&quot;, line 265, in closure
dilled_user_defined_function = dill.dumps(user_defined_function)

File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 263, in dumps
dump(obj, file, protocol, byref, fmode, recurse, **kwds)#, strictio)

File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 235, in dump
Pickler(file, protocol, **_kwds).dump(obj)

File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 394, in dump
StockPickler.dump(self, obj)

File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 487, in dump
self.save(obj)
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 388, in save
StockPickler.save(self, obj, save_persistent_id)

File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 560, in save
f(self, obj) # Call unbound method with explicit self
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 1824, in save_function
_save_with_postproc(pickler, (_create_function, (
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 1089, in _save_with_postproc
pickler.save_reduce(*reduction)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 692, in save_reduce
save(args)
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 388, in save
StockPickler.save(self, obj, save_persistent_id)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 560, in save
f(self, obj) # Call unbound method with explicit self
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 886, in save_tuple
save(element)
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 388, in save
StockPickler.save(self, obj, save_persistent_id)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 603, in save
self.save_reduce(obj=obj, *rv)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 717, in save_reduce
save(state)
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 388, in save
StockPickler.save(self, obj, save_persistent_id)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 560, in save
f(self, obj) # Call unbound method with explicit self
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 1186, in save_module_dict
StockPickler.save_dict(pickler, obj)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 971, in save_dict
self._batch_setitems(obj.items())
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 997, in _batch_setitems
save(v)
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 388, in save
StockPickler.save(self, obj, save_persistent_id)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 603, in save
self.save_reduce(obj=obj, *rv)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 717, in save_reduce
save(state)
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 388, in save
StockPickler.save(self, obj, save_persistent_id)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 560, in save
f(self, obj) # Call unbound method with explicit self
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 1186, in save_module_dict
StockPickler.save_dict(pickler, obj)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 971, in save_dict
self._batch_setitems(obj.items())
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 997, in _batch_setitems
save(v)
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 388, in save
StockPickler.save(self, obj, save_persistent_id)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 603, in save
self.save_reduce(obj=obj, *rv)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 717, in save_reduce
save(state)
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 388, in save
StockPickler.save(self, obj, save_persistent_id)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 560, in save
f(self, obj) # Call unbound method with explicit self
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 1186, in save_module_dict
StockPickler.save_dict(pickler, obj)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 971, in save_dict
self._batch_setitems(obj.items())
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 997, in _batch_setitems
save(v)
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 388, in save
StockPickler.save(self, obj, save_persistent_id)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 603, in save
self.save_reduce(obj=obj, *rv)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 717, in save_reduce
save(state)
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 388, in save
StockPickler.save(self, obj, save_persistent_id)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 560, in save
f(self, obj) # Call unbound method with explicit self
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 1186, in save_module_dict
StockPickler.save_dict(pickler, obj)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 971, in save_dict
self._batch_setitems(obj.items())
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 1002, in _batch_setitems
save(v)
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 388, in save
StockPickler.save(self, obj, save_persistent_id)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 603, in save
self.save_reduce(obj=obj, *rv)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 717, in save_reduce
save(state)
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 388, in save
StockPickler.save(self, obj, save_persistent_id)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 560, in save
f(self, obj) # Call unbound method with explicit self
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 1186, in save_module_dict
StockPickler.save_dict(pickler, obj)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 971, in save_dict
self._batch_setitems(obj.items())
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 997, in _batch_setitems
save(v)
File &quot;/usr/local/lib/python3.8/site-packages/dill/_dill.py&quot;, line 388, in save
StockPickler.save(self, obj, save_persistent_id)
File &quot;/usr/lib64/python3.8/pickle.py&quot;, line 578, in save
rv = reduce(self.proto)

TypeError: cannot pickle 'SSLContext' object
</code></pre>
<p>My code structure with pandarallel call looks like this:</p>
<pre><code>
import pandas as pd
from pandarallel import pandarallel
from openai import AzureOpenAI

openai_client = AzureOpenAI(
             api_key=config_nlp.openai_api_key,
             api_version=config_nlp.openai_api_version,
            azure_endpoint =config_nlp.openai_api_base,)
            
            
def call_summ_logic_new(self,prompt,max_len):


    api_start_time = datetime.now()
    response = self.openai_client.chat.completions.create(model=summary_model,
    messages = messages,
    temperature=0.8,
    max_tokens=int(max_len)
    )



pandarallel.initialize(progress_bar=True, nb_workers = 7)
recs_pre_summ_tkn = [(&quot;data1&quot;,23),(&quot;data2&quot;,24),(&quot;data4&quot;,123),(&quot;data5&quot;,243)]
prompt_df_1 = pd.DataFrame(recs_pre_summ_tkn,columns =[&quot;to_be_summarized&quot;,&quot;token_len&quot;])
prompt_df_1[&quot;result&quot;] = prompt_df_1.parallel_apply(lambda x: self.summarize(x[&quot;to_be_summarized&quot;],x[&quot;token_len&quot;]), axis=1)
</code></pre>
<p>Any suggestions apart from reverting back to openai 0.28.0?</p>
<p>Thanks</p>
","pandas, nlp, openai-api, azure-openai, pandarallel",
"AttributeError: module &#39;click.utils&#39; has no attribute &#39;_expand_args&#39;, when i&#39;m tring to install en_core_web_sm","<p>I am trying to install &quot;en_core_web_sm&quot;,
the commands i ran is:</p>
<ol>
<li>pip install spacy ( which ran perfectly and got installed)</li>
<li>python -m spacy download en_core_web_sm ( here i'm getting error &quot;AttributeError: module 'click.utils' has no attribute '_expand_args'&quot; )</li>
</ol>
<p><a href=""https://i.sstatic.net/fpCqO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fpCqO.png"" alt=""enter image description here"" /></a></p>
<p>Please help to me resolve this problem.</p>
<p>#CODE</p>
<p>import nl_core_news_sm</p>
<h1>Load pre-trained Dutch language model</h1>
<p>nlp = nl_core_news_sm.load()</p>
<h1>File Extension. set as 'pdf' or as 'doc(x)'</h1>
<p>extension = 'pdf'</p>
<p>def create_tokenized_texts_list(extension):
'''Create two lists, one with the names of the candidate and one with the tokenized
resume texts extracted from either a .pdf or .doc'''
resume_texts, resume_names = [], []</p>
<pre><code># Loop over the contents of the directory containing the resumes, filtering by .pdf or .doc(x)
for resume in list(filter(lambda x: extension in x, os.listdir(PROJECT_DIR + '/CV'))):
    if extension == 'pdf':
        # Read in every resume with pdf extension in the directory
        resume_texts.append(nlp(extract_text_from_pdf(PROJECT_DIR + '/CV/' + resume)))
    elif 'doc' in extension:
        # Read in every resume with .doc or .docx extension in the directory
        resume_texts.append(nlp(extract_text_from_word(PROJECT_DIR + '/CV/' + resume)))
        
    resume_names.append(resume.split('_')[0].capitalize())
</code></pre>
<p>here it the code that i'm trying to run</p>
","python-3.x, nlp, spacy, spacy-3",
&quot;Text Input in Streamlit Removes Entered Text Upon Hitting Enter Key: How to Fix?&quot;,"<pre><code>the input stored in user_question i need when press enter the txt in textbox getting clear and the output displing what can i did i tried js and streamlit function but when enter input and press send btn or press enter no output displaying 
</code></pre>
<p><a href=""https://i.sstatic.net/MheP5.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>here is an example of an image entered word pressed, and enter<br />
textbox didnot not get clear</p>
","python, nlp, artificial-intelligence, streamlit, large-language-model",
How to install Detectron2,"<p>I am installing layout-parser and following this <a href=""https://layout-parser.readthedocs.io/en/latest/notes/installation.html"" rel=""nofollow noreferrer"">link</a>. Did not face any issues with the following packages.  </p>
<pre><code>pip install layoutparser    
pip install &quot;layoutparser[effdet]&quot;    
pip install layoutparser torchvision     
pip install &quot;layoutparser[paddledetection]&quot;    
pip install &quot;layoutparser[ocr]&quot; 
</code></pre>
<p>But I am not able to install detectron2</p>
<pre><code>pip install &quot;git+https://github.com/facebookresearch/detectron2.git@v0.5#egg=detectron2&quot; 
</code></pre>
<p>while installing this package I am getting this error    </p>
<blockquote>
<p>ERROR: Could not find a version that satisfies the requirement detectron2 (unavailable) (from versions: none)</p>
<p>ERROR: No matching distribution found for detectron2 (unavailable)</p>
</blockquote>
<p>I followed the same installation guide in Google collab and it worked but not able to install them in my Azure workspace.  </p>
","python, nlp, data-science, ocr, python-3.10",
Is there any way to standardize different text that represents the same thing?,"<p>I am collecting product data from different supermarket's website. But I found that each store uses different name to represent the product.</p>
<p>For example:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Product name</th>
<th>Store</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>360° Toothbrush With Tongue And Cheek Cleaner</td>
<td>NoFrills</td>
<td><a href=""https://www.nofrills.ca/360-toothbrush-with-tongue-and-cheek-cleaner/p/20306410001_EA"" rel=""nofollow noreferrer"">product link</a></td>
</tr>
<tr>
<td>MEDIUM TOOTHBRUSH, 360°</td>
<td>FoodBasics</td>
<td><a href=""https://www.foodbasics.ca/aisles/health-beauty/oral-care/toothbrushes/medium-toothbrush/p/058000006140"" rel=""nofollow noreferrer"">product link</a></td>
</tr>
</tbody>
</table></div>
<p>and I want to standardized this as &quot;360° Toothbrush&quot;.</p>
<p>My approach is first try to tokenized the product name and then compare the similarity by cosine law. But I found that this is not the right direction as I am seeking the <strong>standarized name</strong> before flushing to database (in order to categorize the &quot;same&quot; product by the store).</p>
<p>Please find my testing script below (thanks ChatGPT!)</p>
<pre><code>import torch
from transformers import BertTokenizer, BertModel

def get_sentence_vector(text):
  
    # Initialize the tokenizer and model
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')

    # Ensure the model is in evaluation mode
    model.eval()

    # Encode the text
    inputs = tokenizer(text, return_tensors='pt')

    # Perform a forward pass without gradient calculation
    with torch.no_grad():
        outputs = model(**inputs)

    # Extract the last hidden state
    last_hidden_states = outputs.last_hidden_state

    # Aggregate the hidden states using mean pooling
    sentence_vector = torch.mean(last_hidden_states, dim=1)

    return sentence_vector


def cosine_similarity(vec1, vec2):
    # Calculate the cosine similarity
    cosine_sim = torch.nn.functional.cosine_similarity(vec1, vec2, dim=1)
    
    return cosine_sim.item() 


example_text = &quot;360° Toothbrush With Tongue And Cheek Cleaner&quot;
example_text_2 = &quot;MEDIUM TOOTHBRUSH, 360°&quot;
example_text_3 = &quot;Hair Expertise Hyaluron Plump Shampoo, with Hyaluronic Acid&quot;

vector = get_sentence_vector(example_text)
vector_2 = get_sentence_vector(example_text_2)
vector_3 = get_sentence_vector(example_text_3)

print(cosine_similarity(vector, vector_2)) # 0.8426903486251831
print(cosine_similarity(vector, vector_3)) # 0.7190334796905518
print(cosine_similarity(vector_2, vector_3)) # 0.622465193271637
</code></pre>
<p>The above approach indeed give me some sense, the same product gives higher similarity. But how to make use of this result to give a standardized name?</p>
","python, machine-learning, nlp, word-embedding",
No Improvement in Results after Implementing Unsupervised Denoising Training Technique for T5 Model using Hugging Face,"<p>I am currently working on implementing an unsupervised denoising training technique using the Hugging Face library for the T5 model. I have written several versions of the code, but only one seems to run without errors. However, after training the model with this code, I am not seeing any improvement in the results.</p>
<p>Here is the code that I have been using:</p>
<pre class=""lang-py prettyprint-override""><code>

import torch
import random
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments

def generate_masked_sequence(paragraph, mask_prob):
    words = paragraph.split()
    num_words = len(words)
    num_extra_ids = max(1, round(mask_prob * num_words))
    extra_id_positions = random.sample(range(num_words), num_extra_ids)

    extra_id_positions.sort()
    input_ids = []
    labels = []
    for i, word in enumerate(words):
        if extra_id_positions and i == extra_id_positions[0]:
            input_ids.append(f&quot;&lt;extra_id_{len(input_ids)}&gt;&quot;)
            labels.append(word)
            extra_id_positions.pop(0)  # Remove the used position
        else:
            input_ids.append(word)
            labels.append(f&quot;&lt;extra_id_{len(labels)}&gt;&quot;)
    
    return &quot; &quot;.join(input_ids), &quot; &quot;.join(labels)

paragraphs = load_dataset('nima-nLc/dsm', split='train')

tokenizer = AutoTokenizer.from_pretrained(&quot;google/flan-t5-small&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;google/flan-t5-small&quot;)


device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model.to(device)

# Hyperparameters
batch_size = 32
learning_rate = 5e-5
epochs = 3
mask_prob = 0.15

def preprocess_data(examples):
    input_seqs = []
    label_seqs = []
    for paragraph in examples['text']:
        input_seq, label_seq = generate_masked_sequence(paragraph, mask_prob)
        input_seqs.append(input_seq)
        label_seqs.append(label_seq)
    return {&quot;input_ids&quot;: tokenizer(input_seqs, return_tensors=&quot;pt&quot;, padding=True, truncation=True).input_ids,
            &quot;labels&quot;: tokenizer(label_seqs, return_tensors=&quot;pt&quot;, padding=True, truncation=True).input_ids}
train_dataset = paragraphs.map(preprocess_data, batched=True)

training_args = TrainingArguments(
    per_device_train_batch_size=batch_size,
    learning_rate=learning_rate,
    num_train_epochs=epochs,
    report_to=&quot;none&quot;,  
    logging_steps=500,  
    save_steps=1000,  
    output_dir=&quot;./checkpoints&quot; , 
    resume_from_checkpoint = True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=reduced_dataset
)

print('Training Started')
trainer.train()
print('Training Finished')


</code></pre>
<p>Despite the code running successfully, the results from the model post-training do not show any noticeable improvements. I am unsure if the issue lies in the code itself or the approach I am using to implement the unsupervised denoising training technique.</p>
<p>I would greatly appreciate any insights or suggestions on what might be going wrong, or how I could modify my approach or code to improve the results.</p>
<p>Thank you in advance for your help!</p>
","nlp, huggingface-transformers, large-language-model, encoder-decoder",
How to Generate Text with Specific Length Using AI API,"<p>I am attempting to generate text outputs that are exactly a certain number of characters or words long using AI API (OpenAI GPT, Claude, Gemini...), but I'm facing difficulties. Here's what I've tried so far:</p>
<p><strong>Setting Max Tokens</strong>: I've used the max_tokens parameter hoping to limit the output length, but then text is truncated.</p>
<p><strong>Explicit Prompt Requests</strong>: I've tried including explicit instructions in the prompt about the desired length. However, this approach has not produced the precise output lengths I need (ex: gives 600 words instead of 800).</p>
<p>I am looking for suggestions on how to configure the API calls or promtp to achieve exact output lengths. Is there a way to better utilize OpenAI/Claude’s parameters, or is there a method to post-process the text to fit the required length?</p>
<p>I need this because I inject the text in slides. Any alternative solution?</p>
","python, nlp, openai-api, text-generation","<p>This won't work. LLMs are bad at counting. There are a lot of humorous examples over internet.</p>
"
Training a LLM to execute functions,"<p>I want develop a chatbot that is able to perform actions and answer questions in a given Smart Home environment.</p>
<p>I am curious how to do this with an LLM. How can I customize/train a model to execute code?
Just a simple example: When I tell the chatbot &quot;turn on the light in the living room&quot; it should answer &quot;I will turn on the light in the living room&quot; and at the same time turn it on in the background (assuming I have an API / code that I am able to call).</p>
<p>Are there some resources or even examples you can share to learn about the process?</p>
<p>I know a bit about customizing models like adding a System Message or adjusting the temperature of the model and I also trained an LLM to generate software requirements before. But I don't know how to train a model to perform an action like turning a smart device on or off.</p>
<p>I am currently using Ollama to manage and customize models.</p>
","python, machine-learning, nlp, chatbot, large-language-model",
Python Libraries for Sentence Classification Based on Keyword,"<p>I want to separate a list of sentences into two lists based on whether they match the sentiment of a specific keyword. For example:</p>
<pre><code>valid_keyword = &quot;Guest Accepted&quot;

sentences = [
    &quot;Guest Allowed&quot;, &quot;Max Guest Allowed 1&quot;, &quot;Guest Not Allowed&quot;, 
    &quot;No Guest Allowed&quot;, &quot;Guest Restricted&quot;, &quot;Max Guest Allowed 0&quot;, 
    &quot;Guest Allowed on Request&quot;
]
</code></pre>
<p>I want to separate the list in the following way:</p>
<p>valid_sentences, which contains sentences that match the sentiment of the valid_keyword (&quot;Guest Accepted&quot;).</p>
<p>invalid_sentences, which contains sentences that do not match the sentiment of the valid_keyword.</p>
<p>The expected output would be:</p>
<pre><code>valid_sentences = [&quot;Guest Allowed&quot;, &quot;Max Guest Allowed 1&quot;, &quot;Guest Allowed on Request&quot;]
invalid_sentences = [&quot;Guest Not Allowed&quot;, &quot;No Guest Allowed&quot;, &quot;Guest Restricted&quot;, &quot;Max Guest Allowed 0&quot;]
</code></pre>
<p>I already tried <a href=""https://vadersentiment.readthedocs.io/en/latest/index.html#"" rel=""nofollow noreferrer"">VaderSentiment</a>, but didn't get what I expected.
What are the suitable Python libraries that I should go for this process that will give maximum accuracy?</p>
","python, nlp, sentiment-analysis",
How to get all noun phrases in Spacy,"<p>I am new to <code>Spacy</code> and I would like to extract ""all"" the noun phrases from a sentence. I'm wondering how I can do it. I have the following code:</p>

<pre><code>import spacy

nlp = spacy.load(""en"")

file = open(""E:/test.txt"", ""r"")
doc = nlp(file.read())
for np in doc.noun_chunks:
    print(np.text)
</code></pre>

<p>But it returns only the base noun phrases, that is, phrases which don't have any other <code>NP</code> in them. That is, for the following phrase, I get the result below:</p>

<p>Phrase: <code>We try to explicitly describe the geometry of the edges of the images.</code></p>

<p>Result: <code>We, the geometry, the edges, the images</code>.</p>

<p>Expected result: <code>We, the geometry, the edges, the images, the geometry of the edges of the images, the edges of the images.</code></p>

<p>How can I get all the noun phrases, including nested phrases?</p>
","python, python-3.x, nlp, spacy","<p>Please see commented code below to recursively combine the nouns. Code inspired by the <a href=""https://spacy.io/usage/linguistic-features#navigating-around"" rel=""noreferrer"">Spacy Docs here</a></p>

<pre><code>import spacy

nlp = spacy.load(""en"")

doc = nlp(""We try to explicitly describe the geometry of the edges of the images."")

for np in doc.noun_chunks: # use np instead of np.text
    print(np)

print()

# code to recursively combine nouns
# 'We' is actually a pronoun but included in your question
# hence the token.pos_ == ""PRON"" part in the last if statement
# suggest you extract PRON separately like the noun-chunks above

index = 0
nounIndices = []
for token in doc:
    # print(token.text, token.pos_, token.dep_, token.head.text)
    if token.pos_ == 'NOUN':
        nounIndices.append(index)
    index = index + 1


print(nounIndices)
for idxValue in nounIndices:
    doc = nlp(""We try to explicitly describe the geometry of the edges of the images."")
    span = doc[doc[idxValue].left_edge.i : doc[idxValue].right_edge.i+1]
    span.merge()

    for token in doc:
        if token.dep_ == 'dobj' or token.dep_ == 'pobj' or token.pos_ == ""PRON"":
            print(token.text)
</code></pre>
"
Custom Name Entity Regognition,"<p>I have the following sentence:</p>
<pre><code>text=&quot;The weather is extremely severe in England&quot;
</code></pre>
<p>I want to perform a custom <code>Name Entity Recognition (NER)</code> procedure</p>
<p>First a normal <code>NER</code> procedure will output <code>England</code> with a <code>GPE</code> label</p>
<pre><code>pip install spacy

!python -m spacy download en_core_web_lg

import spacy
nlp = spacy.load('en_core_web_lg')

doc = nlp(text)

for ent in doc.ents:
    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))

Result: England - GPE - Countries, cities, states
</code></pre>
<p>However, I want the whole sentence to take the tag <code>High-Severity</code>.</p>
<p>So I am doing the following procedure:</p>
<pre><code>from spacy.strings import StringStore

new_hash = StringStore([u'High_Severity']) # &lt;-- match id
nlp.vocab.strings.add('High_Severity')

from spacy.tokens import Span

# Get the hash value of the ORG entity label
High_Severity = doc.vocab.strings[u'High_Severity']  

# Create a Span for the new entity
new_ent = Span(doc, 0, 7, label=High_Severity)

# Add the entity to the existing Doc object
doc.ents = list(doc.ents) + [new_ent]
</code></pre>
<p>I am taking the following error:</p>
<pre><code>ValueError: [E1010] Unable to set entity information for token 6 which is included in more than one span in entities, blocked, missing or outside.
</code></pre>
<p>From my understanding, this is happening because <code>NER</code> has already recognised <code>England</code> as <code>GRE</code> and cannot add a label over the existing label.</p>
<p>I tried to execute the custom <code>NER</code> code (i.e, without first running the normal <code>NER</code> code) but this did not solve my problem.</p>
<p>Any ideas on how to Solve this problem?</p>
","python, python-3.x, nlp, spacy, named-entity-recognition","<p>Indeed it looks like NER do not allow overlapping, and that is your problem, your second part of the code tries to create a ner containing another ner, hence, it fails.
see in:</p>
<p><a href=""https://github.com/explosion/spaCy/discussions/10885"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/discussions/10885</a></p>
<p>and therefore spacy has spans categorization.</p>
<p>I did not find yet the way to characterized a predefined span (not coming from a trained model)</p>
"
ValueError: Cannot use a compiled regex as replacement pattern with regex=False,"<p>I'm doing a project, on Google Colab, where I use the following version:
<code>!pip install &quot;gensim==4.2.0&quot; !pip install &quot;texthero==1.0.5&quot;</code></p>
<p>Until recently, I received the following warning:
<em>FutureWarning: The default value of regex will change from True to False in a future version.
return input.str.replace(r&quot;^\d+\s|\s\d+\s|\s\d+$&quot;, &quot; &quot;)</em></p>
<p>But the execution worked normally. Now, I'm getting the following error:
<img src=""https://github.com/jbesomi/texthero/assets/43450181/99ad9b5c-7c64-4edb-8b83-3448475a4cef"" alt=""image"" /></p>
<p>How should I proceed?</p>
<p>I tried different versions, but the problem persists.</p>
","python, text, nlp","<p>This is a texthero bug triggering a pandas error.</p>
<p>Pandas <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html"" rel=""nofollow noreferrer""><code>str.replace</code></a> now uses <code>regex=False</code> by default:</p>
<p>Texthero's <a href=""https://github.com/jbesomi/texthero/blob/master/texthero/preprocessing.py#L104"" rel=""nofollow noreferrer""><code>replace_digits</code></a> function hasn't been updated in two years and doesn't explicitly pass <code>regex=True</code>:</p>
<pre><code>    if only_blocks:
        pattern = r&quot;\b\d+\b&quot;
        return s.str.replace(pattern, symbols)
    else:
        return s.str.replace(r&quot;\d+&quot;, symbols)
</code></pre>
<p>You should fill a bug report to texthero, there are probably several other occurrences of <code>str.replace</code> to fix.</p>
<p>In there meantime you can patch the library by changing the code to:</p>
<pre><code>    if only_blocks:
        pattern = r&quot;\b\d+\b&quot;
        return s.str.replace(pattern, symbols, regex=True)
    else:
        return s.str.replace(r&quot;\d+&quot;, symbols, regex=True)
</code></pre>
<p>Or use a pandas version prior to <code>2</code> (e.g. <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.2/reference/api/pandas.Series.str.replace.html"" rel=""nofollow noreferrer""><code>1.5.2</code></a>)</p>
"
tokenize multilple files python,"<p>I am currently trying to attempting to tokenize large text, however I have a lot of files in the directory that I want to tokenize as this is very time consuming to do 1 by 1.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch


tokenizer = AutoTokenizer.from_pretrained(&quot;joeddav/distilbert-base-uncased-go-emotions-student&quot;)
model = AutoModelForSequenceClassification.from_pretrained(&quot;joeddav/distilbert-base-uncased-go-emotions-student&quot;)

txt=&quot;....&quot;

words_input_dir = &quot;/content/sample_data/&quot;

for filename in os.listdir(words_input_dir):
    if filename.endswith(&quot;.txt&quot;):
        with open(filename, &quot;r&quot;) as input_file:
            input_tokens = word_tokensize(input_file.read())

tokens = tokenizer.encode_plus(input_file.read(), add_special_tokens = False, return_tensors = 'pt')

print(len(tokens))
</code></pre>
<p>tokens</p>
<p>before I added the loop the original read</p>
<pre><code>tokens = tokenizer.encode_plus(txt, add_special_tokens = False, return_tensors = 'pt')
</code></pre>
<p>Regards,</p>
<p>I tried to loop the tokens function however it appears to only be taking specific printed text.</p>
","python, nlp, token, huggingface-transformers, huggingface-tokenizers",
converting safe.tensor to pytorch bin files,"<p>I fine-tuned my transformer model with HuggingFace. It has provided me a <code>model.safetensor</code> file for later use.</p>
<p>I want to plug-in the model to a old framework which takes <code>pytorch.bin</code> files only. I wonder how can I <strong>downgrade</strong> my model to fit the framework.</p>
<p>I am not sure what is the best way to do such moodel format conversion.</p>
<p>Thanks for your help.</p>
","pytorch, nlp, huggingface-transformers, tensor",
Training process with the default Spacy configuration file does not produce any log output,"<p>My config.cfg is generated according to the instructions provided at <a href=""https://spacy.io/usage/training/"" rel=""nofollow noreferrer"">Spacy Quickstart</a>
I run the train command:<code>python -m spacy train config.cfg --output ./output --paths.train .\train.spacy --paths.dev .\dev.spacy </code></p>
<pre><code>[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = &quot;pytorch&quot;
seed = 0

[nlp]
lang = &quot;en&quot;
pipeline = [&quot;transformer&quot;,&quot;ner&quot;]
batch_size = 128
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}
vectors = {&quot;@vectors&quot;:&quot;spacy.Vectors.v1&quot;}

[components]

[components.ner]
factory = &quot;ner&quot;
incorrect_spans_key = null
moves = null
scorer = {&quot;@scorers&quot;:&quot;spacy.ner_scorer.v1&quot;}
update_with_oracle_cut_size = 100

[components.ner.model]
@architectures = &quot;spacy.TransitionBasedParser.v2&quot;
state_type = &quot;ner&quot;
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = false
nO = null

[components.ner.model.tok2vec]
@architectures = &quot;spacy-transformers.TransformerListener.v1&quot;
grad_factor = 1.0
pooling = {&quot;@layers&quot;:&quot;reduce_mean.v1&quot;}
upstream = &quot;*&quot;

[components.transformer]
factory = &quot;transformer&quot;
max_batch_items = 4096
set_extra_annotations = {&quot;@annotation_setters&quot;:&quot;spacy-transformers.null_annotation_setter.v1&quot;}

[components.transformer.model]
@architectures = &quot;spacy-transformers.TransformerModel.v3&quot;
name = &quot;roberta-base&quot;
mixed_precision = false

[components.transformer.model.get_spans]
@span_getters = &quot;spacy-transformers.strided_spans.v1&quot;
window = 128
stride = 96

[components.transformer.model.grad_scaler_config]

[components.transformer.model.tokenizer_config]
use_fast = true

[components.transformer.model.transformer_config]

[corpora]

[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[corpora.train]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.train}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[training]
accumulate_gradient = 3
dev_corpus = &quot;corpora.dev&quot;
train_corpus = &quot;corpora.train&quot;
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
patience = 1600
max_epochs = 0
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null
before_update = null

[training.batcher]
@batchers = &quot;spacy.batch_by_padded.v1&quot;
discard_oversize = true
size = 2000
buffer = 256
get_length = null

[training.logger]
@loggers = &quot;spacy.ConsoleLogger.v1&quot;
progress_bar = false

[training.optimizer]
@optimizers = &quot;Adam.v1&quot;
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001

[training.optimizer.learn_rate]
@schedules = &quot;warmup_linear.v1&quot;
warmup_steps = 250
total_steps = 20000
initial_rate = 0.00005

[training.score_weights]
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0
ents_per_type = null

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]
</code></pre>
<p>There are no logs generated during the training process:</p>
<p>=========================== Initializing pipeline ===========================
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaMode
l: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_
head.layer_norm.weight', 'lm_head.layer_norm.bias']</p>
<ul>
<li>This IS expected if you are initializing RobertaModel from the checkpoint of a model trained o
n another task or with another architecture (e.g. initializing a BertForSequenceClassification m
odel from a BertForPreTraining model).</li>
<li>This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that
you expect to be exactly identical (initializing a BertForSequenceClassification model from a Be
rtForSequenceClassification model).
✔ Initialized pipeline</li>
</ul>
<p>============================= Training pipeline =============================
ℹ Pipeline: ['transformer', 'ner']
ℹ Initial learn rate: 0.0
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE</p>
<hr />
<p>The initial_rate specified in the config file is 0.00005, however, during training, it displays 'Initial learn rate: 0.0'.  Is this inconsistency the cause of the problem?
I want to know how to make training process normal.</p>
","nlp, spacy, named-entity-recognition, spacy-transformers",
How to fix error in google Colab when using model and tokenizer from hugging face: OSError: Can&#39;t load tokenizer for &#39;google/mobilebert-uncased&#39;,"<p>from transformers import MobileBertTokenizer, MobileBertForSequenceClassification</p>
<p>model_name = &quot;google/mobilebert-uncased&quot;
num_labels = 4  # Adjust this based on the number of distinct intents you have</p>
<p>tokenizer = MobileBertTokenizer.from_pretrained(model_name)
model = MobileBertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))</p>
<h2>getting this error:</h2>
<p>OSError                                   Traceback (most recent call last)
 in &lt;cell line: 10&gt;()
8 num_labels = 4  # Adjust this based on the number of distinct intents you have
9
---&gt; 10 tokenizer = MobileBertTokenizer.from_pretrained(model_name)
11 model = MobileBertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))</p>
<p>/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)
2030
2031         if all(full_file_name is None for full_file_name in resolved_vocab_files.values()):
-&gt; 2032             raise EnvironmentError(
2033                 f&quot;Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from &quot;
2034                 &quot;'https://huggingface.co/models', make sure you don't have a local directory with the same name. &quot;</p>
<p>OSError: Can't load tokenizer for 'google/mobilebert-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'google/mobilebert-uncased' is the correct path to a directory containing all relevant files for a MobileBertTokenizer tokenizer.</p>
<p>The code runs perfectly fine on jupyter notebook but takes a long time. it is supposed to take the model and tokenizer from transformers and just get running</p>
","nlp, huggingface-transformers, bert-language-model",
How can I implement a multi-label sequence classification model using TF-IDF vectorization followed by LSTM algorithm?,"<p>This is the link for the dataset- <a href=""https://huggingface.co/datasets/surrey-nlp/PLOD-CW"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/surrey-nlp/PLOD-CW</a></p>
<p>You can load the dataset using this line- load_dataset(&quot;surrey-nlp/PLOD-CW&quot;)</p>
<p>I have been trying to code in which first I am vectorising the dataset using word2vec then applying LSTM algorithm to for multilabel sequence classifocation but I am facing the error.</p>
<p>This is the code that I have created-</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

datasets = load_dataset(&quot;surrey-nlp/PLOD-CW&quot;)

# Preprocess the data for a single dictionary
def preprocess_data(data):
    processed_data = []
    for sentence in data['tokens']:
        processed_sentence = []
        for word in sentence:
            processed_sentence.append(word.lower())
        processed_data.append(processed_sentence)
    return processed_data

# Convert the data into Tfidf vectors
# Convert the data into Tfidf vectors
def tfidf_vectorization(data):
    data_strings = [' '.join(sentence) for sentence in data]
    vectorizer = TfidfVectorizer(max_features=5000)
    tfidf_matrix = vectorizer.fit_transform(data_strings)
    return tfidf_matrix, vectorizer

# Build the LSTM model
def build_lstm_model(input_shape, num_labels):
    inputs = Input(shape=input_shape)
    embedding_layer = Embedding(input_dim=5000, output_dim=100, input_length=input_shape[0])(inputs)
    lstm_layer = LSTM(units=100, return_sequences=False)(embedding_layer)
    dense_layer = Dense(units=num_labels, activation='sigmoid')(lstm_layer)
    model = Model(inputs=inputs, outputs=dense_layer)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Preprocess the data for the list of dictionaries
processed_data = [preprocess_data(dataset) for dataset in datasets.values()]

# Convert the data into Tfidf vectors
tfidf_matrix, vectorizer = tfidf_vectorization(processed_data)

# Prepare the data for LSTM input
max_sequence_length = 50
X = tfidf_matrix.toarray()
X = pad_sequences(X, maxlen=max_sequence_length, padding='post', truncating='post')

# Prepare the labels
num_labels = len(np.unique(np.concatenate([dataset['train']['ner_tags'] for dataset in datasets])))
Y = np.concatenate([np.array(dataset['train']['ner_tags']) for dataset in datasets])
Y = to_categorical(Y, num_labels)

# Split the data into training and validation sets
X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=42)

# Build the LSTM model
input_shape = (max_sequence_length,)
model = build_lstm_model(input_shape, num_labels)

# Train the model
checkpointer = ModelCheckpoint(filepath='best_model.h5', verbose=1, save_best_only=True)
model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, batch_size=32, callbacks=[checkpointer])

# Evaluate the model
model.load_weights('best_model.h5')
loss, accuracy, f1_score = model.evaluate(X_val, Y_val, verbose=1)
print('Loss:', loss)
print('Accuracy:', accuracy)
print('F1 Score:', f1_score)
</code></pre>
<p>The error that I am getting is-</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [67], in &lt;cell line: 45&gt;()
     42 processed_data = [preprocess_data(dataset) for dataset in datasets.values()]
     44 # Convert the data into Tfidf vectors
---&gt; 45 tfidf_matrix, vectorizer = tfidf_vectorization(processed_data)
     47 # Prepare the data for LSTM input
     48 max_sequence_length = 50

Input In [67], in tfidf_vectorization(data)
     25 def tfidf_vectorization(data):
---&gt; 26     data_strings = [' '.join(sentence) for sentence in data]
     27     vectorizer = TfidfVectorizer(max_features=5000)
     28     tfidf_matrix = vectorizer.fit_transform(data_strings)

Input In [67], in &lt;listcomp&gt;(.0)
     25 def tfidf_vectorization(data):
---&gt; 26     data_strings = [' '.join(sentence) for sentence in data]
     27     vectorizer = TfidfVectorizer(max_features=5000)
     28     tfidf_matrix = vectorizer.fit_transform(data_strings)

TypeError: sequence item 0: expected str instance, list found
</code></pre>
<p>I have been trying a way out to make this code run so that I can create the code for multilabel sequence classification using TFIDF vectorisation and LSTM algorithm.</p>
<p>The datsets consist of-</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['tokens', 'pos_tags', 'ner_tags'],
        num_rows: 1072
    })
    validation: Dataset({
        features: ['tokens', 'pos_tags', 'ner_tags'],
        num_rows: 126
    })
    test: Dataset({
        features: ['tokens', 'pos_tags', 'ner_tags'],
        num_rows: 153
    })
})
</code></pre>
<p>For these 2 lines-</p>
<pre><code>data=datasets['train'][0:3]
print(data)
</code></pre>
<p>The output comes-</p>
<pre><code>{'tokens': [['For', 'this', 'purpose', 'the', 'Gothenburg', 'Young', 'Persons', 'Empowerment', 'Scale', '(', 'GYPES', ')', 'was', 'developed', '.'], ['The', 'following', 'physiological', 'traits', 'were', 'measured', ':', 'stomatal', 'conductance', '(', 'gs', ',', 'mol', 'H2O', 'm-2', 's-1', ')', ',', 'transpiration', 'rate', '(', 'E', ',', 'mmol', 'H2O', 'm-2', 's-1', ')', ',', 'net', 'photosynthetic', 'rate', '(', 'PN', ',', 'μmol', 'm-2', 's-1', ')', 'and', 'intercellular', 'CO2', 'concentration', 'CO2', '(', 'Ci', ',', 'μmol', 'm-2', 's-1', ')', '.'], ['Minor', 'H', 'antigen', 'alloimmune', 'responses', 'readily', 'occur', 'in', 'the', 'setting', 'of', 'human', 'leukocyte', 'antigen', '(', 'HLA)–matched', 'allogeneic', 'solid', 'organ', 'and', 'stem', 'cell', 'transplantation', '(', 'SCT', ')', '[', '3,4', ']', '.']], 'pos_tags': [['ADP', 'DET', 'NOUN', 'DET', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'AUX', 'VERB', 'PUNCT'], ['DET', 'ADJ', 'ADJ', 'NOUN', 'AUX', 'VERB', 'PUNCT', 'ADJ', 'NOUN', 'PUNCT', 'PROPN', 'PUNCT', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT', 'PROPN', 'PUNCT', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'PUNCT', 'ADJ', 'ADJ', 'NOUN', 'PUNCT', 'PROPN', 'PUNCT', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'CCONJ', 'ADJ', 'NOUN', 'NOUN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'PUNCT'], ['ADJ', 'PROPN', 'NOUN', 'ADJ', 'NOUN', 'ADV', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'PUNCT', 'PROPN', 'ADJ', 'ADJ', 'NOUN', 'CCONJ', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'PROPN', 'PUNCT', 'PUNCT', 'NUM', 'PUNCT', 'PUNCT']], 'ner_tags': [['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O'], ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-AC', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O'], ['B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O']]}
</code></pre>
<p>Please help me out in making the changes to the code so that I can obtain the output. Please give me an alternative or make correction according to the dataset so that I can create a code for sequence classification using Tfidf vectorisation and LSTM algorithm.</p>
","python, nlp, lstm, tf-idf, tfidfvectorizer",
How do I improve hybrid search on Weaviate?,"<p>I've been working on Hybrid Search using Weaviate. I use OpenAI's latest embeddings model, and then some other stuff. So, the problem is, for some queries, I would like to focus on specific properties while for others, I would like to focus on other properties more. I also have 2 axes on which I want to rank the recommendations - relevance and excellence.</p>
<p>Relevance would be how relevant they are to my search, and excellence would be how excellent the &quot;document&quot; is based on some score that I give it.
So far, the things I've tried are:</p>
<ol>
<li>Cohere reranking. I saw that v3 reranking gave marginally better
results for short queries than the &quot;Hybrid Score&quot; for Weaviate</li>
<li>For shorter queries, I do more of a keyword search and for longer
queries, more of a semantic search (shifting the alpha value based
on word count)</li>
<li>Assigning weights for keyword search in Weaviate</li>
<li>Tried using a linear combination of my in house eval and relevancy
(reranked score/hybrid score) and sorted based on that. This didnt
really provide satisfactory results at all.</li>
</ol>
<p>Are there any suggestions based on which I could try improving the Search results? I want:</p>
<ol>
<li>To be able to &quot;understand&quot; what the query is for, and focus on that
property more in my vector DB schema for the search</li>
<li>For common queries, I want to be able to surface more &quot;excellent&quot;
recommendations, as if its common, rather than focusing on very very
relevant stuff, if it meets a certain level of relevancy and then is
really excellent, that is the best way to go and looks really good
in search results</li>
<li>For larger/more niche queries, focus on the relevancy a lot more</li>
<li>I think fine tuning Cohere's reranking model might be an option
here?</li>
<li>How do I factor in the excellence?</li>
</ol>
<p>What are my options here, and where do I go from here? Also, I've been checking distributions of scores that are returned from Weaviate upon the hybrid search, and I see that in lot of cases, if, say, I return the top 800 people from my query, most of them (~500-700) fall in the range of &lt;0.3 vector/keyword scores or they do not have keyword/vector scores at all in which case one score is just 0.</p>
<p>What are my options here?</p>
","nlp, ranking, word-embedding, weaviate, semantic-search",
BERT models are working slow on production,"<p>I have trained three BERT models using huggingface transformers. Let's call these models S, T and I models.</p>
<p>The <em>model.py</em> file is as below. <em>init_model()</em> function is for initializing the models. I will be calling that function in <em>main.py</em></p>
<pre><code>import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

def init_model():
    device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)
    
    tokenizer = AutoTokenizer.from_pretrained(&quot;google-bert/bert-base-uncased&quot;)
    model_t = AutoModelForSequenceClassification.from_pretrained(&quot;google-bert/bert-base-uncased&quot;, num_labels=2)
    model_s = AutoModelForSequenceClassification.from_pretrained(&quot;google-bert/bert-base-uncased&quot;, num_labels=2)
    model_i = AutoModelForSequenceClassification.from_pretrained(&quot;google-bert/bert-base-uncased&quot;, num_labels=2)

    model_t.load_state_dict(torch.load(r&quot;PATH/TO/MODEL_T&quot;))
    model_s.load_state_dict(torch.load(r&quot;PATH/TO/MODEL_S&quot;))
    model_i.load_state_dict(torch.load(r&quot;PATH/TO/MODEL_I&quot;))

    model_t.to(device)
    model_s.to(device)
    model_i.to(device)
    return model_t,model_s,model_i, device, tokenizer
</code></pre>
<p>The <em>predict.py</em> file is as below. <em>predict_fn</em> function takes model, device, tokenizer_fn and text as input and returns the output probability.</p>
<pre><code>import torch
from torch.nn import functional as F

def predict_fn(mdl, device, tokenizer_fn, text):
    encoded_text = tokenizer_fn.encode_plus(
    text,
    add_special_tokens=True,
    return_token_type_ids=True,
    padding=True,
    return_attention_mask=True,
    truncation=True,
    return_tensors='pt',
    )

    with torch.no_grad():
        input_ids = encoded_text['input_ids'].to(device)
        token_type_ids = encoded_text['token_type_ids'].to(device)
        attention_mask = encoded_text['attention_mask'].to(device)
        output = mdl(input_ids,token_type_ids, attention_mask)
    probs = F.softmax(output[0], dim=1)
    return probs[0][1].cpu().detach().numpy()
</code></pre>
<p>In the <em>main.py</em> file I am building the API service using Flask as below. Firstly I initialize the models calling the <em>init_model()</em> function. And then I build a <em>get_prediction()</em> function for returning outputs of all three models together for each input text. After that I build the endpoint <em>/predict</em></p>
<pre><code>from predict import predict_fn
from model import init_model
import numpy as np
from flask import Flask, request, jsonify

app = Flask(__name__)
model_t,model_s,model_i, device, tokenizer  = init_model()
def get_prediction(text):
    t_val = predict_fn(model_t, device, tokenizer, text)
    s_val = predict_fn(model_s, device, tokenizer, text)
    i_val = predict_fn(model_i, device, tokenizer, text)
    return t_val, s_val, i_val

@app.route(&quot;/predict&quot;, methods=[&quot;POST&quot;])
def get_results():    
    data = request.get_json(force=True)
    texts = [text for text in data.values()]
    i_vals = np.array([])
    s_vals = np.array([])
    t_vals = np.array([])
    for text in texts:
        t_val, s_val, i_val = get_prediction(text)
        t_vals = np.append(t_vals,t_val)
        s_vals = np.append(s_vals,s_val)
        i_vals = np.append(i_vals,i_val)
        
    return jsonify({
                    &quot;i_scores&quot;: i_vals.tolist(),
                    &quot;t_scores&quot;: t_vals.tolist(),
                    &quot;s_scores&quot;: s_vals.tolist(),
 })

if __name__ == &quot;__main__&quot;:
    app.run(debug=True, host='0.0.0.0', port=8000)
</code></pre>
<p>In the current scenario, I send the request to the server as below.</p>
<pre><code>import requests

ip_address = &quot;XXXXXXXXX&quot;
url = f&quot;http://{ip_address}:8000/predict&quot;  
data = {&quot;text1&quot;: &quot;TEXT1_COMES_HERE&quot;,
      &quot;text2&quot;: &quot;TEXT2_COMES_HERE&quot;,
      &quot;text3&quot;: &quot;TEXT3_COMES_HERE&quot;,
         }
    
response = requests.post(url, json=data)
print(response.json())
</code></pre>
<p>And I get the response successfully.</p>
<p>The GPU built-on our server is Tesla T4. When I sent request of consisting 1000 texts, I get the respond in 22 seconds which means models can process 44-45 texts per second. And no it is not about slow internet connection or anything. Models are working slow. The models are working on GPU, I checked that. However, I cannot understand where the problem is exactly? How should I optimize the system?</p>
<p>PS. This is my first time deploying a model so in case I am doing something very wrong, don't be harsh please. And also I had to anonymize the code so it might look non-sense at some points, it is mostly because of anonymization. But I am open to any suggestions.</p>
","python, flask, pytorch, nlp, huggingface-transformers",
Error in implementing LIME for Text data using scispacy en_core_m.... models,"<p>ValueError: [E1041] Expected a string, Doc, or bytes as input, but got: &lt;class 'list'&gt;</p>
<p>Please help</p>
<p>I want to build the explainable part for text data using LIME. I am getting the above error when i run the following code</p>
<pre><code>from lime.lime_text import LimeTextExplainer

# Load your spaCy model
nlp = scispacy_medical


input_text = &quot;Your large input text here...&quot;

# Create a spaCy compatible tokenizer function for lime
def spacy_tokenizer(text):
    return [token.text for token in nlp(text)]

# Create a LIME explainer
explainer = LimeTextExplainer(class_names=[&quot;class_1&quot;, &quot;class_2&quot;])

# Explain the model's predictions on the input text
exp = explainer.explain_instance(input_text, spacy_tokenizer, num_features=10)

# Visualize the explanation
exp.show_in_notebook(text=True)```
</code></pre>
","nlp, spacy, named-entity-recognition, lime",
Text conversion processing and extracting information,"<p>I have a call center software which consist of industry specific calls, in which all calls are recorded and then all calls transform into text. After transforming into text, every calls is passed to other agent who is human, who extract all information from text. I have to extract information from AI, or some sort of services and analyze conversation between two if them, then save it in DB.</p>
<p>Let say a call center person name is Bob and the person whom need assisstance from bob is foo. the conversation between the two person is below.</p>
<pre><code>Bob : Hey Foo, I am Bob, How may I help you today?
Foo: Hey, I am good, I have a issue with product which I have purchased from you.

Bob : We never like to hear customers are unhappy. Why don’t you start by giving me your full name and order number so I can try to address this issue for you?
Foo : Yeah, it is a electric bike, and Number is BBM-3344.

and so on...
</code></pre>
<p>Now I have to extract all information from conversation like</p>
<ol>
<li>sentiments (light or harsh mood, happy or sad etc.)</li>
<li>filler words (um, ha, etc words)</li>
<li>confident level</li>
<li>answer is appropriate, answer related to topic</li>
<li>engagements (engagement between the two)</li>
<li>talk time</li>
<li>conversation topic</li>
<li>Number of questions asked.</li>
</ol>
<p>and all other information.</p>
<p>Now my questions are:</p>
<ol>
<li>What information we extract from conversation other than that?</li>
<li>How to extract all information from python and their libraries/packages?</li>
</ol>
","python, nlp, bots, artificial-intelligence",
Auto-identification of abbreviations,"<p>I want to realize some function that will decrypt on-the-fly abbreviations based on a pre-trained dictionary. How do I do it in Python? For example, the phrase &quot;The president of the U.S. visited N.Y. City&quot; should be transformed to &quot;The president of the United States visited New York City.&quot; Suppose the abbreviations are popular, and we don't know what they would be.</p>
","python, nlp, abbreviation",
Preprocessing a large dataset with tf.layers.TextVectorization gives Memory Errors,"<p>I have around 300k files, which is around 9GB of medical literature.</p>
<p>My goal is to establish the frequency of ALL tokens in the dataset and serialize them to a csv file (token, frequency).</p>
<p>To achieve this, I used <code>layers.TextVectorization</code> with <code>output_mode='count'</code>.</p>
<p>Adapting the huge dataset went fine, but when retrieving the vocabulary I got:</p>
<pre><code>2024-04-20 22:38:56.832518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-20 22:38:56.833545: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Discoverd 323719 files
Finished adapting
Traceback (most recent call last):
  File &quot;(file_location_of_source_code)&quot;, line 55, in &lt;module&gt;
    inverse_vocab = vectorize_layer.get_vocabulary()
  File &quot;D:\Anaconda\envs\src\lib\site-packages\keras\layers\preprocessing\text_vectorization.py&quot;, line 487, in get_vocabulary
    return self._lookup_layer.get_vocabulary(include_special_tokens)
  File &quot;D:\Anaconda\envs\src\lib\site-packages\keras\layers\preprocessing\index_lookup.py&quot;, line 385, in get_vocabulary
    self._tensor_vocab_to_numpy(vocab),
  File &quot;D:\Anaconda\envs\src\lib\site-packages\keras\layers\preprocessing\string_lookup.py&quot;, line 416, in _tensor_vocab_to_numpy
    [tf.compat.as_text(x, self.encoding) for x in vocabulary]
  File &quot;D:\Anaconda\envs\src\lib\site-packages\keras\layers\preprocessing\string_lookup.py&quot;, line 416, in &lt;listcomp&gt;
    [tf.compat.as_text(x, self.encoding) for x in vocabulary]
MemoryError

Process finished with exit code 1
</code></pre>
<p>Relevant piece from my code:</p>
<pre><code>files_root = pathlib.Path(r&quot;directoryname&quot;)
files = tf.data.TextLineDataset.list_files(str(files_root/'*'))
text_ds = tf.data.TextLineDataset(files).filter(lambda x: tf.cast(tf.strings.length(x), bool))
# by default this tokenizes elements by new line
# we use dataset class bec it is for large amounts of data

# Converting the vocab to integer indexes, wrt their frequency
vectorize_layer = layers.TextVectorization(
    standardize=custom_standardization,
    output_mode='count')
# vectorize_layer.adapt(text_ds.batch(1024))
print(f&quot;Discoverd {len(files)} files&quot;)
vectorize_layer.adapt(text_ds.batch(1024))
print(&quot;Finished adapting&quot;)
inverse_vocab = vectorize_layer.get_vocabulary() #&lt;----ERROR 
print(&quot;Vocabulary Retrieved, with the len:&quot;)
size_vocab = len(inverse_vocab) 
print(size_vocab)
</code></pre>
<p>Further, I was planning to combine the frequency arrays for all sequences, the approach worked on <em>considerably</em> smaller datasets:</p>
<pre><code>text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()
print(&quot;Finished vectorization&quot;)
it = text_vector_ds.as_numpy_iterator()
freq_arr = None
for i, entry in enumerate(text_vector_ds.as_numpy_iterator()):
    if i == 0:
        freq_arr = np.zeros(len(entry))
        freq_arr += entry.astype(int)
    else:
        freq_arr += entry.astype(int)
</code></pre>
<p>What could be the solution to this issue? I need the vocabulary in order to map the actual tokens to their frequency.</p>
<p>I am fairly new to tensorflow and keras, any advice or critique to my approach are welcome. I really need some guidance on working with huge datasets. My ultimate goal is to feed them to a neural network (more specific, some skip grams).
Many thanks.</p>
","python, tensorflow, keras, nlp, vectorization",
TypeError in quartznet.transcribe() from neMO,"<p>First time using this, so dk what's wrong.</p>
<pre><code>files = [&quot;/content/harvard.wav&quot;]  # Assuming you have defined this list elsewhere
raw_text = ''
text = ''
for fname, transcription in zip(files, quartznet.transcribe(paths2audio_files=files)):
    raw_text = transcription

text = raw_text[0]
print(text)
</code></pre>
<p>TypeError: Output shape mismatch occured for audio_signal in module AudioToCharDataset :
Output shape expected = (batch, time) |
Output shape found : torch.Size([1, 293700, 2])</p>
<p>Tried using different ways to write it. but error exists in the parameters.</p>
<p><code>audio_file_path = &quot;/content/harvard.wav&quot;  # Path to your audio file transcript = quartznet.transcribe(paths2audio_files=audio_file_path) print(transcript)</code></p>
<p><code>quartznet = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=&quot;QuartzNet15x5Base-En&quot;) wave_file = [&quot;/content/harvard.wav&quot;]  # List containing the path to your audio file transcript = quartznet.transcribe(paths2audio_files=wave_file) print(transcript)</code></p>
","nlp, text-to-speech, nvidia, automatic-speech-recognition",
How to tag words that not include one specific symbol in Spacy?,"<p>I'm trying to tag one word in Spacy using regex, but I want to add one condition: it can't contain symbol '/' in any place inside. My code looks like this:</p>
<pre><code>[{'lower': {&quot;regex&quot;: &quot;^.*(word).*?&quot;}}]
</code></pre>
<p>I tried using ^ to exclude this but It didn't work.</p>
<p>So examples:</p>
<ol>
<li>'subwordw' tagged: 'subword'</li>
<li>'subword/w' tagged nothing</li>
</ol>
","python, nlp, pattern-matching, spacy","<p>try this:
<code>{'lower': {'REGEX': &quot;^([^\/]*word[^\/]*)$&quot;}}</code></p>
"
Unable to read docx file using python-docx,"<p>I have this code:</p>
<pre><code>import os
import traceback

import pdfplumber
from docx import Document

def read_docx(file_path):
    try:
        doc = Document(file_path)
        content = [paragraph.text for paragraph in doc.paragraphs]
        return '\n'.join(content)
    except Exception as e:
        return f&quot;Error reading DOCX file: {e}&quot;

def read_pdf(file_path):
    try:
        text = ''
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ''
        return text
    except Exception as e:
        return f&quot;Error reading PDF file: {e}&quot;

def read_txt(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    except Exception as e:
        return f&quot;Error reading TXT file: {e}&quot;

def read_file(folder_path, patent_name=None, file_name=None):
    if file_name:
        file_path = os.path.join(folder_path, patent_name, file_name)
    elif patent_name:
        file_path = os.path.join(folder_path, patent_name)
    else:
        file_path = os.path.join(folder_path)

    if os.path.basename(file_path).startswith('~$'):
        return &quot;Skipping temporary file.&quot;

    if file_path.endswith('.docx'):
        return read_docx(file_path)
    elif file_path.endswith('.pdf'):
        return read_pdf(file_path)
    elif file_path.endswith('.txt') and file_name:
        return read_txt(file_path)
    else:
        raise ValueError(&quot;Unsupported file format or missing file name for .txt file&quot;)

def save_text_to_file(folder_path, text, patent_name, file_name):
    try:
        file_path = os.path.join(folder_path, patent_name)
        if not os.path.exists(file_path):
            os.makedirs(file_path)
        with open(os.path.join(file_path, file_name), &quot;w&quot;, encoding='utf-8') as f:
            f.write(text)
    except Exception as err:
        print(err, traceback.format_exc())
</code></pre>
<p>For some reason, I am able to read all of the docx files except one. The files are read on the basis of <code>docID</code>. Here's the structure of <code>docID</code>:</p>
<pre><code>Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
d-----        16/04/2024     18:00                custom_template
-a----        16/04/2024     17:48         162129 input_disclosure.docx
</code></pre>
<pre><code>Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
-a----        16/04/2024     17:47          39928 input_customTemplate.docx
</code></pre>
<p>Here's the error that I am getting:</p>
<pre><code>raise PackageNotFoundError(&quot;Package not found at '%s'&quot; % pkg_file)
docx.opc.exceptions.PackageNotFoundError: Package not found at 'C:\Users\hp\Desktop\Projects\PatentGenie\temp\234\custom_template\~$put_customTemplate.docx'
</code></pre>
<p>Note:</p>
<ul>
<li>I haven't opened the file on MS Word or any other editor during execution.</li>
<li>All other files are getting read except this one.</li>
<li>I have copied and pasted the file by manually creating the directory; still no luck.</li>
<li>I have also changed the content inside of the file (by adding couple of spaces) still no luck.</li>
<li>I have also made a new file with the same name. Used the 'Save As' method. Still no luck.</li>
</ul>
","python, file, ms-word, nlp, python-docx",
How to remove negation from japanese sentence and extract keywords (tfidf search )?,"<p>I want to remove negation from user query and extarct keywords from it for a search engine, for example .</p>
<p>`NISAを除いてリストを見せ</p>
<p>English translation : Show me the list excluding NISA</p>
<p>For this I don't want nisa as a keyword as user is excluding it.</p>
<p>I am using tfidf for keyword search .I am not able to handle negation in user sentences.</p>
<p>I tried using llm to manipulate my query but is not giving proper result below is the prompt I am using.</p>
<pre><code>[
{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Analyze user query and return only relevant keywords from it in . Give answer in this format 'keyword, keyword, keyword'. Give keywords in same language as the user query. Skip verbs like show, display. Also change the keywords to lowercase if they are in english language. Whenever query mentions 'not nisa' , 'excluding nisa' for this keyword would be 'tsumitate'. If a keyword is preceded by negation then don't return that keyword.Keep the tense of the keywords as it is. Do not skip any keyword&quot;}, 
{{#session}}
{{^last}}
{&quot;role&quot;:&quot;{{{role}}}&quot;, &quot;content&quot;:&quot;{{{content}}}&quot;},
{{/last}}
{{#last}}
{&quot;role&quot;:&quot;{{{role}}}&quot;, &quot;content&quot;:&quot; user query : '{{content}} ' &quot;}
{{/last}}
{{/session}}
]



</code></pre>
<p>Model I am using : gpt35-turbo chat model</p>
","node.js, nlp, search-engine, tf-idf, large-language-model",
ModuleNotFoundError: No java install detected. Please install java to use language-tool-python,"<p>I would like to check the number if issues in a given sentence.</p>
<p>my code is</p>
<pre><code>import language_tool_python
tl = language_tool_python.LanguageTool('en-US')

txt = &quot;good mooorning sirr and medam my namee anderen i am from amerecia !&quot;
m = tl.check(txt)
len(m)
</code></pre>
<p>Instead of returning the number i am getting error message as shown below.</p>
<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-1-1c4c9134d6f4&gt; in &lt;module&gt;
      1 import language_tool_python
----&gt; 2 tool = language_tool_python.LanguageTool('en-US')
      3 
      4 text = &quot;Your the best but their are allso  good !&quot;
      5 matches = tool.check(text)

E:\Anaconda\lib\site-packages\language_tool_python\server.py in __init__(self, language, motherTongue, remote_server, newSpellings, new_spellings_persist)
     43             self._update_remote_server_config(self._url)
     44         elif not self._server_is_alive():
---&gt; 45             self._start_server_on_free_port()
     46         if language is None:
     47             try:

E:\Anaconda\lib\site-packages\language_tool_python\server.py in _start_server_on_free_port(self)
    212             self._url = 'http://{}:{}/v2/'.format(self._HOST, self._port)
    213             try:
--&gt; 214                 self._start_local_server()
    215                 break
    216             except ServerError:

E:\Anaconda\lib\site-packages\language_tool_python\server.py in _start_local_server(self)
    222     def _start_local_server(self):
    223         # Before starting local server, download language tool if needed.
--&gt; 224         download_lt()
    225         err = None
    226         try:

E:\Anaconda\lib\site-packages\language_tool_python\download_lt.py in download_lt(update)
    142     ]
    143 
--&gt; 144     confirm_java_compatibility()
    145     version = LATEST_VERSION
    146     filename = FILENAME.format(version=version)

E:\Anaconda\lib\site-packages\language_tool_python\download_lt.py in confirm_java_compatibility()
     73         # found because of a PATHEXT-related issue
     74         # (https://bugs.python.org/issue2200).
---&gt; 75         raise ModuleNotFoundError('No java install detected. Please install java to use language-tool-python.')
     76 
     77     output = subprocess.check_output([java_path, '-version'],

ModuleNotFoundError: No java install detected. Please install java to use language-tool-python.
</code></pre>
<p>When I run the code I get no java install detected
How to solve this issue?</p>
","python, nlp, grammar","<p>I think this is not an issue with the Code itself when I run the code you provided</p>
<pre><code>import language_tool_python
tl = language_tool_python.LanguageTool('en-US')

txt = &quot;good mooorning sirr and medam my namee anderen i am from amerecia !&quot;
m = tl.check(txt)
len(m)
</code></pre>
<p>I get as result a number in this case</p>
<pre><code> OUT: 8
</code></pre>
<p>In the Documentation of the <a href=""https://pypi.org/project/language-tool-python/"" rel=""nofollow noreferrer"">language-tool-python</a> is written:</p>
<blockquote>
<p>By default, language_tool_python will download a LanguageTool server .jar and run that in the background to detect grammar errors locally. However, LanguageTool also offers a Public HTTP Proofreading API that is supported as well. Follow the link for rate-limiting details. (Running locally won't have the same restrictions.)</p>
</blockquote>
<p>So You will need Java (JRE and SKD). Also it's Written in the Requirements of the library:</p>
<blockquote>
<p>Prerequisites
Python 3.5+
LanguageTool (Java 8.0 or higher)
The installation process should take care of downloading LanguageTool (it may take a few minutes). Otherwise, you can manually download LanguageTool-stable.zip and unzip it into where the language_tool_python package resides.</p>
</blockquote>
<p>Source:</p>
<ul>
<li><a href=""https://pypi.org/project/language-tool-python/"" rel=""nofollow noreferrer"">https://pypi.org/project/language-tool-python/</a></li>
<li><a href=""https://stackoverflow.com/questions/45377463/python-2-7-javaerror-when-using-grammar-check-1-3-1-library"">Python 2.7 - JavaError when using grammar-check 1.3.1 library</a></li>
</ul>
<p>I Hope I could help.</p>
"
Stanford NLP Annotation pipeline.annotate resulting into OutOfMemoryError in Java,"<p>So we are using Stanford NLP to annotate input text, and these input texts are laughably small. Below is one example of the same.</p>
<blockquote>
<p>&quot;Can you give me details about Mohammad Siva John with identifiers
6745-3876-1354-8790 and 313-31-333&quot;</p>
</blockquote>
<p>Below is the Java code snippet to annotate.</p>
<pre><code>    final Properties properties = new Properties();
    properties.setProperty(&quot;annotators&quot;, &quot;tokenize, ssplit, pos, lemma&quot;);
    final StanfordCoreNLP pipeline = new StanfordCoreNLP(properties);
    final Annotation document = new Annotation(text);
    pipeline.annotate(document);
</code></pre>
<p>Below is the Maven dependency.</p>
<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
        &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
        &lt;version&gt;4.5.4&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
<p>This works fine, but after couple of days, the JVM crashes with a coredump. Coredump analysis shows that below line resulted into OutOfMemoryError</p>
<pre><code>pipeline.annotate(document);
</code></pre>
<p>Any thoughts on how to resolve this? There are no field level variables in the class, and all of them are method level, and so should be 'freed' once the execution is done. So, there should be no OutOfMemoryError first of all to begin with.</p>
<p>Quite perplexing. Any thoughts?</p>
","java, nlp, out-of-memory","<p>So found this in StanfordNLP Javadoc of <em>void edu.stanford.nlp.pipeline.StanfordCoreNLP.clearAnnotatorPool()</em></p>
<blockquote>
<p>Call this if you are no longer using StanfordCoreNLP and want torelease the memory associated with the annotators.</p>
</blockquote>
<p>Calling this method itself almost has no impact, but the real game changer is calling 'System.gc()'. Below is the fix for this, just call <em>clearAnnotatorPool</em> and <em>gc</em> after <em>annotate</em>.</p>
<pre><code>final Properties properties = new Properties();
properties.setProperty(&quot;annotators&quot;, &quot;tokenize, ssplit, pos, lemma&quot;);
final StanfordCoreNLP pipeline = new StanfordCoreNLP(properties);
final Annotation document = new Annotation(text);
pipeline.annotate(document);

// Below two calls would fix memory issue.
StanfordCoreNLP.clearAnnotatorPool();
System.gc();
</code></pre>
<p>This is how used memory is with <strong>only StanfordCoreNLP.clearAnnotatorPool()</strong> call. Note that this is <strong>without System.gc()</strong> call. Notice that for 1000 calls to 'annotate' in a loop the used memory crosses 2500 MB and then falls down below 500 MB. This is the case when we are letting JVM call gc.</p>
<p><a href=""https://i.sstatic.net/b1OHC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/b1OHC.png"" alt=""Used Memory with and without annotation pool clearing up and no explicit GC call"" /></a></p>
<p>However, when I call both <strong>StanfordCoreNLP.clearAnnotatorPool(); and System.gc();</strong> the results are drastically different. Note that used memory is within the range of 132 to 133 MB irrespective of whether the annotation pool has been cleared.</p>
<p><a href=""https://i.sstatic.net/XdLrg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XdLrg.png"" alt=""Used Memory with and without annotation pool clearing up with explicit GC call"" /></a></p>
<p>One starts seeing the real of 'StanfordCoreNLP.clearAnnotatorPool()' around after 150 <strong>pipeline.annotate</strong> hits. Below is memory utilization observed over 1000 hits to pipeline.annotate. Observe that with 'StanfordCoreNLP.clearAnnotatorPool() and System.gc()', the memory utilization hovers just above 40 MB.</p>
<p><a href=""https://i.sstatic.net/Pu2VF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Pu2VF.png"" alt=""Used Memory with and without annotation pool clearing up with explicit GC call and for 1000 hits to pipeline.annotate"" /></a></p>
<p>If you rightly understand, this only signifies that somehow the JVM default gc execution is not releasing as much memory as when an explicit call is being made. I understand there would be timing difference and all, which only means that one needs to further research on the default gc and the JDK on being used (am on JDK 21 with IDE enforcing JDK 17 compliance level. The graphs are all almost the same when run on a JDK 17 directly too) and how this changes with various gc strategies.</p>
<p>But then I'm happy with this and conclude! Hope this helps someone.</p>
"
How to use forward() method instead of model.generate() for T5 model,"<p>For my use case, I need to use the model.forward() instead of the model.generate() method
i.e instead of the below code</p>
<pre><code>outs = model.model.generate(input_ids=batch['source_ids'],
                                 attention_mask=batch['source_mask'],
                                 output_scores=True,
                                 max_length=model.model_arguments.max_output_seq_length)

preds_cleaned = [model.tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=True) for ids in outs]
</code></pre>
<p>I need to use</p>
<pre><code>model_outputs = model.model(
            input_ids=batch[&quot;source_ids&quot;],
            attention_mask=batch[&quot;source_mask&quot;],
            labels=lm_labels.to(device),
            decoder_attention_mask=batch['target_mask']
        )
logits = model_outputs.logits
softmax_logits = m(logits)
max_logits = torch.max(softmax_logits, dim=2)

    
</code></pre>
<p>decoding these logits gives unprocessed text that has many issues like repetition of words at the end etc.
What do I need to do to get the same result as model.generate() ?</p>
","nlp, huggingface-transformers",
BERT MLM model fine-tuning bad results on new dataset,"<p>I'm trying to fine tune a MLM model on new kind of small data (train.csv 43285 lines, validation.csv 3597), my data looks like this:</p>
<pre><code>text
בראשית ברא אלהים את השמים ואת הארץ
והארץ היתה תהו ובהו וחשך על פני תהום ורוח אלהים מרחפת על פני המים
ויאמר אלהים יהי אור ויהי אור
וירא אלהים את האור כי טוב ויבדל אלהים בין האור ובין החשך
ויקרא אלהים לאור יום ולחשך קרא לילה ויהי ערב ויהי בקר יום אחד
ויאמר אלהים יהי רקיע בתוך המים ויהי מבדיל בין מים למים
ויעש אלהים את הרקיע ויבדל בין המים אשר מתחת לרקיע ובין המים אשר מעל לרקיע ויהי כן
</code></pre>
<p>My results are poor, on old data everything works as expected, on my new data not so much:</p>
<pre><code>PHRASE (old data): ומשה ואהרן עשו את כל המופתים האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו
PHRASE (with mask): ומשה ואהרן עשו את כל [MASK] האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו

&gt;&gt;&gt; ומשה ואהרן עשו את כל המופתים האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו
&gt;&gt;&gt; ומשה ואהרן עשו את כל האותות האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו
&gt;&gt;&gt; ומשה ואהרן עשו את כל המעשים האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו
&gt;&gt;&gt; ומשה ואהרן עשו את כל הדברים האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו
&gt;&gt;&gt; ומשה ואהרן עשו את כל השפטים האלה לפני פרעה ויחזק יהוה את לב פרעה ולא שלח את בני ישראל מארצו

PHRASE (new data): ומשה ואהרן עבדו ית כל פליאתה אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה
PHRASE (with mask): ומשה ואהרן עבדו ית כל [MASK] אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה

&gt;&gt;&gt; ומשה ואהרן עבדו ית כל פתגמיא אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה
&gt;&gt;&gt; ומשה ואהרן עבדו ית כל ממלל אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה
&gt;&gt;&gt; ומשה ואהרן עבדו ית כל עובדוי אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה
&gt;&gt;&gt; ומשה ואהרן עבדו ית כל מלי אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה
&gt;&gt;&gt; ומשה ואהרן עבדו ית כל אלין אלין לקדם פרעה ותקף יהוה ית לב פרעה ולא שלח ית בני ישראל מן ארעה

PHRASE (new data): ואמר משה אכהן אמר יהוה כפלגות ליליה אנה נפק בגו ארע מצרים
PHRASE (with mask): ואמר משה אכהן אמר יהוה כפלגות [MASK] אנה נפק בממצית ארע מצרים

&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות ##ה אנה נפק בממ ##צית ארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות הלא אנה נפק בממ ##צית ארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות ##ן אנה נפק בממ ##צית ארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות ##יה אנה נפק בממ ##צית ארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות משה אנה נפק בממ ##צית ארע מצרים

PHRASE (new data): ואמר משה אכהן אמר יהוה כפלגות ליליה אנא נפק בגו ארע מצרים
PHRASE (with mask): ואמר משה אכהן אמר יהוה כפלגות ליליה אנא נפק בגו [MASK] מצרים

&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו ארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו ארץ מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו בארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו משרית מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו נהר מצרים
</code></pre>
<p>I have trained the model with the script <code>run_mlm.py</code> from <code>transformers/examples/pytorch/language-modeling</code> as follows:</p>
<blockquote>
<p>python run_mlm.py --model_name_or_path dicta-il/BEREL_2.0 --train_file
./train.csv --validation_file ./validation.csv --line_by_line
--num_train_epochs 5 --warmup_steps 500 --gradient_accumulation_steps 16 --per_device_train_batch_size 4 --per_device_eval_batch_size 4
--learning_rate 1e-4 --optim adamw_torch --evaluation_strategy steps --load_best_model_at_end --do_train --do_eval --push_to_hub --output_dir ./BEREL_2.0-sam-finetune-v4</p>
</blockquote>
<p>My model: <code>johnlockejrr/BEREL_2.0-sam-finetune-v4</code>
My dataset: <code>johnlockejrr/samv2</code></p>
<p>What did I do wrong? In my dataset or my arguments to the trainer?</p>
<p>EDIT: maybe should I use &quot;Whole Word Masking&quot; to get the same results?</p>
<p>EDIT 2: I just trained the model with WWM, same problem...</p>
<pre><code>&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו ארע מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו ארץ מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו תחום מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו בתי מצרים
&gt;&gt;&gt; ואמר משה אכהן אמר יהוה כפל ##גות לילי ##ה אנא נפק בגו ארעא מצרים
</code></pre>
","nlp, huggingface-transformers, bert-language-model, huggingface-tokenizers",
"Error when calling Hugging Face load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)","<p>I'm following the huggingface tutorial <a href=""https://huggingface.co/learn/nlp-course/chapter3/2?fw=pt"" rel=""nofollow noreferrer"">here</a> and it's giving me a strange error. When I run the following code:</p>
<pre><code>from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
from torch.utils.data import DataLoader

raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)
</code></pre>
<p>Here is what I see:</p>
<pre><code>Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 151k/151k [00:00&lt;00:00, 3.35MB/s]
Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11.1k/11.1k [00:00&lt;00:00, 6.63MB/s]
Downloading data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:32&lt;00:00, 10.89s/it]
Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00&lt;00:00, 127.92it/s]
Traceback (most recent call last):
  File &quot;/Users/ameenizhac/Downloads/transformers_playground.py&quot;, line 5, in &lt;module&gt;
    raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/load.py&quot;, line 1782, in load_dataset
    builder_instance.download_and_prepare(
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py&quot;, line 872, in download_and_prepare
    self._download_and_prepare(
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py&quot;, line 967, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py&quot;, line 1709, in _prepare_split
    split_info = self.info.splits[split_generator.name]
                 ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/splits.py&quot;, line 530, in __getitem__
    instructions = make_file_instructions(
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py&quot;, line 112, in make_file_instructions
    name2filenames = {
                     ^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py&quot;, line 113, in &lt;dictcomp&gt;
    info.name: filenames_for_dataset_split(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/naming.py&quot;, line 70, in filenames_for_dataset_split
    prefix = filename_prefix_for_split(dataset_name, split)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/naming.py&quot;, line 54, in filename_prefix_for_split
    if os.path.basename(name) != name:
       ^^^^^^^^^^^^^^^^^^^^^^
  File &quot;&lt;frozen posixpath&gt;&quot;, line 142, in basename
TypeError: expected str, bytes or os.PathLike object, not NoneType
</code></pre>
<p>I don't know where to start because I don't understand where the error is coming from.</p>
","python, nlp, huggingface, huggingface-datasets","<p>I tried on my PC and on Google Colab. The strange thing is that on Colab it works, on my PC it does not.</p>
<p>Anyway, a possible workaround is the following:</p>
<pre><code>raw_datasets = load_dataset(&quot;SetFit/mrpc&quot;)
</code></pre>
<p>If you print it, you will see that the dataset is the same, it just has a different name:</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['text1', 'text2', 'label', 'idx', 'label_text'],
        num_rows: 3668
    })
    test: Dataset({
        features: ['text1', 'text2', 'label', 'idx', 'label_text'],
        num_rows: 1725
    })
    validation: Dataset({
        features: ['text1', 'text2', 'label', 'idx', 'label_text'],
        num_rows: 408
    })
})
</code></pre>
"
How to use spaCy Matcher to create a pattern for rule-based matching for a sequence that is only interpreted as a single token,"<p>I am new to nlp and spaCy but I am using it for my project. I am trying to use spaCy's Matcher class to create a pattern to extract information from clinical summaries, specifically mentions of IQ scores. I want to use Matcher to extract sequences in the clinical summaries that occur frequently such as &quot;IQ=68&quot;. Initially it seemed relatively straight forward and I created the following pattern:</p>
<p>pattern2 = [{&quot;LOWER&quot;: &quot;iq&quot;}, {&quot;TEXT&quot;: &quot;=&quot;, &quot;OP&quot;: &quot;?&quot;}, {&quot;IS_DIGIT&quot;: True}]</p>
<p>However, this hasn't worked and I think this is because spaCy's tokenization is treating the sequence &quot;IQ=68&quot; as a single token and so pattern2 does not work. Is there a solution to this using Matcher? I have been using Matcher as it has been helpful with creating patterns that extract IQ from sequences such as &quot;IQ is 68&quot; as this is treated as three easy to identify tokens, where a pattern can be easily made and used.</p>
<p>Any help would be really appreciated! Apologies for any incorrect terms used, I am still learning!</p>
<p>Thanks.
Tarran :)</p>
","regex, nlp, pattern-matching, spacy, tokenize",
Error when uploading files using Streamlit and Langchain,"<p>I'mtrying to build a Chatbot with langchain and when I'm trying to use streamlit to interact with it, I have an error as follows:</p>
<pre><code>File &quot;/mount/src/cenace-llm/streamlit_app.py&quot;, line 33, in load_pdf
    pdf_loader = PyPDFLoader(file)
                 ^^^^^^^^^^^^^^^^^
File &quot;/home/adminuser/venv/lib/python3.11/site-packages/langchain_community/document_loaders/pdf.py&quot;, line 154, in __init__
    raise ImportError(
</code></pre>
<p>So far I've tried different ways to solve the problem, but none of them have worked, in my code I have this:</p>
<pre class=""lang-py prettyprint-override""><code>def load_pdf(file):
    &quot;&quot;&quot;Load a PDF file and split it into pages.&quot;&quot;&quot;
    pdf_loader = PyPDFLoader(file)
    page = pdf_loader.load_and_split()
    return page

with st.sidebar:
        st.title(&quot;Menú:&quot;)
        pdf_docs = st.file_uploader(&quot;Cargue los documentos&quot;, accept_multiple_files=True)
        pdf_docs = os.path.join(&quot;/tmp&quot;, pdf_docs) if pdf_docs else None
        if st.button(&quot;Submit and Process&quot;, key=&quot;process_button&quot;):
            with st.spinner(&quot;Procesando...&quot;):
                raw_text = load_pdf(pdf_docs)
</code></pre>
<p>I know that the problem is that <code>langchain.document_loaders.PyPDFLoader</code> takes in <code>file_path</code> which is a string but I can't seem to find the solution.</p>
","python, nlp, streamlit, langchain",
Is it possible to fine-tune a pretrained word embedding model like vec2word?,"<p>I'm working on semantic matching in my search engine system. I saw that word embedding can be used for this task. However, my dataset is very limited and small, so I don't think that training a word embedding model such as word2vec from scratch will yield good results. As such, I decided to fine-tune a pre-trained model with my data.</p>
<p>However, I can't find a lot of information, such as articles or documentation, about fine-tuning. Some people even say that it's impossible to fine-tune a word embedding model.</p>
<p>This raises my question: is fine-tuning a pre-trained word embedding model possible and has anyone tried this before? Currently, I'm stuck and looking for more information. Should I try to train a word embedding model from scratch or are there other approaches?</p>
","python, nlp, artificial-intelligence, word2vec, word-embedding","<p>As has been pointed out <a href=""https://stackoverflow.com/questions/76161758/fine-tune-a-custom-word2vec-model-with-gensim-4?rq=2"">before</a>, there is no &quot;go-to&quot; way for fine-tuning Word2Vec type models.</p>
<p>I would suggest training your own model from scratch, combining your data with other available data from a similar domain. Word2vec models are fairly quick to train and this would probably give you the best results. If you do not need static word-level embeddings, I would recommend considering contextualized embeddings, for example through the use of <a href=""https://sbert.net/"" rel=""nofollow noreferrer"">sentence-transformers</a> or similar frameworks, which has a wide selection of already pre-trained models you can choose from. You can fine-tune these types of models on your specific data rather easily, and there are tons of resources online on how to do that.</p>
<p>For your use case, you can embed all the documents into dense vector representations using the abovementioned library, and then construct a searchable index over this semantic space. In order to match queries, all you have to do then is to embed the query using the same model and then retrieve the documents with the highest approximate inner product, often referred to as a MIPS search. An example library to take a look at would be <a href=""https://github.com/facebookresearch/faiss"" rel=""nofollow noreferrer"">faiss</a>.</p>
"
Spacy matching countries in messy data,"<p>I have some natural language text that I am trying to parse using <code>spacy</code> but having a hard time getting this to cover all possible variations.</p>
<p>Data is basically a set of recommendations for a set of countries, embedded together in a single text block. I want to extract the list of countries and their associated recommendations.</p>
<p>For example:</p>
<pre><code>Consider implementing the recommendations of the Special Rapporteur on violence against women and CEDAW (India) (Thailand), and France recommends to strengthen measures to increase the participation by ethnic minority women in line with CEDAW recommendations, and consider intensifying human rights education (Ghana).   
</code></pre>
<p>Should be processed into 2 lists:</p>
<pre><code>states = [[&quot;india&quot;, &quot;thailand&quot;], [&quot;ghana&quot;]]
recommendations = [&quot;Consider implementing the recommendations of the Special Rapporteur on violence against women and CEDAW&quot;, &quot;France recommends to strengthen measures to increase the participation by ethnic minority women in line with CEDAW recommendations, and consider intensifying human rights education&quot;]
</code></pre>
<p>So far I have been getting away with a custom sentence segmentation in <code>spacy</code> but this is growing out of hand to handle lots of edge cases such as countries appearing in the form <code>(Country1, Country2 and Country3)</code> or sometimes missing parenthesis or typos in countries. Some countries can also have very different spellings, like <code>iran</code>, <code>islamic republic of iran</code>, <code>iran (islamic republic of)</code>, <code>iran, islamic republic of</code>.</p>
<p>Looking for some guidance on what a proper way would be to handle this. I Was thinking using <code>spacy</code>'s <code>Matcher</code> but it wasn't clear how to apply it to such a use case.</p>
","python, nlp, spacy",
Sklearm FeatureHasher not working on a single column in a dataframe,"<p>I tried performing the feature hasher on a single column in my dataframe but it keeps on giving the error:</p>
<blockquote>
<p>ValueError: Samples can not be a single string. The input must be an iterable over iterables of strings.</p>
</blockquote>
<pre><code>from sklearn.feature_extraction import FeatureHasher

hash_vector_size = 50
fh = FeatureHasher(n_features=hash_vector_size, input_type='string')
hashed_df = pd.DataFrame(fh.transform(X_train[&quot;Item_Identifier&quot;]).toarray(),
                         columns=['H'+str(i) for i in range (hash_vector_size)])
</code></pre>
<p>I was expecting a dataframe of 50 columns where the data would have been hashed in</p>
","pandas, machine-learning, scikit-learn, nlp","<p>You were almost there:</p>
<pre><code>from sklearn.feature_extraction import FeatureHasher
import pandas as pd

data = {&quot;Item_Identifier&quot;: [&quot;ID1&quot;, &quot;ID2&quot;, &quot;ID3&quot;, &quot;ID4&quot;, &quot;ID5&quot;]}
X_train = pd.DataFrame(data)

hash_vector_size = 50
fh = FeatureHasher(n_features=hash_vector_size, input_type='string')
hashed_features = fh.transform([[item] for item in X_train[&quot;Item_Identifier&quot;]])

hashed_df = pd.DataFrame(hashed_features.toarray(),
                         columns=['H'+str(i) for i in range(hash_vector_size)])

print(hashed_df)
</code></pre>
<p>which gives</p>
<pre><code>H0   H1   H2   H3   H4   H5   H6   H7   H8   H9  ...  H40  H41  H42  H43  \
0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -1.0  ...  0.0  0.0  0.0  0.0   
1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   
2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   
3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   
4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   

   H44  H45  H46  H47  H48  H49  
0  0.0  0.0  0.0  0.0  0.0  0.0  
1  0.0  0.0  0.0  0.0  0.0  0.0  
2  0.0  0.0  0.0  0.0  0.0  0.0  
3  0.0  0.0  0.0  0.0  0.0  0.0  
4  0.0  0.0  0.0  0.0  0.0  0.0  

[5 rows x 50 columns]
</code></pre>
"
What is the best function/stage to use tokenizer in Pytorch&#39;s data processing?,"<p>I am subclassing <code>torch.utils.data.Dataset</code> and writing a collate function to be passed to Dataloader's <code>dataset</code> and <code>collate_fn</code> argument respectively.</p>
<p>Between Dataset's <code>__getitem__</code> or <code>collate_fn</code> I was wondering what would be the best function to use tokenizer (huggingface's FastTokenizer) in (or is there another better option out of these two that I don't know of?)</p>
<p><code>collate_fn</code> seems to me as the best option since I can use tokenizer to tokenize and adding padding for the full batch together, which should also help with tokenization speed. But I am not sure if it can cause any problems, in future if not now as some of my teammates start using/customizing my code.</p>
","pytorch, nlp","<p>It is more efficient applying it on the <code>collate_fn</code> as the tokenizer accepts a list of texts (the batch), specially if the sequences need to be padded at a fixed length, you would be processing batch-wise</p>
"
Can I appoint a Masked Language Model&#39;s outputs&#39; range?,"<p>When different kinds of models are trained with masked language modeling, the input embeddings at masked positions are replaced with a MASK token. I'm wondering if I could appoint the range of a MASK token? For example:</p>
<pre><code>1) &quot;What a [MASK] weather!&quot; 
2) &quot;What a [MASK] person he is!&quot;
3) &quot;How can you do such a [MASK] thing!&quot; 

...
</code></pre>
<p>Instead of let a pretrained model using its ability to find a suitable word in its whole vocab, I want the pretrained model to pick a word which is from a specific token set, e.g {&quot;good&quot;,&quot;great&quot;,&quot;stupid&quot;,&quot;bad&quot;}, to replace the MASK token. In another words, when facing all different kinds of input, I wish the model could replace the MASK token using the word from the specific token set. Could anyone give me some hints to do this? Thanks!</p>
","nlp, huggingface-transformers, transformer-model, mlmodel",
I cannot get past data(stop_words) to analyze text in text mining,"<p>It's my first attempt at text mining and I have run into a wall. This is what I have done thus far:</p>
<pre class=""lang-r prettyprint-override""><code>library(tm)
library(tidytext)
library(dplyr)
library(ggplot2)

text1 &lt;- c(&quot;Dear land of Guyana, of rivers and plains,
Made rich by the sunshine, and lush by the rains,
Set gem-like and fair between mounts and sea-
Your children salute you. dear land of the free.
Green land of Guyana, our heroes of yore,
Both bondsman and free, laid their bones on your shore,
This soil so they hallowed, and from them are we,
All sons of one mother, Guyana the free
Great land of Guyana, diverse though our strains,
We are born of their sacrifice, heirs of their pains,
And ours is the glory their eyes did not see –
One Land of six peoples, united and free.
Dear Land of Guyana, to you will we give
Our homage, our service each day that we live;
God guard you, great Mother, and make us to be
More worthy our heritage – land of the free.&quot;)

text1 
newtext1 &lt;- data_frame(line = 1:16, text = text1)
newtext1

newtext1 %&gt;%
  unnest_tokens(word, text)

data(stop_words)

newtext1 &lt;- newtext1 %&gt;%
  anti_join(newtext1)

newtext1 %&gt;%
  count(newtext1, sort = TRUE)
</code></pre>
<p>I have not been able to move forward from <code>data(stop_words)</code>. Thanks in advance.</p>
<p>Rohan</p>
","r, dplyr, nlp, text-mining, tidy",
Should I use model.eval() when using transformers.pipeline for inference with a fine-tuned BERT model in PyTorch?,"<p>When training transformer models with <code>Trainer()</code>, documentation shows the following usage:</p>
<pre class=""lang-py prettyprint-override""><code>model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=5)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
</code></pre>
<p>If you check <code>model.training</code> flag, it's set to <code>false</code> by default, but <code>Trainer</code> has a logic which calls <code>model.train()</code> to set it to <code>True</code>, which makes sense.</p>
<p>When using this fine-tuned model for inference, you can leverage <code>transformers.pipeline</code>, which accepts model object as an argument. But, <code>pipeline</code> doesn't have a logic to check if model is in training mode. I haven't found it in source code, and I don't see it anywhere in the documentation. When I use <code>pipeline</code> for predictions, results are not deterministic, which is another indicator that model is not in eval mode.</p>
<pre><code>generator = pipeline(
                &quot;some_task&quot;,
                model=model,
                tokenizer=tokenizer,
                aggregation_strategy=aggregation_strategy,
                ignore_labels=[],
            )

generator(&quot;Example&quot;)    # returns score X
generator(&quot;Example&quot;)    # returns score Y

# but if model.eval() is used before creating pipeline object
generator(&quot;Example&quot;)    # returns score X
generator(&quot;Example&quot;)    # returns score X
</code></pre>
<p>Should I call <code>model.eval()</code> before using the model in <code>pipeline</code> or should <code>pipeline</code> handle it by itself but because of some reason it doesn't?</p>
","python, machine-learning, pytorch, nlp, huggingface-transformers",
I am getting multiple tabs of figure when i input the year... i need only one animated tab and planet img must show in different tab,"<pre><code>import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from astropy.time import Time
from astropy.coordinates import solar_system_ephemeris, get_body_barycentric
from astropy.coordinates import CartesianRepresentation
from astropy import units as u
from transformers import pipeline
import tensorflow as tf
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
import spacy
from spacy.tokens import Span
from matplotlib.patches import Rectangle
from PIL import Image
import mplcursors

nlp = spacy.load(&quot;en_core_web_sm&quot;)

def build_cnn_model(input_shape):
    model = tf.keras.Sequential([
        Conv1D(64, 3, activation='relu', input_shape=input_shape),
        MaxPooling1D(2),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    return model

def analyze_orbital_data(orbital_data, labels):
    cnn_model = build_cnn_model(input_shape=orbital_data.shape[1:])
    cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    cnn_model.fit(orbital_data, labels, epochs=10, batch_size=32)
    return cnn_model

def compute_positions_and_orbits(year):
    positions = {}
    orbits = {}
    
    start_time = Time(f'{year}-01-01', scale='utc')
    end_time = start_time + 365 * u.day  # Simulate one year of data

    with solar_system_ephemeris.set('builtin'):
        planets = ['mercury', 'venus', 'earth', 'mars', 'jupiter', 'saturn', 'uranus', 'neptune']

        for planet_name in planets:
            times = Time(np.linspace(start_time.jd, end_time.jd, 1000), format='jd')
            planet = get_body_barycentric(planet_name, times)
            positions[planet_name] = planet.represent_as(CartesianRepresentation).xyz.value
            orbits[planet_name] = planet.represent_as(CartesianRepresentation).xyz.value

    return positions, orbits

def update(frame, current_year, positions, orbits, planet_images):
    plt.cla()
    plt.xlabel('X (AU)')
    plt.ylabel('Y (AU)')
    plt.title(f'Planetary Orbits and Positions in {current_year}')
    plt.gca().set_facecolor('black')  # Set background color to black

    for i, (planet_name, orbit) in enumerate(orbits.items()):
        x_orbit, y_orbit, _ = orbit
        x_pos, y_pos, _ = positions[planet_name]

        # Plot full orbit path
        plt.plot(x_orbit, y_orbit, color='white', alpha=0.3)

        # Load planet image and resize
        planet_image_path = planet_images.get(planet_name)
        if planet_image_path:
            planet_image = Image.open(planet_image_path)
            planet_image = planet_image.resize((500, 500))  # Resize image for better visibility

            # Plot the planet image at the current position
            plt.imshow(planet_image, extent=(x_pos[frame] - 0.5, x_pos[frame] + 0.5, y_pos[frame] - 0.5, y_pos[frame] + 0.5), alpha=0.8, label=planet_name)

    # Define the hover function
    def on_hover(event):
        planet_name = &quot;Earth&quot;
        planet_image = &quot;earth.jpg&quot;
        display_zoomed_image(planet_name, planet_image)

    # Connect hover event to the plot
    mplcursors.cursor(hover=True).connect(&quot;add&quot;, on_hover)

    # Set aspect ratio to equal and adjust axis limits for the animation space
    plt.gca().set_aspect('equal')
    plt.xlim(-50, 50)  # Adjust x-axis limits for the animation space
    plt.ylim(-50, 50)  # Adjust y-axis limits for the animation space

    plt.show()

def display_zoomed_image(planet_name, planet_image):
    print(f&quot;Hovering over planet: {planet_name}&quot;)
    fig, ax = plt.subplots()
    ax.imshow(planet_image)
    ax.set_title(planet_name)
    plt.show()

def animate(year):
    positions, orbits = compute_positions_and_orbits(year)
    num_frames = len(next(iter(positions.values()))[0])  # Number of frames based on position data length

    # Load planet images for each planet
    planet_images = {
        'mercury': r&quot;C:\Users\Dell\Desktop\planet imgs\mercury.jpeg&quot;,
        'venus': r&quot;C:\Users\Dell\Desktop\planet imgs\venus.jpeg&quot;,
        'earth': r&quot;C:\Users\Dell\Desktop\planet imgs\earth.jpeg&quot;,
        'mars': r&quot;C:\Users\Dell\Desktop\planet imgs\mars.jpeg&quot;,
        'jupiter': r&quot;C:\Users\Dell\Desktop\planet imgs\jupyter.png&quot;,
        'saturn': r&quot;C:\Users\Dell\Desktop\planet imgs\saturn.png&quot;,
        'uranus': r&quot;C:\Users\Dell\Desktop\planet imgs\uranus.jpeg&quot;,
        'neptune': r&quot;C:\Users\Dell\Desktop\planet imgs\neptune.jpg&quot;
    }

    fig = plt.figure(figsize=(10, 10))
    ani = animation.FuncAnimation(fig, update, frames=num_frames, fargs=(year, positions, orbits, planet_images),
                                  interval=50, repeat=False)

    plt.show()

def process_user_input(input_text):
    nlp = pipeline(&quot;ner&quot;, grouped_entities=True)
    entities = nlp(input_text)
    year = None

    for entity in entities:
        if entity['entity'] == 'DATE':
            try:
                year = int(entity['word'])
                break
            except ValueError:
                continue

    return year

def main():
    while True:
        year_input = input(&quot;Enter a year between 1900 and 2040: &quot;)

        try:
            year = int(year_input)
            if 1900 &lt;= year &lt;= 2040:
                animate(year)
                break
            else:
                print(&quot;Invalid year. Please enter a year between 1900 and 2040.&quot;)
        except ValueError:
            print(&quot;Invalid input. Please enter a valid year.&quot;)

if __name__ == &quot;__main__&quot;:
    main()

</code></pre>
<p>i have tried troubleshooting and working on instances but the result is not cmg as i want it...
I'm struggling to display planet images correctly during the animation. The orbits of the planets show up fine, but the images themselves don't appear at their expected positions. I've verified the image paths and used PIL to resize them, but the images still don't show. Additionally, the hover function I've defined doesn't trigger as intended. Any insights or suggestions would be helpful!</p>
","python, tensorflow, image-processing, nlp, astronomy",
word2vec not working using gensim library,"<p>I am trying to vectorise my dataset using word2vec model provided by gensim. I am facing the error raised by scipy.</p>
<pre><code>from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

# Sample sentences
sentences = [
    &quot;This is a sample sentence.&quot;,
    &quot;Word embeddings are cool.&quot;,
    &quot;I love natural language processing.&quot;
]

# Tokenize the sentences
tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]

# Train the Word2Vec model
model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)

# Get the vector representation of a word
word_vector = model.wv['sample']
print(&quot;Vector representation of 'sample':&quot;, word_vector)

# Get the most similar words to a given word
similar_words = model.wv.most_similar('sample')
print(&quot;Words most similar to 'sample':&quot;, similar_words)
</code></pre>
<p>This is the code. I want to apply word2vec vectorisation on it.</p>
<p>I am facing the error-
ImportError: cannot import name 'triu' from 'scipy.linalg.special_matrices' (C:\Users\onkar\anaconda3\lib\site-packages\scipy\linalg\special_matrices.py)</p>
<p>I have tried using the older versions (1.10 &amp; 1.12) as well as the latest versions of scipy. Please help me find out the way to resolve this error or please suggest an alternative method.</p>
","python-3.x, nlp, vectorization, gensim, word2vec",
Keras Fit Method &#39;Unrecognized Data Type&#39; for NLP Classification Problem with TextVectorization,"<p>I have an LSTM based model.  I am using a text vectorization layer and an embedding layer prior to the LSTM layers.  I am also using two Dense layers with a softmax output.  I am trying to go for binary classification across text inputs.</p>
<p>Here is my code (the inputs are just examples and my larger data set looks just like them, but with far more entries).</p>
<p>Libraries (some not necessary for this portion of the project):</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import numpy as np
import itertools
from sklearn.metrics import confusion_matrix
from tabulate import tabulate
from sklearn import svm
from sklearn.model_selection import GridSearchCV

import tensorflow as tf
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense, Embedding, Input
from keras.layers import TextVectorization
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing import sequence
</code></pre>
<p>Simple inputs:</p>
<pre><code>text_data = [
    &quot;The movie was fantastic! I really enjoyed it and would watch it again.&quot;,
    &quot;I didn't like the movie. The plot had too many holes.&quot;,
    &quot;One of the best movies I've seen this year. Highly recommended!&quot;,
    &quot;The acting was poor and the storyline was boring. I wouldn't recommend it.&quot;
]

labels = [1, 0, 1, 0]
</code></pre>
<p>Text Vectorization and Adapt:</p>
<pre><code>vectorize_layer = TextVectorization(max_tokens=9000,output_mode='int',output_sequence_length = 50,                 pad_to_max_tokens = True,vocabulary = None,standardize = &quot;lower_and_strip_punctuation&quot;,
split = 'whitespace',ngrams = None)

vectorize_layer.adapt(text_data)

</code></pre>
<p>Model definition:</p>
<pre><code>lstm_model = Sequential()
lstm_model.add(Input(shape = (1,), dtype = &quot;string&quot;))
lstm_model.add(vectorize_layer)
lstm_model.add(Embedding(input_dim = 9000, output_dim = 128,embeddings_initializer= 'uniform'))
lstm_model.add(LSTM(64, return_sequences = True, recurrent_dropout = 0.25, dropout = 0.25))
lstm_model.add(LSTM(64))
lstm_model.add(Dense(32, activation='relu'))
lstm_model.add(Dense(1, activation = 'softmax'))
lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
lstm_model.build()
lstm_model.summary()
</code></pre>
<p>The model seems correct:</p>
<p>┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ text_vectorization_25           │ (None, 50)             │             0 │
│ (TextVectorization)             │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ embedding_26 (Embedding)        │ (None, 50, 128)        │     1,152,000 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm_49 (LSTM)                  │ (None, 50, 64)         │        49,408 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm_50 (LSTM)                  │ (None, 64)             │        33,024 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_49 (Dense)                │ (None, 32)             │         2,080 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_50 (Dense)                │ (None, 1)              │            33 │</p>
<p>Call model fit:</p>
<pre><code>lstm_model.fit(text_data, labels, epochs=10)
</code></pre>
<p>Error:</p>
<p>ValueError: Unrecognized data type: x=['The movie was fantastic! I really enjoyed it and would watch it again.', &quot;I didn't like the movie. The plot had too many holes.&quot;, &quot;One of the best movies I've seen this year. Highly recommended!&quot;, &quot;The acting was poor and the storyline was boring. I wouldn't recommend it.&quot;] (of type &lt;class 'list'&gt;)</p>
<p>Tried converting to np arrays:</p>
<pre><code>a = np.array(text_data)
b = np.array(labels)
</code></pre>
<p>Call model fit:</p>
<pre><code>lstm_model.fit(a, b, epochs=10)
</code></pre>
<p>Error:</p>
<p>ValueError: Invalid dtype: str2368</p>
","tensorflow, keras, nlp, classification, lstm",
How to use BERT for identify words unrelated to the content of a sentence and replace them with suitable words?,"<p>For this question we can see below example:</p>
<p>A: Jack continues to pray well.
B: Jack continues to play well.</p>
<p>In the first sentence, the word &quot;pray&quot; is written in a typographical error and the word &quot;play&quot; should have been written instead.
1- How can we recognize such words in a sentence?
2- How can we replace these recognized words with suitable words.(If we can consider a factor that the proposed word can be written similar to the word in the first sentence, the result will probably be more accurate.)</p>
<p>We also have this challenge in Farsi(Persian) language, which I will explain below in an example:</p>
<p>A: قیمت بابا میرود.
B: قیمت بالا میرود.</p>
<p>In the example above, the word &quot;father (بابا)&quot; is written incorrectly, and the intended word was the word &quot;َup (بالا)&quot;.
In Persian, the first sentence has no meaning, but the second sentence is completely meaningful and understandable.</p>
<p>Do you have any solution or guide that I can solve this issue using the BERT language model?</p>
<p>We can use the [Mask] feature to predict the replacement word for a word that is written incorrectly, but I have no idea how to determine whether a word is written incorrectly or not in terms of the meaning of the whole sentence.</p>
","python, deep-learning, nlp, huggingface-transformers, bert-language-model",
(NLP Modelling) How to get SpaCy matcher to parse names and education history from resumes to append to an Excel file,"<p>I am building an NLP model to parse through resumes for names and education history which I will then append to a larger Excel file with other applicant data to use for broader analysis. I am introducing patterns into the SpaCy matcher to identify the broad range of names for educational institutions around the world. I would like the end result to be a table with rows for each applicant and a column with the educational institution they've attended possibly concatenated as a string or multiple columns instead. When I run my model on a test resume, each of the 15 patterns I've passed through the matcher shows the same result, rather than one or two results corresponding to the pattern that hit. I'm also wondering if my method of approach is the most efficient way of going about this. I am a very beginner level data scientist and this is basically my first complex program outside of my studies. I have never used SpaCy nor NLP techniques before. Learning it with little experience has been challenging but very fun and rewarding.</p>
<pre><code># Patterns to identify possible educational institutions of applicants
x_university_pattern = [{&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}, {&quot;SPACY&quot;: True}, {&quot;TEXT&quot;: &quot;university&quot;, &quot;OP&quot;: &quot;+&quot;}]# not tested 
university_of_x_pattern = [{&quot;TEXT&quot;: &quot;university&quot;}, {&quot;SPACY&quot;: True}, {&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}]# tested and working
x_college_pattern = [{&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}, {&quot;SPACY&quot;: True}, {&quot;TEXT&quot;: &quot;college&quot;, &quot;OP&quot;: &quot;+&quot;}]# not tested
college_of_x_pattern = [{&quot;TEXT&quot;: &quot;college&quot;}, {&quot;SPACY&quot;: True}, {&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}]# should work
x_community_college_pattern = [{&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}, {&quot;SPACY&quot;: True}, {&quot;TEXT&quot;: &quot;community college&quot;, &quot;OP&quot;: &quot;+&quot;}]# not tested
community_college_of_x_pattern = [{&quot;TEXT&quot;: &quot;community college&quot;}, {&quot;SPACY&quot;: True}, {&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}]# should work
csu_pattern = [{&quot;TEXT&quot;: &quot;csu&quot;}, {&quot;SPACY&quot;: True}, {&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}]# not tested
california_state_university_pattern = [{&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}, {&quot;SPACY&quot;: True}, {&quot;TEXT&quot;: &quot;california state university&quot;}]# not tested
x_state_university_pattern = [{&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}, {&quot;SPACY&quot;: True}, {&quot;TEXT&quot;: &quot;state university&quot;}]# not tested
x_state_pattern = [{&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}, {&quot;SPACY&quot;: True}, {&quot;TEXT&quot;: &quot;state&quot;}]# not tested
state_university_of_x_pattern = [{&quot;TEXT&quot;: &quot;state university&quot;}, {&quot;SPACY&quot;: True}, {&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}]# not tested
university_of_california_pattern = [{&quot;TEXT&quot;: &quot;university of california&quot;}, {&quot;SPACY&quot;: True}, {&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}]# not tested
uc_pattern = [{&quot;TEXT&quot;: &quot;uc&quot;}, {&quot;SPACY&quot;: True}, {&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}]# not tested
institute_of_x_pattern = [{&quot;TEXT&quot;: &quot;institute&quot;}, {&quot;SPACY&quot;: True}, {&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}]# should work
x_institute_pattern = [{&quot;POS&quot;: &quot;PROPN&quot;, &quot;OP&quot;: &quot;+&quot;}, {&quot;SPACY&quot;: True}, {&quot;TEXT&quot;: &quot;institute&quot;, &quot;OP&quot;: &quot;+&quot;}]# not tested

pattern_names = [
    &quot;x_university&quot;,
    &quot;university_of_x&quot;,
    &quot;x_college&quot;,
    &quot;college_of_x&quot;,
    &quot;x_community_college&quot;,
    &quot;community_college_of_x&quot;,
    &quot;csu&quot;,
    &quot;california_state_university&quot;,
    &quot;x_state_university&quot;,
    &quot;x_state&quot;,
    &quot;state_university_of_x&quot;,
    &quot;university_of_california&quot;,
    &quot;uc&quot;,
    &quot;institute_of_x&quot;,
    &quot;x_institute&quot;
]

patterns = [
    x_university_pattern,
    university_of_x_pattern,
    x_college_pattern,
    college_of_x_pattern,
    x_community_college_pattern,
    community_college_of_x_pattern,
    csu_pattern,
    california_state_university_pattern,
    x_state_university_pattern,
    x_state_pattern,
    state_university_of_x_pattern,
    university_of_california_pattern,
    uc_pattern,
    institute_of_x_pattern,
    x_institute_pattern
]

def contains_any_substring(text, substrings):
    &quot;&quot;&quot;Check if any of the specified substrings are present in the text. Will be iterated through each applicant folder. Will check if the file has resume or cv in it.&quot;&quot;&quot;
    for substring in substrings:
        if substring.lower() in text.lower():
            return True
    return False

def remove_stop_words(string):
    stop_words = set(stopwords.words('english'))
    words = string.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    new_string = ' '.join(filtered_words)
    return new_string


# Path to resume/cv, which then gets used as a doc
test_applicant_folder = &quot;Path/To/Applicants Directory/Doe J&quot;

results = {}
nlp = spacy.load(&quot;en_core_web_sm&quot;)
matcher = Matcher(nlp.vocab)

#for loop which goes through each folder, finds the resume/cv, opens it, joins it to a string, standardizes string, assigns that string as the doc object, sets up matcher with patterns above, feeds doc object, into matcher, stores matches.
for file_name in os.listdir(test_applicant_folder):
    file_path = os.path.join(test_applicant_folder, file_name)
    if file_name.endswith(&quot;.pdf&quot;) and any(substring in file_name.lower() for substring in [&quot;resume&quot;, &quot;cv&quot;]):
        with open(file_path, &quot;rb&quot;) as file:
            pdf_reader = PyPDF2.PdfReader(file)
            text = &quot; &quot;.join(page.extract_text() for page in pdf_reader.pages)
        text = text.lower()
        punc = &quot;''!()-[];:',&lt;&gt;./#$%^&amp;*_~`''&quot;
        for ele in text:
            if ele in punc:
                text = text.replace(ele, &quot;&quot;)
        remove_stop_words(text)
        doc = nlp(text)
        
        file_matches = {}

        for i, pattern_name in enumerate(pattern_names):
            file_matches[pattern_name] = []

        # Apply each pattern to the resume/cv and store the matches
        for pattern_name, pattern in zip(pattern_names, patterns):
            matcher.add(pattern_name, [pattern])

            matches = matcher(doc)
            matches.sort(key=lambda x: x[1])

            for i, pattern_name in enumerate(pattern_names):
                file_matches[pattern_name] = [doc[start:end].text for _, start, end in matches]
            
        for pattern_name in pattern_names:
            matcher.remove(pattern_name)
        
        # Store the matches for this file in the results dictionary
        results[file_name] = file_matches

print(len(results))
test_table = pd.DataFrame(results).transpose()
test_table
</code></pre>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>x_university</th>
<th>university_of_x</th>
<th>x_college</th>
<th>college_of_x</th>
<th>x_community_college</th>
<th>community_college_of_x</th>
<th>csu</th>
<th>california_state_university</th>
<th>x_state_university</th>
<th>x_state</th>
<th>state_university_of_x</th>
<th>university_of_california</th>
<th>uc</th>
<th>institute_of_x</th>
<th>x_institute</th>
</tr>
</thead>
<tbody>
<tr>
<td>DOE_JANE_RESUME.pdf</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
<td>[university of washington]</td>
</tr>
<tr>
<td>JAMES_MICHAEL_RESUME.pdf (one example when I iterate over every resume. When they get a result, it usually looks like this)</td>
<td>[NaN]</td>
<td>[Nan]</td>
<td>[NaN]</td>
<td>[{'x_university': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'university_of_x': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'x_college': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'college_of_x': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'x_community_college': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'community_college_of_x': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'csu': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'california_state_university': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'x_state_university': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'x_state': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'state_university_of_x': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'university_of_california': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'uc': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'institute_of_x': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla'], 'x_institute': ['university of california', 'university of california san', 'university of california san diego', 'university of california san diego la', 'university of california san diego la jolla']}]</td>
<td>[NaN]</td>
<td>[NaN]</td>
<td>[NaN]</td>
<td>[NaN]</td>
<td>[NaN]</td>
<td>[NaN]</td>
<td>[NaN]</td>
<td>[NaN]</td>
<td>[NaN]</td>
<td>[NaN]</td>
<td>[NaN]</td>
</tr>
</tbody>
</table></div>
<p>I tried adding in the for loop that removes the pattern from the matcher every time the matcher is done</p>
<pre><code>for pattern_name in pattern_names:
            matcher.remove(pattern_name)
</code></pre>
<p>because I thought that having all the patterns into the matcher at one time was causing any match from any pattern to apply to all pattern_names. But this didn't do anything. I tried it as a nested for loop and outside but nothing.</p>
<p>Using all those nested for loops was also something that I tried so that each pattern is getting applied to the resume one by one. Not much there though.</p>
<p>My hope is for the table to look like this:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th></th>
<th>x_university</th>
<th>university_of_x</th>
<th>x_college</th>
<th>college_of_x</th>
<th>x_community_college</th>
<th>community_college_of_x</th>
<th>csu</th>
<th>california_state_university</th>
<th>x_state_university</th>
<th>x_state</th>
<th>state_university_of_x</th>
<th>university_of_california</th>
<th>uc</th>
<th>institute_of_x</th>
<th>x_institute</th>
</tr>
</thead>
<tbody>
<tr>
<td>DOE_JANE_RESUME.pdf</td>
<td>[]</td>
<td>[university of washington]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
</tr>
<tr>
<td>JAMES_MICHAEL_CV.pdf</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
<td>[university of california san diego]</td>
<td>[]</td>
<td>[]</td>
<td>[]</td>
</tr>
</tbody>
</table></div>
","python, parsing, nlp, pattern-matching, spacy",
Gensim&#39;s Doc2Vec with documents in multiple languages,"<p>I'm building a content based recommender system using similarities on vector representations of documents.
My documents are descriptions of books. Most of them are in English, but some of them are in other languages. I used gensim's Doc2Vec for building vector representations of the documents.</p>
<p>Based on my understanding of this model, documents in different languages should have very low similarity, because the words are not even overlapping. But this is not what's happening. Documents in different languages in my dataset can have a high similarity as 0.9.</p>
<p>Why is this happening?</p>
<pre><code>import pandas as pd
from pathlib import Path
from gensim.models.doc2vec import TaggedDocument, Doc2Vec
from gensim.utils import simple_preprocess
import numpy as np
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
data_folder = Path.cwd().parent / 'data'
transformed_folder = data_folder / 'transformed'
BOOKS_PATH = Path(transformed_folder, &quot;books_final.csv&quot;)
import seaborn as sns

book_df = pd.read_csv(BOOKS_PATH)
languages=book_df.language.unique()

training_corpus = np.empty(len(book_df), dtype=object)
for i, (isbn, desc) in tqdm(enumerate(zip(book_df['isbn'], book_df['description']))):
    training_corpus[i] = TaggedDocument(simple_preprocess(desc), [str(i)])
model=Doc2Vec(vector_size=50, min_count=2, epochs=40)
model.build_vocab(training_corpus)
#train the model
model.train(training_corpus, total_examples=model.corpus_count, epochs=model.epochs)
#get the keys of the trained model
model_keys=list(model.dv.key_to_index.keys())
matrix = model.dv.vectors
similarity_matrix = cosine_similarity(matrix)

indices_for_language = {}
for language, group_df in book_df.groupby('language'):
    indices_for_language[language] = group_df.index.to_list()

sim_japanese_italian = similarity_matrix[indices_for_language['Japanese']][:, indices_for_language['Italian']]
#make a heatmap
sns.heatmap(sim_japanese_italian)

</code></pre>
<p><a href=""https://i.sstatic.net/9sjkH.png"" rel=""nofollow noreferrer"">Heatmap of similarities between Italian and Japanese documents</a></p>
","python, nlp, gensim, recommendation-engine, doc2vec","<p>You corpus is a bit small compared to published results for the <code>Doc2Vec</code> (&quot;Paragraph Vectors&quot;) algorithm, which is usually demonstrated on set of tens-of-thousands to millions of documents.</p>
<p>In particular, if you only have (4742-4655=) 93 non-English <code>description</code> texts, and those are split over 16 non-English languages, it seems unlikely to me that <em>any</em> of the words from those other languages will have enough in-context usage examples to either (1) survive the <code>min_count</code> cutoff (which generally shouldn't be as low as 2!); or (2) obtain meaningful/generalizable implications in the model.</p>
<p>That is: all those very-rare words will have fairly idiosyncratic/arbitrary meanings, and <code>description</code> texts filled with all rare/weakly-understood words will tend to have weak, almost random doc-vectors. So, spurious similarities aren't that much of a surprise.</p>
<p>If those &quot;non-English&quot; <code>description</code> texts do have a few English-looking or omnilingual tokens in them – stray words or false cognates or punctuation or glitches – those might also dominate the <code>description</code>-to-<code>description</code> comparison.</p>
<p>You should check, on your suspiciously-too-similar pairs, how many of the words in each document are actually in the trained model – keeping in mind words not in the trained model are essentially ignored in training or post-rtraining inference. (A document of 100 words, in a language where there are only a few other 100-word documents, might in fact be dominated by single-appearance words, that don't even meet the <code>min_count=2</code>, and so when slimmed to the surviving words be short docs filled with weakly-understood words.)</p>
<p>If your similarity scores between English documents are generally sensible, then the model is doing what it can, where it has enough data. You really want this family of algorithms – word2vec, <code>Doc2Vec</code>, FastText, etc – to have many, varied, representative examples of any word's usage in sensible contrasting contexts for it to be able to do any meaningful later modeling of those words (and the documents that contain them).</p>
<p>So: if you can't manage to use a <code>min_count</code> of at least the usual default (<code>5</code>), or ideally even higher, you likely don't have sufficient data for these algorithms to show their strengths. Extending your corpus – even with other texts not of direct interest, but of sufficient variety to better represent the languages used in your texts of interest – might help. (That is: even if your universe-of-recommendations is &lt;5000 <code>descriptions</code>, mixing another 200,000 similar short blurbs from other sources, so that words and langguages of interest have enough coverage, could be worthwhile.)</p>
"
R Tidymodels textrecipes - tokenizing with spacyR - how to remove punctuations from produced list of tokens,"<p>I would like to tokenize my text by using the step_tokenize with the spacyR engine before proceeding to lemmatisation using step_lemma. Following that, i would like to remove for example punctuations from the list of tokens.</p>
<p>When using the default tokenizers::tokenize_words you can pass this option through a list of options in step_tokenize().</p>
<p>However, my understanding is that step_tokenize uses spacy_parse on the backend which does not provide such an option.</p>
<p>Is there a way to remove for e.g. punctuations or numeric tokens from the tokens produced after lemmatisation using step_lemma()?</p>
<p>A reprex:</p>
<pre><code>library(tidyverse)
library(tidymodels)
library(textrecipes)
library(spacyr)

text = &quot;It was a day, Tuesday. It wasn't Thursday!&quot;

df &lt;- tibble(text)

spacyr::spacy_initialize(entity = FALSE)

lexicon_features_tokenized_lemmatised &lt;-
  recipe(~ text, data = df%&gt;%head(1)) %&gt;%
  step_tokenize(text, engine = &quot;spacyr&quot;) %&gt;%
  step_lemma(text) %&gt;%
  prep() %&gt;%
  bake(new_data = NULL) 

lexicon_features_tokenized_lemmatised %&gt;% pull(text) %&gt;%textrecipes:::get_tokens()
</code></pre>
<p>Output:
&quot;it&quot;, &quot;be&quot;, &quot;a&quot;, &quot;day&quot;, &quot;,&quot;, &quot;Tuesday&quot;, &quot;.&quot;, &quot;it&quot;, &quot;be&quot;, &quot;not&quot;, &quot;Thursday&quot;, &quot;!&quot;</p>
<p>Desired output (Removal of &quot;!&quot;, &quot;,&quot; and &quot;.&quot;):
&quot;it&quot;, &quot;be&quot;, &quot;a&quot;, &quot;day&quot;, &quot;Tuesday&quot;, &quot;it&quot;, &quot;be&quot;, &quot;not&quot;, &quot;Thursday&quot;</p>
","r, nlp, spacy, tidymodels","<p>You want to use the <code>step_pos_filter()</code> to filter the output of spacy by POS.</p>
<p>It is a little annoying, because you have to specify the types to keep. Full list of tags found here <a href=""https://github.com/explosion/spaCy/blob/master/spacy/glossary.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/spacy/glossary.py</a></p>
<pre class=""lang-r prettyprint-override""><code>library(tidyverse)
library(tidymodels)
library(textrecipes)
library(spacyr)

text = &quot;It was a day, Tuesday. It wasn't Thursday!&quot;

df &lt;- tibble(text)

spacyr::spacy_initialize(entity = FALSE)

pos &lt;- c(&quot;ADJ&quot;, &quot;ADP&quot;, &quot;ADV&quot;, &quot;AUX&quot;, &quot;CONJ&quot;, &quot;CCONJ&quot;, &quot;DET&quot;, &quot;INTJ&quot;, &quot;NOUN&quot;, 
         &quot;NUM&quot;, &quot;PART&quot;, &quot;PRON&quot;, &quot;PROPN&quot;, &quot;SCONJ&quot;, &quot;SYM&quot;, &quot;VERB&quot;, &quot;X&quot;, &quot;EOL&quot;, 
         &quot;SPACE&quot;)

lexicon_features_tokenized_lemmatised &lt;-
  recipe(~ text, data = df %&gt;% head(1)) %&gt;%
  step_tokenize(text, engine = &quot;spacyr&quot;) %&gt;%
  step_pos_filter(text, keep_tags = pos) %&gt;%
  step_lemma(text) %&gt;%
  prep() %&gt;%
  bake(new_data = NULL) 

lexicon_features_tokenized_lemmatised %&gt;% 
  pull(text) %&gt;%
  textrecipes:::get_tokens()
#&gt; [[1]]
#&gt; [1] &quot;it&quot;       &quot;be&quot;       &quot;a&quot;        &quot;day&quot;      &quot;Tuesday&quot;  &quot;it&quot;       &quot;be&quot;      
#&gt; [8] &quot;not&quot;      &quot;Thursday&quot;
</code></pre>
"
"LangChain agent parsing error with structured_chat_agent and Wikipedia tool, handle_parsing_errors hits limit","<p>I am trying to ask GPT 4 to use Wikipedia for a prompt, using agents and tools via LangChain.</p>
<p>The difficulty I'm running into is the book I've been using, <em>Developing Apps with GPT-4 and ChatGPT: Build Intelligent Chatbots, Content Generators, and More</em>, while published in 2023, already has code examples that are deprecated.</p>
<p>For example, I am trying to do something similar to the code provided on page 114 of that book:</p>
<pre><code>from langchain.chat_models import ChatOpenAI
from langchain.agents import load_tools, initialize_agent, AgentType llm = ChatOpenAI(model_name=&quot;gpt-3.5-turbo&quot;, temperature=0)
tools = load_tools([&quot;wikipedia&quot;, &quot;llm-math&quot;], llm=llm)
agent = initialize_agent(
tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True )
    question = &quot;&quot;&quot;What is the square root of the population of the capital of the
    Country where the Olympic Games were held in 2016?&quot;&quot;&quot;
    agent.run(question)
</code></pre>
<p>I see much of this is deprecated (e.g., <a href=""https://api.python.langchain.com/en/latest/agents/langchain.agents.initialize.initialize_agent.html#langchain.agents.initialize.initialize_agent"" rel=""nofollow noreferrer"">initialize_agent</a>), so I have looked around StackOverflow, GitHub, and the LangChain Python documents to come up with this:</p>
<pre><code>from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.agents import (
  load_tools, create_structured_chat_agent, AgentExecutor
)

model = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0)
tools = load_tools([&quot;wikipedia&quot;])
prompt = ChatPromptTemplate.from_template(
  &quot;&quot;&quot;
  You are a research assistant, and your job is to retrieve information about
  movies and movie directors.
  
  Use the following tool: {tools}
  
  Use the following format:

  Question: the input question you must answer
  Thought: you should always think about what to do
  Action: the action to take, should be one of [{tool_names}]
  Action Input: the input to the action
  Observation: the result of the action
  ... (this Thought/Action/Action Input/Observation can repeat N times)
  Thought: I now know the final answer
  Final Answer: the final answer to the original input question. You only
  need to give the number, no other information or explanation is necessary.

  Begin!

  Question: How many movies did the director of the {year} movie {name} direct
  before they made {name}?
  Thought: {agent_scratchpad}
  &quot;&quot;&quot;
)
agent = create_structured_chat_agent(model, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools)
agent_executor.invoke({&quot;year&quot;: &quot;1991&quot;, &quot;name&quot;: &quot;thelma and louise&quot;})
</code></pre>
<p>I'm going to be running this through a loop of many movies, so <strong>I'd like it to only return one integer</strong> (in this case, 6). But it seems like I need to give it that full thought process prompt; I can't get it to run if I don't include <code>{tools}</code>, <code>{tool_names}</code>, and <code>{agent_scratchpad}</code> in the prompt (<a href=""https://github.com/langchain-ai/langchain/discussions/20199"" rel=""nofollow noreferrer"">per this GitHub post</a>).</p>
<p>The frustrating thing is I eventually do get the correct answer, but note that it is throwing an error:</p>
<pre><code>ValueError: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: First, I need to find out who directed the movie &quot;Thelma and Louise&quot; in 1991. 
  Action: wikipedia
  Action Input: {'query': 'Thelma and Louise'}
  Observation: 
  &quot;Thelma &amp; Louise&quot; is a 1991 American female buddy road film directed by Ridley Scott and written by Callie Khouri. It stars Geena Davis as Thelma and Susan Sarandon as Louise, two friends who embark on a road trip with unforeseen consequences. The film became a critical and commercial success, receiving six Academy Award nominations and winning one for Best Original Screenplay for Khouri. Scott was nominated for Best Director.
  Thought: 
  Ridley Scott directed the movie &quot;Thelma and Louise&quot;. Now I need to find out how many movies he directed before this one.
  Action: wikipedia
  Action Input: {'query': 'Ridley Scott filmography'}
  Observation: 
  Ridley Scott is an English filmmaker. Following his commercial breakthrough with the science fiction horror film Alien (1979), his best known works are the neo-noir dystopian science fiction film Blade Runner (1982), historical drama Gladiator (2000), and science fiction film The Martian (2015). Scott has directed more than 25 films and is known for his atmospheric, highly concentrated visual style. His films are also known for their strong female characters. Here is a list of his films before &quot;Thelma &amp; Louise&quot;: 
  1. The Duellists (1977)
  2. Alien (1979)
  3. Blade Runner (1982)
  4. Legend (1985)
  5. Someone to Watch Over Me (1987)
  6. Black Rain (1989)
  Thought: 
  Ridley Scott directed six movies before &quot;Thelma and Louise&quot;.
  Final Answer: 6
</code></pre>
<p>This seems to be very common (<a href=""https://github.com/langchain-ai/langchain/issues/1358"" rel=""nofollow noreferrer"">here</a>, <a href=""https://github.com/langchain-ai/langchain/discussions/4065"" rel=""nofollow noreferrer"">and here</a>, and <a href=""https://stackoverflow.com/questions/77391069/parsing-error-on-langchain-agent-with-gpt4all-llm"">also here</a>, and <a href=""https://www.reddit.com/r/LangChain/comments/16bztmo/outputparserexception_could_not_parse_llm_output/"" rel=""nofollow noreferrer"">lastly here</a>).</p>
<p>So, I do what it tells me (<a href=""https://python.langchain.com/docs/modules/agents/how_to/handle_parsing_errors/"" rel=""nofollow noreferrer"">see docs also</a>) and update my AgentExecutor to:</p>
<pre><code>agent_executor = AgentExecutor(
  agent=agent, 
  tools=tools,
  handle_parsing_errors=True
)
</code></pre>
<p>And that returns:</p>
<pre><code>{'year': '1991', 'name': 'thelma and louise', 'output': 'Agent stopped due to iteration limit or time limit.'}
</code></pre>
<p>My question: <strong>How can I use LangChain to combine GPT 4 and Wikipedia to get an answer to a query, when all I want back is an integer?</strong></p>
","python, nlp, openai-api, langchain, large-language-model","<p>Author of the book <em>Developing Apps with GPT-4 and ChatGPT</em> here, I already answered by mail, but just in case someone else stumbles upon this question... You can find updated code at <a href=""https://github.com/malywut/gpt_examples"" rel=""nofollow noreferrer"">https://github.com/malywut/gpt_examples</a>.</p>
<p>The updated code looks like this:</p>
<pre><code>from langchain_openai import ChatOpenAI
from langchain.agents import load_tools, create_react_agent, AgentExecutor
from langchain import hub

llm = ChatOpenAI(model_name=&quot;gpt-3.5-turbo&quot;)
tools = load_tools([&quot;wikipedia&quot;, &quot;llm-math&quot;], llm=llm)
agent = create_react_agent(
    tools=tools,
    llm=llm,
    prompt = hub.pull(&quot;hwchase17/react&quot;),
)
question = &quot;...&quot;
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
agent_executor.invoke({&quot;input&quot;: question})
</code></pre>
<p>Hope this helps.</p>
"
Using word2vec to classify words in categories,"<p><strong>BACKGROUND</strong></p>

<p>I have vectors with some sample data and each vector has a category name (Places,Colors,Names).</p>

<pre><code>['john','jay','dan','nathan','bob']  -&gt; 'Names'
['yellow', 'red','green'] -&gt; 'Colors'
['tokyo','bejing','washington','mumbai'] -&gt; 'Places'
</code></pre>

<p>My objective is to train a model that take a new input string and predict which category it belongs to. For example if a new input is ""purple"" then I should be able to predict 'Colors' as the correct category. If the new input is ""Calgary"" it should predict 'Places' as the correct category.</p>

<p><strong>APPROACH</strong></p>

<p>I did some research and came across <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">Word2vec</a>. This library has a ""similarity"" and ""mostsimilarity"" function which i can use. So one brute force approach I thought of is the following:</p>

<ol>
<li>Take new input.</li>
<li>Calculate it's similarity with each word in each vector and take an average.</li>
</ol>

<p>So for instance for input ""pink"" I can calculate its similarity with words in vector ""names"" take a average and then do that for the other 2 vectors also. The vector that gives me the highest similarity average would be the correct vector for the input to belong to.</p>

<p><strong>ISSUE</strong></p>

<p>Given my limited knowledge in NLP and machine learning I am not sure if that is the best approach and hence I am looking for help and suggestions on better approaches to solve my problem. I am open to all suggestions and also please point out any mistakes I may have made as I am new to machine learning and NLP world.</p>
","python, machine-learning, nlp, word2vec, gensim","<p>If you're looking for the simplest / fastest solution then I'd suggest you take the pre-trained word embeddings (Word2Vec or GloVe) and just build a simple query system on top of it. The vectors have been trained on a huge corpus and are likely to contain good enough approximation to your domain data.</p>

<p>Here's my solution below:</p>

<pre><code>import numpy as np

# Category -&gt; words
data = {
  'Names': ['john','jay','dan','nathan','bob'],
  'Colors': ['yellow', 'red','green'],
  'Places': ['tokyo','bejing','washington','mumbai'],
}
# Words -&gt; category
categories = {word: key for key, words in data.items() for word in words}

# Load the whole embedding matrix
embeddings_index = {}
with open('glove.6B.100d.txt') as f:
  for line in f:
    values = line.split()
    word = values[0]
    embed = np.array(values[1:], dtype=np.float32)
    embeddings_index[word] = embed
print('Loaded %s word vectors.' % len(embeddings_index))
# Embeddings for available words
data_embeddings = {key: value for key, value in embeddings_index.items() if key in categories.keys()}

# Processing the query
def process(query):
  query_embed = embeddings_index[query]
  scores = {}
  for word, embed in data_embeddings.items():
    category = categories[word]
    dist = query_embed.dot(embed)
    dist /= len(data[category])
    scores[category] = scores.get(category, 0) + dist
  return scores

# Testing
print(process('pink'))
print(process('frank'))
print(process('moscow'))
</code></pre>

<p>In order to run it, you'll have to download and unpack the pre-trained GloVe data from <a href=""http://nlp.stanford.edu/data/glove.6B.zip"" rel=""noreferrer"">here</a> (careful, 800Mb!). Upon running, it should produce something like this:</p>

<pre><code>{'Colors': 24.655489603678387, 'Names': 5.058711671829224, 'Places': 0.90213905274868011}
{'Colors': 6.8597321510314941, 'Names': 15.570847320556641, 'Places': 3.5302454829216003}
{'Colors': 8.2919375101725254, 'Names': 4.58830726146698, 'Places': 14.7840416431427}
</code></pre>

<p>... which looks pretty reasonable. And that's it! If you don't need such a big model, you can filter the words in <code>glove</code> according to their <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""noreferrer"">tf-idf</a> score. Remember that the model size only depends on the data you have and words you might want to be able to query.</p>
"
Pytesseract to return text inside bounding box,"<p>I am currently trying to do named entity extraction on a set of documents. My plan is:</p>
<ol>
<li>Do OCR using pytesseract</li>
<li>Extract the text</li>
<li>Apply an LLM to get the entities like patient name, age etc.</li>
</ol>
<p>One of the example scans look like this: <a href=""https://i.sstatic.net/4XDAH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4XDAH.png"" alt=""enter image description here"" /></a></p>
<p>The output of pytesseract using: <code>text = pytesseract.image_to_string(image, lang='eng', config='--psm 12')</code> is this:</p>
<pre><code>HR

Community General Hospital

Patient Account #

Medical Record #

12345

INPATIENT REGISTRATION AND SUMMARY FORM

215043

Patient Name (Last) (Fir) (Middle)

‘Attending Physician Number and Name

Patieat Type

Tae Date

“Adin time

Brown, John
</code></pre>
<p>Now, as you can clearly see, <strong>the &quot;Patient Name (Last)(First)(Middle)&quot; comes first</strong> and the <strong>actual name of the patient (Brown, John) comes at the end.</strong> How can I make sure the pytesseract returns the text according to the boxes below, i.e. &quot;Patient Name (Last)(First)(Middle)&quot; followed by &quot;Brown, John&quot; and then proceed to &quot;Attending Physician Number and Name&quot;? If tesseract cannot do it, is there a way to get this for any document?</p>
<p>The reason for this requirement is that the LLM can more accurately tell the patient name if it falls right after &quot;Patient Name&quot; rather than later in the output text.</p>
","nlp, tesseract, python-tesseract, named-entity-recognition, large-language-model",
Unable to locally save some models locally with SentenceTransformers,"<p>Tried many times to have SentenceTransformers and having stripped down my code to the following:</p>
<pre><code>from sentence_transformers import SentenceTransformer
modelPath = &quot;/Users/bob/.cache&quot;
model = SentenceTransformer(&quot;Salesforce/SFR-Embedding-Mistral&quot;)
model.save(modelPath)
</code></pre>
<p>always fails with timeouts(?) while downloading the model, not always at the same exact spot. Here follows an example:</p>
<pre><code>llm-fast-py3.11bob /Volumes/2TBWDB/code/llm_fast [main] $ /Volumes/2TBWDB/code/llm_fast/.venv/bin/python /Volumes/2TBWDB/code/llm_fast/llm_fast/prove/scarica
.py
modules.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 229/229 [00:00&lt;00:00, 413kB/s]
config_sentence_transformers.json: 100%|█████████████████████████████████████████████████████████████████████████████████████| 123/123 [00:00&lt;00:00, 288kB/s]
README.md: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 85.3k/85.3k [00:00&lt;00:00, 756kB/s]
sentence_bert_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 54.0/54.0 [00:00&lt;00:00, 187kB/s]
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 663/663 [00:00&lt;00:00, 1.99MB/s]
model.safetensors.index.json: 100%|█████████████████████████████████████████████████████████████████████████████████████| 22.2k/22.2k [00:00&lt;00:00, 29.4MB/s]
model-00001-of-00003.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████| 4.94G/4.94G [00:46&lt;00:00, 106MB/s]
model-00002-of-00003.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████| 5.00G/5.00G [00:46&lt;00:00, 107MB/s]
model-00003-of-00003.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████| 4.28G/4.28G [00:40&lt;00:00, 107MB/s]
Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:14&lt;00:00, 44.72s/it]
Loading checkpoint shards:  33%|███████████████████████████████▋                                                               | 1/3 [00:17&lt;00:35, 17.53s/it]zsh: killed     /Volumes/2TBWDB/code/llm_fast/.venv/bin/python 
/Users/bob/.pyenv/versions/3.11.7/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
</code></pre>
<p>Same happens with other largish models such as intfloat/e5-mistral-7b-instruct</p>
<p>I have a good 200MBit fiber connection. Can I do anything else to save these models locally?</p>
<p>Python 3.11.7 - sentence-transformers 2.6.1 - Mac M1 16GB with Sonoma 14.4.1 - 55GB free disk space</p>
","python, nlp, huggingface-transformers, large-language-model, sentence-transformers",
Tensorflow unicode text encoding-decoding,"<p>I just started working with cyrillic text. Cannot properly print russian text after text preprocessing. How can I set encoding during text loading?</p>
<pre><code>import pathlib
text = pathlib.Path('rus.txt').read_text(encoding='utf-8')

lines = text.splitlines()
pairs = [line.split('\t') for line in lines]
inp = [inp for targ, inp, tag in pairs]
targ = [targ for targ, inp, tag in pairs]
inp[:20]
</code></pre>
<p>Output1:</p>
<pre><code>['Марш!',  'Иди.',  'Идите.',  'Здравствуйте.',  'Привет!',  'Хай.', 
   'Здрасте.',  'Здоро́во!',  'Приветик!',  'Беги!',  'Бегите!',...
</code></pre>
<p>Creating dataset:</p>
<pre><code>BUFFER_SIZE = len (inp)
BATCH_SIZE = 64
    
dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)
dataset = dataset.batch(BATCH_SIZE)

for example_input_batch, example_target_batch in dataset.take(1):
  print(example_input_batch[:5]) --Russian input
  print()
  print(example_target_batch[:5]) --English target
  break
</code></pre>
<p>Output2:</p>
<pre><code> tf.Tensor(
    [b'\xd0\xa2\xd0\xbe\xd0\xbc \xd0\xbf\xd0\xbe\xd1\x81\xd1\x82\xd1\x83\xd0\xbf\xd0\xb8\xd0\xbb \xd1\x85\xd0\xbe\xd1\x80\xd0\xbe\xd1\x88\xd0\xbe.'
     b'\xd0\xa2\xd1\x8b \xd1\x81\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd0\xbb\xd0\xb0 \xd1\x8d\xd1\x82\xd0\xbe \xd1\x81\xd0\xbf\xd0\xb5\xd1\x86\xd0\xb8\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe.'
     b'\xd0\xa2\xd0\xbe\xd0\xbc \xd0\xb5\xd1\x89\xd1\x91 \xd0\xbd\xd0\xb5 \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82 \xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd1\x8c \xd1\x81\xd0\xb0\xd0\xbc.'
     b'\xd0\x94\xd1\x83\xd0\xbc\xd0\xb0\xd1\x8e, \xd0\xbf\xd0\xbe\xd1\x80\xd0\xb0 \xd0\xbc\xd0\xbd\xd0\xb5 \xd0\xbf\xd0\xbe\xd0\xb3\xd0\xbe\xd0\xb2\xd0\xbe\xd1\x80\xd0\xb8\xd1\x82\xd1\x8c \xd0\xbe\xd0\xb1 \xd1\x8d\xd1\x82\xd0\xbe\xd0\xb9 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb1\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5 \xd1\x81 \xd0\xbd\xd0\xb0\xd1\x87\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xb8\xd0\xba\xd0\xbe\xd0\xbc.'
     b'\xd0\xaf \xd0\xbc\xd0\xbe\xd0\xb3\xd1\x83 \xd1\x8d\xd1\x82\xd0\xbe \xd1\x83\xd0\xbb\xd0\xb0\xd0\xb4\xd0\xb8\xd1\x82\xd1\x8c.'], shape=(5,), dtype=string)

tf.Tensor(
[b'Tom did a good thing.' b'You did that on purpose.'
 b&quot;Tom can't walk on his own yet.&quot;
 b&quot;I think it's time for me to talk to the boss about this problem.&quot;
 b'I can arrange that.'], shape=(5,), dtype=string)
</code></pre>
<p>Can you please advise what is the problem here with printing russian text? English text prints ok.</p>
","python, tensorflow, nlp, machine-translation",
What&#39;s inside inner vertices in Word2Vec Hierarchical Softmax?,"<p>I have a question about Hierarchical Softmax. Actually, I do not quite understand what is stored in inner vertices (which are not leaf vertices). I clearly understand the main idea of this algorithm, but each step we calculate dot product of input word embedding with the word embedding of inner vertice. So what vectors are inside these inner vertices? Is it randomly initialized vectors of size that equals to embedding_size and then their coordinates change due to backpropagation step until we stop?</p>
","machine-learning, nlp, word2vec, hierarchical, softmax","<p>While there are many slightly-different ways to think about it, it may help to consider the values in the (trained, frozen) neural network as associated with edges moreso than vertexes (nodes).</p>
<p>The network &quot;projection weights&quot; leading from an (abstract) one-hot encoding of each known word into the network are essentially the actual per-word word-vectors. Assuming a common case of 300d vectors, the 300 edges from the single-word node to the inner nodes are that words.</p>
<p>Then, there's another set of edge-weights from the internal activation to the &quot;output&quot; layer, which is offering the network's training goal of in-context word-projection.</p>
<p>In the (more common &amp; usual default) negative-sampling approach, each output node corresponds to a single predictable word. That's a very easy output-shape to visualize. And the value of negative-sampling is that you only check the activations of the desired word, and <code>n</code> more randomly chosen negative words, to perform your training updates. That's way less calculation than if you checked the output values at all <code>V</code> (size of vocabulary) output nodes, and still works pretty well, and doesn't get more expensive with larger vocabularies (unless you choose for other reasons to also increase your choice of <code>n</code> negative samples).</p>
<p>In hierarchical softmax, the interpretation of the output nodes is more complicated. Rather than the activation at a single node indicating the prediction of a single word, a (varying) set of nodes must have the right on/off activations to communicate the variable-length huffman-code of a word.</p>
<p>So in HS, to backprop the network more towards predicting your desired &quot;positive&quot; word (from one input context), the algorithm considers just those nodes involved in that words nique coding – a smaller set of nodes for the most-ommon words, but a larger set of nodes for rarer words – and nudges each of them more towards the pattern that predicts the desired word. Again, you get the sparse training efficiency of updating only a tiny subset, far smaller than all <code>V</code> nodes, each training-step. But, the cost will vary based on the target word, and grow with the log of <code>V</code> as vocabulary-size grows. Further, as the original/naive assignment of codes is based strictly on word-frequency, quite-dissimilar words may have very-similar codings, perhaps causing more word-to-word interference. (There were hints in the original <code>word2vec.c</code> release of refining the HS word-codings over time to ensure similar words share similar Huffman codings, but I've seen litle followup on that idea, perhaps because of the dominance of negative-sampling.)</p>
<p>So, in an HS network, the weights from the inner-activations, to the output nodes, are tuned to indicate, by Huffman code, which word is the preferred prediction from a context.</p>
<p>In the word2vec implementation I'm most familiar with, Python Gensim, these &quot;hidden to output&quot; weights are not even randomly initialized at the beginning, instead left as 0.0 – and I think this was directly copied from the initialization of Google's <code>word2vec.c</code> release. But, as soon as training begins, the explict random initialization of those &quot;input weights&quot; (initial random input word-vectors) means those weights are immediately perturbed in a way that at 1st is nearly all random but becomes more helpful over the SGD training.</p>
<p>So:</p>
<ul>
<li>those inner weights start 0.0 but quickly start reflecting the influence of the (initially-random) word vectors and training examples</li>
<li>they're usually <em>not</em> harvested from the network after training, like the final word-vectors are – but would be kept around if you wanted to continue training later, and perhaps provide a running start to future training runs</li>
</ul>
<p>(In the negative-sampling case, some research suggested those hidden-to-output weights could also be interpreted as same <code>embedding_size</code> per-word vectors, with some usefulness: see paper by Mitra et al at Microsoft about &quot;Dual Word Embeddings&quot;. But given the varying-length codings of output words in the HS case, extracting/interpretating those output-weights would be trickier.)</p>
<p>In the implementation I'm most familiar with, the It may help to think instead of the shallow neural network's &quot;projection layer&quot; (effectively the word-vectors themselves, as each virtual &quot;single node&quot; 1-hot word has its <code>embedding_size</code> out-weights) and &quot;hidden layer</p>
"
How to train Hugging Face Model On Multiple Datasets?,"<p>I am trying to fine tune a model based on two datasets, following the example on the Hugging Face website, I have my model training on the Yelp Review dataset, but I also want to train my model on the Short Jokes dataset.</p>
<p>These two datasets are picked just to eximplify that the datasets I would like to fine tune the model on are totally unrelated.</p>
<p>I have seen the <code>interleave_datasets</code> function, but im not sure if its exactly what i should be using.</p>
<p>I've tried this to train on one dataset:</p>
<pre><code>from datasets import load_dataset

yelp_dataset = load_dataset(&quot;yelp_review_full&quot;)
jokes_dataset = load_dataset(&quot;short-jokes&quot;)

from transformers import AutoTokenizer, Trainer

tokenizer = AutoTokenizer.from_pretrained(&quot;google-bert/bert-base-cased&quot;)

def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)

tokenized_datasets = yelp_dataset.map(tokenize_function, batched=True)

small_train_yelp_dataset = tokenized_datasets[&quot;train&quot;].shuffle(seed=42).select(range(1000))
small_eval_yelp_dataset = tokenized_datasets[&quot;test&quot;].shuffle(seed=42).select(range(1000))

# Where do these go?
small_train_short_jokes_dataset = jokes_dataset[&quot;train&quot;].shuffle(seed=42).select(range(1000))
small_eval_short_jokes_dataset = jokes_dataset[&quot;test&quot;].shuffle(seed=42).select(range(1000))

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(&quot;google-bert/bert-base-cased&quot;, num_labels=5)

from transformers import TrainingArguments

training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_yelp_dataset,
    eval_dataset=small_eval_yelp_dataset,

)

trainer.train()

</code></pre>
<p><strong>How would I train my model on two different sets at one time?</strong></p>
","python, nlp, huggingface-transformers, bert-language-model, huggingface-datasets",
Loading a safetensor file in transformers,"<p>I have downloaded this <a href=""https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g"" rel=""nofollow noreferrer"">model</a> from huggingface. I am trying to load this model in transformers so I can do inferencing:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(&quot;path_to/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g&quot;)

model = AutoModelForCausalLM.from_pretrained(&quot;path_to/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g&quot;)
</code></pre>
<p>But i get the error saying that it expects a .bin or .h5 or .ckpt files but the above has only .safetensors or .pt files</p>
<p>How do i load the model?</p>
","python-3.x, machine-learning, nlp, huggingface-transformers",
Mosestokenizer issue: [WinError 2] The system cannot find the file specified,"<p>Can't figure out why is this problem appearing.</p>

<pre><code>from mosestokenizer import MosesDetokenizer

with MosesDetokenizer('en') as detokenize:
    print(detokenize([""hi"", 'my', 'name', 'is', 'artem']))
</code></pre>

<p>This is what I get:</p>

<pre><code>stdbuf was not found; communication with perl may hang due to stdio buffering.
Traceback (most recent call last):
  File ""C:\Users\ArtemLaptiev\Documents\GitHub\temp\foo.py"", line 3, in &lt;module&gt;
    with MosesDetokenizer('en') as detokenize:
  File ""C:\ProgramFiles\Anaconda\lib\site-packages\mosestokenizer\detokenizer.py"", line 47, in __init__
    super().__init__(argv)
  File ""C:\ProgramFiles\Anaconda\lib\site-packages\toolwrapper.py"", line 52, in __init__
    self.start()
  File ""C:\ProgramFiles\Anaconda\lib\site-packages\toolwrapper.py"", line 92, in start
    cwd=self.cwd
  File ""C:\ProgramFiles\Anaconda\lib\subprocess.py"", line 709, in __init__
    restore_signals, start_new_session)
  File ""C:\ProgramFiles\Anaconda\lib\subprocess.py"", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] The system cannot find the file specified
</code></pre>

<p>Thank you for help!</p>
","python, nlp, anaconda, nltk, tokenize","<p>use <code>sacremoses</code> instead of <code>moses</code>.</p>

<pre><code>pip install -U sacremoses
</code></pre>

<p>and</p>

<pre><code>from sacremoses import MosesTokenizer, MosesDetokenizer
with MosesDetokenizer() as detokenize:
    print(detokenize([""hi"", 'my', 'name', 'is', 'artem']))
</code></pre>

<p>for complete details <a href=""https://github.com/alvations/sacremoses"" rel=""nofollow noreferrer"">sacremoses</a></p>
"
ImportError: cannot import name &#39;LayoutlmConfig&#39; from &#39;layoutlm,"<p>I am trying to save the predictions for the LayoutLM model.</p>
<p>paper - <a href=""https://arxiv.org/abs/1912.13318"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1912.13318</a></p>
<p>Notebook - <a href=""https://www.kaggle.com/code/iamarjunchandra/layoutlm-document-sequence-labeling-model/notebook"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/iamarjunchandra/layoutlm-document-sequence-labeling-model/notebook</a></p>
<pre><code>The problem arises when I run the below code for making prediction 
! python unilm/layoutlm/examples/seq_labeling/run_seq_labeling.py 
                               --do_predict 
                               --data_dir data 
                               --model_type layoutlm 
                               --model_name_or_path output 
                               --output_dir output 
                               --labels data/labels.txt \
                               --fp16
</code></pre>
<p>I am getting the below error.</p>
<pre><code>Traceback (most recent call last):
  File &quot;unilm\\layoutlm\\examples\\seq_labeling\\run_seq_labeling.py&quot;, line 53, in &lt;module&gt;
    from layoutlm import FunsdDataset, LayoutlmConfig, LayoutlmForTokenClassification
ImportError: cannot import name 'LayoutlmConfig' from 'layoutlm' (c:\Users\jyoti\anaconda3\lib\site-packages\layoutlm\__init__.py)
</code></pre>
","python, machine-learning, pdf, nlp, ocr",
Spacy: Can&#39;t find model &#39;en_core_web_sm&#39;. It doesn&#39;t seem to be a Python package or a valid path to a data directory,"<p>I'm trying to load the <code>en_core_web_sm</code> spaCy model, but I have been unsuccessful in doing so.</p>
<p>The error that occurs is the following:</p>
<pre><code>OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.
</code></pre>
<p>I'm working in a Anaconda virtual environment. The following checkboxes are ticked:</p>
<ul>
<li>Did <code>conda activate gcp-env</code> prior to installing spaCy and the english language model</li>
<li>Have run <code>conda install -c conda-forge spacy</code> while on the right environment</li>
<li>Then, have run <code>python -m spacy download en</code>, still while on the right environment</li>
<li>Also tried adding <code>spacy</code> to the <code>requirements.txt</code> , and installing dependencies via that route, after first attempts failed</li>
</ul>
<p><code>spacy info</code> produces this output:</p>
<pre><code>spacy info

============================== Info about spaCy ==============================

spaCy version    3.3.0                         
Location         /Users/simonmortensen/opt/anaconda3/envs/gcp-env/lib/python3.10/site-packages/spacy
Platform         macOS-11.6.5-x86_64-i386-64bit
Python version   3.10.4                        
Pipelines        en_core_web_sm (3.3.0)
</code></pre>
<p><code>python -m spacy validate</code> produces this output:</p>
<pre><code>================= Installed pipeline packages (spaCy v3.3.0) =================
ℹ spaCy installation:
/Users/simonmortensen/opt/anaconda3/envs/gcp-env/lib/python3.10/site-packages/spacy

NAME             SPACY                 VERSION                            
en_core_web_sm   &gt;=3.3.0.dev0,&lt;3.4.0   3.3.0   ✔
</code></pre>
<p>I've been through several previous StackOverflow posts on the same topic. Those have often been solved, but my issue remains.</p>
<p>Any advice would be very much appreciated. Thanks in advance!</p>
<p>Simon</p>
<p>EDIT:
For additional context, <code>pip list</code> on the environment contains both</p>
<pre><code>spacy                         3.3.0
spacy-legacy                  3.0.9
spacy-loggers                 1.0.2
</code></pre>
<p>and</p>
<pre><code>en-core-web-sm                3.3.0
</code></pre>
<p>Even so, <code>import en_core_web_sm</code> also doesn't work:</p>
<pre><code>import en_core_web_sm
Traceback (most recent call last):

  Input In [65] in &lt;cell line: 1&gt;
    import en_core_web_sm

ModuleNotFoundError: No module named 'en_core_web_sm'
</code></pre>
","python, nlp, anaconda, conda, spacy","<p>Spyder was the villain.</p>
<p>All packages were correctly installed on the virtual environment, but Spyder was not running that environment (even if the IDE was launched with the <code>spyder</code> command from a terminal where the environment was in fact activated).</p>
<p>In order to make Spyder run the correct environment, you needed to change the Python interpreter in the Spyder preferences:
<a href=""https://i.sstatic.net/UweOn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UweOn.png"" alt=""enter image description here"" /></a>
... and then restart the kernel.</p>
<p>I got an error prompting me to <code>pip install spyder-kernels==2.1.*</code>, but once that was done (make sure to do it on the right venv), I restarted Spyder, and it finally worked!</p>
<p>See discussions in thread: <a href=""https://github.com/explosion/spaCy/discussions/10895"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/discussions/10895</a>.</p>
"
Langchain ParentDocumetRetriever: Save and load,"<p>I am using the PartentDocumentRetriever from Langchain.
Now I first want to build my vector database and then want to retrieve stuff.</p>
<p>Here is my file that builds the database:</p>
<pre><code># =========================
#  Module: Vector DB Build
# =========================
import box
import yaml
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.storage import InMemoryStore
from langchain.retrievers import ParentDocumentRetriever
from langchain.vectorstores import Chroma

# Import config vars
with open('config/config.yml', 'r', encoding='utf8') as ymlfile:
    cfg = box.Box(yaml.safe_load(ymlfile))

# Build vector database
def run_db_build():
    loader = DirectoryLoader(cfg.DATA_PATH,
                             glob='*.pdf',
                             loader_cls=PyPDFLoader)
    documents = loader.load()
    
    embeddings = HuggingFaceEmbeddings(model_name=cfg.EMBEDDING_MODEL_NAME,
                                       model_kwargs={'device': 'mps'}, encode_kwargs={'device': 'mps', 'batch_size': 32})
    
    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)
    store = InMemoryStore()

    vectorstore = Chroma(collection_name=&quot;split_parents&quot;, embedding_function=embeddings,
                         persist_directory=&quot;chroma_db/&quot;) 
    big_chunks_retriever = ParentDocumentRetriever(
        vectorstore=vectorstore,
        docstore=store,
        child_splitter=child_splitter,
        parent_splitter=parent_splitter,
    )
    big_chunks_retriever.add_documents(documents)

if __name__ == &quot;__main__&quot;:
    run_db_build()
</code></pre>
<p>So I am saving the Chroma Database in the folder &quot;chroma_db&quot;. However I want to save PartentDocumentRetriever (big_chunk_objects) with the added documents to use it later when building a RetrievalQa chain. So how do I load &quot;big_chunk_objects&quot; in the following code?</p>
<pre><code>def build_retrieval_qa(llm, prompt, vectordb):
    chain_type_kwargs={
        #&quot;verbose&quot;: True,
        &quot;prompt&quot;: prompt,
        &quot;memory&quot;: ConversationBufferMemory(
            memory_key=&quot;history&quot;,
            input_key=&quot;question&quot;)}
    
    dbqa = RetrievalQA.from_chain_type(llm=llm,
                                       chain_type='stuff',
                                       retriever=&quot;HOW TO SET PARENTDOCUMENTRETRIEVER HERE?&quot;,                                       
                                       return_source_documents=cfg.RETURN_SOURCE_DOCUMENTS,
                                       chain_type_kwargs=chain_type_kwargs,
                                      )
return dbqa
</code></pre>
","python, nlp, langchain",
How to interpret scikit&#39;s learn confusion matrix and classification report?,"<p>I have a sentiment analysis task, for this Im using this <a href=""http://pastebin.com/ikbKQcsc"" rel=""noreferrer"">corpus</a> the opinions have 5 classes (<code>very neg</code>, <code>neg</code>, <code>neu</code>, <code>pos</code>, <code>very pos</code>), from 1 to 5. So I do the classification as follows:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
tfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True,
                            sublinear_tf=False, ngram_range=(2,2))
from sklearn.cross_validation import train_test_split, cross_val_score

import pandas as pd

df = pd.read_csv('/corpus.csv',
                     header=0, sep=',', names=['id', 'content', 'label'])

X = tfidf_vect.fit_transform(df['content'].values)
y = df['label'].values


from sklearn import cross_validation
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,
                                                    y, test_size=0.33)


from sklearn.svm import SVC
svm_1 = SVC(kernel='linear')
svm_1.fit(X, y)
svm_1_prediction = svm_1.predict(X_test)
</code></pre>

<p>Then with the metrics I obtained the following confusion matrix and classification report, as follows:</p>

<pre><code>print '\nClasification report:\n', classification_report(y_test, svm_1_prediction)
print '\nConfussion matrix:\n',confusion_matrix(y_test, svm_1_prediction)
</code></pre>

<p>Then, this is the result:</p>

<pre><code>Clasification report:
             precision    recall  f1-score   support

          1       1.00      0.76      0.86        71
          2       1.00      0.84      0.91        43
          3       1.00      0.74      0.85        89
          4       0.98      0.95      0.96       288
          5       0.87      1.00      0.93       367

avg / total       0.94      0.93      0.93       858


Confussion matrix:
[[ 54   0   0   0  17]
 [  0  36   0   1   6]
 [  0   0  66   5  18]
 [  0   0   0 273  15]
 [  0   0   0   0 367]]
</code></pre>

<p>How can I interpret the above confusion matrix and classification report. I tried reading the <a href=""http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html"" rel=""noreferrer"">documentation</a> and this <a href=""https://stats.stackexchange.com/questions/95209/how-can-i-interpret-sklearn-confusion-matrix"">question</a>. But still can interpretate what happened here particularly with this data?. Wny this matrix is somehow ""diagonal""?. By the other hand what means the recall, precision, f1score and support for this data?. What can I say about this data?. Thanks in advance guys</p>
","machine-learning, nlp, scikit-learn, svm, confusion-matrix","<p>Classification report must be straightforward - a report of P/R/F-Measure for each element in your test data. In Multiclass problems, it is not a good idea to read Precision/Recall and F-Measure over the whole data any imbalance would make you feel you've reached better results. That's where such reports help.</p>

<p>Coming to confusion matrix, it is much detailed representation of what's going on with your labels. So there were 71 points in the first class (label 0). Out of these, your model was successful in identifying 54 of those correctly in label 0, but 17 were marked as label 4. Similarly look at second row. There were 43 points in class 1, but 36 of them were marked correctly. Your classifier predicted 1 in class 3 and 6 in class 4.</p>

<p><img src=""https://i.sstatic.net/0yLu8.png"" alt=""enter image description here""></p>

<p>Now you can see the pattern this follows. An ideal classifiers with 100% accuracy would produce a pure diagonal matrix which would have all the points predicted in their correct class. </p>

<p>Coming to Recall/Precision. They are some of the mostly used measures in evaluating how good your system works. Now you had 71 points in first class (call it 0 class). Out of them your classifier was able to get 54 elements correctly. That's your recall. 54/71 = 0.76. Now look only at first column in the table. There is one cell with entry 54, rest all are zeros. This means your classifier marked 54 points in class 0, and all 54 of them were actually in class 0. This is precision. 54/54 = 1. Look at column marked 4. In this column, there are elements scattered in all the five rows. 367 of them were marked correctly. Rest all are incorrect. So that reduces your precision.</p>

<p>F Measure is harmonic mean of Precision and Recall. 
Be sure you read details about these. <a href=""https://en.wikipedia.org/wiki/Precision_and_recall"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Precision_and_recall</a></p>
"
Why does the Cloud Natural Language Model API return so many NULLs?,"<p>I have been working with <a href=""https://www.samthebrand.com/sentiment-analysis-using-sql-ai-bigquery/"" rel=""nofollow noreferrer"">Google's Cloud Natural Language Model in BigQuery</a>, and I have noticed that a significant percent of requests generate a <code>NULL</code> response. Why?</p>
<p>Here's an example...</p>
<p>This code creates a table with 54 movie reviews from the publicly available IMDB movie reviews dataset, then creates a remote connection to the API, and then uses the function <code>ML.UNDERSTAND_TEXT</code> to do NLP on the reviews via the API. 10 out of the 54 results = <code>NULL</code>. I have tried this again with a different sample of movie reviews with the same result.</p>
<p>The code:</p>
<pre><code>-- from: https://www.samthebrand.com/sentiment-analysis-using-sql-ai-bigquery/
-- Isolate a sample of natural language data

CREATE OR REPLACE TABLE `[project].[dataset].[table]` AS
SELECT review, movie_url, label, reviewer_rating
FROM `bigquery-public-data.imdb.reviews`
WHERE reviewer_rating IS NOT NULL AND RAND() &lt; 0.001;

-- Create a connection to a remote model

CREATE OR REPLACE MODEL `[project].[dataset].[model_name_a]`
REMOTE WITH CONNECTION `[dataset].[location].[dataset]`
OPTIONS (REMOTE_SERVICE_TYPE = 'CLOUD_AI_NATURAL_LANGUAGE_V1');

-- Run the data through your model to extract sentiment

CREATE OR REPLACE TABLE `[project].[dataset].[table]` AS
SELECT *, float64(ml_understand_text_result.document_sentiment.score) as sentiment_score, float64(ml_understand_text_result.document_sentiment.magnitude) as magnitude_score, 
FROM ML.UNDERSTAND_TEXT(
  MODEL `[project].[dataset].[model_name_a]`,
  (SELECT review AS text_content, movie_url, label, reviewer_rating from `[dataset].[table]`)
  STRUCT('analyze_sentiment' AS nlu_option));

-- see how many NULLs

SELECT sentiment_score, COUNT(*)
FROM `[project].[dataset].[table]`
GROUP BY 1 ORDER BY 1;
</code></pre>
","sql, google-cloud-platform, google-bigquery, nlp, google-natural-language",
How to convert Doccano exported JSONL format to spaCy format？,"<p>I want to use my own data set to train a named entity recognition model. The data set is exported by the annotation tool Doccano. The format is JSONL (not JSON), but spaCy does not support such a data format input model. How do I convert it? ?
Here's what my dataset looks like:</p>
<pre><code>{&quot;id&quot;:17,&quot;text&quot;:&quot;In this work, the effect of CFs with zeolite on the mechanical, tribological prop-erties, and structure of PTFE was investigated. \nThe developed materials with a CF content of 1–5 wt.% retained their deformation and strength properties at the level of the initial polymer. \nThe compressive stress of PCM increased by 7–53%, and the yield point by 30% relative to the initial polymer. \nIt was found that with an increase in the content of fillers, the degree of crystallinity increased, and the density decreased in comparison with unfilled PTFE. \nCombining fillers (CF\/Zt) into PTFE reduced the wear rate by 810 times relative to the initial polymer. Tribochemical reactions were shown by IR spectroscopy.\nSEM established the formation of secondary structures in the form of tribofilms on the friction surface, which, together with CFs, protect the surface layer of the material from destruction during friction. \nThe wear resistance of the composite material PTFE\/CF\/Zt was effectively improved, and the coefficient of friction was low compared to PTFE\/CF\/Kl and PTFE\/CF\/Vl.&quot;,&quot;entities&quot;:[{&quot;id&quot;:298,&quot;label&quot;:&quot;composite&quot;,&quot;start_offset&quot;:1049,&quot;end_offset&quot;:1059},{&quot;id&quot;:545,&quot;label&quot;:&quot;composite&quot;,&quot;start_offset&quot;:960,&quot;end_offset&quot;:971},{&quot;id&quot;:299,&quot;label&quot;:&quot;composite&quot;,&quot;start_offset&quot;:1064,&quot;end_offset&quot;:1074},{&quot;id&quot;:607,&quot;label&quot;:&quot;value&quot;,&quot;start_offset&quot;:176,&quot;end_offset&quot;:184}],&quot;relations&quot;:[],&quot;Comments&quot;:[]}
</code></pre>
<p>I have also tried many online methods, but none of them seem to work.</p>
","nlp, spacy, named-entity-recognition, doccano","<p>You can modify the script given at <a href=""https://github.com/explosion/projects/blob/v3/pipelines/ner_demo/scripts/convert.py"" rel=""nofollow noreferrer"">explosion/projects/pipelines/ner_demo/scripts
/convert.py</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import json
import warnings

import spacy
from spacy.tokens import DocBin

def read_jsonl(fpath):
    with open(fpath, &quot;r&quot;) as f:
        for line in f:
            yield json.loads(line)


nlp = spacy.blank(&quot;en&quot;)
doc_bin = DocBin()
docs = []
for data in read_jsonl(&quot;data.jsonl&quot;):
    doc = nlp.make_doc(data[&quot;text&quot;])
    ents = []
    for entity in data[&quot;entities&quot;]:
        start = entity[&quot;start_offset&quot;]
        end = entity[&quot;end_offset&quot;]
        label = entity[&quot;label&quot;]
        span = doc.char_span(
            start_idx=start,
            end_idx=end,
            label=label,
            alignment_mode=&quot;strict&quot;,
        )
        if span is None:
            msg = (
                f&quot;Skipping entity [{start}, {end}, {label}] in the &quot;
                &quot;following text because the character span &quot;
                &quot;'{doc.text[start:end]}' does not align with token &quot;
                &quot;boundaries:\n\n{repr(text)}\n&quot;
            )
            warnings.warn(msg)
        else:
            ents.append(span)
    doc.set_ents(entities=ents)
    doc_bin.add(doc)

doc_bin.to_disk(&quot;train.spacy&quot;)
</code></pre>
<p>The data format you have given looks a bit different from the Doccano format that I'm used to, but the above should work.</p>
"
Error in importing SimpleInputPrompt from llama index,"<h2>Code:
from llama_index.prompts.prompts import SimpleInputPrompt</h2>
<p>ModuleNotFoundError                       Traceback (most recent call last)</p>
<p> in &lt;cell line: 1&gt;()
----&gt; 1 from llama_index.prompts.prompts import SimpleInputPrompt</p>
<p>ModuleNotFoundError: No module named 'llama_index.prompts'</p>
<hr />
<p>NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.</p>
<p>To view examples of installing some common dependencies, click the
&quot;Open Examples&quot; button below.</p>
<p>I have already pip installed llama index on my colab notebook and a simple import of SimpleInputPrompt failed</p>
<p>Has the documentation changed? I dont find it anywhere in docs anymore.</p>
","nlp, llama-index",
Semantic search with NLP and elasticsearch,"<p>I am experimenting with elasticsearch as a search server and my task is to build a ""semantic"" search functionality. From a short text phrase like ""I have a burst pipe"" the system should infer that the user is searching for a plumber and return all plumbers indexed in elasticsearch. </p>

<p>Can that be done directly in a search server like elasticsearch or do I have to use a natural language processing (NLP) tool like e.g. Maui Indexer. What is the exact terminology for my task at hand, text classification? Though the given text is very short as it is a search phrase.</p>
","search, nlp","<p>There may be several approaches with different implementation complexity. </p>

<p>The easiest one is to create <strong>list of topics</strong> (like plumbing), attach <strong>bag of words</strong> (like ""pipe""), identify search request by majority of keywords and search only in specified topic (you can add field <code>topic</code> to your elastic search documents and set it as mandatory with <code>+</code> during search). </p>

<p>Of course, if you have lots of documents, manual creation of topic list and bag of words is very time expensive. You can use <strong>machine learning</strong> to automate some of tasks. Basically, it is enough to have distance measure between words and/or documents to automatically discover topics (e.g. by <strong>data clustering</strong>) and <strong>classify</strong> query to one of these topics. Mix of these techniques may also be a good choice (for example, you can manually create topics and assign initial documents to them, but use classification for query assignment). Take a look at Wikipedia's article on <a href=""http://en.wikipedia.org/wiki/Latent_semantic_analysis""><strong>latent semantic analysis</strong></a> to better understand the idea. Also pay attention to the 2 linked articles on <a href=""http://en.wikipedia.org/wiki/Data_clustering"">data clustering</a> and <a href=""http://en.wikipedia.org/wiki/Document_classification"">document classification</a>. And yes, <a href=""http://code.google.com/p/maui-indexer/"">Maui Indexer</a> may become good helper tool this way. </p>

<p>Finally, you can try to build an engine that ""understands"" meaning of the phrase (not just uses terms frequency) and searches appropriate topics. Most probably, this will involve <strong>natural language processing</strong> and <strong>ontology-based knowledgebases</strong>. But in fact, this field is still in active research and without previous experience it will be very hard for you to implement something like this. </p>
"
How to workaround an installation issue on installing the fastText library on Windows?,"<p>I am new to this field and experimenting different models in NLP domain. While am trying to install the <a href=""https://fasttext.cc/"" rel=""nofollow noreferrer"">fastText</a> libary using the command prompt, it is showing an error:</p>
<pre><code>pip install wheel
pip install pybind11
pip install fasttext
</code></pre>
<p>The error is:</p>
<pre class=""lang-none prettyprint-override""><code>Collecting fasttext
  Using cached fasttext-0.9.2.tar.gz (68 kB)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: pybind11&gt;=2.2 in c:\users\manid\appdata\local\programs\python\python312\lib\site-packages (from fasttext) (2.11.1)
Requirement already satisfied: setuptools&gt;=0.7.0 in c:\users\manid\appdata\local\programs\python\python312\lib\site-packages (from fasttext) (69.2.0)
Requirement already satisfied: numpy in c:\users\manid\appdata\local\programs\python\python312\lib\site-packages (from fasttext) (1.26.4)
Building wheels for collected packages: fasttext
  Building wheel for fasttext (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─&gt; [75 lines of output]
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\setuptools\dist.py:476: SetuptoolsDeprecationWarning: Invalid dash-separated options
      !!

              ********************************************************************************
              Usage of dash-separated 'description-file' will not be supported in future
              versions. Please use the underscore name 'description_file' instead.

              By 2024-Sep-26, you need to update your project and remove deprecated calls
              or your builds will no longer be supported.

              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.
              ********************************************************************************

      !!
        opt = self.warn_dash_deprecation(opt, section)
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build\lib.win-amd64-cpython-312
      creating build\lib.win-amd64-cpython-312\fasttext
      copying python\fasttext_module\fasttext\FastText.py -&gt; build\lib.win-amd64-cpython-312\fasttext
      copying python\fasttext_module\fasttext\__init__.py -&gt; build\lib.win-amd64-cpython-312\fasttext
      creating build\lib.win-amd64-cpython-312\fasttext\util
      copying python\fasttext_module\fasttext\util\util.py -&gt; build\lib.win-amd64-cpython-312\fasttext\util
      copying python\fasttext_module\fasttext\util\__init__.py -&gt; build\lib.win-amd64-cpython-312\fasttext\util
      creating build\lib.win-amd64-cpython-312\fasttext\tests
      copying python\fasttext_module\fasttext\tests\test_configurations.py -&gt; build\lib.win-amd64-cpython-312\fasttext\tests
      copying python\fasttext_module\fasttext\tests\test_script.py -&gt; build\lib.win-amd64-cpython-312\fasttext\tests
      copying python\fasttext_module\fasttext\tests\__init__.py -&gt; build\lib.win-amd64-cpython-312\fasttext\tests
      running build_ext
      building 'fasttext_pybind' extension
      creating build\temp.win-amd64-cpython-312
      creating build\temp.win-amd64-cpython-312\Release
      creating build\temp.win-amd64-cpython-312\Release\python
      creating build\temp.win-amd64-cpython-312\Release\python\fasttext_module
      creating build\temp.win-amd64-cpython-312\Release\python\fasttext_module\fasttext
      creating build\temp.win-amd64-cpython-312\Release\python\fasttext_module\fasttext\pybind
      creating build\temp.win-amd64-cpython-312\Release\src
      &quot;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.39.33519\bin\HostX86\x64\cl.exe&quot; /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include -IC:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include -Isrc -IC:\Users\manid\AppData\Local\Programs\Python\Python312\include -IC:\Users\manid\AppData\Local\Programs\Python\Python312\Include &quot;-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.39.33519\include&quot; &quot;-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\include&quot; &quot;-IC:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt&quot; &quot;-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um&quot; &quot;-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared&quot; &quot;-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt&quot; &quot;-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt&quot; &quot;-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um&quot; /EHsc /Tppython/fasttext_module/fasttext/pybind/fasttext_pybind.cc /Fobuild\temp.win-amd64-cpython-312\Release\python/fasttext_module/fasttext/pybind/fasttext_pybind.obj /EHsc /DVERSION_INFO=\\\&quot;0.9.2\\\&quot;
      fasttext_pybind.cc
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(171): error C2065: 'ssize_t': undeclared identifier
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(171): error C2672: 'pybind11::init': no matching overloaded function found
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1932): note: could be 'Ret pybind11::init(CFunc &amp;&amp;,AFunc &amp;&amp;)'
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(171): note: 'pybind11::init': invalid template argument for 'CFunc', type expected
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1924): note: or       'Ret pybind11::init(Func &amp;&amp;)'
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(171): note: 'pybind11::init': invalid template argument for 'Func', type expected
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1912): note: or       'pybind11::detail::initimpl::constructor&lt;Args...&gt; pybind11::init(void)'
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(171): note: 'pybind11::init': invalid template argument for 'Args', type expected
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(171): error C2672: 'pybind11::class_&lt;fasttext::Vector&gt;::def': no matching overloaded function found
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1631): note: could be 'pybind11::class_&lt;fasttext::Vector&gt; &amp;pybind11::class_&lt;fasttext::Vector&gt;::def(pybind11::detail::initimpl::pickle_factory&lt;Args...&gt; &amp;&amp;,const Extra &amp;...)'
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1625): note: or       'pybind11::class_&lt;fasttext::Vector&gt; &amp;pybind11::class_&lt;fasttext::Vector&gt;::def(pybind11::detail::initimpl::factory&lt;Args...&gt; &amp;&amp;,const Extra &amp;...)'
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1618): note: or       'pybind11::class_&lt;fasttext::Vector&gt; &amp;pybind11::class_&lt;fasttext::Vector&gt;::def(const pybind11::detail::initimpl::alias_constructor&lt;Args...&gt; &amp;,const Extra &amp;...)'
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1611): note: or       'pybind11::class_&lt;fasttext::Vector&gt; &amp;pybind11::class_&lt;fasttext::Vector&gt;::def(const pybind11::detail::initimpl::constructor&lt;Args...&gt; &amp;,const Extra &amp;...)'
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1599): note: or       'pybind11::class_&lt;fasttext::Vector&gt; &amp;pybind11::class_&lt;fasttext::Vector&gt;::def(const T &amp;,const Extra &amp;...)'
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1574): note: or       'pybind11::class_&lt;fasttext::Vector&gt; &amp;pybind11::class_&lt;fasttext::Vector&gt;::def(const char *,Func &amp;&amp;,const Extra &amp;...)'
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(171): note: 'pybind11::class_&lt;fasttext::Vector&gt; &amp;pybind11::class_&lt;fasttext::Vector&gt;::def(const char *,Func &amp;&amp;,const Extra &amp;...)': expects 3 arguments - 1 provided
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(185): error C2065: 'ssize_t': undeclared identifier
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(185): error C2065: 'ssize_t': undeclared identifier
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(185): error C2672: 'pybind11::init': no matching overloaded function found
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1932): note: could be 'Ret pybind11::init(CFunc &amp;&amp;,AFunc &amp;&amp;)'
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(185): note: 'pybind11::init': invalid template argument for 'CFunc', type expected
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1924): note: or       'Ret pybind11::init(Func &amp;&amp;)'
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(185): note: 'pybind11::init': invalid template argument for 'Func', type expected
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1912): note: or       'pybind11::detail::initimpl::constructor&lt;Args...&gt; pybind11::init(void)'
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(185): note: 'pybind11::init': invalid template argument for 'Args', type expected
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(185): error C2672: 'pybind11::class_&lt;fasttext::DenseMatrix&gt;::def': no matching overloaded function found
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1631): note: could be 'pybind11::class_&lt;fasttext::DenseMatrix&gt; &amp;pybind11::class_&lt;fasttext::DenseMatrix&gt;::def(pybind11::detail::initimpl::pickle_factory&lt;Args...&gt; &amp;&amp;,const Extra &amp;...)'
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1625): note: or       'pybind11::class_&lt;fasttext::DenseMatrix&gt; &amp;pybind11::class_&lt;fasttext::DenseMatrix&gt;::def(pybind11::detail::initimpl::factory&lt;Args...&gt; &amp;&amp;,const Extra &amp;...)'
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1618): note: or       'pybind11::class_&lt;fasttext::DenseMatrix&gt; &amp;pybind11::class_&lt;fasttext::DenseMatrix&gt;::def(const pybind11::detail::initimpl::alias_constructor&lt;Args...&gt; &amp;,const Extra &amp;...)'
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1611): note: or       'pybind11::class_&lt;fasttext::DenseMatrix&gt; &amp;pybind11::class_&lt;fasttext::DenseMatrix&gt;::def(const pybind11::detail::initimpl::constructor&lt;Args...&gt; &amp;,const Extra &amp;...)'
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1599): note: or       'pybind11::class_&lt;fasttext::DenseMatrix&gt; &amp;pybind11::class_&lt;fasttext::DenseMatrix&gt;::def(const T &amp;,const Extra &amp;...)'
      C:\Users\manid\AppData\Local\Programs\Python\Python312\Lib\site-packages\pybind11\include\pybind11\pybind11.h(1574): note: or       'pybind11::class_&lt;fasttext::DenseMatrix&gt; &amp;pybind11::class_&lt;fasttext::DenseMatrix&gt;::def(const char *,Func &amp;&amp;,const Extra &amp;...)'
      python/fasttext_module/fasttext/pybind/fasttext_pybind.cc(185): note: 'pybind11::class_&lt;fasttext::DenseMatrix&gt; &amp;pybind11::class_&lt;fasttext::DenseMatrix&gt;::def(const char *,Func &amp;&amp;,const Extra &amp;...)': expects 3 arguments - 1 provided
      error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\bin\\HostX86\\x64\\cl.exe' failed with exit code 2
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for fasttext
  Running setup.py clean for fasttext
Failed to build fasttext
ERROR: Could not build wheels for fasttext, which is required to install pyproject.toml-based projects
</code></pre>
","python, installation, cmd, nlp, fasttext",
Out of RAM while tokenize a 12GB xml with SpaCy,"<p>I'm trying to tokenize a 12GB of text in xml. The file contains only &quot;content words&quot; with no stopwords. I am trying to implement a function in order to tokenize by chunks of text and clean the RAM. (I have a core i7 and 32GB of RAM)</p>
<p>I have tried the following without success;</p>
<pre class=""lang-py prettyprint-override""><code>from spacy.util import minibatch
import spacy

nlp = spacy.load(&quot;es_core_news_sm&quot;)
nlp.disable_pipes('tok2vec', 'morphologizer', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner')

def tokenizer(corpus_file):
    all_tokens = []

    with open(corpus_file, 'r', encoding='utf-8') as file:
        for batch in minibatch(file, size=100):
            docs = nlp.pipe(batch)
            for doc in docs:
                tokens = [token.text for token in doc if not token.is_space]
                all_tokens.extend(tokens)
    return all_tokens
</code></pre>
","nlp, spacy, tokenize",
Extracting dates from a sentence in spaCy,"<p>I have a string like so:</p>
<pre><code>&quot;The dates are from 30 June 2019 to 1 January 2022 inclusive&quot;
</code></pre>
<p>I want to extract the dates from this string using spaCy.</p>
<p>Here is my function so far:</p>
<pre><code>def extract_dates_with_year(text):
    doc = nlp(text)
    dates_with_year = []
    for ent in doc.ents:
        if ent.label_ == &quot;DATE&quot;:
            dates_with_year.append(ent.text)
    return dates_with_year
</code></pre>
<p>This returns the following output:</p>
<pre><code>['30 June 2019 to 1 January 2022']
</code></pre>
<p>However, I want output like:</p>
<pre><code>['30 June 2019', '1 January 2022']
</code></pre>
","python, regex, nlp, spacy, named-entity-recognition","<p>The issue is that <code>&quot;to&quot;</code> is considered part of the date. So when you do <code>for ent in doc.ents</code>, your loop only has one iteration, as <code>&quot;30 June 2019 to 1 January 2022&quot;</code> is considered one entity.</p>
<p>As you don't want this behaviour, you can amend your function to split on <code>&quot;to&quot;</code>:</p>
<pre class=""lang-py prettyprint-override""><code>def extract_dates_with_year(text):
    doc = nlp(text)
    dates_with_year = []
    for ent in doc.ents:
        if ent.label_ == &quot;DATE&quot;:
            for ent_txt in ent.text.split(&quot;to&quot;):
                dates_with_year.append(ent_txt.strip())
    return dates_with_year
</code></pre>
<p>This will correctly handle dates like these, as well as single dates, and strings with multiple dates:</p>
<pre class=""lang-py prettyprint-override""><code>txt = &quot;&quot;&quot;
     The dates are from 30 June 2019 to 1 January 2022 inclusive.
     And oddly also 5 January 2024.
     And exclude 21 July 2019 until 23 July 2019.
&quot;&quot;&quot;

extract_dates_with_year(txt)

# Output:
[
 '30 June 2019',
 '1 January 2022',
 '5 January 2024',
 '21 July 2019',
 '23 July 2019'
]
</code></pre>
"
GloVe embedding for empty string,"<p>It looks like the embedding for the empty string in the <code>glove.twitter.27B.200d.txt</code> file that's part of this zip file:</p>
<p><a href=""https://nlp.stanford.edu/data/glove.twitter.27B.zip"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/data/glove.twitter.27B.zip</a></p>
<p>is provided on line 38523, but I'm not 100% sure this is what I think it is. I haven't worked with these embeddings much and was wondering if someone could verify that this the case (or not)?</p>
","nlp, stanford-nlp, word-embedding",
Clarification on T5 Model Pre-training Objective and Denoising Process,"<p>I am currently developing a T5 model (encoder-decoder architecture) from scratch for educational purposes. While working on this project, I've encountered some confusion regarding the pre-training objective, specifically the <em>denoising objective</em>. I would like to clarify my understanding and have some questions about the process.</p>
<p>Given the sentence:</p>
<blockquote>
<p>Thank you for inviting me to your party last week.</p>
</blockquote>
<p>Based on my understanding, during the pre-training phase with a denoising objective, the model works as follows:</p>
<ul>
<li><strong>Encoder input</strong>: <code>Thank you &lt;X&gt; me to your party &lt;Y&gt; week</code></li>
<li><strong>Decoder input</strong>: <code>&lt;X&gt; for inviting &lt;Y&gt; last</code></li>
<li><strong>Decoder labels (true labels)</strong>: <code>for inviting &lt;Y&gt; last &lt;Z&gt;</code></li>
</ul>
<p>Here are my questions:</p>
<ol>
<li>Is my interpretation of how the encoder input, decoder input, and decoder labels are constructed correct?</li>
<li>In this setup, the model is expected to predict sentinel tokens (e.g., <code>&lt;X&gt;</code>, <code>&lt;Y&gt;</code>). Could this potentially introduce confusion for the model, for example, it may take the idea that it is possible for the word &quot;last&quot; to come after the token ? Or does the model naturally learn to interpret these situations correctly?</li>
</ol>
<hr />
<p><strong>Accordingly to the paper:</strong></p>
<p><a href=""https://i.sstatic.net/QYUco.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QYUco.png"" alt=""denoising objective"" /></a></p>
<blockquote>
<p>we process the sentence <code>Thank you for inviting me to your party last week.</code> The words <code>for</code>, <code>inviting</code> and <code>last</code> are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as <code>&lt;X&gt;</code> and <code>&lt;Y&gt;</code>) that is unique over the example. Since <code>for</code> and <code>inviting</code> occur consecutively, they are replaced by a single sentinel <code>&lt;X&gt;</code>. The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token <code>&lt;Z&gt;</code>.</p>
</blockquote>
","nlp, large-language-model",
Retrieval Augmented generation vs. LLM context,"<p>I am still learning the concepts behind RAG but I was wondering,
alot if references explain RAG by saying that you will be able to increase the LLM knowledge by augmenting a new knowledge using an external retrieving system.</p>
<p>But how is that different than just add a context (even if its long as some LLMs has a huge window size) to the model prompt?</p>
","nlp, information-retrieval, data-augmentation",
Need Guidance on Implementing Stratified K-Fold for Sequence Labeling (NER) Dataset,"<p>I'm currently working on a Named Entity Recognition (NER) task and I'm looking to implement Stratified K-Fold cross-validation for my dataset. However, I'm struggling to find specific references or guidelines on how to perform this technique for sequence labeling tasks like NER.</p>
<p>In fact, I already trying to perform this but the process always resulting an error.</p>
<p>Here are the key points I'm interested in:</p>
<ol>
<li><p>Stratified K-Fold for Sequence Labeling: I want to understand how to adapt the standard Stratified K-Fold cross-validation technique for sequence labeling tasks. In NER, each token in a sequence is labeled with a specific entity type (e.g., person, organization, location). How can I ensure that each fold in the cross-validation maintains the same distribution of entity types as the original dataset?</p>
</li>
<li><p>Handling Sequential Dependence: Since sequence labeling tasks have a sequential dependency between tokens, how should I ensure that this dependency is preserved during the cross-validation process? Should I shuffle the entire sequences or just the samples?</p>
</li>
<li><p>Existing References or Implementations: I've searched for resources or existing implementations that demonstrate how to apply Stratified K-Fold specifically to sequence labeling datasets, but I haven't found much relevant information. Are there any papers, articles, or code examples that address this topic directly?</p>
</li>
</ol>
<p>Any guidance, references, or insights on how to implement Stratified K-Fold cross-validation for sequence labeling datasets like NER would be greatly appreciated. Thank you!</p>
","nlp, named-entity-recognition, k-fold",
How do I make sure answers are from a customized (fine-tuning) dataset?,"<p>I'm using customized text with 'Prompt' and 'Completion' to train new model.</p>
<p>Here's the tutorial I used to create customized model from my data:</p>
<p><a href=""https://beta.openai.com/docs/guides/fine-tuning/advanced-usage"" rel=""nofollow noreferrer"">beta.openai.com/docs/guides/fine-tuning/advanced-usage</a></p>
<p>However even after training the model and sending prompt text to the model, I'm still getting generic results which are not always suitable for me.</p>
<p>How I can make sure completion results for my prompts will be only from the text I used for the model and not from the generic OpenAI models?</p>
<p>Can I use some flags to eliminate results from generic models?</p>
","nlp, customization, openai-api, gpt-3",
Textual similarity between two tags in Nodejs,"<p>I want to rate the similarity between two tags. For example the words <em>technology</em>, <em>computer</em> and <em>chip</em> should have <strong>high similarity</strong>, a word like <em>food</em> should be low similarity.</p>
<p>Given the recent advancements in AI and more traditional approaches from NLP, how would you solve that problem nowadays in Node JS?</p>
<p>I tried to get the <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Google News dataset</a> from 2013 to run with <a href=""https://www.npmjs.com/package/word2vec"" rel=""nofollow noreferrer"">word2vec</a>, but neither this or other of the word2vec libraries seem to be running with modern versions of node (or at least i didnt manage to make any work yet with the dataset). There are also repos like <a href=""https://github.com/loretoparisi/fasttext.js"" rel=""nofollow noreferrer"">fast-text</a> or <a href=""https://github.com/AndriyMulyar/semantic-text-similarity"" rel=""nofollow noreferrer"">Text Similarity Test (using TensorFlow.js)</a>.</p>
<p>Right now I'm wondering both which approach would be technically the best to fullfil my problem (also approaches that I didn't consider yet) <em>and</em> how to technically implement it in code.</p>
","javascript, node.js, typescript, nlp, similarity",
Dynamic Filtering of Non-Descriptive Adjectives and Nouns in Text Data?,"<p>How can I dynamically filter out non-descriptive adjectives and nouns from text data without relying on predefined lists?</p>
<p>I'm using spaCy for part-of-speech tagging and Named Entity Recognition, but I want a more dynamic approach. Any suggestions? Or should I try a whole different method?</p>
<pre><code>import spacy
from collections import Counter

nlp = spacy.load(&quot;en_core_web_sm&quot;)

text = &quot;&quot;
doc = nlp(text)

def is_descriptive(adjective, noun):
    common_nouns = [list of common nouns that are not usefull]
    if noun.lower() in common_nouns:
        return False
    generic_adjectives = [list of generic adjectives that are not useful]
    if adjective.lower() in generic_adjectives:
        return False
    return True

phrases = []
for i in range(len(doc) - 1):
    if doc[i].pos_ == &quot;ADJ&quot; and doc[i + 1].pos_ == &quot;NOUN&quot; and is_descriptive(doc[i].text, doc[i + 1].text):
        phrases.append(f&quot;{doc[i].text} {doc[i + 1].text}&quot;)

word_freq = Counter(phrases)

most_used_phrases = word_freq.most_common(10)
print(&quot;Adjective and Phrase Frequency&quot;)
for phrase, freq in most_used_phrases:
    print(f&quot;{phrase} {freq}&quot;)

</code></pre>
","python, nlp, spacy",
Why sentencepiece tokenizer in nllb returns id&#39;s that differ by 1 when I specify model name or file?,"<p>When I'm using hf tokenizers and specify the model by name <code>facebook/nllb-200-distilled-600M</code> I get such tokens: <code>256047, 30311, 104, 253990</code> and so on. But when I take the <code>sentencepiece.bpe.model</code> from the repo and load it as a file I get this: <code>30310, 103, 253989</code>. I understand that the first token is the language that this model requires for knowing the translation, but why the next tokens differ by 1? The input string is &quot;English (<code>eng_Latn</code>) is set as the default language from which to translate.&quot; And this difference is consistent on the whole string.</p>
","nlp, huggingface-transformers, sentencepiece",
Is there any OCR or technique that can recognize/identify radio buttons printed out in the form of pdf document?,"<p>I have a pdf document with radio responses like attached screenshot. I want to extract the selected response only through python or any OCR technique. Is there  any way of doing it?
(<a href=""https://i.sstatic.net/3fXu6.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/3fXu6.png</a>)</p>
<p>I have tried pdfplumber,pdfminer,pytesseract but they are not able to extract the response only.</p>
","python, nlp, ocr, large-language-model, information-extraction",
How to get up and running with spaCy for Vietnamese?,"<p>I success with English</p>
<pre><code>python -m spacy download en_core_web_lg
python -m spacy download en_core_web_sm

python -m spacy download en
</code></pre>
<p>I read <a href=""https://spacy.io/models/xx"" rel=""nofollow noreferrer"">https://spacy.io/models/xx</a> . How to use with Vietnamese use <code>xx</code>?</p>
","python, nlp, spacy","<p>Call Python terminal from Anaconda Navigator.</p>
<pre><code>python -m spacy download xx_sent_ud_sm
</code></pre>
<p>or</p>
<pre><code>python -m spacy download xx_ent_wiki_sm
</code></pre>
<p>test</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy import displacy

nlp = spacy.load(&quot;xx_sent_ud_sm&quot;)
doc = nlp(&quot;Hôm nay trời nắng to&quot;)
displacy.serve(doc, style=&quot;dep&quot;)
</code></pre>
<p>or</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy import displacy

nlp = spacy.load(&quot;xx_ent_wiki_sm&quot;)
doc = nlp(&quot;Hôm nay trời nắng to&quot;)
displacy.serve(doc, style=&quot;ent&quot;)
</code></pre>
<p>result
[1]: <a href=""https://i.sstatic.net/OjTqJ.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/OjTqJ.png</a></p>
"
How to Predict New Data Topics with BERTopic Model Loaded from Hugging Face in Python? (Automated solution),"<p>I've developed a BERTopic model for analyzing mobile app reviews and have successfully pushed it to Hugging Face. I'm able to load the model in my Python script but am facing challenges in predicting topics for new incoming feedback.</p>
<p>My goal is to automatically assign a topic to new app feedback using the trained BERTopic model. Here's the workflow I've implemented:</p>
<p>I load the BERTopic model from Hugging Face.
I use the SentenceTransformer model to encode the new documents (app feedback).
I attempt to transform the new documents using the loaded BERTopic model to predict their topics.</p>
<p>However, I'm uncertain about the last part of my code, where I aim to merge the predicted topics with the original dataset containing app feedback. I want to end up with a dataset that includes the original feedback and the assigned topic names, ideally with automatic incrementation for new data.</p>
<p>Here's the code snippet I'm using:</p>
<pre><code>from huggingface_hub import login
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import pandas as pd

# Login to Hugging Face
access_token_read = &quot;hf_xynbKhivF..........&quot;
login(token=access_token_read)

# Load the BERTopic model
model = BERTopic.load(&quot;shantanudave/BERTopic_ArXiv&quot;)

# Sample new document

new_docs = [&quot;There is an issue with payment on the checkout.&quot;]

# Generate embeddings for the new document
embedding_model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)
embeddings = embedding_model.encode(new_docs, show_progress_bar=True)

# Predict the topic for the new document
new_topics, new_probs = model.transform(new_docs, embeddings)

# Code for merging predicted topics with the original dataset (uncertain part)
...

</code></pre>
<p>Questions:</p>
<p>Is the approach I'm using to predict and assign topics to new feedback using BERTopic correct?
How can I effectively merge the predicted topics back into the original dataset, ensuring that each piece of feedback gets its corresponding topic and topic name?</p>
<pre><code>#My manual approach : - 

T = topic_model.get_document_info(docs)
T_df = T[['Document', 'Topic', 'Name', 'CustomName']]
# Remove duplicates from this new DataFrame
T_clean = T_df.drop_duplicates(subset=['Document'], keep='first')
T_clean

# Performing a left join where 'Document' in T_clean matches 'eng_content' in df , 'eng_content' is the review/feedback

merged_df = pd.merge(df, T_clean, how='left', left_on='eng_content', right_on='Document')
topic_doc_df_1 = merged_df
# Rename columns
topic_doc_df_1 = topic_doc_df_1.rename(columns={
    'CustomName': 'Topic_name',
    'score': 'Rating',
    'date': 'Date',
    'language': 'Language',
    'sentiment': 'Sentiment',
    'probability': 'Probability',
    'app_version': 'App_version'
})

# Rearrange columns
topic_doc_df_1 = topic_doc_df_1[['Topic', 'Topic_name', 'Document', 'Rating', 'Date', 'Language', 'Sentiment', 'Probability', 'App_version']]

topic_doc_df_1

![image](https://github.com/MaartenGr/BERTopic/assets/54185486/d1a96b23-7766-4b34-b615-4a9ce8e804f9)


</code></pre>
<p>I have another function clean_doc() which basically reads new data from df['eng_content'] and provide list 'new_docs'</p>
<p>I am sure there must be a better way of doing this that I am missing,</p>
<p>Thanks in advance,
Shantanu</p>
","python, nlp, huggingface",
How to create an Embedding Model for Recipe Dataset Using Deep Metric Learning?,"<p>I want to create an <strong>embedding model</strong> for my own dataset of recipes. The goal is to create a neural network that takes each recipe, represented by a list of its ingredients, a list of recipe tags, and its title, as input. The network should use <strong>deep metric learning</strong> to create an embedding model that represents recipes in a way that distinguishes them well.</p>
<p>I have considered using a triplet network with the <strong>triplet loss function</strong>, trained on a dataset of triplets. Each triplet would consist of an anchor (the recipe itself), a positive sample (a similar recipe), and a negative sample (a very different recipe). The objective would be to minimize the distances between the anchor and positive samples while maximizing the distances between the anchor and negative samples.</p>
<p>Since I'm relatively new to neural networks and natural language processing, I would greatly appreciate your thoughts and insights on this approach. Do you think this method is suitable for my task? Are there any potential pitfalls or challenges that I should be aware of?
Additionally, I would be grateful if you could provide me with a guideline on how I can successfully execute this project.</p>
<p>Thank you.</p>
","deep-learning, nlp, neural-network, embedding",
Huggingface pretrained model&#39;s tokenizer and model objects have different maximum input length,"<p>I'm using <em><strong>symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli</strong></em> pretrained model from huggingface. My task requires to use it on pretty large texts, so it's essential to know maximum input length.</p>
<p>The following code is supposed to load pretrained model and its tokenizer:</p>
<pre><code>encoding_model_name = &quot;symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli&quot;
encoding_tokenizer = AutoTokenizer.from_pretrained(encoding_model_name)
encoding_model = SentenceTransformer(encoding_model_name)
</code></pre>
<p>So, when I print info about them:</p>
<pre><code>encoding_tokenizer
encoding_model
</code></pre>
<p>I'm getting:</p>
<pre><code>PreTrainedTokenizerFast(name_or_path='symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli', vocab_size=250002, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'sep_token': '&lt;/s&gt;', 'pad_token': '&lt;pad&gt;', 'cls_token': '&lt;s&gt;', 'mask_token': AddedToken(&quot;&lt;mask&gt;&quot;, rstrip=False, lstrip=True, single_word=False, normalized=False)})
</code></pre>
<pre><code>SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)
</code></pre>
<p>As you can see, <strong>model_max_len=512</strong> parameter in tokenizer doesn't match <strong>max_seq_length=128</strong> parameter in model</p>
<p>How can I figure out which one is true? Or, probably, if they somehow respond to different features, how I can check maximum input length for my model?</p>
","nlp, huggingface-transformers, huggingface-tokenizers, sentence-transformers","<p>Since you are using a SentenceTransformer and load it to the SentenceTransformer class, it will truncate your input at 128 tokens as stated by the <a href=""https://www.sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.max_seq_length"" rel=""nofollow noreferrer"">documentation</a> (the relevant code is <a href=""https://github.com/UKPLab/sentence-transformers/blob/b90970c190658f7588c86e8a3fb9cc400716ecd4/sentence_transformers/models/Transformer.py#L113"" rel=""nofollow noreferrer"">here</a>):</p>
<blockquote>
<p>property max_seq_length<br />
Property to get the maximal input sequence length for the model. Longer inputs will be truncated.</p>
</blockquote>
<p>You can also check this by yourself:</p>
<pre class=""lang-py prettyprint-override""><code>fifty = model.encode([&quot;This &quot;*50], convert_to_tensor=True)
two_hundered = model.encode([&quot;This &quot;*200], convert_to_tensor=True)
four_hundered = model.encode([&quot;This &quot;*400], convert_to_tensor=True)

print(torch.allclose(fifty, two_hundered))
print(torch.allclose(two_hundered,four_hundered))
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>False
True
</code></pre>
<p>The underlying model (xlm-roberta-base) is able to handle sequences with up to 512 tokens, but I assume <a href=""https://huggingface.co/symanto"" rel=""nofollow noreferrer"">Symanto</a> limited it to 128 because they also used this limit during training (i.e. the embeddings might be not good for sequences longer than 128 tokens).</p>
"
How can you use an already created chromadb collection with a LLM using openai and langchain?,"<p>I already have a chromadb collection created with its documents and metadata.</p>
<p>The problem is when I want to use langchain to create a llm and pass this chromadb collection to use as a knowledge base.</p>
<pre><code>langchain_chroma = Chroma(
client=persistent_client,
collection_name=collection.name,
embedding_function=openai_ef,
)

llm_model = &quot;gtp35turbo-latest&quot;

llm = AzureChatOpenAI(
   api_key=openai_api_key,
   api_version=openai_api_version,
   azure_endpoint=openai_api_base,
   model=llm_model)

qa_chain = RetrievalQA.from_chain_type(
   llm,
   retriever=langchain_chroma.as_retriever(),
   chain_type=&quot;refine&quot;
)
</code></pre>
<p>When I want to run:</p>
<pre><code>qa_chain.run(&quot;How many datascientist do I need for a Object detection problem&quot;)
</code></pre>
<p>I got this error:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-81-3cdb65aeb43e&gt; in &lt;cell line: 1&gt;()
----&gt; 1 qa.run(&quot;How many datascientist do I need for a Object detection problem&quot;)

9 frames
/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py in similarity_search_with_score(self, query, k, filter, where_document, **kwargs)
    430             )
    431         else:
--&gt; 432             query_embedding = self._embedding_function.embed_query(query)
    433             results = self.__query_collection(
    434                 query_embeddings=[query_embedding],

AttributeError: 'OpenAIEmbeddingFunction' object has no attribute 'embed_query'
</code></pre>
<p>How to solve it?</p>
","python, machine-learning, nlp, openai-api, langchain",
Parsing city of origin / destination city from a string,"<p>I have a pandas dataframe where one column is a bunch of strings with certain travel details. My goal is to parse each string to extract the city of origin and destination city (I would like to ultimately have two new columns titled 'origin' and 'destination').</p>

<p>The data:</p>

<pre><code>df_col = [
    'new york to venice, italy for usd271',
    'return flights from brussels to bangkok with etihad from â‚¬407',
    'from los angeles to guadalajara, mexico for usd191',
    'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags'
]
</code></pre>

<p>This should result in:</p>

<pre><code>Origin: New York, USA; Destination: Venice, Italy
Origin: Brussels, BEL; Destination: Bangkok, Thailand
Origin: Los Angeles, USA; Destination: Guadalajara, Mexico
Origin: Paris, France; Destination: Australia / New Zealand (this is a complicated case given two countries)
</code></pre>

<p>Thus far I have tried:
A variety of NLTK methods, but what has gotten me closest is using the <code>nltk.pos_tag</code> method to tag each word in the string. The result is a list of tuples with each word and associated tag. Here's an example...</p>

<pre><code>[('Fly', 'NNP'), ('to', 'TO'), ('Australia', 'NNP'), ('&amp;', 'CC'), ('New', 'NNP'), ('Zealand', 'NNP'), ('from', 'IN'), ('Paris', 'NNP'), ('from', 'IN'), ('â‚¬422', 'NNP'), ('return', 'NN'), ('including', 'VBG'), ('2', 'CD'), ('checked', 'VBD'), ('bags', 'NNS'), ('!', '.')]
</code></pre>

<p>I am stuck at this stage and am unsure how to best implement this. Can anyone point me in the right direction, please? Thanks.</p>
","python, regex, pandas, nlp, nltk","<h1>TL;DR</h1>

<p>Pretty much impossible at first glance, unless you have access to some API that contains pretty sophisticated components. </p>

<h1>In Long</h1>

<p>From first look, it seems like you're asking to solve a natural language problem magically. But lets break it down and scope it to a point where something is buildable. </p>

<p>First, to identify countries and cities, you need data that enumerates them, so lets try: <a href=""https://www.google.com/search?q=list+of+countries+and+cities+in+the+world+json"" rel=""noreferrer"">https://www.google.com/search?q=list+of+countries+and+cities+in+the+world+json</a> </p>

<p>And top of the search results, we find <a href=""https://datahub.io/core/world-cities"" rel=""noreferrer"">https://datahub.io/core/world-cities</a> that leads to the world-cities.json file. Now we load them into sets of countries and cities. </p>

<pre><code>import requests
import json

cities_url = ""https://pkgstore.datahub.io/core/world-cities/world-cities_json/data/5b3dd46ad10990bca47b04b4739a02ba/world-cities_json.json""
cities_json = json.loads(requests.get(cities_url).content.decode('utf8'))

countries = set([city['country'] for city in cities_json])
cities = set([city['name'] for city in cities_json])
</code></pre>

<h2>Now given data, lets try to build <strong>component ONE</strong>:</h2>

<ul>
<li><strong>Task:</strong> Detect if any substring in the texts matches a city/country.</li>
<li><strong>Tool:</strong> <a href=""https://github.com/vi3k6i5/flashtext"" rel=""noreferrer"">https://github.com/vi3k6i5/flashtext</a> (a fast string search/match)</li>
<li><strong>Metric:</strong> No. of correctly identified cities/countries in string</li>
</ul>

<p>Lets put them together.</p>

<pre><code>import requests
import json
from flashtext import KeywordProcessor

cities_url = ""https://pkgstore.datahub.io/core/world-cities/world-cities_json/data/5b3dd46ad10990bca47b04b4739a02ba/world-cities_json.json""
cities_json = json.loads(requests.get(cities_url).content.decode('utf8'))

countries = set([city['country'] for city in cities_json])
cities = set([city['name'] for city in cities_json])


keyword_processor = KeywordProcessor(case_sensitive=False)
keyword_processor.add_keywords_from_list(sorted(countries))
keyword_processor.add_keywords_from_list(sorted(cities))


texts = ['new york to venice, italy for usd271',
'return flights from brussels to bangkok with etihad from â‚¬407',
'from los angeles to guadalajara, mexico for usd191',
'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags']
keyword_processor.extract_keywords(texts[0])
</code></pre>

<p>[out]:</p>

<pre><code>['York', 'Venice', 'Italy']
</code></pre>

<h2>Hey, what went wrong?!</h2>

<p>Doing due diligence, first hunch is that ""new york"" is not in the data, </p>

<pre><code>&gt;&gt;&gt; ""New York"" in cities
False
</code></pre>

<p>What the?! #$%^&amp;* For sanity sake, we check these:</p>

<pre><code>&gt;&gt;&gt; len(countries)
244
&gt;&gt;&gt; len(cities)
21940
</code></pre>

<p>Yes, you cannot just trust a single data source, so lets try to fetch all data sources.</p>

<p>From <a href=""https://www.google.com/search?q=list+of+countries+and+cities+in+the+world+json"" rel=""noreferrer"">https://www.google.com/search?q=list+of+countries+and+cities+in+the+world+json</a>, you find another link <a href=""https://github.com/dr5hn/countries-states-cities-database"" rel=""noreferrer"">https://github.com/dr5hn/countries-states-cities-database</a> Lets munge this...</p>

<pre><code>import requests
import json

cities_url = ""https://pkgstore.datahub.io/core/world-cities/world-cities_json/data/5b3dd46ad10990bca47b04b4739a02ba/world-cities_json.json""
cities1_json = json.loads(requests.get(cities_url).content.decode('utf8'))

countries1 = set([city['country'] for city in cities1_json])
cities1 = set([city['name'] for city in cities1_json])

dr5hn_cities_url = ""https://raw.githubusercontent.com/dr5hn/countries-states-cities-database/master/cities.json""
dr5hn_countries_url = ""https://raw.githubusercontent.com/dr5hn/countries-states-cities-database/master/countries.json""

cities2_json = json.loads(requests.get(dr5hn_cities_url).content.decode('utf8'))
countries2_json = json.loads(requests.get(dr5hn_countries_url).content.decode('utf8'))

countries2 = set([c['name'] for c in countries2_json])
cities2 = set([c['name'] for c in cities2_json])

countries = countries2.union(countries1)
cities = cities2.union(cities1)
</code></pre>

<h2>And now that we are neurotic, we do sanity checks.</h2>

<pre><code>&gt;&gt;&gt; len(countries)
282
&gt;&gt;&gt; len(cities)
127793
</code></pre>

<p>Wow, that's a lot more cities than previously. </p>

<p>Lets try the <code>flashtext</code> code again.</p>

<pre><code>from flashtext import KeywordProcessor

keyword_processor = KeywordProcessor(case_sensitive=False)
keyword_processor.add_keywords_from_list(sorted(countries))
keyword_processor.add_keywords_from_list(sorted(cities))

texts = ['new york to venice, italy for usd271',
'return flights from brussels to bangkok with etihad from â‚¬407',
'from los angeles to guadalajara, mexico for usd191',
'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags']

keyword_processor.extract_keywords(texts[0])
</code></pre>

<p>[out]:</p>

<pre><code>['York', 'Venice', 'Italy']
</code></pre>

<h2>Seriously?! There is no New York?! $%^&amp;*</h2>

<p>Okay, for more sanity checks, lets just look for ""york"" in the list of cities.</p>

<pre><code>&gt;&gt;&gt; [c for c in cities if 'york' in c.lower()]
['Yorklyn',
 'West York',
 'West New York',
 'Yorktown Heights',
 'East Riding of Yorkshire',
 'Yorke Peninsula',
 'Yorke Hill',
 'Yorktown',
 'Jefferson Valley-Yorktown',
 'New York Mills',
 'City of York',
 'Yorkville',
 'Yorkton',
 'New York County',
 'East York',
 'East New York',
 'York Castle',
 'York County',
 'Yorketown',
 'New York City',
 'York Beach',
 'Yorkshire',
 'North Yorkshire',
 'Yorkeys Knob',
 'York',
 'York Town',
 'York Harbor',
 'North York']
</code></pre>

<h2>Eureka! It's because it's call ""New York City"" and not ""New York""!</h2>

<p><strong>You:</strong> What kind of prank is this?! </p>

<p><strong>Linguist:</strong> Welcome to the world of <strong>natural language</strong> processing, where natural language is a social construct subjective to communal and idiolectal variant. </p>

<p><strong>You</strong>: Cut the crap, tell me how to solve this. </p>

<p><strong>NLP Practitioner</strong> (A real one that works on noisy user-generate texts): You just have to add to the list. But before that, check your <em>metric</em> given the list you already have.</p>

<h3>For every texts in your sample ""test set"", you should provide some truth labels to make sure you can ""measure your metric"".</h3>

<pre><code>from itertools import zip_longest
from flashtext import KeywordProcessor

keyword_processor = KeywordProcessor(case_sensitive=False)
keyword_processor.add_keywords_from_list(sorted(countries))
keyword_processor.add_keywords_from_list(sorted(cities))

texts_labels = [('new york to venice, italy for usd271', ('New York', 'Venice', 'Italy')),
('return flights from brussels to bangkok with etihad from â‚¬407', ('Brussels', 'Bangkok')),
('from los angeles to guadalajara, mexico for usd191', ('Los Angeles', 'Guadalajara')),
('fly to australia new zealand from paris from â‚¬422 return including 2 checked bags', ('Australia', 'New Zealand', 'Paris'))]

# No. of correctly extracted terms.
true_positives = 0
false_positives = 0
total_truth = 0

for text, label in texts_labels:
    extracted = keyword_processor.extract_keywords(text)

    # We're making some assumptions here that the order of 
    # extracted and the truth must be the same.
    true_positives += sum(1 for e, l in zip_longest(extracted, label) if e == l)
    false_positives += sum(1 for e, l in zip_longest(extracted, label) if e != l)
    total_truth += len(label)

    # Just visualization candies.
    print(text)
    print(extracted)
    print(label)
    print()
</code></pre>

<p>Actually, it doesn't look that bad. We get an accuracy of 90%:</p>

<pre><code>&gt;&gt;&gt; true_positives / total_truth
0.9
</code></pre>

<h3>But I %^&amp;*(-ing want 100% extraction!!</h3>

<p>Alright, alright, so look at the ""only"" error that the above approach is making, it's simply that ""New York"" isn't in the list of cities. </p>

<p><strong>You</strong>: Why don't we just add ""New York"" to the list of cities, i.e. </p>

<pre><code>keyword_processor.add_keyword('New York')

print(texts[0])
print(keyword_processor.extract_keywords(texts[0]))
</code></pre>

<p>[out]:</p>

<pre><code>['New York', 'Venice', 'Italy']
</code></pre>

<p><strong>You</strong>: See, I did it!!! Now I deserve a beer.
<strong>Linguist</strong>: How about <code>'I live in Marawi'</code>?</p>

<pre><code>&gt;&gt;&gt; keyword_processor.extract_keywords('I live in Marawi')
[]
</code></pre>

<p><strong>NLP Practitioner</strong> (chiming in): How about <code>'I live in Jeju'</code>? </p>

<pre><code>&gt;&gt;&gt; keyword_processor.extract_keywords('I live in Jeju')
[]
</code></pre>

<p><strong>A Raymond Hettinger fan</strong> (from farway): ""There must be a better way!""</p>

<p>Yes, there is what if we just try something silly like adding keywords of cities that ends with ""City"" into our <code>keyword_processor</code>?</p>

<pre><code>for c in cities:
    if 'city' in c.lower() and c.endswith('City') and c[:-5] not in cities:
        if c[:-5].strip():
            keyword_processor.add_keyword(c[:-5])
            print(c[:-5])
</code></pre>

<h1>It works!</h1>

<p>Now lets retry our regression test examples:</p>

<pre><code>from itertools import zip_longest
from flashtext import KeywordProcessor

keyword_processor = KeywordProcessor(case_sensitive=False)
keyword_processor.add_keywords_from_list(sorted(countries))
keyword_processor.add_keywords_from_list(sorted(cities))

for c in cities:
    if 'city' in c.lower() and c.endswith('City') and c[:-5] not in cities:
        if c[:-5].strip():
            keyword_processor.add_keyword(c[:-5])

texts_labels = [('new york to venice, italy for usd271', ('New York', 'Venice', 'Italy')),
('return flights from brussels to bangkok with etihad from â‚¬407', ('Brussels', 'Bangkok')),
('from los angeles to guadalajara, mexico for usd191', ('Los Angeles', 'Guadalajara')),
('fly to australia new zealand from paris from â‚¬422 return including 2 checked bags', ('Australia', 'New Zealand', 'Paris')),
('I live in Florida', ('Florida')), 
('I live in Marawi', ('Marawi')), 
('I live in jeju', ('Jeju'))]

# No. of correctly extracted terms.
true_positives = 0
false_positives = 0
total_truth = 0

for text, label in texts_labels:
    extracted = keyword_processor.extract_keywords(text)

    # We're making some assumptions here that the order of 
    # extracted and the truth must be the same.
    true_positives += sum(1 for e, l in zip_longest(extracted, label) if e == l)
    false_positives += sum(1 for e, l in zip_longest(extracted, label) if e != l)
    total_truth += len(label)

    # Just visualization candies.
    print(text)
    print(extracted)
    print(label)
    print()
</code></pre>

<p>[out]:</p>

<pre><code>new york to venice, italy for usd271
['New York', 'Venice', 'Italy']
('New York', 'Venice', 'Italy')

return flights from brussels to bangkok with etihad from â‚¬407
['Brussels', 'Bangkok']
('Brussels', 'Bangkok')

from los angeles to guadalajara, mexico for usd191
['Los Angeles', 'Guadalajara', 'Mexico']
('Los Angeles', 'Guadalajara')

fly to australia new zealand from paris from â‚¬422 return including 2 checked bags
['Australia', 'New Zealand', 'Paris']
('Australia', 'New Zealand', 'Paris')

I live in Florida
['Florida']
Florida

I live in Marawi
['Marawi']
Marawi

I live in jeju
['Jeju']
Jeju
</code></pre>

<h3>100% Yeah, NLP-bunga !!!</h3>

<p>But seriously, this is only the tip of the problem. What happens if you have a sentence like this:</p>

<pre><code>&gt;&gt;&gt; keyword_processor.extract_keywords('Adam flew to Bangkok from Singapore and then to China')
['Adam', 'Bangkok', 'Singapore', 'China']
</code></pre>

<p><strong>WHY is <code>Adam</code> extracted as a city?!</strong></p>

<p>Then you do some more neurotic checks:</p>

<pre><code>&gt;&gt;&gt; 'Adam' in cities
Adam
</code></pre>

<p>Congratulations, you've jumped into another NLP rabbit hole of polysemy where the same word has different meaning, in this case, <code>Adam</code> most probably refer to a person in the sentence but it is also coincidentally the name of a city (according to the data you've pulled from).</p>

<h3>I see what you did there... Even if we ignore this polysemy nonsense, you are still not giving me the desired output:</h3>

<p>[in]:</p>

<pre><code>['new york to venice, italy for usd271',
'return flights from brussels to bangkok with etihad from â‚¬407',
'from los angeles to guadalajara, mexico for usd191',
'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags'
]
</code></pre>

<p>[out]:</p>

<pre><code>Origin: New York, USA; Destination: Venice, Italy
Origin: Brussels, BEL; Destination: Bangkok, Thailand
Origin: Los Angeles, USA; Destination: Guadalajara, Mexico
Origin: Paris, France; Destination: Australia / New Zealand (this is a complicated case given two countries)
</code></pre>

<p><strong>Linguist</strong>: Even with the assumption that the preposition (e.g. <code>from</code>, <code>to</code>) preceding the city gives you the ""origin"" / ""destination"" tag, how are you going to handle the case of ""multi-leg"" flights, e.g. </p>

<pre><code>&gt;&gt;&gt; keyword_processor.extract_keywords('Adam flew to Bangkok from Singapore and then to China')
</code></pre>

<p>What's the desired output of this sentence:</p>

<pre><code>&gt; Adam flew to Bangkok from Singapore and then to China
</code></pre>

<p>Perhaps like this? What is the specification? How (un-)structured is your input text?</p>

<pre><code>&gt; Origin: Singapore
&gt; Departure: Bangkok
&gt; Departure: China
</code></pre>

<h2>Lets try to build component TWO to detect prepositions.</h2>

<p>Lets take that assumption you have and try some hacks to the same <code>flashtext</code> methods. </p>

<p><strong>What if we add <code>to</code> and <code>from</code> to the list?</strong></p>

<pre><code>from itertools import zip_longest
from flashtext import KeywordProcessor

keyword_processor = KeywordProcessor(case_sensitive=False)
keyword_processor.add_keywords_from_list(sorted(countries))
keyword_processor.add_keywords_from_list(sorted(cities))

for c in cities:
    if 'city' in c.lower() and c.endswith('City') and c[:-5] not in cities:
        if c[:-5].strip():
            keyword_processor.add_keyword(c[:-5])

keyword_processor.add_keyword('to')
keyword_processor.add_keyword('from')

texts = ['new york to venice, italy for usd271',
'return flights from brussels to bangkok with etihad from â‚¬407',
'from los angeles to guadalajara, mexico for usd191',
'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags']


for text in texts:
    extracted = keyword_processor.extract_keywords(text)
    print(text)
    print(extracted)
    print()
</code></pre>

<p>[out]:</p>

<pre><code>new york to venice, italy for usd271
['New York', 'to', 'Venice', 'Italy']

return flights from brussels to bangkok with etihad from â‚¬407
['from', 'Brussels', 'to', 'Bangkok', 'from']

from los angeles to guadalajara, mexico for usd191
['from', 'Los Angeles', 'to', 'Guadalajara', 'Mexico']

fly to australia new zealand from paris from â‚¬422 return including 2 checked bags
['to', 'Australia', 'New Zealand', 'from', 'Paris', 'from']
</code></pre>

<h3>Heh, that's pretty crappy rule to use to/from,</h3>

<ol>
<li>What if the ""from"" is referring the price of the ticket?</li>
<li>What if there's no ""to/from"" preceding the country/city? </li>
</ol>

<p>Okay, lets work with the above output and see what we do about the problem 1. <strong>Maybe check if the term after the from is city, if not, remove the to/from?</strong></p>

<pre><code>from itertools import zip_longest
from flashtext import KeywordProcessor

keyword_processor = KeywordProcessor(case_sensitive=False)
keyword_processor.add_keywords_from_list(sorted(countries))
keyword_processor.add_keywords_from_list(sorted(cities))

for c in cities:
    if 'city' in c.lower() and c.endswith('City') and c[:-5] not in cities:
        if c[:-5].strip():
            keyword_processor.add_keyword(c[:-5])

keyword_processor.add_keyword('to')
keyword_processor.add_keyword('from')

texts = ['new york to venice, italy for usd271',
'return flights from brussels to bangkok with etihad from â‚¬407',
'from los angeles to guadalajara, mexico for usd191',
'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags']


for text in texts:
    extracted = keyword_processor.extract_keywords(text)
    print(text)

    new_extracted = []
    extracted_next = extracted[1:]
    for e_i, e_iplus1 in zip_longest(extracted, extracted_next):
        if e_i == 'from' and e_iplus1 not in cities and e_iplus1 not in countries:
            print(e_i, e_iplus1)
            continue
        elif e_i == 'from' and e_iplus1 == None: # last word in the list.
            continue
        else:
            new_extracted.append(e_i)

    print(new_extracted)
    print()
</code></pre>

<p>That seems to do the trick and remove the <code>from</code> that doesn't precede a city/country. </p>

<p>[out]:</p>

<pre><code>new york to venice, italy for usd271
['New York', 'to', 'Venice', 'Italy']

return flights from brussels to bangkok with etihad from â‚¬407
from None
['from', 'Brussels', 'to', 'Bangkok']

from los angeles to guadalajara, mexico for usd191
['from', 'Los Angeles', 'to', 'Guadalajara', 'Mexico']

fly to australia new zealand from paris from â‚¬422 return including 2 checked bags
from None
['to', 'Australia', 'New Zealand', 'from', 'Paris']
</code></pre>

<h3>But the ""from New York"" still isn't solve!!</h3>

<p><strong>Linguist</strong>: Think carefully, should ambiguity be resolved by making an informed decision to make ambiguous phrase obvious? If so, what is the ""information"" in the informed decision? Should it follow a certain template first to detect the information before filling in the ambiguity?</p>

<p><strong>You</strong>: I'm losing my patience with you... You're bringing me in circles and circles, where's that AI that can understand human language that I keep hearing from the news and Google and Facebook and all?!</p>

<p><strong>You</strong>: What you gave me are rule based and where's the AI in all these?</p>

<p><strong>NLP Practitioner</strong>: Didn't you wanted 100%? Writing ""business logics"" or rule-based systems would be the only way to really achieve that ""100%"" given a specific data set without any preset data set that one can use for ""training an AI"".</p>

<p><strong>You</strong>: What do you mean by training an AI? Why can't I just use Google or Facebook or Amazon or Microsoft or even IBM's AI? </p>

<p><strong>NLP Practitioner</strong>: Let me introduce you to </p>

<ul>
<li><a href=""https://learning.oreilly.com/library/view/data-science-from/9781492041122/"" rel=""noreferrer"">https://learning.oreilly.com/library/view/data-science-from/9781492041122/</a></li>
<li><a href=""https://allennlp.org/tutorials"" rel=""noreferrer"">https://allennlp.org/tutorials</a></li>
<li><a href=""https://www.aclweb.org/anthology/"" rel=""noreferrer"">https://www.aclweb.org/anthology/</a></li>
</ul>

<p>Welcome to the world of Computational Linguistics and NLP!</p>

<h1>In Short</h1>

<p>Yes, there's no real ready-made magical solution and if you want to use an ""AI"" or machine learning algorithm, most probably you would need a lot more training data like the <code>texts_labels</code> pairs shown in the above example.</p>
"
Python Spacy Pattern- How to tag a word based on another word?,"<p>I'm trying to write a pattern that would tag the whole word as unit based on one substring.
Here's example:</p>
<pre><code>terms = [{'ent': &quot;UNIT&quot;,
         'patterns':[
            [{'lemma':'liter'}]]}]

text = &quot;There were 46 kiloliters of juice available&quot;
</code></pre>
<p>I wanna tag 'kiloliters' as Unit based on this pattern. I tried using 'lemma&quot; but it won't work in this case.</p>
","python, nlp, spacy","<p>You haven't said which model you're using so I'll use <code>en_web_core_sm</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.matcher import Matcher

nlp = spacy.load(&quot;en_core_web_sm&quot;)
matcher = Matcher(nlp.vocab)
doc = nlp(&quot;There were 46 kiloliters of juice available&quot;)
</code></pre>
<p>The first thing is that none of these have an <code>ent_type</code> of <code>UNIT</code>:</p>
<pre class=""lang-py prettyprint-override""><code>for tok in doc:
    print(f&quot;'{tok}': ent_type: '{tok.ent_type_}', lemma: '{tok.lemma_}'&quot;)

'There': ent_type: '', lemma: 'there'
'were': ent_type: '', lemma: 'be'
'46': ent_type: 'CARDINAL', lemma: '46'
'kiloliters': ent_type: '', lemma: 'kiloliter'
'of': ent_type: '', lemma: 'of'
'juice': ent_type: '', lemma: 'juice'
'available': ent_type: '', lemma: 'available'
</code></pre>
<p>Also, as you can see, the lemma of <code>kiloliters</code> is <code>kiloliter</code>. This is a bit annoying as you don't want to have to specify milliliters, liters etc. separately. One alternative is to look for a <code>CARDINAL</code> token (which also includes words e.g. <code>&quot;two liters&quot;</code>) followed a regex:</p>
<pre class=""lang-py prettyprint-override""><code>doc = nlp(&quot;&quot;&quot;
          There were 46 kiloliters of juice available.
          I could not drink more than two liters a day.
          I would only give a child 500 milliliters.
          &quot;&quot;&quot;
)
pattern = [{'ENT_TYPE': 'CARDINAL'},
           {&quot;TEXT&quot;: {&quot;REGEX&quot;: &quot;^.*(liter)s?$&quot;}}]

matcher.add(&quot;unit&quot;, [pattern])

matches = matcher(doc, as_spans=True)
for span in matches:
    print(span[-1].text)
</code></pre>
<p>Output:</p>
<pre><code>kiloliters
liters
milliliters
</code></pre>
"
Python Counter() function to count words in documents with more then one occurrence,"<p>I am working on an NLP (Natural Language Processing) project where I used the Python Counter() function from collections library. I am getting the results in the following form:</p>

<p><strong>OUTPUT</strong>:</p>

<pre><code>Counter({'due': 23, 'support': 20, 'ATM': 16, 'come': 12, 'case': 11, 'Sallu': 10, 'tough,': 9, 'team': 8, 'evident': , 'likely': 6, 'rupee': 4, 'depreciated': 2, 'senior': 1, 'neutral': 1, 'told': 1, 'tour\n\nRussia’s': 1, 'Vladimir': 1, 'indeed,': 1, 'welcome,”': 1, 'player': 1, 'added': 1, 'Games,': 1, 'Russia': 1, 'arrest': 1, 'system.\nBut': 1, 'rate': 1, 'Tuesday': 1, 'February,': 1, 'idea': 1, 'ban': 1, 'data': 1, 'consecutive': 1, 'interbank': 1, 'man,': 1, 'involved': 1, 'aggressive': 1, 'took': 1, 'sure': 1, 'market': 1, 'custody': 1, 'gang.\nWithholding': 1, 'cricketer': 1})
</code></pre>

<p>The problem is, I want to extract the words having count more than 1. In other words, I am trying to get only those words whose count is greater than 1 or 2.</p>

<p>I want to use the output to make a vocabulary list after reducing the words with low frequency.</p>

<p><strong>PS</strong>: I have more than 100 documents to test my data with almost 2000 distinct words.</p>

<p><strong>PPS</strong>: I have tried everything to get the results but unable to do so. I only need a logic and will be able to implement. </p>
","python, python-3.x, nlp, words, python-collections","<p>You can iterate over the key, value pairs in the dict and add them to a separate list. This is just that you wanted to produce a list in the end, otherwise @jpp has the better solution.</p>

<pre><code>from collections import Counter

myStr = ""This this this is really really good.""
myDict = Counter(myStr.split())

myList = [k for k, v in myDict.items() if v &gt; 1]

# ['this', 'really']
</code></pre>
"
How to fix this error No module named &#39;llama_index.llms.llama_cpp&#39;,"<p>I am trying to use mixtral-8x7b with my own data with no luck. Here is my code</p>
<pre><code>import torch
from llama_index.llms.llama_cpp import LlamaCPP
from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt, completion_to_prompt
llm = LlamaCPP(
    model_url=None, # We'll load locally.
    model_path='./Models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf', # 4-bit model
    temperature=0.1,
    max_new_tokens=1024, # Increasing to support longer responses
    context_window=8192, # Mistral7B has an 8K context-window
    generate_kwargs={},
    # set to at least 1 to use GPU
    model_kwargs={&quot;n_gpu_layers&quot;: 40}, # 40 was a good amount of layers for the RTX 3090, you may need to decrease yours if you have less VRAM than 24GB
    messages_to_prompt=messages_to_prompt,
    completion_to_prompt=completion_to_prompt,
    verbose=True
)
</code></pre>
<p>Which gives error No module named 'llama_index.llms.llama_cpp'.</p>
<p>I have installed llama_index, used my MAC Mini and also Google Colab's GPUs</p>
<p>Any suggestions?</p>
","machine-learning, nlp",
RAG: Injecting entire retrieved documents into prompt vs just the page_content,"<p>When using a <a href=""https://python.langchain.com/docs/expression_language/cookbook/retrieval"" rel=""nofollow noreferrer"">rag chain</a>, I realized that the entire <code>Document</code> is injected into the prompt. Would it not make more sense to just pass the <code>page_content</code> of the document? I am concerned that seeing <code>[Document(page_content='</code> will affect the results in unexpected ways.</p>
<pre class=""lang-py prettyprint-override""><code>from operator import itemgetter

from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

vectorstore = FAISS.from_texts(
    [&quot;harrison worked at kensho&quot;, &quot;harrison worked for 3 years&quot;], embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()

template = &quot;&quot;&quot;Answer the question based only on the following context:
{context}

Question: {question}
&quot;&quot;&quot;
prompt = ChatPromptTemplate.from_template(template)

model = ChatOpenAI()

chain = (
    {&quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough()}
    | prompt
)

chain.invoke(&quot;where did harrison work?&quot;)
</code></pre>
<pre><code>ChatPromptValue(messages=[HumanMessage(content=&quot;Answer the question based only on the following context:\n[Document(page_content='harrison worked for 3 years'), Document(page_content='harrison worked at kensho')]\n\nQuestion: where did harrison work?\n&quot;)])
</code></pre>
","nlp, langchain",
Replace personal pronoun with previous person mentioned (noisy coref),"<p>I want to do a noisy resolution such that given a personal prounoun, that pronoun is replace by the previous(nearest) person.</p>
<p>For example:</p>
<p><code>Alex is looking at buying a U.K. startup for $1 billion. He is very confident that this is going to happen. Sussan is also in the same situation. However, she has lost hope.</code></p>
<p>the output is:</p>
<p><code>Alex is looking at buying a U.K. startup for $1 billion. Alex is very confident that this is going to happen. Sussan is also in the same situation. However, Susan has lost hope.</code></p>
<p>Another example,</p>
<p><code>Peter is a friend of Gates. But Gates does not like him. </code></p>
<p>In this case, the output would be :</p>
<p><code>Peter is a friend of Gates. But Gates does not like Gates.</code></p>
<p>Yes! This is super noisy.</p>
<p>Using spacy:
I have extracted the <code>Person</code> using NER, but how can I replace pronouns appropriately?</p>
<p>Code:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)
for ent in doc.ents:
  if ent.label_ == 'PERSON':
    print(ent.text, ent.label_)
</code></pre>
","python, python-3.x, nlp, spacy, coreference-resolution","<p>I have written a function that works for your two examples:</p>
<p>Consider using a larger model such as <code>en_core_web_lg</code> for more accurate tagging.</p>
<pre><code>import spacy
from string import punctuation

nlp = spacy.load(&quot;en_core_web_lg&quot;)

def pronoun_coref(text):
    doc = nlp(text)
    pronouns = [(tok, tok.i) for tok in doc if (tok.tag_ == &quot;PRP&quot;)]
    names = [(ent.text, ent[0].i) for ent in doc.ents if ent.label_ == 'PERSON']
    doc = [tok.text_with_ws for tok in doc]
    for p in pronouns:
        replace = max(filter(lambda x: x[1] &lt; p[1], names),
                      key=lambda x: x[1], default=False)
        if replace:
            replace = replace[0]
            if doc[p[1] - 1] in punctuation:
                replace = ' ' + replace
            if doc[p[1] + 1] not in punctuation:
                replace = replace + ' '
            doc[p[1]] = replace
    doc = ''.join(doc)
    return doc
</code></pre>
"
Updating old code with new torch vocab methods (stoi and itos methods changed),"<p>I am trying to create a Japanese-English translation model following this Medium article.
<a href=""https://arusl.medium.com/japanese-english-language-translation-with-transformer-using-pytorch-243738146806"" rel=""nofollow noreferrer"">https://arusl.medium.com/japanese-english-language-translation-with-transformer-using-pytorch-243738146806</a>
Everything runs perfectly until the second to last cell, when I get an error running the translate function. The error is specifically on this line.</p>
<pre><code>tokens = [BOS_IDX] + [src_vocab.stoi[tok] for tok in src_tokenizer.encode(src, out_type=str)]+ [EOS_IDX]
</code></pre>
<p>The error: AttributeError: 'Vocab' object has no attribute 'stoi'. Since the article was written, the method .stoi has changed to get_stoi() → Dict[str, int] according to the torchtext documentation (<a href=""https://pytorch.org/text/stable/vocab.html"" rel=""nofollow noreferrer"">https://pytorch.org/text/stable/vocab.html</a>). When I attempt to change the line to the following, however, I get the error &quot;Counter object has no attribute 'get_stoi'.&quot;</p>
<pre><code>tokens = [BOS_IDX] + [src_vocab.get_stoi()[tok] for tok in src_tokenizer.encode(src, out_type=str)]+ [EOS_IDX]
</code></pre>
<p>The same goes for the itos and get_itos() method. If I try to use the method as
Any help for how to make this work would be greatly appreciated as I'm very dumbfounded at the moment.</p>
<p>A similar question was asked here but I don't see how to implement the answer or make it work in this case. <a href=""https://stackoverflow.com/questions/68743912/vocab-object-has-no-attribute-itos"">&#39;Vocab&#39; object has no attribute &#39;itos&#39;</a></p>
<p>Edit: This function seems suspect as it is creating vocab out of a counter... is there a better way to do this?</p>
<pre><code>def build_vocab(sentences, tokenizer):
  counter = Counter()
  for sentence in sentences:
    counter.update(tokenizer.encode(sentence, out_type=str))
  return Vocab(counter)
</code></pre>
<p>Thank you!</p>
","python, pytorch, nlp, torchtext",
similarity_search with chromadb and default setting returns 4 identical document objects,"<p>i'm new to vectorstore and in general doing my first steps with RAG</p>
<p>relevant pip freeze info:</p>
<pre><code>chroma-hnswlib==0.7.3
chromadb==0.4.24
langchain==0.1.13
langchain-community==0.0.29
langchain-core==0.1.34
langchain-text-splitters==0.0.1
</code></pre>
<p>here is the code:</p>
<pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings



loader = PyPDFLoader(&quot;paper1.pdf&quot;)
pages = loader.load()
pages = pages[9:360]
text = &quot;&quot;
for page in pages:
    text += page.page_content
text = text.replace('\t', ' ')
text_splitter = RecursiveCharacterTextSplitter(separators=[&quot;\n\n&quot;, &quot;\n&quot;, &quot;\t&quot;], chunk_size=chunk_size,
                                               chunk_overlap=chunk_overlap)
docs = text_splitter.create_documents([text])

DB = Chroma.from_documents(docs,SentenceTransformerEmbeddings(model_name=&quot;all-MiniLM-L6- 
v2&quot;),persist_directory=&quot;./chroma_db&quot;)
most_similar = self.DB.similarity_search(
        'some sentence about apples')
</code></pre>
<p>so far things work well, but when i inspect most_similar this is what i see.</p>
<pre><code>[Document(page_content='bla bla bla apples bla bla bla'), Document(page_content='bla bla bla 
               apples bla bla bla'), Document(page_content='bla bla bla apples bla bla bla'),  
               Document(page_content='bla bla bla apples bla bla bla')]
</code></pre>
<p>the point being, the same chunk is being retrieved again and again.</p>
","python, nlp, langchain, chromadb",
Error loading Hugging Face model: SafeTensorsInfo.__init__() got an unexpected keyword argument &#39;sharded&#39;,"<p>I have been using Hugging Face transformers <a href=""https://huggingface.co/TheBloke/Llama-2-7B-Chat-AWQ"" rel=""nofollow noreferrer"">quantized Llama2 model</a>. Suddenly, code I was able to run earlier today is throwing an error when I try to load the model.</p>
<p>This code is straight from the docs:</p>
<pre class=""lang-py prettyprint-override""><code>from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_name_or_path = &quot;TheBloke/Llama-2-7b-Chat-AWQ&quot;

# Load model
model = AutoAWQForCausalLM.from_quantized(
    model_name_or_path,
    fuse_layers = True,
    trust_remote_code = False,
    safetensors = True
)
</code></pre>
<p>This suddenly generates this error:</p>
<pre><code>TypeError: SafeTensorsInfo.__init__() got an unexpected keyword argument 'sharded'
</code></pre>
<p>The bizarre thing is that I was able to run this code earlier today on the same machine. It is the same model. I haven't installed or updated any new Python (or other) packages.</p>
<p>This is using Ubuntu 20.04. Here are the relevant package versions:</p>
<pre><code>accelerate==0.28.0
autoawq==0.2.4
autoawq_kernels==0.0.6
bitsandbytes==0.43.0
huggingface-hub==0.22.2
torch==2.2.2+cu121
torchaudio==2.2.2+cu121
torchvision==0.17.2+cu121
transformers==4.38.2
</code></pre>
<p>How can an error suddenly arise when I haven't changed anything? It might make sense of my cached version of the model was no longer available, and it had downloaded a newer version. The way that <code>transformers</code> downloads models is a bit mysterious to me, but the <a href=""https://huggingface.co/TheBloke/Llama-2-7B-Chat-AWQ/tree/main"" rel=""nofollow noreferrer"">model files</a> page doesn't indicate anything has changed. What can I do to return to the position I was in this morning when this code would run?</p>
","python, nlp, huggingface-transformers, huggingface, llama",
Get previous sentence while using SpaCy matcher,"<p>I am running a SpaCy Matcher line-by-line on a text file. My file has each text entry on a separate line. I am trying to extract 1) the matched instance, 2) the full sentence, and 3) the previous sentence. I am able to get the first two, <strong>but I am having trouble getting the previous sentence</strong>, given that there isn't a sentence index (from <a href=""https://stackoverflow.com/questions/58599421/get-previous-and-following-sentences-in-spacy"">this post</a>). Here is my code:</p>
<pre><code>with open('file.txt', 'r') as f:
    for line in iter(f.readline, ''):
        doc = nlp(line)
        matcher = Matcher(nlp.vocab)
        matcher.add(&quot;pattern_of_interest&quot;, [pattern])
        matches = matcher(doc)
    
        for match_id, start, end in matches:
            string_id = nlp.vocab.strings[match_id]
            span = doc[start:end] 
            
        for sent in doc.sents:
            if matcher(sent):
                instances.append(pd.Series({&quot;instance&quot;:str(span.text), 
                                        &quot;sentence&quot;:str(sent.text),
                                        &quot;previous_sentence&quot;:str(sent[-1].text)}))
</code></pre>
<p>I understand that the bolded part is giving me the previous token, not sentence (I tried to get around this with the list, but it doesn't work). Any advice for retrieving the previous sentence would be greatly appreciated. Thank you!</p>
","python, nlp, spacy","<p>Adjustments:
<strong>Track Previous Sentence:</strong> We now maintain a prev_sent variable that tracks the previous sentence as we iterate through all sentences in a document. <strong>Matcher Usage:</strong> We only need to create the Matcher instance once, outside the loop through lines in the file, and then apply it to each sentence within the loop. This is more efficient than recreating it for every line. <strong>Check for Previous Sentence:</strong> We handle cases where there might not be a previous sentence (e.g., the match is found in the first sentence of the document) by checking if prev_sent is None. If it is, we set the &quot;previous_sentence&quot; field to &quot;N/A&quot; or any placeholder text you find suitable.</p>
<pre><code>import spacy
from spacy.matcher import Matcher
import pandas as pd

# Load the SpaCy model
nlp = spacy.load(&quot;en_core_web_sm&quot;)  # Adjust model as necessary

# Define your pattern here
pattern = [{&quot;LOWER&quot;: &quot;example&quot;}]  # Example pattern

# Initialize matcher with the vocab
matcher = Matcher(nlp.vocab)
matcher.add(&quot;pattern_of_interest&quot;, [pattern])

instances = []  # List to hold match details

with open('file.txt', 'r') as f:
    for line in iter(f.readline, ''):
        doc = nlp(line)
        
        prev_sent = None  # Variable to keep track of the previous sentence
        for sent in doc.sents:
            matches = matcher(sent)
            if matches:
                for match_id, start, end in matches:
                    instance_text = sent[start:end].text  # The matched instance
                    current_sentence = sent.text
                    previous_sentence = prev_sent.text if prev_sent else &quot;N/A&quot;  # Handle the case where there's no previous sentence
                    
                    # Append the extracted information to your instances list
                    instances.append(pd.Series({&quot;instance&quot;: instance_text, 
                                                 &quot;sentence&quot;: current_sentence,
                                                 &quot;previous_sentence&quot;: previous_sentence}))
            prev_sent = sent  # Update the previous sentence for the next iteration

# Convert instances list to DataFrame
df_instances = pd.DataFrame(instances)
print(df_instances)
</code></pre>
"
"How to detect if two sentences are simmilar, not in meaning, but in syllables/words?","<p><strong>Here are some examples of the types of sentences that need to be considered &quot;similar&quot;</strong></p>
<pre><code>there was a most extraordinary noise going on shrinking rapidly she soon made out
there was a most extraordinary noise going on shrinking rapid
</code></pre>
<pre><code>that will be a very little alice knew it was just possible it had
thou wilt be very little alice i knew it was possible to add
</code></pre>
<pre><code>however at last it sat down and looked very anxiously into her face and
however that lives in sadtown and look very anxiously into him facing it
</code></pre>
<pre><code>she went in search of her or of anything to say she simply bowed
she went in the search of her own or of anything to say
</code></pre>
<pre><code>and she squeezed herself up on tiptoe and peeped over the wig he did
and she squeezed herself up on the tiptoe and peeped over her wig he did
</code></pre>
<pre><code>she had not noticed before and behind it was very glad to find that
she had not noticed before and behind it it was very glad to find that
</code></pre>
<pre><code>as soon as the soldiers had to fall a long hookah and taking not
soon as the soldiers have to fall along huka and taking knots
</code></pre>
<p><strong>And here are some examples of more difficult edge cases I would be able to like to catch, but are not as necessary</strong></p>
<pre><code>so she tucked it under her arm with its head it would not join
she tucked it under her arm with its head
</code></pre>
<pre><code>let me see four times five is twelve and four times five is twelve 
let me see  times  is  and  times  is
</code></pre>
<pre><code>let me see four times seven is oh dear run home this moment and 
times  is o dear run home this moment and
</code></pre>
<pre><code>in a minute or two she walked sadly down the middle being held up 
and then well see you sidely down the middle in health often
</code></pre>
<p>Sentences that are somewhat different and have no such similarities need to be marked as dissimilar. If there is an algorithm that exists that outputs a &quot;score&quot; versus just a boolean similar or not, I could determine what threshold would be necessary through my own testing.</p>
<p>The top sentence in each example is randomly generated; the bottom sentence is the output of a speech-to-text neural network, from an audio file of someone reading out the top line. If there is some syllabic comparison method that would be much more accurate given that I have the initial source text as well as the audio, I could also employ that instead of this word comparison technique.</p>
<p>My current method involves indexing each word, once forwards, and once reverse, and then checking how many words line up. If at least 10 words match in either indexing order, I count the sentences as similar. However, all of the presented examples are cases where this strategy does not work.</p>
","search, nlp, full-text-search, similarity, sentence-similarity","<p>This is exactly similar to my answer above, but this is in nodejs. Apart from the language difference, code works exactly the same.</p>
<p>First you need to install the natural module using npm.</p>
<pre class=""lang-js prettyprint-override""><code>    npm install natural
</code></pre>
<pre class=""lang-js prettyprint-override""><code>
    const natural = require('natural');
    
    function dotProduct(vector1, vector2) {
        return vector1.reduce((acc, val, index) =&gt; acc + val * vector2[index], 0);
    }
    
    function magnitude(vector) {
        return Math.sqrt(vector.reduce((acc, val) =&gt; acc + val * val, 0));
    }
    
    function cosineSimilarity(vector1, vector2) {
        const dotProd = dotProduct(vector1, vector2);
        const mag1 = magnitude(vector1);
        const mag2 = magnitude(vector2);
    
        if (mag1 === 0 || mag2 === 0) {
            return 0; // Avoid division by zero
        }
    
        return dotProd / (mag1 * mag2);
    }
    
    function sentenceSimilarity(sentence1, sentence2) {
        // Tokenizing sentences
        const tokenizer = new natural.WordTokenizer();
        const sentence1Tokens = tokenizer.tokenize(sentence1);
        const sentence2Tokens = tokenizer.tokenize(sentence2);
    
        // Creating a bade of words from tokens
        const bagOfWords = new Set([...sentence1Tokens, ...sentence2Tokens]);
    
        // Convert tokens to vectors
        const vector1 = Array.from(bagOfWords).map(word =&gt; sentence1Tokens.includes(word) ? 1 : 0);
        const vector2 = Array.from(bagOfWords).map(word =&gt; sentence2Tokens.includes(word) ? 1 : 0);
    
        // Calculate cosine similarity
        const similarity = cosineSimilarity(vector1, vector2);
    
        return similarity;
    }
    
    // Example usage
    const sentence1 = &quot;This is a sentence.&quot;;
    const sentence2 = &quot;This is another sentence.&quot;;
    const similarityScore = sentenceSimilarity(sentence1, sentence2);
    console.log(&quot;Similarity score:&quot;, similarityScore);

</code></pre>
<p>Function dotProduct(), magnitude(), and cosineSimilarity() were needed to be defined since I was not able to find a library that provides these in node unlike in python. Apart from that, all the other logic are similar to the python code above.</p>
"
How to use SpaCy NER?,"<p>I am working on a mini-project to cluster similar sentences together. Before I can achieve that, I have to perform pre-processing to the extremely dirty data (these data are all user inputs, free text).</p>
<p>One of the pre-processing step that I thought of is to identify each sentence and classify it with a category. Even though it is free text, there are some key token per sentence such as &quot;LOOK&quot;, &quot;REPLACE&quot;, &quot;CHECK&quot;.</p>
<p>To classify each sentences, I looked into NLP and discovered SpaCy. One of the component in SpaCy, NER, seems like the perfect task for this. I believe my use case is that I do not need to use the full SpaCy pipeline for this classification. I also understand that I need to add in my custom labels into the NER.</p>
<p>My question is - can I just add my custom label into SpaCy NER using <code>add_label()</code> and run the NER onto my dataset without the having to re-train the SpaCy model? My end goal is to simply classify the sentences into a category, based on the keywords.</p>
<p>This is quite unclear despite several days of researching.</p>
<p>Greatly appreciate any clarification on this. TIA.</p>
","python, nlp, spacy, named-entity-recognition",
Alternative to one-hot encoding for output to a model when vocabulary size is very large,"<p>I was following <a href=""https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"" rel=""nofollow noreferrer"">this blog</a>. In it he talks about how to build a language model in keras. He shows how to build a simple model in keras.</p>
<blockquote>
<p>After separating, we need to one hot encode the output word. This means converting it from an integer to a vector of 0 values, one for each word in the vocabulary, with a 1 to indicate the specific word at the index of the words integer value.</p>
<p>This is so that the model learns to predict the probability distribution for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next.</p>
<p>Keras provides the to_categorical() that can be used to one hot encode the output words for each input-output sequence pair.</p>
</blockquote>
<p>He uses the following:</p>
<p><code>y = to_categorical(y, num_classes=vocab_size)</code></p>
<p>In his case, the vocabulary size is manageable. I am working with vocabulary having size &gt; 100 million. I guess I should not use a one-hot encoding for the output <code>y</code> as done by him. Is there any alternative?</p>
","nlp, keras, language-model",
Output of Cosine Similarity is not as expected,"<p>I am trying to generate the Cosine similarity between two words in a sentence. The sentence is &quot;The black cat sat on the couch and the brown dog slept on the rug&quot;.</p>
<p>My Python code is below:</p>
<pre><code>from nltk.tokenize import sent_tokenize, word_tokenize
import warnings
 
warnings.filterwarnings(action = 'ignore')
 
import gensim
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity

sentence = &quot;The black cat sat on the couch and the brown dog slept on the rug&quot;
# Replaces escape character with space
f = sentence.replace(&quot;\n&quot;, &quot; &quot;)
 
data = []

# sentence parsing
for i in sent_tokenize(f):
    temp = []
    # tokenize the sentence into words
    for j in word_tokenize(i):
        temp.append(j.lower())
    data.append(temp)
print(data)
# Creating Skip Gram model
model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 512, window = 5, sg = 1)

# Print results
print(&quot;Cosine similarity between 'black' &quot; +
          &quot;and 'brown' - Skip Gram : &quot;,
    model2.wv.similarity('black', 'brown'))

</code></pre>
<p>As &quot;black&quot; and &quot;brown&quot; are of colour type, their cosine similarity should be maximum (somewhere around 1). But my result shows following:</p>
<pre><code>[['the', 'black', 'cat', 'sat', 'on', 'the', 'couch', 'and', 'the', 'brown', 'dog', 'slept', 'on', 'the', 'rug']]
Cosine similarity between 'black' and 'brown' - Skip Gram :  0.008911405
</code></pre>
<p>Any idea what is wrong here? Is my understanding about cosine similarity correct?</p>
","python, nlp, transform, word2vec, cosine-similarity",
Error while installing medaCy package while following instructions from github repo,"<p><strong>I wanted to run this command to install medaCy's Prediction and Model Training (stable version)</strong> :</p>
<p>! pip install git+https://github.com/NLPatVCU/medaCy.git</p>
<p><strong>But I've encountered an error as shown below :</strong></p>
<p><a href=""https://i.sstatic.net/M81sz.png"" rel=""nofollow noreferrer"">img1</a></p>
<p><a href=""https://i.sstatic.net/pk76l.png"" rel=""nofollow noreferrer"">img2</a></p>
<p><a href=""https://i.sstatic.net/xAv29.png"" rel=""nofollow noreferrer"">img3</a></p>
<p><a href=""https://i.sstatic.net/WZxgl.png"" rel=""nofollow noreferrer"">img4</a></p>
<p>medaCy's github repo : <a href=""https://github.com/NLPatVCU/medaCy"" rel=""nofollow noreferrer"">https://github.com/NLPatVCU/medaCy</a></p>
<p>I wanted to run the command to install medaCy's Prediction and Model Training (stable version), and train the model on my own data.
expected work :</p>
<p><a href=""https://i.sstatic.net/brSrs.png"" rel=""nofollow noreferrer"">img5</a></p>
<p>But I've encountered an error while installing the medaCy model. Probably there is an issue with the version specifier format : <strong>spacy_lookups_data&gt;=0.0.5&lt;0.2.0</strong></p>
","github, installation, pip, nlp, package",
&#39;Unknown Encoding&#39; TikToken Error in exe that is compiled with Nuitka,"<p>I'm working with the tiktoken library in Python to count the number of tokens in a series of messages. My code runs well when executed as a Python script. However, when I compile it into an executable, I encounter an 'unknown encoding' error. Attempts to debug the library or replace it with custom code have so far been unsuccessful.</p>
<p>In both scenario in code below failed to get the encoding model from tiktoken for unknown reason. I've tried to load the cl100k_based.tiktoken file from local PC rather than sending the link to openai in the openai_public.py in tiktoken_ext. For pyinstaller we only need to add this line --hidden-import=tiktoken_ext.openai_public --hidden-import=tiktoken_ext to solve the problem. I've used below command</p>
<pre><code>python -m nuitka script.py --onefile --show-modules --include-package=tiktoken --include-package=tiktoken_ext --include-package=blobfile
</code></pre>
<p>But the problem still persist.</p>
<pre><code>def num_tokens_from_messages(messages, model):
    &quot;&quot;&quot;Returns the number of tokens used by a list of messages.&quot;&quot;&quot;
    try:
        encoding = tiktoken.encoding_for_model(model)
        print(&quot;getting encoding_for_mdel&quot;,encoding)
    except Exception as e:
        print(&quot;Warning: model not found. Using cl100k_base encoding.&quot;,e)
        encoding = tiktoken.get_encoding(&quot;cl100k_base&quot;)
        print(&quot;in exception&quot;,encoding)
</code></pre>
<p>Any pointers or insights would be greatly appreciated.</p>
","python, c, nlp, compiler-errors, nuitka",
How to speed up computation time for stopword removal and lemmatization in NLP,"<p>As part of pre-processing for a text classification model, I have added stopword removal and lemmatization steps, using the NLTK library. The code is below:</p>
<pre><code>import pandas as pd
import nltk; nltk.download(&quot;all&quot;)
from nltk.corpus import stopwords; stop = set(stopwords.words('english'))
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

# Stopwords removal


def remove_stopwords(entry):
  sentence_list = [word for word in entry.split() if word not in stopwords.words(&quot;english&quot;)]
  return &quot; &quot;.join(sentence_list)

df[&quot;Description_no_stopwords&quot;] = df.loc[:, &quot;Description&quot;].apply(lambda x: remove_stopwords(x))

# Lemmatization

lemmatizer = WordNetLemmatizer()

def punct_strip(string):
  s = re.sub(r'[^\w\s]',' ',string)
  return s

def get_wordnet_pos(word):
    &quot;&quot;&quot;Map POS tag to first character lemmatize() accepts&quot;&quot;&quot;
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {&quot;J&quot;: wordnet.ADJ,
                &quot;N&quot;: wordnet.NOUN,
                &quot;V&quot;: wordnet.VERB,
                &quot;R&quot;: wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

def lemmatize_rows(entry):
  sentence_list = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in punct_strip(entry).split()]
  return &quot; &quot;.join(sentence_list)

df[&quot;Description - lemmatized&quot;] = df.loc[:, &quot;Description_no_stopwords&quot;].apply(lambda x: lemmatize_rows(x))

</code></pre>
<p>The problem is that, when I pre-process a dataset with 27k entries (my test set), it takes 40-45 seconds for stopwords removal and just as long for lemmatization. By contrast, model evaluation only takes 2-3 seconds.</p>
<p>How can I re-write the functions to optimise computation speed? I have read something about vectorization, but the example functions were much simpler than the ones that I have reported, and I wouldn't know how to do it in this case.</p>
","python, pandas, performance, nlp",
Upgrading accelerate while using Trainer class,"<p>I am facing an issue whilst using Trainer class with Pytorch on Google Colab as it demands accelarate&gt;=0.21.0 even though I have updated all the requirements, is there any alternative to it?</p>
<p>&quot;Using the <code>Trainer</code> with <code>PyTorch</code> requires <code>accelerate&gt;=0.21.0</code>: Please run <code>pip install transformers[torch]</code> or <code>pip install accelerate -U</code>&quot;.
I have tried both the above suggestions but none worked. I don't know what am I doing wrong or how to proceed?</p>
","deep-learning, pytorch, nlp, huggingface-transformers, huggingface-trainer",
How to create table of contents using unstructured (the python package),"<h2>tl;dr</h2>
<p>How can I extract a clean table of contents from a pdf document that has hierarchical section headers using the <a href=""https://unstructured.io/"" rel=""nofollow noreferrer""><code>unstructured</code></a> package?</p>
<h2>Some more details</h2>
<p>I have a pdf document that is multiple pages long. The text in the document is organised into multiple sections, each with a header/title. Each of these sections are potentially split up into subsections with their own header/title. These subsections can have subsubsections, etc.</p>
<p>The document does not have a table of contents page. How can I use the <a href=""https://unstructured.io/"" rel=""nofollow noreferrer""><code>unstructured</code></a> package to automatically extract a table of contents from my document? The table of contents should have the same hierarchy as the sections and subsections in my document.</p>
<h2>Example</h2>
<p>If my document looks like this:</p>
<blockquote>
<p><strong>This is the title of section 1</strong></p>
<p>Bla bla bla.</p>
<p><strong>This is the title of subsection 1.1</strong></p>
<p>More bla bla bla.</p>
<p><strong>This is the title of subsubsection 1.1.1</strong></p>
<p>More bla bla bla.</p>
<p><strong>This is the title of subsection 1.2</strong></p>
<p>More bla bla bla.</p>
<p><strong>This is the title of section 2</strong></p>
<p>Even more bla bla.</p>
</blockquote>
<p>Then I would like to extract a table of contents from this that includes the hierarchy of headers. For example:</p>
<pre class=""lang-py prettyprint-override""><code>{
    &quot;This is the title of section 1&quot;: 0,
    &quot;This is the title of subsection 1.1&quot;: 1,
    &quot;This is the title of subsubsection 1.1.1&quot;: 2,
    &quot;This is the title of subsection 1.2&quot;: 1,
    &quot;This is the title of section 2&quot;: 0,
}
</code></pre>
<p>Where the number indicates the level of the header in the hierarchy.</p>
","python, nlp",
"ChatBot langchain, set memory+retriever+map_reduce","<p>I am new on this topics.I'd like to get a chain that allows me to use a retriever ,a memory and to set the chain_type = 'map_reduce'.In addition to that I'd like to know if is it possible to use the map_reduce only in the case in which the number of token exceeds the limit.Thanks in advice.</p>
<p>I tried to use the 'ConversationalRetrievalChain.from_llm' or 'RetrievalQA.from_llm' but I'm not figuring out how to combine the different functions of the chains.</p>
","python, nlp, chatbot, openai-api, langchain",
converting spacy token vectors into text,"<p>I am using spacy to create vectors of a sentence. If the sentence is 'I am working', it gives me a vector of shape (3, 300). Is there any way to get back the text in the sentence using those vectors?</p>

<p>Thank in advance,
Harathi</p>
","python, vector, text, nlp, spacy",
Cleaning text data that has extra whitespaces between letters of a word,"<p>Using R, I have read text from PDFs, and some words were read in with a space within the word, and I can't find any way to clean it.</p>
<p>For example, I need to turn &quot;Then I cre ated a c h a r t using Excel&quot; into &quot;Then I created a chart using Excel&quot;</p>
<p>The problem is that text analysis tools first separate words into tokens (separate rows) based on where spaces occur with the assumption that a space denotes a new word.</p>
<pre><code>library(tidytext)
library(dplyr)

## Example dataframe of text strings with erroneous spaces within words
txt &lt;- structure(list(id = 1:3, 
                      data = c(&quot;Then I cre ated a c h a r t using Excel&quot;, 
                               &quot;other p r inc ip le is that when a person&quot;, 
                               &quot;M R . C O O K S O N : Mr . Speaker , on behal f of&quot;)), 
                 class = &quot;data.frame&quot;, 
                 row.names = c(NA, -3L))


## Unnest text data so each word becomes a separate row in the dataframe
result &lt;- tidytext::unnest_tokens(tbl = txt,
                        output = text, 
                        input = data, 
                        token = &quot;words&quot;, 
                        to_lower = F)

print(result)

   id    text
1   1    Then
2   1       I
3   1     cre
4   1    ated
5   1       a
6   1       c
7   1       h
8   1       a
9   1       r
10  1       t
11  1   using
12  1   Excel
...
</code></pre>
<p>Is there a way to collapse all words with extra whitespaces <em><strong>within</strong></em> them without collapsing white space that occurs <em><strong>between</strong></em> words? All I can think is some kind of function that would:</p>
<ul>
<li><p>find occurrences of single characters with whitespace on either side (besides &quot;a&quot; or &quot;I&quot;)</p>
</li>
<li><p>remove whitespace from either side of the character, but not if that space is between the character and a whole word (i.e., a string of two or more characters)</p>
</li>
</ul>
<p>Maybe something like: if you find a letter besides &quot;a&quot; or &quot;I&quot; with a space on both sides, which is followed by a single letter that is not &quot;a&quot; or &quot;I&quot; followed by a space, then remove that first space?</p>
<p>I tried:</p>
<pre><code>txt &lt;- structure(list(id = 1:3, 
                      data = c(&quot;Then I cre ated a c h a r t using Excel&quot;, 
                               &quot;other p r inc ip le is that when a person&quot;, 
                               &quot;M R . C O O K S O N : Mr . Speaker , on behal f of&quot;)), 
                 class = &quot;data.frame&quot;, 
                 row.names = c(NA, -3L))

df1 &lt;- txt %&gt;%
  mutate(revised = gsub(&quot;([A-Za-z])\\s(?=[A-Za-z]\\b)&quot;, &quot;\\1&quot;, data, perl = TRUE))

df2 &lt;- txt %&gt;%
  mutate(revised = gsub(&quot;(?&lt;=\\b\\w)\\s(?=\\w\\b)&quot;, &quot;&quot;, data, perl=T))
</code></pre>
","r, nlp",
Automatic Word Boundary Detection for German,"<p>I want to rephrase that: I need a corpus of German words so that I can check if a segment is a word. My solution so far is to take the string, check if it's in the dictionary and if not, delete the last character and check again, and so on. I just need a German word list now. Does anyone know something?</p>
<p>I have a bunch of German texts, but lost all whitespaces. Now I need to perform some kind of word boundary detection to get from &quot;NamensänderungimNamenderIntegration&quot; to [&quot;Namensänderung&quot;, &quot;im&quot;, &quot;Namen&quot;, &quot;der&quot;, &quot;Integration&quot;].</p>
<p>I found the python package wordsegment and it works okay, but not ideally. I also found the german_compound_splitter, but that would also split &quot;Namensänderung&quot; in &quot;Namens&quot; &quot;änderung&quot;. Does anyone have any experience with that or knows how I could build a solution?</p>
","python, nlp, linguistics, word-boundary",
How to assign topics to individual documents/ tweets in Bi-term Topic Modeling?,"<p>I am a newbie at this, so I apologize if I am asking the obvious here. I ran a bi-term topic modeling algorithm to model short text data and discover topics among them. I am using LDAvis package to visualize and understand the data. However, as I understand, reading the raw data qualitatively will help me understand what the underlying topics are and what people are talking about. A keyword search from the raw text data isn't helping since keywords typically overlap with several topics and selecting one keyword may lead to subset which may contain data related to several topics. To this end, I want to assign each tweet/ document to one topic to be able to analyze them manually and get what the topic is talking about since the customer feedback data that I am analyzing typically contains keywords that could indicate several things at once.</p>
<p>I tried assigning the max value from each row in the scores array generated by BTM package <a href=""https://cran.r-project.org/web/packages/BTM/BTM.pdf"" rel=""nofollow noreferrer"">scores array indicated here</a> as the topic that is most likely associated with each document, and that was syntaxically successful. However the generated topics didn't match visually from LDAvis. For instance, I could see a keyword &quot;apps&quot; as being present in only topic &quot;10&quot; as indicated by LDAvis but manually selecting the subset of data that contained keyword &quot;apps&quot; led to two tweets/ documents that were both assigned to different topics per scores array. Am I doing this correctly? I am currently writing the topics to a csv file since I am new to R and didn't want to figure out a way to add the topic to the original dataframe (named data) and then write that file to disk.</p>
<p>#Run bi-term topic modeling</p>
<pre><code>
set.seed(9082374)
model &lt;- BTM(x, k = 10, alpha = 0.1, beta = 0.01, iter = 2000, trace = 100, detailed=TRUE)

</code></pre>
<p>#Predict topic scores for new data
<code>scores &lt;- predict(model, x)</code></p>
<p>#assign topic to each score</p>
<pre><code>final.topic&lt;-apply(scores, 1, which.min)
fwrite(list(final.topic), file=&quot;topic_max.csv&quot;)

</code></pre>
<p>Here is the code if you want to see the code I am running. Specially, the LDAvis part since I did have some challenges in running LDAvis.</p>
<pre><code>term.table1 &lt;- table(x$text)

# Create JSON data for LDAvis
docsize &lt;- table(x$id)
scores &lt;- scores[names(docsize), ]
json &lt;- createJSON(
  phi = t(model$phi),
  theta = scores,
  doc.length = as.integer(docsize),
  vocab = as.character(rownames(model$phi)),
  term.frequency =  term.table1)
serVis(json)
library(data.table)
final.topic&lt;-apply(scores, 1, which.max)
fwrite(list(final.topic), file=&quot;topic_max.csv&quot;)
</code></pre>
","r, nlp, lda, topic-modeling, libshorttext",
How to remove layers in Huggingface&#39;s transformers GPT2 pre-trained models?,"<p>My code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2Config, GPT2Model
from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(&quot;openai-community/gpt2&quot;)
print(decoder)
</code></pre>
<p>Here is the output of the console, listing the model architecture:</p>
<pre><code>GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
</code></pre>
<p>I want to remove the first layer:</p>
<pre><code>(wte): Embedding(50257, 768)
</code></pre>
<p>I've tried the following way:</p>
<pre class=""lang-py prettyprint-override""><code>def deleteEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model
    oldModuleList = model.bert.encoder.layer
    newModuleList = nn.ModuleList()

    # Now iterate over all layers, only keepign only the relevant layers.
    for i in range(0, len(num_layers_to_keep)):
        newModuleList.append(oldModuleList[i])

    # create a copy of the model, modify it with the new list, and return
    copyOfModel = copy.deepcopy(model)
    copyOfModel.bert.encoder.layer = newModuleList

    return copyOfModel
</code></pre>
<p>But it didn't work. Who knows how to fix it?</p>
","python, machine-learning, deep-learning, nlp, transformer-model","<p>Try these parameters to bypass the embedding layer :</p>
<pre><code>class GPT2WithoutWTE(GPT2Model):
def __init__(self, config):
    super().__init__(config)
    # Remove the word token embedding layer
    del self.wte

def forward(
    self,
    inputs_embeds,
    attention_mask=None,
    token_type_ids=None,
    position_ids=None,
    head_mask=None,
    inputs=None,
    encoder_hidden_states=None,
    encoder_attention_mask=None,
    past_key_values=None,
    use_cache=None,
    output_attentions=None,
    output_hidden_states=None,
    return_dict=None,
):
    # here you will bypass the embedding layer and use inputs_embeds directly
    return super().forward(
        inputs_embeds=inputs_embeds,
        attention_mask=attention_mask,
        token_type_ids=token_type_ids,
        position_ids=position_ids,
        head_mask=head_mask,
        encoder_hidden_states=encoder_hidden_states,
        encoder_attention_mask=encoder_attention_mask,
        past_key_values=past_key_values,
        use_cache=use_cache,
        output_attentions=output_attentions,
        output_hidden_states=output_hidden_states,
        return_dict=return_dict,
    )
</code></pre>
<p>for the input embeddings you can use the following</p>
<pre><code>inputs_embeds = torch.rand(1, 10, config.n_embd)
</code></pre>
<p>Load the config of GPT2, send it to the class and then use the inputs_embeds for the new model.</p>
"
Swift Natural Language and CoreML: How to improve NLTagger to read Card Holder,"<p>I am using Natural Language framework to find personal name on credit card. 
Firstly I read credit card texts using Vision framework. Then I concatenate it. </p>

<p>So I have text that contains format similar to this: </p>

<pre><code>""Citi 6011 1111 1111 1117 07/25 ELON MUSK Discover Debit""
</code></pre>

<p>I have tired to simply find .personalName NLTags in such string
But it doesn't work perfect. </p>

<pre><code>let tagger = NLTagger(tagSchemes: [.nameType])
    tagger.string = text
    tagger.setLanguage(.english, range: range)

    var fullNames = [String]()

    // 1) personalName
    let options : NLTagger.Options = [.omitPunctuation, .omitWhitespace, .omitOther, .joinNames]
    let foundTags = tagger.tags(in: range, unit: .word, scheme: .nameType, options: options)

    foundTags.forEach { (tag, tokenRange) in
        if tag == .personalName {
            let name = text[tokenRange]
            fullNames.append(String(name))
        }
    }
</code></pre>

<p>So I am trying to use CreateML in macOS playeground to create CoreML model that will be used to tag custom CARDHOLDER tags in such string. </p>

<p><strong>How many example labeled data there is needed to train such model to be usable?</strong></p>

<p>Now I have such relatively simple case </p>

<pre><code>[
    {
        ""tokens"": [""Inteligo"", ""4242 4242 4242 4242"", ""09/21"", ""JOHN SMITH"", ""VISA"", ""Debit""],
        ""labels"": [""ORG"", ""NONE"", ""NONE"", ""CARDHOLDER"", ""ORG"", ""NONE""]
    },
    {
        ""tokens"": [""mBank"", ""4000 0566 5566 5556"", ""10/23"", ""STEVE JOBS"", ""VISA"", ""Debit""],
        ""labels"": [""ORG"", ""NONE"", ""NONE"", ""CARDHOLDER"", ""ORG"", ""NONE""]
    },
    {
        ""tokens"": [""ING"", ""5555 5555 5555 4444"", ""03/22"", ""EMMA WATSON"", ""mastercard"", ""Debit""],
        ""labels"": [""ORG"", ""NONE"", ""NONE"", ""CARDHOLDER"", ""ORG"", ""NONE""]
    },
    {
        ""tokens"": [""Bank of America"", ""3782 822463 10005"", ""05/24"", ""JULIA ROBERTS"", ""American Express"", ""Debit""],
        ""labels"": [""ORG"", ""NONE"", ""NONE"", ""CARDHOLDER"", ""ORG"", ""NONE""]
    },
    {
        ""tokens"": [""Citi"", ""6011 1111 1111 1117"", ""07/25"", ""ELON MUSK"", ""Discover"", ""Debit""],
        ""labels"": [""ORG"", ""NONE"", ""NONE"", ""CARDHOLDER"", ""ORG"", ""NONE""]
    }
]
</code></pre>

<p>Then I create CoreML model from this </p>

<pre><code>import Foundation
import CreateML

let trainingData = try MLDataTable(contentsOf:
    Bundle.main.url(forResource: ""data"", withExtension: ""json"")!)

let model = try MLWordTagger(trainingData: trainingData, tokenColumn: ""tokens"", labelColumn: ""labels"")

try model.write(to: URL(fileURLWithPath: ""#path/to/Desktop/creditcardtagger.mlmodel""))
</code></pre>

<p>Then I use it: </p>

<pre><code>let tagger = CreditCardTagger.shared
        tagger.string = text
        tagger.setLanguage(.english, range: range)
tagger.enumerateTags(in: range, unit: .word, scheme: CreditCardTagger.Scheme, options: options) { (tag, tokenRange) -&gt; Bool in

            if tag == CreditCardTagger.cardHolderTag {
                let cardHolder = text[tokenRange]
                print(""CARD HOLDER: \(cardHolder)"")
            }
            return true
        }
</code></pre>

<p>But I think that data I am using for training are insufficient. Do you know how much such records of data is needed to cover most of credit card cases?</p>
","swift, nlp, coreml, createml",
Different embedding checksums after encoding with SentenceTransformers?,"<p>I am calculating some embeddings with SentenceTransformers Library. However, I get different results when encoding the sentences and calculating their embeddings when checking the sum of their values. For instance:</p>
<p>In:</p>
<pre><code>
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)


transformer_models = [
    'M-CLIP/M-BERT-Distil-40', 
                     ]

sentences = df['content'].tolist()


for transformer_model in tqdm(transformer_models, desc=&quot;Transformer Models&quot;):
    tqdm.write(f&quot;Processing with Transformer Model: {transformer_model}&quot;)
    model = SentenceTransformer(transformer_model)
    embeddings = model.encode(sentences)
    print(f&quot;Embeddings Checksum for {transformer_model}:&quot;, np.sum(embeddings))
</code></pre>
<p>Out:</p>
<pre><code>Embeddings Checksum for M-CLIP/M-BERT-Distil-40: 1105.9185
</code></pre>
<p>Or</p>
<pre><code>Embeddings Checksum for M-CLIP/M-BERT-Distil-40: 1113.5422
</code></pre>
<p>I noticed this situation happens when I restart and clear the output of the jupyter notebook, and then re-run the full notebook. Any idea of how to fix this issue?</p>
<p>Alternative I tried to set after and before the embeddings calculation the reandom seeds:</p>
<pre><code>import torch
import numpy as np
import random
import tensorflow as tf
from sentence_transformers import SentenceTransformer
from tqdm.auto import tqdm

RANDOM_SEED = 42

# Setting seeds
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

# Ensuring PyTorch determinism
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

transformer_models = ['M-CLIP/M-BERT-Distil-40']

sentences = df['content'].tolist()

for transformer_model in tqdm(transformer_models, desc=&quot;Transformer Models&quot;):
    # Set the seed again right before loading the model
    np.random.seed(RANDOM_SEED)
    random.seed(RANDOM_SEED)
    tf.random.set_seed(RANDOM_SEED)
    torch.manual_seed(RANDOM_SEED)

    tqdm.write(f&quot;Processing with Transformer Model: {transformer_model}&quot;)
    model = SentenceTransformer(transformer_model, device='cpu')  # Force to use CPU

    embeddings = model.encode(sentences, show_progress_bar=False)  # Disable progress bar and parallel tokenization
    print(f&quot;Embeddings Checksum for {transformer_model}:&quot;, np.sum(embeddings))
</code></pre>
<p>However I am getting the same inconsistent behavior.</p>
<p><strong>UPDATE</strong></p>
<p>What I tried now, and seem to work is that now I store all the calculated embeddings in files. However, I find weird that when doing this I get different results. Does anyone has experience this before?</p>
","python, jupyter-notebook, nlp, sentence-transformers",
NER grouping into objects,"<p>Given a text, I am able to recognise all entities by using a discriminative NER model. Let us imagine that we have a page of text in which the different entities that make up a group of people are described and for each of them we have data such as 'first name', 'surname' and 'date of birth'. The NER model is able to find all these entities but is unable to group them into the 'person' object. How could I solve this NLP task?</p>
<p>I have tried to group these entities based on the span in the original text using heuristics. but this requires a lot of effort for more complex cases.
Also, if these data are extracted from different pages?</p>
","nlp, entity-relationship, named-entity-recognition, large-language-model, entity-linking",
Very long training times in pyTorch compared to Gensim,"<p>I have trained a word2vec model using the Brown corpus with gensim as follows:</p>
<pre><code>model = gensim.models.Word2Vec(brown.sents(),min_count = 5,
                              vector_size = 30, window = 5, negative=5) 
</code></pre>
<p>This took just a couple of seconds to train.</p>
<p>Then I tried to build a word2vec model using pyTorch. The training takes more than 5 hours according to my estimation.</p>
<p>I was expecting a speed difference since Gensim's word2vec uses optimized C routines. However, I was not expecting this much difference.</p>
<p>Is it normal for the training to take this much time with pyTorch? For both cases I'm not using any GPU's. What should I expect the running time of PyTorch for this task on a standard 4-cpu Mac?</p>
","pytorch, nlp, gensim, word2vec",
How to calculate cosine similarity with bert over 1000 random example,"<p>I'm trying to calculate over random <strong>1000</strong> quest and <strong>1000</strong> answer using <strong>cosine similarity</strong> with <strong>bert-base-uncased</strong>, and after I want to find most similar 5 asnwer, after calculate top1 and top5 real answer accuracy. But im receiving output always <strong>0.0</strong> accuracy and answers not similar.</p>
<pre><code>sample_1000_quest = train_ds['questions'].sample(1000)
sample_1000_answer = train_ds['answers'].sample(1000)

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)


tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')
model_bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True).eval()


selected_question = sample_1000_quest.iloc[1]

selected_question_idx = sample_1000_quest.index.get_loc(30574)


encoded_question = tokenizer_bert(selected_question, return_tensors='pt', padding=True, truncation=True)


with torch.no_grad():
    outputs = model_bert(**encoded_question)
    question_embedding = outputs.last_hidden_state.mean(dim=1)


encoded_answers = []
answer_embeddings = []
for answer in sample_1000_answer:
    encoded_answer = tokenizer_bert(answer, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        outputs = model_bert(**encoded_answer.to(device))
        answer_embedding = outputs.last_hidden_state.mean(dim=1)
        answer_embeddings.append(answer_embedding)


similarities = []
for answer_embedding in answer_embeddings:
    similarity = cosine_similarity(question_embedding, answer_embedding)
    similarities.append(similarity.item())


most_similar_indices = np.argsort(similarities)[-5:][::-1]


ground_truth_idx = train_ds['answers'].iloc[selected_question_idx]


top1_accuracies = []
top5_accuracies = []

top1_idx = most_similar_indices[0]
top1_accuracy = 1 if top1_idx == ground_truth_idx else 0
top5_accuracy = 1 if ground_truth_idx in most_similar_indices else 0

top1_accuracies.append(top1_accuracy)
top5_accuracies.append(top5_accuracy)

print(&quot;Selected Question:&quot;, selected_question)
print(&quot;Most similar 5 asnwer:&quot;)
for i, idx in enumerate(most_similar_indices):
    print(f&quot;{i+1}. {sample_1000_answer.iloc[idx]}&quot;)

print(&quot;Top-1 Accuracy:&quot;, top1_accuracy)
print(&quot;Top-5 Accuracy:&quot;, top5_accuracy)
</code></pre>
<p>Output:</p>
<pre><code>Selected Question:  bir sunum oluşturmak için beş adım yazın.
Most similar 5 asnwer:
1.  doğum günü gülüm bütün yaz aldığım en güzel hediyeydi.
2.  bu deneyin amacı ilkeleri anlamaktır.
3.  bir satış elemanı sunum yapıyor.
4.  hangi konuda yardıma ihtiyacın olduğunu söyle.
5.  konuşmanın içeriği, projede bir sonraki adım için onay almakla ilgilidir.
Top-1 Accuracy: 0
Top-5 Accuracy: 0
</code></pre>
","python, nlp, bert-language-model, cosine-similarity",
In which data type text data extracted from news site should be stored for NLP?,"<p>I extracted following text data using <code>beautifulsoup</code></p>
<pre><code>for data in soup2.find_all(class_=&quot;td-post-content&quot;):
         data.get_text()
</code></pre>
<p>In which type of data type should I store above extracted data?</p>
<p>I stored above extracted data into the list but cant perform text analysis
it does not contain any headline etc..</p>
","python, web-scraping, beautifulsoup, nlp",
How to get probability of prediction per entity from Spacy NER model?,"<p>I used this <a href=""https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py"" rel=""noreferrer"">official example code</a> to train a NER model from scratch using my own training samples. </p>

<p>When I predict using this model on new text, I want to get the probability of prediction of each entity.</p>

<blockquote>
<pre><code>    # test the saved model
    print(""Loading from"", output_dir)
    nlp2 = spacy.load(output_dir)
    for text, _ in TRAIN_DATA:
        doc = nlp2(text)
        print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
        print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])
</code></pre>
</blockquote>

<p>I am unable to find a method in Spacy to get the probability of prediction of each entity.</p>

<p>How do I get this probability from Spacy? I need it to apply a cutoff on it.</p>
","python, deep-learning, nlp, spacy, named-entity-recognition",
How does one use accelerate with the hugging face (HF) trainer?,"<p>What are the code changes one has to do to run accelerate with a trianer?
I keep seeing:</p>
<pre><code>from accelerate import Accelerator

accelerator = Accelerator()

model, optimizer, training_dataloader, scheduler = accelerator.prepare(
    model, optimizer, training_dataloader, scheduler
)

for batch in training_dataloader:
    optimizer.zero_grad()
    inputs, targets = batch
    outputs = model(inputs)
    loss = loss_function(outputs, targets)
    accelerator.backward(loss)
    optimizer.step()
    scheduler.step()
</code></pre>
<p>but when I tried the analogous thing it didn't work:</p>
<pre><code>!pip
install
accelerate
!pip
install
datasets
!pip
install
transformers

# %%
from accelerate import Accelerator
from datasets import load_dataset
from transformers import GPT2LMHeadModel, GPT2TokenizerFast, TrainingArguments, Trainer

# Initialize accelerator
accelerator = Accelerator()

# Specify dataset
dataset = load_dataset('imdb')

# Specify tokenizer and model
tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.to(accelerator.device)


# Tokenize and format dataset
def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], truncation=True, max_length=512)


tokenized_datasets = dataset.map(
    tokenize_function,
    batched=True,
    num_proc=accelerator.num_processes,
    remove_columns=[&quot;text&quot;]
)

# Training configuration
training_args = TrainingArguments(
    output_dir=&quot;output&quot;,
    overwrite_output_dir=True,
    # num_train_epochs=3,
    max_steps=10,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=2,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=True,
    fp16=False,  # Set to True for mixed precision training (FP16)
    fp16_full_eval=False,  # Set to True for mixed precision evaluation (FP16)
    dataloader_num_workers=accelerator.num_processes,  # Use multiple processes for data loading
)

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets[&quot;train&quot;],
    eval_dataset=tokenized_datasets[&quot;test&quot;],
    tokenizer=tokenizer,
)

# Train model
trainer.train()

</code></pre>
<p>why?</p>
<p>related:</p>
<ul>
<li><a href=""https://discuss.huggingface.co/t/trainer-and-accelerate/26382/5"" rel=""noreferrer"">https://discuss.huggingface.co/t/trainer-and-accelerate/26382/5</a></li>
</ul>
","pytorch, nlp, huggingface-transformers, huggingface, accelerate",
How can i get the first content of a python synsets list?,"<p><a href=""https://i.sstatic.net/gN3qc.png"" rel=""nofollow noreferrer"">enter image description here</a>I have a scrapped text stored under the variable &quot;message&quot;.
I have removed the StopWords and stored the result with the variable &quot;without_stop_words&quot;.
I want to loop through each words in the 'without_stop_words' and get their MEANINGS AND PRONOUNS.</p>
<p>Currently i am trying to get the MEANINGS but I'm getting an error: &quot;IndexError: list index out of range&quot;</p>
<p><a href=""https://i.sstatic.net/V55ve.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<pre><code>   
 for writeup in writeups:
        message = writeup.text
        #Stop Words
        stop_words = set(stopwords.words('english'))
        #print(stop_words)

        tokenized_words = word_tokenize(message)
        #Filtering Stop Words
        without_stop_words = []
        for word in tokenized_words:
            if word not in stop_words:
                without_stop_words.append(word)            
                #Word Meanings
        word_meanings = []
        for each_word in without_stop_words:
            sync_words = wordnet.synsets(each_word)
            meaning = sync_words[0].definition()
            print(meaning)

</code></pre>
<p>I want to get the MEANING of each word in the &quot;without_stop_words&quot;.</p>
","python, nlp, nltk, sentiment-analysis, synset","<p>The error comes from this line</p>
<pre><code>meaning = sync_words[0].definition()
</code></pre>
<p>And it indicates that <code>sync_words</code> is empty</p>
<pre><code>for each_word in without_stop_words:
    sync_words = wordnet.synsets(each_word)
    if sync_words:
        meaning = sync_words[0].definition()
        word_meanings.append(meaning)
    else:
        # Whatever you want to do if it's empty
</code></pre>
<p>This will stop the error, but you should try to find out why <code>sync_words</code> is empty in the first place.</p>
"
How do non-LLM models compare to LLMs for Abstractive Summaries of HTML content?,"<p>I'm interested in utilizing an NLP model to provide short (one sentence in length) abstractive summaries of web pages, providing the model a set of commonly occurring HTML content from each web page (for example, heading tags, meta, the title, and so on).</p>
<p>In my experience, large language models could do this quite effectively. However, I'm interested in alternatives to LLMs -- how they perform, how costly in terms of computational resources they are compared to LLMs, and what degree of setup might be required for them compared to the comparatively simple prompt-and-respond format of commercial LLMs.</p>
","html, nlp, summarization",
Subsampling when training word embeddings,"<p>NLP newbie here with a question about word embeddings. As a learning exercise, I'm trying to train my own set of word embeddings based on word2vec. I have a corpus of english sentences that I've downloaded and cleaned and I think I have a decent grasp of how the training is supposed to work, but there's something I still don't really understand.</p>
<p>As one might imagine, the corpus contains many more instances of common words like 'the', 'and', and so on. The word frequency distribution is a fairly extreme power law, which makes sense. My question is this: what are the best practices to deal with this when I'm generating samples to train the word embeddings?</p>
<p>I can see a few of options:</p>
<ol>
<li>When I'm generating training samples, do some sort of probabilistic sampling based on the frequency of the input token in the dataset. My newbie intuition is that this makes some sense, but I'm not 100% sure how the sampling should work.</li>
<li>With some probability, drop the most common words from the vocabulary altogether and don't learn embeddings for them at all. I've seen some guidance on the web (and in <a href=""https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf"" rel=""nofollow noreferrer"">the original word2vec paper</a>) that recommends doing this and just treating them as an OOV token when looking up the embedding, but it just feels ... weird. After all, I do want to have an embedding for the word 'the', even if it appears very frequently.</li>
<li>Just power through and live with the fact that I'm going to have a lot more training samples for the word 'the' than the word 'persnickety'. This will make a training epoch take a lot longer.</li>
</ol>
<p>Can anyone give me some guidance here? How do people usually deal with this kind of imbalance?</p>
","pytorch, nlp, word-embedding, torchtext",
ResourceExhaustedError In Tensorflow BERT Classifier,"<p>I am trying to use the BertClassifier from the keras_nlp library but when I train the model I get this error:</p>
<pre><code>2024-03-22 22:53:03.932926: W external/local_tsl/tsl/framework/bfc_allocator.cc:487] Allocator (GPU_0_bfc) ran out of memory trying to allocate 192.00MiB (rounded to 201326592)requested by op StatelessRandomUniformV2
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2024-03-22 22:53:03.933015: I external/local_tsl/tsl/framework/bfc_allocator.cc:1044] BFCAllocator dump for GPU_0_bfc
2024-03-22 22:53:03.933046: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (256):    Total Chunks: 74, Chunks in use: 74. 18.5KiB allocated for chunks. 18.5KiB in use in bin. 549B client-requested in use in bin.
2024-03-22 22:53:03.933066: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (512):    Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-03-22 22:53:03.933086: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (1024):   Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2024-03-22 22:53:03.933105: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (2048):   Total Chunks: 111, Chunks in use: 111. 333.2KiB allocated for chunks. 333.2KiB in use in bin. 333.0KiB client-requested in use in bin.
2024-03-22 22:53:03.933121: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (4096):   Total Chunks: 1, Chunks in use: 1. 6.0KiB allocated for chunks. 6.0KiB in use in bin. 6.0KiB client-requested in use in bin.
...
3.30GiB allocated for chunks. 3.30GiB in use in bin. 3.19GiB client-requested in use in bin.
2024-03-22 22:53:03.933449: I external/local_tsl/tsl/framework/bfc_allocator.cc:1067] Bin for 192.00MiB was 128.00MiB, Chunk State: 
2024-03-22 22:53:03.933468: I external/local_tsl/tsl/framework/bfc_allocator.cc:1080] Next region of size 10344333312
2024-03-22 22:53:03.933495: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 78f6b2000000 of size 256 next 1
2024-03-22 22:53:03.933513: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 78f6b2000100 of size 1280 next 2
2024-03-22 22:53:03.933533: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 78f6b2000600 of size 256 next 3
...
---------------------------------------------------------------------------
ResourceExhaustedError                    Traceback (most recent call last)
Cell In[38], line 1
----&gt; 1 classifer.fit(X_train, y_train, epochs=1, batch_size=128)

File /usr/local/lib/python3.11/dist-packages/keras_nlp/src/utils/pipeline_model.py:188, in PipelineModel.fit(self, x, y, batch_size, sample_weight, validation_data, validation_split, **kwargs)
    181         (vx, vy, vsw) = keras.utils.unpack_x_y_sample_weight(
    182             validation_data
    183         )
    184         validation_data = _convert_inputs_to_dataset(
    185             vx, vy, vsw, batch_size
    186         )
--&gt; 188 return super().fit(
    189     x=x,
    190     y=None,
    191     batch_size=None,
    192     sample_weight=None,
    193     validation_data=validation_data,
    194     **kwargs,
    195 )

File /usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:123, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    120     filtered_tb = _process_traceback_frames(e.__traceback__)
    121     # To get the full stack trace, call:
    122     # `keras.config.disable_traceback_filtering()`
--&gt; 123     raise e.with_traceback(filtered_tb) from None
    124 finally:
    125     del filtered_tb

File /usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:123, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    120     filtered_tb = _process_traceback_frames(e.__traceback__)
    121     # To get the full stack trace, call:
    122     # `keras.config.disable_traceback_filtering()`
--&gt; 123     raise e.with_traceback(filtered_tb) from None
    124 finally:
    125     del filtered_tb

ResourceExhaustedError: Exception encountered when calling Dropout.call().

{{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[128,512,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2] name: 

Arguments received by Dropout.call():
  • inputs=tf.Tensor(shape=(128, 512, 768), dtype=float32)
  • training=True
</code></pre>
<p>Here is the code I am running:</p>
<pre><code>import keras_nlp

classifer = keras_nlp.models.BertClassifier.from_preset(
    'bert_base_en_uncased',
    num_classes=num_classes,
    activation='softmax',
)

classifer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

import os
os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'

classifer.fit(X_train, y_train, epochs=1, batch_size=128)
</code></pre>
<p>I am using a docker container (tensorflow:latest-gpu-jupyter) and I have a Nvidia RTX 3060 and a Core i5 CPU with all the drivers updated.</p>
<p>I have tried adding the environment variable but that didn't work. I have tried reducing the batch size even all the way down to a batch size of 1.</p>
<p>Please answer if you have a solution or a suggestion. Anything will be appreciated.</p>
","python, tensorflow, keras, nlp, bert-language-model",
Output parser of langchain Agent,"<p>I am initializing a langchain agent as:</p>
<pre><code>agent_output_parser=AgentOutputParser()
     self.mrkl = initialize_agent(
                tools, 
                llm,
                output_parser= agent_output_parser,
                agent_executor_kwargs={ &quot;output_parser&quot;: agent_output_parser}
            )
</code></pre>
<p>I have also created an AgentParser subclass as:</p>
<pre><code>class AgentParser(BaseLLMOutputParser):

    def __init__(self):
        super().__init__()
      
    
    def parse(self, text):
        print(text)
        return text
      

    def parse_result(self, output):
        print(output)
        return output
</code></pre>
<p>However, using print I would expect to see the output of the agent parsed and somehow manipulate it.<br />
In my current code I can't seem to handle the output of the agent and the prints i wrote never happen.<br />
Is there an alternative way to pass a custom output parser?</p>
","python, python-3.x, nlp, artificial-intelligence, langchain",
How to prevent DataCollatorForLanguageModelling from using input_ids as labels in CLM tasks?,"<p>How to instruct<code>DataCollatorForLanguageModeling</code> to not use shifted inputs as labels but my own labels?</p>
<p>Here's a MWE:</p>
<pre><code>data = {
'sources': [&quot;This is some text&quot;, &quot;Another text athta ljdlsfjsdlf&quot;, &quot;Also some bulshit type text who knows wtf?&quot;],
'targets': [&quot;Some potential target.&quot;, &quot;The answer is JoLo!&quot;, &quot;Who killed margaret and what was the motive and poential causes!&quot;]
}

tokenizer = AutoTokenizer.from_pretrained(&quot;openai-community/gpt2&quot;)
config = AutoConfig.from_pretrained(&quot;openai-community/gpt2&quot;)
gpt2model = AutoModelForCausalLM.from_config(config)
tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

&gt;&gt; &quot;Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained&quot;


tokenized_data = tokenizer(data['sources'])
with tokenizer.as_target_tokenizer():
    tokenized_data['labels'] = tokenizer(data['targets'])

&gt;&gt; tokenized_data
{'input_ids': [[1212, 318, 617, 2420], [6610, 2420, 379, 4352, 64, 300, 73, 67, 7278, 69, 8457, 67, 1652], [7583, 617, 4807, 16211, 2099, 2420, 508, 4206, 266, 27110, 30]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],
'labels': {'input_ids': [[4366, 2785, 2496, 13], [464, 3280, 318, 5302, 27654, 0], [8241, 2923, 6145, 8984, 290, 644, 373, 262, 20289, 290, 745, 1843, 5640, 0]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}}


tokenized_labels = tokenized_data.pop('labels')

outputs = data_collator(tokenized_data)

&gt;&gt; outputs
{'input_ids': tensor([[ 1212,   318,   617,  2420, 50257, 50257, 50257, 50257, 50257, 50257,
         50257, 50257, 50257],
        [ 6610,  2420,   379,  4352,    64,   300,    73,    67,  7278,    69,
          8457,    67,  1652],
        [ 7583,   617,  4807, 16211,  2099,  2420,   508,  4206,   266, 27110,
            30, 50257, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]), 'labels': tensor([[ 1212,   318,   617,  2420,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100],
        [ 6610,  2420,   379,  4352,    64,   300,    73,    67,  7278,    69,
          8457,    67,  1652],
        [ 7583,   617,  4807, 16211,  2099,  2420,   508,  4206,   266, 27110,
            30,  -100,  -100]])}

</code></pre>
<p>Now the <code>outputs['labels']</code> are just the shifted <code>outputs['input_ids']</code> which is happening automatically from the <code>DataColaltorForLanguageModeling</code>.</p>
<p>The question is since I do have proper labels for the data, in this case <code>tokenized_data['labels']</code> or the variable <code>tokenized_labels</code>, how do I use that in the Trainer class?</p>
<p>So, the <code>dataset.map(...)</code> will tokenize the whole dataset and will return tokens for both <code>text</code> and <code>labels</code>.</p>
<p>Then using <code>data_collator</code> will create <code>labels</code> by shifting the <code>input_ids</code> and feed that to the model.</p>
<p>How do I tell <code>Trainer</code> or <code>DataCollator</code> to use my <code>tokenized_labels</code> instead of creating them based on the inputs?</p>
","nlp, huggingface-transformers, huggingface, huggingface-trainer",
Non linear programming float decision variable in CPLEX,"<p>I m working in a <em>non linear programming</em> and got some problems that explain me that we can't use float decision variable in CPLEX when using solver <strong>CP</strong>. Help me, if there is some approach or other methods to solve this problem. Thanks</p>
<p>I had just try to use CP in CPLEX studio:</p>
","nlp, mathematical-optimization",
is there any way to use RL for decoder only models,"<p>Could you please assist me in finding resources related to training models like BERT using RLHF? Additionally, I'm curious about the scarcity of research on applying reinforcement learning to decoder-only models. Are there any specific challenges or issues associated with this area that limit the research? Any guidance or insights would be greatly appreciated. Thank you!</p>
<p>while exploring trl library i found this issue (<a href=""https://github.com/huggingface/trl/issues/747"" rel=""nofollow noreferrer"">https://github.com/huggingface/trl/issues/747</a>)
so have brainstorming a lot about how to define the trajectories for this</p>
","nlp, transform, huggingface-transformers, reinforcement-learning, transformer-model",
Can I monitor progress of spacy parsing?,"<p>I have a simple program to process English text with spacy and output some of the info about the tokens. For a big text it takes a long time for spacy to process it. Is there a way to see how far the processing has progressed ideally as a percentage? I'm not using my own models, just ones provided by spacy.</p>
<pre class=""lang-py prettyprint-override""><code>import spacy

// load big text file into `text` variable

nlp = spacy.load(&quot;en_core_web_sm&quot;)
nlp.max_length = len(text)+1
doc = nlp(text)

// output info
</code></pre>
","python, nlp, monitoring, spacy, spacy-3",
Extracting full-text research papers,"<p>I am working on an NLP project which deals with full-text research papers. I have a list of DOIs and I want to store all the text of these research papers in one .txt file. Currently, I am downloading the pdfs from scihub, and then extracting text from these pdfs. But, this is very slow, especially when I have a lot of papers. Are there better alternatives?</p>
<p>At a high-level, this is how I get the text for one paper:</p>
<pre><code>!python -m PyPaperBot --doi=&quot;10.2196/29324&quot; --dwn-dir=&quot;C:\&quot;
</code></pre>
<p>and then</p>
<pre><code>with fitz.open(&quot;sample.pdf&quot;) as doc:
    text = &quot;&quot;
    for page in doc:
        text += page.get_text()
</code></pre>
<p>I took a look at some related questions (<a href=""https://stackoverflow.com/questions/30904755/extract-abstract-full-text-from-scientific-literature-given-doi-or-title"">Extract abstract / full text from scientific literature given DOI or Title</a>), but they seem very outdated. I've also looked into PubMedCentral, but it has a smaller database of research papers than sci-hub.</p>
","python, web-scraping, nlp",
Can I use LoRa and Prompt Tuning at the same time for text summarization with GPT?,"<p>LoRA is to insert and learn the rank composition matrix created by dimensionally reducing the weight matrix in the transformer. Prompt Tuning, on the other hand, typically uses a soft prompt that encodes the prompt within the model to learn, rather than a hard prompt that a person gives the task directly. Both are effective in lightening, especially prompt tuning, which is better than hard prompt use.</p>
<p>Both techniques can also be implemented using the peft module.</p>
<pre><code>from peft import get_peft_model, PeftModel, TaskType, LoraConfig, PromptTuningConfig, PromptTuningInit

for path,dirs,files in os.walk('/root/.cache/huggingface/hub/models--kakaobrain--kogpt'):
  for file in files:
    if file.endswith('tokenizer.json'):
      tokenizer_path = path
print(tokenizer_path)

prompt_config = PromptTuningConfig(
    task_type=TaskType.CAUSAL_LM,
    num_virtual_tokens=10,
    prompt_tuning_init=PromptTuningInit.TEXT,
    prompt_tuning_init_text=&quot;Read the following and summarize:&quot;,
    tokenizer_name_or_path=tokenizer_path
)

lora_config = LoraConfig(
    task_type = TaskType.CAUSAL_LM,
    r=8, lora_alpha=32, lora_dropout=0.1,
    target_modules = ['q_proj', 'v_proj'],
    # target_modules = r&quot;.*(q_proj|v_proj)&quot;,
)
</code></pre>
<p>However, the get_feft_model function receives only the model and one peft_config as parameters.</p>
<pre><code>peft_model = get_peft_model(base_model, prompt_config)
</code></pre>
<p>I want to use both techniques at the same time. How shall I do it?</p>
","nlp, huggingface-transformers, transformer-model, summarization, huggingface",
Error when I trying to run a trained ner model on local pc,"<pre><code>
import re
import pickle
import keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import TFSMLayer
import numpy as np

class CustomNonPaddingTokenLoss(keras.losses.Loss):
    def __init__(self, reduction=tf.keras.losses.Reduction.AUTO, name=&quot;custom_ner_loss&quot;):
        super().__init__(reduction=reduction, name=name)

    def call(self, y_true, y_pred):
        loss_fn = keras.losses.SparseCategoricalCrossentropy(
            from_logits=False, reduction=self.reduction
        )
        loss = loss_fn(y_true, y_pred)
        mask = tf.cast((y_true &gt; 0), dtype=tf.float32)
        loss = loss * mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)

def map_record_to_training_data(record):
    record = tf.strings.split(record, sep=&quot;\t&quot;)
    length = tf.strings.to_number(record[0], out_type=tf.int32)
    tokens = record[1 : length + 1]
    tags = record[length + 1 :]
    tags = tf.strings.to_number(tags, out_type=tf.int64)
    tags += 1
    return tokens, tags

def lookup(tokens):
    # Load the list from the file
    with open('./resources/vocabulary.pkl', 'rb') as f:
        loaded_list = pickle.load(f)
    # The StringLookup class will convert tokens to token IDs
    lookup_layer = keras.layers.StringLookup(vocabulary=loaded_list)

    # No need to lowercase Vietnamese characters
    return lookup_layer(tokens)

def format_datatype(data):
    tokens =  [re.sub(r'[;,]', '', d) for d in data.split(' ')]
    #default is 0, since is for prediction
    ner_tags = [0 for d in data.split(' ')]

    #tab to separate
    string_input = str(len(tokens))+ &quot;\t&quot;+ &quot;\t&quot;.join(tokens)+ &quot;\t&quot;+ &quot;\t&quot;.join(map(str, ner_tags))
    string_input = tf.data.Dataset.from_tensor_slices([string_input])

    
    finalize_input = (string_input.map(map_record_to_training_data)
                      .map(lambda x, y: (lookup(x),  y))
                      .padded_batch(1)
                      )

    return finalize_input

def prediction(data):
    # Register the custom loss function with TensorFlow
    tf.keras.utils.get_custom_objects()['CustomNonPaddingTokenLoss'] = CustomNonPaddingTokenLoss
    loaded_model = Sequential()
    loaded_model.add(TFSMLayer(&quot;./resources/ner_model&quot;, call_endpoint='serving_default'))


    all_predicted_tag_ids = []
    
    for x, _ in data:
        print(&quot;Input Tensor Info:&quot;)
        print(&quot;Data Type:&quot;, x.dtype)
        print(&quot;Shape:&quot;, x.shape)
        x = tf.cast(x, tf.int64)
        output = loaded_model(x, training=False)
        predictions = np.argmax(output, axis=-1)
        predictions = np.reshape(predictions, [-1])
        all_predicted_tag_ids.append(predictions)

    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)

    ner_labels = [&quot;[PAD]&quot;, &quot;N&quot;, &quot;M&quot;, &quot;other&quot;]
    mapping =  dict(zip(range(len(ner_labels)), ner_labels))
    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]

    return predicted_tags

sample_input = &quot;Hello world, my name is John, I live in New York, my birthday is 10/02/1990.&quot;

result = prediction(format_datatype(sample_input))
print(result)
</code></pre>
<p>I trained the model on google colab, and download the model and try to load in on my window pc.
*It work fine on google colab, even when I restart and clear the terminal and just run this block of code, there are no error at all.
When I move to pc, I found that I need to load the model with a lot of pre-step, unlike on google colab just a (model = load_model(&quot;xxx/xxx&quot;))</p>
<p>I am encountering an issue while using the <strong><code>TFSMLayer</code></strong> in TensorFlow, where I receive the following error message</p>
<p>-----------------------------------------</p>
<p>tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling TFSMLayer.call().</p>
<p>cannot compute __inference_signature_wrapper_1285 as input #0(zero-based) was expected to be a int64 tensor but is a float tensor [Op:__inference_signature_wrapper_1285]</p>
<p>-------------------------------------------</p>
<p>I'm trying to load a TensorFlow model that uses the <strong><code>TFSMLayer</code></strong> for inference, but it seems that the input tensor received by the <strong><code>TFSMLayer.call()</code></strong> method is of data type <strong><code>float32</code></strong>, while the model expects <strong><code>int64</code></strong>. This inconsistency leads to an error during computation.</p>
<p>I have already attempted to cast the input tensor to <strong><code>int64</code></strong> using <strong><code>tf.cast</code></strong> before passing it to the loaded model, but the error persists.</p>
<p>I'm uncertain why the input tensor's data type is being converted to <strong><code>float32</code></strong>, and how to resolve this issue. Any insights or suggestions on how to debug and resolve this problem would be greatly appreciated. Thank you!</p>
","python, tensorflow, keras, nlp, named-entity-recognition",
"Using a BERT Model, I keep getting the error: Op type not registered &#39;CaseFoldUTF8&#39; in binary running on MacBook-Pro-21.lan","<p>I am attempting to use a BERT model to help predict whether an update is urgent or not. I am able to tokenize the updates, with this output:</p>
<p>However, when I go to define the model, I keep getting <code>CASEFOLDUTF8</code> errors. All packages are updated, I've attempted in Docker and google colab, optimally running an applemacbook m2 chip.</p>
<blockquote>
<p>RuntimeError: Op type not registered 'CaseFoldUTF8' in binary running on MacBook-Pro-21.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib (e.g. <code>tf.contrib.resampler</code>), accessing should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.</p>
</blockquote>
<p>Updated tensorflow with same result. Restarted kernal, same result. Attempted in google colab, same result.</p>
","python, nlp, data-mining, bert-language-model",
Is there a way to save a pre-compiled AutoTokenizer?,"<p>Sometimes, we'll have to do something like this to extend a pre-trained tokenizer:</p>
<pre><code>from transformers import AutoTokenizer

from datasets import load_dataset


ds_de = load_dataset(&quot;mc4&quot;, 'de')
ds_fr = load_dataset(&quot;mc4&quot;, 'fr')

de_tokenizer = tokenizer.train_new_from_iterator(
    ds_de['text'],vocab_size=50_000
)

fr_tokenizer = tokenizer.train_new_from_iterator(
    ds_fr['text'],vocab_size=50_000
)

new_tokens_de = set(de_tokenizer.vocab).difference(tokenizer.vocab)
new_tokens_fr = set(fr_tokenizer.vocab).difference(tokenizer.vocab)
new_tokens = set(new_tokens_de).union(new_tokens_fr)


tokenizer = AutoTokenizer.from_pretrained(
    'moussaKam/frugalscore_tiny_bert-base_bert-score'
)

tokenizer.add_tokens(list(new_tokens))

tokenizer.save_pretrained('frugalscore_tiny_bert-de-fr')
</code></pre>
<p>And then when loading the tokenizer,</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(
  'frugalscore_tiny_bert-de-fr', local_files_only=True
)
</code></pre>
<p>It takes pretty long to load from <code>%%time</code> in a Jupyter cell:</p>
<pre><code>CPU times: user 34min 20s
Wall time: 34min 22s
</code></pre>
<p>I guess this is due to regex compilation for the added tokens which was also raised in <a href=""https://github.com/huggingface/tokenizers/issues/914"" rel=""noreferrer"">https://github.com/huggingface/tokenizers/issues/914</a></p>
<p>I think it's okay since it'll load once and the work can be done without redoing the regex compiles.</p>
<h3>But, is there a way to just save the tokenizer in binary form and avoid the whole regex compilation the next time?</h3>
","python, nlp, tokenize, huggingface, huggingface-tokenizers",
"Adapters after QLoRA fine-tuning on a llama architecture model reach about 2 GB, which is very far from the general trend seen online","<p>I was Fine Tuning a Llama Architecture Model that supports multiple languages: English, Hindi as well as Roman Hindi.
So, I loaded the model in quantized form using bitsandbytes in nf4 form along with double quantization.</p>
<pre><code>bnb_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.bfloat16,
)

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config = bnb_config,
    device_map = {&quot;&quot;:0},
    trust_remote_code = True,
)
</code></pre>
<p>I was fine tuning this model for chat format. So, I added a  token to the tokenizer for this model, added the  token to the model config and resized the model embeddings according to the final length of the tokenizer.</p>
<pre><code>**# Adding a new padding token to the tokenizer even though one is already present in the tokenizer [PAD]. It would be very helpful if someone can point out the reason for this.**

if '&lt;pad&gt;' not in tokenizer.get_vocab():
    print(&quot;Token Added&quot;)
    # Add the pad token
    tokenizer.add_tokens(['&lt;pad&gt;'])

# Setting the pad token
tokenizer.pad_token = '&lt;pad&gt;'

voca = tokenizer.get_vocab()

# Resize Token Embeddings
model.resize_token_embeddings(len(tokenizer))

# Updating pad token id in model and its config
model.pad_token_id = tokenizer.pad_token_id
model.config.pad_token_id = tokenizer.pad_token_id

assert model.pad_token_id == tokenizer.pad_token_id, &quot;The model's pad token ID does not match the tokenizer' pad token ID&quot;


model = get_peft_model(model, config)
print_trainable_parameters(model)
</code></pre>
<p>Then I set the LoRA Adapters for fine tuning:</p>
<pre><code>r = 16,
lora_rank = 32,
target_modules = [&quot;q_proj&quot;, &quot;v_proj&quot;, &quot;down_proj&quot;, &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;k_proj&quot;],
lora_dropout = 0.05,
task_type = &quot;CAUSAL_LM&quot;,

model = get_peft_model(model, config)

trainable params: 35782656 || all params: 3667800064 || trainable%: 0.9755890554453079
</code></pre>
<p>Now, the vocab size of the model is 48065. After fine tuning the model with about 80 examples in Hindi, Roman Hindi and some English prompts.</p>
<p>Traing Arguments:</p>
<pre><code>args = transformers.TrainingArguments(
        num_train_epochs=1,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=2,
        max_grad_norm=1,
        warmup_ratio=0.1,
        learning_rate=1e-4,
        fp16 = True,
        logging_steps = 1,
        output_dir = &quot;outputs&quot;,
        optim = &quot;paged_adamw_8bit&quot;,
        lr_scheduler_type = 'cosine',
    ),
    data_collator=data_collator,
)

model.config.use_cache = False
</code></pre>
<p>Then after training this model I am pushing this model onto huggingface using model.push_to_hub(repo_id = rep_id) # <strong>This is the adapter and not the merged model, right?</strong></p>
<ul>
<li>This model is 1.72 GB which is not the case anywhere online, Is it ok or what might be the issue?</li>
<li>When wanting to merge this adapter with the base model I get CUDA out of Memory for Turing T4 GPU, which is understandable.</li>
<li>So, I save the adapter and then load model again and resize the embedding for this loaded model and then add the adapters to this model for inference, but still I am not able to do inference due to the huge size of the model with added adapters now because of large adapter file.</li>
<li>How do I resolve this issue of large adapter file, and combine the base model with the adapter?</li>
<li>Also, when trying to load the saved adapter from local using:</li>
</ul>
<pre><code>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available else &quot;cpu&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;path_to_adapter_file&quot;, local_files_only = True, device_map = device)
</code></pre>
<p>The CUDA Memory gets full 16GB, I dont know the reason, even though the adapter file is 1.72 GB???</p>
<p>Am I making any mistake? Is there any solution to this? Thanks</p>
","nlp, chatbot, fine-tuning, quantization-aware-training",
"Kaggle&#39;s packages missing many essential methods, for instance Kaggle&#39;s `Dataset` class has no `from_generator()` method","<p>I have been working on a particular NLP project for a month and have been running into error after error. I built a small model on my potato PC and it works perfectly. I upscaled it to Kaggle and ran into multiple errors, which have frustrated the hell out of me! I've been exploring all individual packages, and lo and behold, many methods are missing from Kaggle packages!</p>
<p>A perfect example is the <code>Dataset</code> class from the <code>datasets</code> package: the one in my PC and on Kaggle are version 2.17.1</p>
<p>But the class in Kaggle is missing so many essential methods, such as <code>from_generator()</code>! You can see for yourself, just install the <code>datasets</code> package, then do the following on your local machine and on Kaggle and note the differences:</p>
<pre><code>from datasets import Dataset 
dir(Dataset)
</code></pre>
<p>This is what led to most of my errors. How and why is this happening? Is there a way to <em>enable</em> all the essential methods on Kaggle, like <code>from_generator()</code>?</p>
","python, nlp, kaggle",
TfidfVectorizer in sklearn how to specifically INCLUDE words,"<p>I have some questions about the <code>TfidfVectorizer</code>.</p>

<p>It is unclear to me how the words are selected. We can give a minimum support, but after that, what will decide which features will be selected (e.g. higher support more chance)? If we say <code>max_features = 10000</code>, do we always get the same? If we say <code>max_features = 12000</code>, will we get the same <code>10000</code> features, but an extra added <code>2000</code>? </p>

<p>Also, is there a way to extend the, say, <code>max_features=20000</code> features? I fit it on some text, but I know of some words that should be included for sure, and also some emoticons "":-)"" etc. How to add these to the <code>TfidfVectorizer</code> object, so that it will be possible to use the object, use it to <code>fit</code> and <code>predict</code></p>

<pre><code>to_include = ["":-)"", "":-P""]
method = TfidfVectorizer(max_features=20000, ngram_range=(1, 3),
                      # I know stopwords, but how about include words?
                      stop_words=test.stoplist[:100], 
                      # include words ??
                      analyzer='word',
                      min_df=5)
method.fit(traindata)
</code></pre>

<h2>Sought result:</h2>

<pre><code>X = method.transform(traindata)
X
&lt;Nx20002 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
 with 1135520 stored elements in Compressed Sparse Row format&gt;], 
 where N is sample size
</code></pre>
","python, machine-learning, nlp, scikit-learn","<p>You are asking several separate questions. Let me answer them separately:</p>

<p><strong>""It is unclear to me how the words are selected.""</strong></p>

<p>From the <a href=""https://github.com/mbatchkarov/scikit-learn/blob/master/sklearn/feature_extraction/text.py"" rel=""noreferrer"">documentation</a>:</p>

<pre><code>max_features : optional, None by default
    If not None, build a vocabulary that only consider the top
    max_features ordered by term frequency across the corpus.
</code></pre>

<p>All the features (in your case unigrams, bigrams and trigrams) are ordered by frequency in the entire corpus, and then the top <code>10000</code> are selected. The uncommon words are thrown out. </p>

<p><strong>""If we say max_features = 10000, do we always get the same? If we say max_features = 12000, will we get the same 10000 features, but an extra added 2000?""</strong></p>

<p>Yes. The process is deterministic: for a given corpus and a given <code>max_features</code>, you will always get the same features.</p>

<p><strong>I fit it on some text, but I know of some words that should be included for sure, [...] How to add these to the TfidfVectorizer object?</strong></p>

<p>You use the <code>vocabulary</code> parameter to specify what features should be used. For example, if you want only emoticons to be extracted, you can do the following:</p>

<pre><code>emoticons = {"":)"":0, "":P"":1, "":("":2}
vect = TfidfVectorizer(vocabulary=emoticons)
matrix = vect.fit_transform(traindata)
</code></pre>

<p>This will return a <code>&lt;Nx3 sparse matrix of type '&lt;class 'numpy.int64'&gt;' with M stored elements in Compressed Sparse Row format&gt;]</code>. Notice there are only 3 columns, one for each feature. </p>

<p>If you want the vocabulary to include the emoticons <strong>as well as the <code>N</code> most common features</strong>, you could calculate the most frequent features first, then merge them with the emoticons and re-vectorize like so:</p>

<pre><code># calculate the most frequent features first
vect = TfidfVectorizer(vocabulary=emoticons, max_features=10)
matrix = vect.fit_transform(traindata)
top_features = vect.vocabulary_
n = len(top_features)

# insert the emoticons into the vocabulary of common features
emoticons = {"":)"":0, "":P"":1, "":("":2)}
for feature, index in emoticons.items():
    top_features[feature] = n + index

# re-vectorize using both sets of features
# at this point len(top_features) == 13
vect = TfidfVectorizer(vocabulary=top_features)
matrix = vect.fit_transform(traindata)
</code></pre>
"
SpaCy v3 custom NER model training,"<p>I am trying to create a NLP project using spacy and python that extracts entities from text.</p>
<p>I need some custom entities so I created a JSON file with annotated articles that I am using to train my model with.</p>
<p>The issue I am facing is that instead of creating a new custom model, I want to add few entities to a pre existing model like &quot;en_core_web_sm&quot; but when I change my source for [components.ner] in my config file to &quot;en_core_web_sm' I am getting only the new entities output. Its like the pre-trained model lost all its entities and is only using the new custom entities.</p>
<p>Note: This custom trained model is much more accurate than when I use the basic &quot;eng&quot; model but it just does not output the existing entities, just the new custom ones.</p>
<p><a href=""https://i.sstatic.net/CK5LY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CK5LY.png"" alt=""custom trained model output"" /></a></p>
<p><a href=""https://i.sstatic.net/hRnAu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hRnAu.png"" alt=""en_core_web_sm model output"" /></a></p>
<p>Please let me know how I could keep those pre-made entites in my new model.</p>
<p>here is the tutorial I used to create this:
<a href=""https://www.youtube.com/watch?v=p_7hJvl7P2A&amp;t=685s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=p_7hJvl7P2A&amp;t=685s</a></p>
","python, nlp, spacy, named-entity-recognition",
how can I build the Jira AI chatbot?,"<p>My company has been using Jira for production issue tracking for last 6~8 years and as a result, <strong>there is a huge amount of production issue details logged in our Jira</strong>.</p>
<p>Usually each Jira ticket for any production support issues consist of some useful information such as:</p>
<ul>
<li>Error Message</li>
<li>System Involved</li>
<li>Root Cause</li>
<li>Resolution</li>
<li>Time Taken</li>
<li>etc</li>
</ul>
<p>My company has its own team chat service that supports the Chatbot API in Java / Python / etc. I would like to build the smart chatbot (if not AI) that is smart enough to exchange conversation like this in the chatroom:</p>
<p><strong>DevOps)</strong> Hey Jirabot, what do you know about this error message? [xyzvxc exception occured at line 82.... ]</p>
<p><strong>Jirabot)</strong> Hi there, in which systems did this occur? Can you choose from one of the followings?</p>
<ol>
<li>System A</li>
<li>System B</li>
</ol>
<p><strong>DevOps)</strong> 1</p>
<p><strong>Jirabot)</strong> Right, it looks like following Jira tickets have experienced the similar issues.. please check the following tickets.</p>
<ol>
<li>Jira-12zx</li>
<li>Jira-52123zz</li>
<li>Jira-vvvbbb</li>
</ol>
<p>I would like to ask people with experiences in implementing something similar to this or have any relevant experience in ML / Neural Network / Natural Language Processing the following questions.</p>
<p><strong>1. Is this even possible for non-NPL Expert?</strong>
Do you think it would be possible to build something like this for the software engineer with 8 years of experience with advanced beginner skills in Machine Learning? (Advanced Beginner in <a href=""https://en.wikipedia.org/wiki/Dreyfus_model_of_skill_acquisition"" rel=""nofollow noreferrer"">Dreyfus skill model</a>)</p>
<p>I work for the company which is known for difficult programming interview questions. And I have about 8 years of extensive programming skills and have completed Andrew Ng's machine learning course honestly and with good marks. I am in the middle of (half way) completing the Deep Learning course in Udacity and from the course I am learning to work with Tensorflow.</p>
<p><strong>2. What frameworks / technologies can I use?</strong>
If you think it is possible, what frameworks / technologies do you recommend I look for? Or is there any example of something similar if not exactly the same?</p>
<p><strong>3. If not possible, what area should I focus?</strong>
If you think it is practically NOT possible for a pure developer to build on, then how and what areas can I focus on improving to be able to build one?</p>
","nlp, neural-network, artificial-intelligence, jira, chatbot",
Solution to solve problem different results when run Doc2vec gensim?,"<p>I try to find information about problem that Doc2vec returns different results when it runs. I saw many previous questions about this and I know It happens because vector is randomly initialize. However, I am creating a website which uses this result to display in frontend. The difference in results makes reliability of systems reduce.
I know my dataset is really small. But <code>infer_vector()</code> can't return same vectors with same documents and results <code>most_similar()</code> are different in each run. How do I prevent this problem or having alternative way to apply doc2vec model in my application to avoid difference of results?</p>
<p>This is some code:</p>
<pre><code>model = gensim.models.doc2vec.Doc2Vec(vector_size=50, dm=1, window=5, min_count=2, epochs=100, negative=0, workers=5)
</code></pre>
<p>But I received warning: <code>You must set either 'hs' or 'negative' to be positive for proper training. When both 'hs=0' and 'negative=0', there will be no training.</code>
I try to set <code>negative=-1</code> but I see explain from <code>gensim</code>: <code>negative</code> must be integer.</p>
","nlp, doc2vec","<p>These are potentially, two different issues.</p>
<p>With regard to the warning you're seeing:</p>
<pre><code>You must set either 'hs' or 'negative' to be positive 
for proper training. When both 'hs=0' and 'negative=0', 
there will be no training.
</code></pre>
<p>The warning is complete and truthful, it already describes what you're doing wrong and how to solve it.</p>
<p>You must set either <code>hs</code> or <code>negative</code> to be <strong>positive</strong> or else <strong>no training will happen in your model</strong>.</p>
<p><code>negative=-1</code> is an illegal setting, and not positive.</p>
<p>If you want to use <code>Doc2Vec</code>, you need to either have the <code>negative</code> parameter as a positive integer (as with its default value <code>negative=5</code>), or if you want to set <code>negative=0</code> then you need to enable the alternative &quot;hierarchical softmax&quot; mode with <code>hs=1</code>.</p>
<p>The algorithm will do nothing but error or given nonsense untrained results if you give it illegal configurations.</p>
<p>As is explained in the <a href=""https://github.com/piskvorky/gensim/wiki/Recipes-&amp;-FAQ#q12-ive-used-doc2vec-infer_vector-on-a-single-text-but-the-resulting-vector-is-different-each-time-is-there-a-bug-or-have-i-made-a-mistake-doc2vec-inference-non-determinism"" rel=""nofollow noreferrer"">Q12 of the Gensim Project FAQ</a> &amp; <a href=""https://stackoverflow.com/questions/71474815/different-results-infer-vector-of-doc2vec-after-saving-to-disk-and-load/71479045#71479045"">other StackOverflow answers</a>, the operation of the <code>Doc2Vec</code> algorithm naturally allows for variance in the vectors returned by <code>infer_vector()</code> from run to run.</p>
<p>And, if that &quot;jitter&quot; between inferences is s making a big difference in results, there are probably <em>other</em> serious problems in your use of <code>Doc2Vec</code>, such as insufficient data or bad parameters, that you should fix, rather than trying to force a false determinism onto your calculations.</p>
<p>In particular, if the model whose changing <code>infer_vector()</code> results was &quot;trained&quot; – not really – with the shown parameters (<code>negative=0</code> without enabled <code>hs</code>), ignoring the warning that won't work, that is the first big problem to solve. It will make <em>all</em> inferred vetor random and meaninglfess (as opposed to just &quot;a little noisy&quot;).</p>
<p>But, if after fixing the total failure of training you then insistently want to do the incorrect thing, you can force inference determinism as is described in another answer at:</p>
<p><a href=""https://stackoverflow.com/questions/44443675/removing-randomization-of-vector-initialization-for-doc2vec/44489964#44489964"">removing randomization of vector initialization for doc2vec</a></p>
"
"Tensorflow embeddings InvalidArgumentError: indices[18,16] = 11905 is not in [0, 11905) [[node sequential_1/embedding_1/embedding_lookup","<p>I am using TF 2.2.0 and trying to create a Word2Vec CNN text classification model. But however I tried there has been always an issue with the model or embedding layers. I could not found clear solutions in the internet so decided to ask it.</p>
<pre><code>import multiprocessing
modelW2V = gensim.models.Word2Vec(filtered_stopwords_list, size= 100, min_count = 5, window = 5, sg=0, iter = 10, workers= multiprocessing.cpu_count() - 1)
model_save_location = &quot;3000tweets_notbinary&quot;
modelW2V.wv.save_word2vec_format(model_save_location)

word2vec = {}
with open('3000tweets_notbinary', encoding='UTF-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vec = np.asarray(values[1:], dtype='float32')
        word2vec[word] = vec

num_words = len(list(tokenizer.word_index))

embedding_matrix = np.random.uniform(-1, 1, (num_words, 100))
for word, i in tokenizer.word_index.items():
    if i &lt; num_words:
        embedding_vector = word2vec.get(word)
        if embedding_vector is not None:
          embedding_matrix[i] = embedding_vector
        else:
          embedding_matrix[i] = np.zeros((100,))
</code></pre>
<p>I have created my word2vec weights by the code above and then converted it to embedding_matrix as I followed on many tutorials. But since there are a lot of words seen by word2vec but not available in embeddings, if there is no embedding I assign 0 vector. And then fed data and this embedding to tf sequential model.</p>
<pre><code>seq_leng = max_tokens
vocab_size = num_words
embedding_dim = 100
filter_sizes = [3, 4, 5]
num_filters = 512
drop = 0.5
epochs = 5
batch_size = 32

model = tf.keras.models.Sequential([
                                    tf.keras.layers.Embedding(input_dim= vocab_size,
                                                              output_dim= embedding_dim,
                                                              weights = [embedding_matrix],
                                                              input_length= max_tokens,
                                                              trainable= False),
                                    tf.keras.layers.Conv1D(num_filters, 7, activation= &quot;relu&quot;, padding= &quot;same&quot;),
                                    tf.keras.layers.MaxPool1D(2),
                                    tf.keras.layers.Conv1D(num_filters, 7, activation= &quot;relu&quot;, padding= &quot;same&quot;),
                                    tf.keras.layers.MaxPool1D(),
                                    tf.keras.layers.Dropout(drop),
                                    tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(32, activation= &quot;relu&quot;, kernel_regularizer= tf.keras.regularizers.l2(1e-4)),
                                    tf.keras.layers.Dense(3, activation= &quot;softmax&quot;)
])

model.compile(loss= &quot;categorical_crossentropy&quot;, optimizer= tf.keras.optimizers.Adam(learning_rate= 0.001, epsilon= 1e-06),
              metrics= [&quot;accuracy&quot;, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])

model.summary()

history = model.fit(x_train_pad, y_train2, batch_size= 60, epochs= epochs, shuffle= True, verbose= 1)
</code></pre>
<p>But when I run this code, tensorflow gives me the following error in any random time of the training process. But I could not find any solution to it. I have tried adding + 1 to vocab_size but when I do that I get size mismatch error which does not let me even compile my model. Can anyone please help me?</p>
<pre><code>InvalidArgumentError:  indices[18,16] = 11905 is not in [0, 11905)
     [[node sequential_1/embedding_1/embedding_lookup (defined at &lt;ipython-input-26-ef1b16cf85bf&gt;:1) ]] [Op:__inference_train_function_1533]

Errors may have originated from an input operation.
Input Source operations connected to node sequential_1/embedding_1/embedding_lookup:
 sequential_1/embedding_1/embedding_lookup/991 (defined at /usr/lib/python3.6/contextlib.py:81)

Function call stack:
train_function
</code></pre>
","tensorflow, nlp, word2vec, embedding, word-embedding","<p>I solved this solution. I was adding a new dimension to vocab_size by doing it vocab_size + 1 as suggested by others. However, since sizes of layer dimensions and embedding matrix don't match I got this issue in my hands. I added a zero vector at the end of my embedding matrix which solved the issue.</p>
"
Word Frequency Analysis of .txt in R Not Returning Expected Output,"<pre><code>analyze_document &lt;- function(filename) {
  # Read text data
  text &lt;- readLines(filename, encoding = &quot;UTF-8&quot;) %&gt;% paste(collapse = &quot; &quot;)
  
  # Preprocessing
  text &lt;- str_to_lower(text)  # Use stringr's function
  text &lt;- gsub(&quot;[^[:alnum:]]&quot;, &quot;&quot;, text)  # Remove punctuation
  tokens &lt;- word_tokenizer(text)
  
  # Remove stop words
  stopwords &lt;- stopwords(&quot;english&quot;)
  filtered_tokens &lt;- tokens[!tokens %in% stopwords]
  
  # Word frequency analysis
  word_counts &lt;- DocumentTermMatrix(Corpus(VectorSource(filtered_tokens))) %&gt;%
      as.matrix() %&gt;%
      apply(1, sum) %&gt;%  # Apply sum to each row (document)
      sort(decreasing = TRUE)

  # Print the top 10 most frequent words and their counts within the function
  print(head(word_counts, 10))  

  # Generate report title
  report_title &lt;- paste(&quot;## Word Frequency Analysis for&quot;, filename, sep = &quot; &quot;)
  
  # Generate abstract
  abstract &lt;- paste0(&quot;This report summarizes the word frequency distribution in the document &quot;, 
                     filename, &quot;. It aims to identify the most frequently used words &quot;,
                     &quot;excluding common stop words.\n\n&quot;)
  
  # Generate table header
  table_header &lt;- paste(&quot;| Word | Frequency |\n&quot;, &quot;|---|---| \n&quot;)
  
  # Generate table rows
  table_rows &lt;- sapply(rownames(word_counts), function(word) paste0(&quot;| &quot;, word, &quot; | &quot;, word_counts[word], &quot; |\n&quot;))
  
  # Generate conclusion
  conclusion &lt;- paste0(&quot;\nThe table displays the most frequent words after removing stop words. &quot;,
                       &quot;Analyzing word frequency can help identify potentially overused words &quot;,
                       &quot;or repetitive language patterns, aiding in revising and refining the text.\n&quot;)
  
  # Combine report sections
  report &lt;- paste(report_title, abstract, table_header, table_rows, conclusion, sep = &quot;&quot;)
  
  # Return word counts in addition to the report
  return(list(report = report, word_counts = word_counts))
}


# Define filename with corrected path
filename &lt;- &quot;C:\\Users\\bruh\\OneDrive\\Documents\\dictionary.txt&quot;

# Call the function with the filename
report &lt;- analyze_document(filename)
# Inside the analyze_document function, after sorting the word_counts:

# Print or save the report
cat(report$report)

# save the report to a text file
write.table(report$word_counts, &quot;word_counts.txt&quot;, row.names = TRUE)  # Use write.table for structured data

</code></pre>
<p>I'm attempting to analyze word frequencies in a text document using R, but the results are sub-optimal.
The code runs without errors, but the generated word counts are only a single-element vector with a value of 1.
The report generated from this data is empty.</p>
<p><strong>Expected Behavior:</strong></p>
<p>I anticipate a list of word frequencies in descending order, along with a report containing a table of the most frequent words.</p>
<p><strong>Attempted Solutions:</strong></p>
<p>I've tried the following:</p>
<ul>
<li>Checked that my .txt file actually has words and that the path is correct</li>
<li>Examined preprocessing steps
(lowercasing, punctuation removal)</li>
</ul>
<p><strong>Additional Information:</strong></p>
<ul>
<li>R version: 4.3.3</li>
<li>RStudio 2023.12.1+402 &quot;Ocean Storm&quot; Release (4da58325ffcff29d157d9264087d4b1ab27f7204, 2024-01-28) for windows</li>
<li>Operating system: Windows 11 Home, 64-bit operating system</li>
<li>Document details: 2,000+ word .txt file</li>
</ul>
<p><strong>Specific Questions:</strong></p>
<ol>
<li>What could be causing the word counts to be inaccurate?</li>
<li>Are there any issues with the function's logic or code structure?</li>
<li>Are any preprocessing steps potentially problematic?</li>
<li>How can I effectively debug and identify the root cause of this issue?</li>
</ol>
<p>I'd greatly appreciate any insights or guidance to resolve this problem and ensure accurate word frequency analysis.</p>
<p>Thank you!</p>
","r, nlp",
removing paywall language from piece of text (pandas),"<p>I'm trying to do some preprocessing on my dataset. Specifically, I'm trying to remove paywall language from the text (in bold below) but I keep getting an empty string as my output.</p>
<p>Here is the sample text:</p>
<blockquote>
<p>In order to put a stop to the invasive bush honeysuckle or Lonicera
Maackii currently taking over forests in Missouri and Kansas,
according to Debbie Neff of Excelsior Springs has organized an…
Premium Content is available to subscribers only. <strong>Please login here to
access content or go here to purchase a subscription.</strong></p>
</blockquote>
<p>and my custom function:</p>
<pre class=""lang-py prettyprint-override""><code>import re
import string
import nltk
from nltk.corpus import stopwords

# function to detect paywall-related text
def detect_paywall(text):
    paywall_keywords = [&quot;login&quot;, &quot;subscription&quot;, &quot;purchase a subscription&quot;, &quot;subscribers&quot;]
    for keyword in paywall_keywords:
        if re.search(r'\b{}\b'.format(keyword), text, flags=re.IGNORECASE):
            return True
    return False

# function for text preprocessing
def preprocess_text(text):
    # Check if the text contains paywall-related content
    if detect_paywall(text):
        # Remove paywall-related sentences or language from the text
        sentences = nltk.sent_tokenize(text)
        cleaned_sentences = [sentence for sentence in sentences if not detect_paywall(sentence)]
        cleaned_text = ' '.join(cleaned_sentences)
        return cleaned_text.strip()  # Remove leading/trailing whitespace

    # Tokenization
    tokens = nltk.word_tokenize(text)
    # Convert to lowercase
    tokens = [token.lower() for token in tokens]
    # Remove punctuation
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in stripped if word.isalpha() and word not in stop_words]
    return ' '.join(words)
</code></pre>
<p>I've tried modifying the list of words to detect but to no avail. However, I found that removing &quot;subscribers&quot; from the list does remove the second sentence of the paywall language. But that's not really ideal because there still remains the other half.</p>
<p>The function is also inconsistent because it works on this piece of text (as it will remove the paywall language), but not the one above.</p>
<blockquote>
<p>Of the hundreds of thousands of high school wrestlers, only a small percentage know what it’s like to win a state title. is part of that percentage. The Richmond junior joined that group by winning… <strong>Premium Content is available to subscribers only. Please login here to access content or go here to purchase a subscription.</strong></p>
</blockquote>
","python, pandas, nlp, nltk",
SpaCy: Regex pattern does not work in rule-based matcher,"<p>I am trying to define a regular expression to use as text pattern in the entity ruler component in my spaCy model.
The aim is to add tokens with &quot;COMP&quot; label whenever it finds words structured like this:</p>
<ul>
<li>XXX-Ynnn</li>
<li>XXX Ynnn
Where 'XXX' are trigrams from a list, 'Y' is a letter and 'nnn' a digit combination.</li>
</ul>
<p>To do so, I use the following method</p>
<pre><code>def add_component_patterns_re(input_references, model_ruler):
    ruler = model_ruler
    ref_patterns = []
    letters = ['V', 'B', 'F', 'K', 'S']

    print(&quot;Adding component patterns&quot;)
    for ref in input_references.iloc[:, 0]:
        # print(f&quot;Adding references for system: {ref}&quot;)
        for letter in letters:
            pattern_text = fr'{ref}(-| ){letter}[0-9]{{3}}'
            pattern = {&quot;TEXT&quot;: {&quot;REGEX&quot;: fr'{ref}(-| ){letter}[0-9]{{3}}'}}
            ref_patterns.append({&quot;label&quot;:&quot;COMP&quot;, &quot;pattern&quot;:pattern})
    ruler.add_patterns(ref_patterns)

    return ref_patterns
</code></pre>
<p>Printing out the added patterns, it seems to me that the output list is correct. So my guess is that I am doing something wrong when defining the pattern to add to the ruler.
For information, i've also tried to change the pattern variable as a list entry, like this:</p>
<p><code>            pattern = [{&quot;TEXT&quot;: {&quot;REGEX&quot;: fr'{ref}(-| ){letter}[0-9]{{3}}'}}]</code></p>
<p>But the result is the same, it can't seem to get any match.</p>
<p>Does someone have any suggestion? Thanks in advance!</p>
","python, nlp, spacy, named-entity-recognition","<p>In the end I got</p>
<pre><code>print(f&quot;Adding references for system: {ref}&quot;)
    for letter in letters:
        for nnn in range(1000):
            pattern = f&quot;{ref}-{letter}{nnn:03d}&quot;
            ref_patterns.append({&quot;label&quot;: &quot;COMP&quot;, &quot;pattern&quot;: pattern})
            pattern = f&quot;{ref} {letter}{nnn:03d}&quot;
            ref_patterns.append({&quot;label&quot;: &quot;COMP&quot;, &quot;pattern&quot;: pattern})
</code></pre>
<p>For each pattern. The code is lengthier and a tad slower but it does the job just fine!</p>
"
Named Entity Recognition on Search Engine Queries with Python,"<p>I'm trying to do Named Entity Recognition on search engine queries with Python.</p>
<p>The big thing about search engine queries are that they are usually incomplete or all lowercase.</p>
<p>For this task, I've been recommended Spacy, NLTK, Stanford NLP, Flair, Transformers by Hugging Face as some approaches to this problem.</p>
<p>I was wondering if anybody in the SO community knew the best approach to dealing with NER for search engine queries, because so far I've ran into problems.</p>
<p>For example, with Spacy:</p>
<pre><code>import spacy

# Load the pre-trained model
nlp = spacy.load(&quot;en_core_web_sm&quot;)

# Process a text
text = &quot;google and apple are looking at buying u.k. startup for $1 billion&quot;
text = &quot;who is barack obama&quot;
doc = nlp(text)

# Extract entities
for ent in doc.ents:
    print(ent.text, ent.label_)
</code></pre>
<p>For the first query I got:</p>
<pre><code>google ORG
u.k. GPE
$1 billion MONEY
</code></pre>
<p>This is a great answer. However, for the search query &quot;who is barack obama&quot;, in lower case, it returned no entities.</p>
<p>I'm sure I'm not the first person to do NER on search engine queries in Python, so I'm hoping to find someone who can point me in the right direction.</p>
","python, nlp, search-engine, named-entity-recognition",
How to optimize the function which uses looping on lists on pandas dataframe?,"<p>I am using a function on a pandas dataframe as :</p>
<pre><code>import spacy
from collections import Counter


# Load English language model
nlp = spacy.load(&quot;en_core_web_sm&quot;)

# Function to filter out only nouns from a list of words
def filter_nouns(words):
    SYMBOLS = '{}()[].,:;+-*/&amp;|&lt;&gt;=~$1234567890#_%'
    filtered_nouns = []
    
    # Preprocess the text by removing symbols and splitting into words
    words = [word.translate({ord(SYM): None for SYM in SYMBOLS}).strip() for word in words.split()]
    
    # Process each word and filter only nouns
    filtered_nouns = [token.text for token in nlp(&quot; &quot;.join(words)) if token.pos_ == &quot;NOUN&quot;]
    
    return filtered_nouns



# Apply filtering logic to all rows in the 'NOTE' column
df['filtered_nouns'] = sf['NOTE'].apply(lambda x: filter_nouns(x))
</code></pre>
<p>I have a dataset containing 6400 rows and <code>df['NOTE']</code> is a very long paragraph converted from the Oracle CLOB datatype.</p>
<p>This function is working quickly for 5-10 rows but for 6400 rows, it is taking a very long time.</p>
<p>Any ways to optimize this.</p>
","python, python-3.x, pandas, list, nlp","<p>The first thing you should do is remove all the repetition in your function. In this line:</p>
<pre class=""lang-py prettyprint-override""><code>words = [word.translate({ord(SYM): None for SYM in SYMBOLS}).strip() for word in words.split()]
</code></pre>
<p>You are building the translation dictionary <em>every</em> time you translate a word, and calling translate for each word in the text. It is far more efficient to do each of those once:</p>
<pre class=""lang-py prettyprint-override""><code>tr = str.maketrans('', '', SYMBOLS)
words = words.strip().translate(tr).split()
</code></pre>
<p>This makes about a 50x speed-up on a 1000-word string on my computer.</p>
<p>In the next line you are then joining all the words for every call to <code>nlp</code>. You should do that once:</p>
<pre><code>text = ' '.join(words)
filtered_nouns = [token.text for token in nlp(text) if token.pos_ == &quot;NOUN&quot;]
</code></pre>
<p>But note that you just split on spaces, so you might as well skip that step completely. In total:</p>
<pre><code>def filter_nouns(text):
    SYMBOLS = '{}()[].,:;+-*/&amp;|&lt;&gt;=~$1234567890#_%'
    tr = str.maketrans('', '', SYMBOLS)
    
    # Preprocess the text by removing symbols
    words = text.strip().translate(tr)
    
    # Process each word and filter only nouns
    filtered_nouns = [token.text for token in nlp(words) if token.pos_ == &quot;NOUN&quot;]
    
    return filtered_nouns
</code></pre>
<p>Finally, note that <code>.apply(lambda x: filter_nouns(x))</code> is the same as <code>.apply(filter_nouns)</code>.</p>
"
Text Generation consistently results in blank characters,"<p>The following code is showing a very normal loss chart after 20 epochs, but when trying to test it with a seed text it consistently outputs blank lines (' '). Its either that I simply do not understand the process of &quot;text seed&quot; preparation or processing the predictions or something subtle is wrong that I have not been able to notice, hence my appeal to this community.</p>
<p><img src=""https://i.sstatic.net/QSWz3.jpg"" alt=""Loss Function"" /></p>
<p>Data used here is a small portion of the nietzsche.txt located here:</p>
<p><a href=""https://s3.amazonaws.com/text-datasets/nietzsche.txt"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/text-datasets/nietzsche.txt</a></p>
<p>The main issue is that the predictions always have <code>argmax() = 1</code>. The word dictionary associated with the test data has ' ' at that index, therefore the output is always series of blank characters. The data being fed to the model however does seem reasonable as shown in the <code>x sequence</code> of integers shown in the [OUTPUT] section below. You can see that after every iteration, a new (and different) character is added to the <code>pattern</code> list and the list is sliced to maintain a constant length by dropping the first character.</p>
<p>I have been trying to figure out where I am going wrong in generating non-blank texts and have not been able to identify the issue. I would be grateful for any help.</p>
<pre><code>import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import LSTM
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical

# load ascii text and covert to lowercase
filename = &quot;/content/drive/MyDrive/Colab Notebooks/TexGen/nietzsche-short.txt&quot;
raw_text = open(filename, 'r', encoding='utf-8').read()
raw_text = raw_text.lower()

# create mapping of unique chars to integers, and a reverse mapping
chars = sorted(list(set(raw_text)))
char_to_int = dict((c, i) for i, c in enumerate(chars))
int_to_char = dict((i, c) for i, c in enumerate(chars))

# summarize the loaded data
n_chars = len(raw_text)
n_vocab = len(chars)

# prepare the dataset of input to output pairs encoded as integers
seq_length = 100
dataX = []
dataY = []

for i in range(0, n_chars - seq_length, 1):
 seq_in = raw_text[i:i + seq_length]
 seq_out = raw_text[i + seq_length]
 dataX.append([char_to_int[char] for char in seq_in])
 dataY.append(char_to_int[seq_out])

n_patterns = len(dataX)

# reshape X to be [samples, time steps, features]
X = np.reshape(dataX, (n_patterns, seq_length, 1))

# normalize
X = X / float(n_vocab)

# one hot encode the output variable
y = to_categorical(dataY)

# define the LSTM model
model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(n_vocab, 50, input_length=seq_length),
    tf.keras.layers.Conv1D(128, 5, activation='relu'),  # CNN layer
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.LSTM(256, return_sequences=True),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(256),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(y.shape[2], activation='softmax')
], name=&quot;LSTM_Model&quot;)

model.compile(loss='categorical_crossentropy', optimizer='adam')

history = model.fit(X, y,
          epochs=25,
          batch_size=128
)

# Test the model with a seed
start = np.random.randint(0, len(dataX)-1)
pattern = dataX[start] # dataX is a list of list 100 characters each
print(&quot;Seed:&quot;)
print(&quot;\&quot;&quot;, ''.join([int_to_char[value] for value in pattern]), &quot;\&quot;&quot;)

# seed:
# &quot; ay upon words, a deception on the part of grammar, or an
# audacious generalization of very restricted &quot;

# pattern[:10]
# [13, 37, 1, 33, 28, 27, 26, 1, 35, 27]

# generate characters
for i in range(5):
  x = np.reshape(pattern, (1, len(pattern), 1))
  x = x / float(n_vocab)
  print(&quot;\n==================&quot;)
  print(&quot;x[:10] : &quot;, x[:, :10, :])
  prediction = model.predict(x, verbose=0)
  index = np.argmax(prediction)
  result = int_to_char[index]
  print(&quot;index = &quot;, index)
  print(&quot;result = &quot;, result)
  # seq_in = [int_to_char[value] for value in pattern]
  #sys.stdout.write(result)
  pattern.append(index)
  pattern = pattern[1:len(pattern)]
print(&quot;\nDone.&quot;)


[output]

# predictions:

array([[0.01481258, 0.1349412 , 0.00109681, 0.00254168, 0.00037268,
        0.00085235, 0.00087828, 0.01556777, 0.00960505, 0.00228236,
        0.00174035, 0.00123438, 0.00138978, 0.07334321, 0.01076234,
        0.01881236, 0.03297085, 0.0944486 , 0.0203624 , 0.01628518,
        0.04297792, 0.05854145, 0.00125222, 0.00374453, 0.02957868,
        0.0199816 , 0.05518206, 0.05479056, 0.02143795, 0.000657  ,
        0.04608261, 0.06542768, 0.08481915, 0.02293939, 0.00776505,
        0.01505006, 0.00033998, 0.01451185, 0.00062015]], dtype=float32)

==================
x[:10] :  [[[0.33333333]
  [0.94871795]
  [0.02564103]
  [0.84615385]
  [0.71794872]
  [0.69230769]
  [0.66666667]
  [0.02564103]
  [0.8974359 ]
  [0.69230769]]]
index =  1
result =   

==================
x[:10] :  [[[0.94871795]
  [0.02564103]
  [0.84615385]
  [0.71794872]
  [0.69230769]
  [0.66666667]
  [0.02564103]
  [0.8974359 ]
  [0.69230769]
  [0.76923077]]]
index =  1
result =   

==================
x[:10] :  [[[0.02564103]
  [0.84615385]
  [0.71794872]
  [0.69230769]
  [0.66666667]
  [0.02564103]
  [0.8974359 ]
  [0.69230769]
  [0.76923077]
  [0.41025641]]]
index =  1
result =   

==================
x[:10] :  [[[0.84615385]
  [0.71794872]
  [0.69230769]
  [0.66666667]
  [0.02564103]
  [0.8974359 ]
  [0.69230769]
  [0.76923077]
  [0.41025641]
  [0.79487179]]]
index =  1
result =   

==================
x[:10] :  [[[0.71794872]
  [0.69230769]
  [0.66666667]
  [0.02564103]
  [0.8974359 ]
  [0.69230769]
  [0.76923077]
  [0.41025641]
  [0.79487179]
  [0.17948718]]]
index =  1
result =   

Done.
[/output]
</code></pre>
<pre><code>model.summary()

Model: &quot;LSTM_Model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (None, 100, 50)           1950      
                                                                 
 conv1d (Conv1D)             (None, 96, 128)           32128     
                                                                 
 max_pooling1d (MaxPooling1  (None, 24, 128)           0         
 D)                                                              
                                                                 
 lstm (LSTM)                 (None, 24, 256)           394240    
                                                                 
 dropout (Dropout)           (None, 24, 256)           0         
                                                                 
 lstm_1 (LSTM)               (None, 256)               525312    
                                                                 
 dropout_1 (Dropout)         (None, 256)               0         
                                                                 
 dense (Dense)               (None, 39)                10023     
                                                                 
=================================================================
Total params: 963653 (3.68 MB)
Trainable params: 963653 (3.68 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________


  [1]: https://i.sstatic.net/QSWz3.jpg
  [2]: https://s3.amazonaws.com/text-datasets/nietzsche.txt
</code></pre>
","nlp, text-generation",
Using MBart50TokenizerFast tokenizer with multiple sentences,"<p>I am trying to use MBart50TokenizerFast with <code>facebook/mbart-large-50-many-to-one-mmt</code> on GPU, and trying to provide multiple sentences in one go (the sentences cannot be combined). Here is my code (based on <a href=""https://stackoverflow.com/a/62688252/194742"">https://stackoverflow.com/a/62688252/194742</a>):</p>
<pre><code>tokenizer.src_lang = source_lang
inputs = tokenizer([title, ftext], return_tensors=&quot;pt&quot;).to(device)
outputs = model.generate(**inputs).to(device)
translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)
translated_title = translations[0]
translated_ftext = translations[1]
</code></pre>
<p>This mostly follows the example given on the page, except that I am trying to include multiple sentences in one go. Here is the error message I get:</p>
<pre><code>Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
</code></pre>
<p>The code does work with this line:</p>
<pre><code>inputs = self.tokenizer(title, return_tensors=&quot;pt&quot;).to(self.device)
</code></pre>
<p>What is the correct way to use multiple sentences? Thanks for any pointers.</p>
","nlp, huggingface-transformers, huggingface-tokenizers, machine-translation",
Error training transformer with QLoRA and Peft,"<p>So I am trying to finetuning google Gemma model using Peft and QLoRA. Yesterday I successfully fine-tuned it for 1 epoch just as a test. However, when I opened the notebook today and ran the cell that loads the model I get a huge error:</p>
<p>The code:</p>
<pre><code>model_id = &quot;google/gemma-7b&quot;

bnb_config = BitsAndBytesConfig(
load_in_4bit=True,
bnb_4bit_use_double_quant=True,
bnb_4bit_quant_type=&quot;nf4&quot;,
bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = 
AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
model_id, 
quantization_config=bnb_config, 
device_map={0:&quot;&quot;})

#model.gradient_checkpointing_enable()

train_dataset, val_dataset, data_collator = load_dataset(train_data_path, val_data_path, tokenizer)
</code></pre>
<p>The error (shortened):</p>
<pre><code>RuntimeError: device &gt;= 0 &amp;&amp; device &lt; num_gpus INTERNAL ASSERT FAILED at &quot;../aten/src/ATen/cuda/CUDAContext.cpp&quot;:50, please report a bug to PyTorch. device=1, num_gpus=

.....

DeferredCudaCallError: CUDA call failed lazily at initialization with error: device &gt;= 0 &amp;&amp; device &lt; num_gpus INTERNAL ASSERT FAILED at &quot;../aten/src/ATen/cuda/CUDAContext.cpp&quot;:50, please report a bug to PyTorch. device=1, num_gpus=
.....

RuntimeError: Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):
CUDA call failed lazily at initialization with error: device &gt;= 0 &amp;&amp; device &lt; num_gpus INTERNAL ASSERT FAILED at &quot;../aten/src/ATen/cuda/CUDAContext.cpp&quot;:50, please report a bug to PyTorch. device=1, num_gpus=

.....
</code></pre>
<p>I have shortened the error for it to be more readable. Has anyone experienced something like this? I can't seem to solve it. Help is much appreciated.</p>
","python, deep-learning, nlp, huggingface-transformers",
What does the vocabulary of a pre-trained / fine-tuned T5 model look like?,"<p>My question is regarding the pre-trained T5 models found on Huggingface. In either case of taking the fully-trained model, or after fine-tuning it, is there an API function for directly downloading the vocabulary?</p>
<p>More specifically, the default <code>vocab_size</code> for T5 is 32128 (<a href=""https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Config.vocab_size"" rel=""nofollow noreferrer"">from the documentation</a>). Does that mean that after the model is trained, its decoder can generate up to 32128 unique words?</p>
<p>As an aside, I have noticed that capitalization does sometimes appear in my fine-tuned T5, does that mean the 32128 vocabulary could also be comprised of capitalized variants of words, e.g., is there one vocab index for &quot;hello&quot; and another index for &quot;Hello&quot;?</p>
","python, pytorch, nlp, huggingface-transformers","<ul>
<li><p>The T5 default vocabulary consists of 32,128 <strong>subword</strong> tokens (utilizing the SentencePiece tokenizer), not word tokens. Thus, it can generate a larger vocabulary than the specified 32,128.</p>
</li>
<li><p>&quot;hello&quot; and &quot;Hello&quot; are treated as different tokens because T5's tokenizer is <strong>case-sensitive</strong>.</p>
</li>
</ul>
"
Extracting difficult words from text,"<p>I need to identify difficult words from input text. I do not want to use common word lists because the level of difficulty will need to be set for children. Is there a scoring mechanism that computes the difficulty level of each word? I can use a threshold for score to separate difficult words from easy ones. The end objective is to provide word meanings for all such difficult words.</p>
<p>There are several ways in which overall text can be scored for complexity or difficulty level, eg. The Dale–Chall formula, The Gunning fog formula, etc. However, these are used for defining &quot;readability&quot; i.e. the ease with which a reader can understand the written text. My requirement is related to the difficulty level of individual words in a text.</p>
<p>There are a few methods that I have come across for defining difficult words such words with more than 2 syllables or any word not appearing in the 10000 most common words, etc. However, none of these methods are useful for me. I am trying to build an application that can identify difficult words, and provides relevant dictionary meanings for only those words. Is there a scoring mechanism that will allow me to use a threshold to separate the difficult words from the easy ones?</p>
",nlp,
Extracting sentences from a text document,"<p>I have a text document from which I'd like to extract the Noun phrases. In the first step I extract sentences and then I do a part of speech (pos) tagging for each sentence and then using the pos I do a chunking. I used StanfordNLP for these task, and this is the code for extracting the sentences.</p>

<pre><code>Reader reader = new StringReader(text);
DocumentPreprocessor dp = new DocumentPreprocessor(reader);
</code></pre>

<p>I think <code>DocumentPreprocessor</code> does a pos under the hood in order to extract the sentences. However, I'm doing another pos for extracting the noun phrases in the second phase as well. That is, pos is done twice and because pos is a computationally expensive task, I'm looking for a way to do it only once. Is there any way to do pos only once to extract sentences and noun phrases?</p>
","nlp, stanford-nlp, part-of-speech",
Export displacy graphics as an image file shows an empty file,"<p>I want to export displacy graphics as an image file. I run the following code block in Jupyter Notebook and generate an svg file name sample.svg.</p>
<p>The code block is</p>
<p>import spacy</p>
<pre><code>from spacy import displacy
from pathlib import Path

nlp = spacy.load('en_core_web_lg')

sentence_nlp = nlp(&quot;John go home to your family&quot;)
svg = displacy.render(sentence_nlp, style=&quot;ent&quot;, jupyter = False)

output_path = Path(&quot;D:\\sample.svg&quot;) 
output_path.open(&quot;w&quot;, encoding=&quot;utf-8&quot;).write(svg)
</code></pre>
<p>The above code gave me an output 393.</p>
<p>When I try to open the svg file by inserting in powerpoint it shows nothing.</p>
<p>I drag and drop the svg file in the Chrome browser.</p>
<p>The result of drag and drop is as follows</p>
<p><a href=""https://i.sstatic.net/1ZEio.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1ZEio.png"" alt=""enter image description here"" /></a></p>
<p>Can any one help me to export the correct svg file format?</p>
<p>Thank you.</p>
","python, jupyter-notebook, nlp, spacy, displacy",
Why do I get different embeddings when I perform batch encoding in huggingface MT5 model?,"<p>I am trying to encode some text using HuggingFace's mt5-base model. I am using the model as shown below</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import MT5EncoderModel, AutoTokenizer

model = MT5EncoderModel.from_pretrained(&quot;google/mt5-base&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;google/mt5-base&quot;)

def get_t5_embeddings(texts):
    last_hidden_state = model(input_ids=tokenizer(texts, return_tensors=&quot;pt&quot;, padding=True).input_ids).last_hidden_state
    pooled_sentence = torch.max(last_hidden_state, dim=1)
    return pooled_sentence[0].detach().numpy()
</code></pre>
<p>I was doing some experiments when I noticed that the same text had a low cosine similarity score with itself. I did some digging and realized that the model was returning very different embeddings if I did the encoding in batches. To validate this, I ran a small experiment that generated embeddings for <code>Hello</code> and a list of 10 <code>Hello</code>s incrementally. and checking the embeddings of the <code>Hello</code> and the first <code>Hello</code> in the list (both of which should be same).</p>
<pre class=""lang-py prettyprint-override""><code>for i in range(1, 10):
    print(i, (get_t5_embeddings([&quot;Hello&quot;])[0] == get_t5_embeddings([&quot;Hello&quot;]*i)[0]).sum())
</code></pre>
<p>This will return the number of values in the embeddings that match each other.
This was the result:</p>
<pre><code>1 768
2 768
3 768
4 768
5 768
6 768
7 768
8 27
9 27
</code></pre>
<p>Every time I run it, I get mismatches if the batch size is more than 768.</p>
<p>Why am I getting different embeddings and how do I fix this?</p>
","python, machine-learning, pytorch, nlp, huggingface-transformers",
Remove everything from text except Hindi and English letters and numbers and punctuation,"<p>I have a text which is mixed in English and Hindi and I want to remove all the characters except Hindi and English characters and numbers and punctuation. That way, I can get rid of &quot;(&quot;, &quot;)&quot;,&quot;@&quot;, etc. Please consider the text below.</p>
<pre><code>text = नई दिल्ली। Navjot Singh Sidhu Resigns. पंजाब विधानसभा चुनाव से पहले पंजाब कांग्रेस में कई बड़े बदलाव देखने को मिल रहे हैं। पहले कैप्टन अमरिंदर सिंह का पंजाब के मुख्यमंत्री पद से इस्तीफा दिया, उसके बाद चरणजीत सिंह चन्नी को राज्य का नया मुख्यमंत्री बनाया गया। वहीं अब नवजोत सिंह सिद्धू ने पंजाब कांग्रेस अध्यक्ष पद से इस्तीफा दे दिया है।I told you so…he is not a stable man and not fit for the border state of punjab.— Capt.Amarinder Singh (@capt_amarinder) September 28, 2021सोनिया गांधी को लिखा पत्र बता दें कि नवजोत सिंह सिद्धू ने काग्रेस अध्यक्ष सोनिया गांधी को एक पत्र लिखकर इस संबंध में जानकारी दी है। पत्र में सिद्धू ने यह भी कह कि वे कांग्रेस का हिस्सा बने रहेंगे।pic.twitter.com/L5wdRql5t3— Navjot Singh Sidhu (@sherryontopp) September 28, 2021
</code></pre>
","python, nlp, hindi",
Torchtext functions in Newest version analogue,"<p>Good day all, i'm trying to solve task, where it was used previously <code>torchtext.dataset.TranslationDataset</code>, <code>torch.data.Field</code> and <code>torch.data.BucketIterator</code>. But, after updating they were removed and i don't know how it can be used now. Has anybody faced with this problem, how have you solved it? I'd be the greatful, if you share a link with advice if it's possible.</p>
<p>I've tried to read docs and find info on the Internet, but unsuccessfully. It always cite <code>torchtext.legacy</code>, but it doesn't have it in up-to-date versions. I understand, that i can download to it's version, but don't want this yet.</p>
","nlp, version, torchtext","<p>I think these are the imports in the newer version of torch and their latest documentation</p>
<ul>
<li><code>torchtext.dataset.TranslationDataset</code> -&gt; <a href=""https://torchtext.readthedocs.io/en/latest/datasets.html#torchtext.datasets.TranslationDataset"" rel=""nofollow noreferrer"">torchtext.datasets.TranslationDataset</a></li>
<li><code>torch.data.Field</code> -&gt; <a href=""https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Field"" rel=""nofollow noreferrer"">torchtext.data.Field</a></li>
<li><code>torch.data.BucketIterator</code> -&gt; <a href=""https://torchtext.readthedocs.io/en/latest/data.html?#bucketiterator"" rel=""nofollow noreferrer"">torchtext.data.BucketIterator</a></li>
</ul>
<p>Let me know if you have trouble importing them</p>
"
Where can I find Twitter messages archive or search old tweets?,"<p>Previously Google launched an application that can search Twitter message OVER Time like in news timeline. But it seems it now can only provide real time (current) message index, not the old and all tweets in history. I want to do research on tweets, but do not know where to download or access to such data based on timeline or geography or demographic or topic list.</p>
","web-services, twitter, nlp",
How to use NTLK (and spacy) to handle sentiment analysis on two topics within one sentence,"<p>I am new to Python but slowly using available resources to make good progress from a coding perspective. However, I can't find help for a specific use case that I am trying to solve. I have survey comments that I want to analyze using sentiment analysis. I have been able to get Python NLTK and spaCy to process the text for sentiment analysis, but the comments I have are complicated, typically containing two independent thoughts.</p>
<p>For example, say I am analyzing text comments about pets and I have this sentence: &quot;Dogs make better pets than Alligators&quot;. What I want to do is get a positive sentiment for Dogs and a negative (or neutral) for Alligators.</p>
<p>I am using spacy to identify sentences with specific phrases (I think of these as topics). I am using SIA from NLTK to do the actual analysis.</p>
<p>For the sentence &quot;Dogs make better pets than Alligators.&quot; the code is correctly processing it twice since it matches the phrase &quot;Dogs&quot; and &quot;Alligtors&quot;. But I need to have SIA process it once with &quot;Dogs&quot; as the topic and once with &quot;Alligators&quot; as the topic and I can't figure out how to get SIA to do that.</p>
<pre><code>import nltk
import spacy

from nltk.sentiment import SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

nlp = spacy.load(&quot;en_core_web_sm&quot;)

text = &quot;&quot;&quot;Dogs and cats make great pets.
Alligators make poor pets.
Dogs make better pets than Alligators.&quot;&quot;&quot;

from spacy.matcher import PhraseMatcher
phrase_matcher = PhraseMatcher(nlp.vocab)
phrases = ['Dogs', 'Alligators']
patterns = [nlp(text) for text in phrases]
phrase_matcher.add('Pets', None, *patterns)

doc = nlp(text)
for sent in doc.sents:
    for match_id, start, end in phrase_matcher(nlp(sent.text)):
        if nlp.vocab.strings[match_id] in [&quot;Pets&quot;]:
            res = sia.polarity_scores(sent.text)
            print(res, &quot; score for sentence: &quot;, sent.text)
</code></pre>
","python, python-3.x, nlp, spacy",
How to identify feature names from indices in a decision tree using scikit-learn’s CountVectorizer?,"<p>I have the following data for training a model to detect whether a sentence is about:</p>
<ul>
<li>a cat or dog</li>
<li>NOT about a cat or dog</li>
</ul>
<p><a href=""https://i.sstatic.net/gNnh6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gNnh6.png"" alt=""screenshot of data consisting of a text column and label column"" /></a></p>
<p>I ran the following code to train a <code>DecisionTreeClassifier()</code> model then view the tree visualisation:</p>
<pre><code>import numpy as np
from numpy.random import seed
import random as rn
import os
import pandas as pd
seed_num = 1
os.environ['PYTHONHASHSEED'] = '0'
np.random.seed(seed_num)
rn.seed(seed_num)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

dummy_train = pd.read_csv('dummy_train.csv')

tree_clf = tree.DecisionTreeClassifier()

X_train = dummy_train[&quot;text&quot;]
y_train = dummy_train[&quot;label&quot;]

dt_tree_pipe = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),
                                                 binary=True)),
                     ('tfidf', TfidfTransformer(use_idf=False)),
                      ('clf', DecisionTreeClassifier(random_state=seed_num,
                                                 class_weight={0:1, 1:1})),
                   ])

tree_model_fold_1 = dt_tree_pipe.fit(X_train, y_train)

tree.plot_tree(dt_tree_pipe[&quot;clf&quot;])
</code></pre>
<p>...resulting in the following tree:</p>
<p><a href=""https://i.sstatic.net/4LBJ4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4LBJ4.png"" alt=""screenshot of decision tree visualisation"" /></a></p>
<p>The first node checks if <code>x[7]</code> is less than or equal to <code>0.177</code>. <strong>How do I find out which word <code>x[7]</code> represents?</strong></p>
<p>I tried the following code but the words returned in the output (&quot;describing&quot; and &quot;the&quot;) don't look correct. I would have thought <code>'cat'</code> and <code>'dog'</code> would be the two words used to split the data into the positive and negative class.</p>
<pre><code>vect_from_pipe = dt_tree_pipe[&quot;vect&quot;]
words = vect_from_pipe.vocabulary_.keys()
print(list(words)[7])
print(list(words)[5])
</code></pre>
<p><a href=""https://i.sstatic.net/qdCbc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qdCbc.png"" alt=""screenshot of the words 'describing' and 'the'"" /></a></p>
","python, scikit-learn, nlp, decision-tree, countvectorizer",
How to revert BERT/XLNet embeddings?,"<p>I've been experimenting with stacking language models recently and noticed something interesting: the output embeddings of BERT and XLNet are not the same as the input embeddings. For example, this code snippet:</p>

<pre><code>bert = transformers.BertForMaskedLM.from_pretrained(""bert-base-cased"")
tok = transformers.BertTokenizer.from_pretrained(""bert-base-cased"")

sent = torch.tensor(tok.encode(""I went to the store the other day, it was very rewarding.""))
enc = bert.get_input_embeddings()(sent)
dec = bert.get_output_embeddings()(enc)

print(tok.decode(dec.softmax(-1).argmax(-1)))
</code></pre>

<p>Outputs this for me:</p>

<pre><code>,,,,,,,,,,,,,,,,,
</code></pre>

<p>I would have expected the (formatted) input sequence to be returned since I was under the impression that the input and output token embeddings were tied.</p>

<p>What's interesting is that most other models do not exhibit this behavior. For example, if you run the same code snippet on GPT2, Albert or Roberta, it outputs the input sequence.</p>

<p>Is this a bug? Or is it expected for BERT/XLNet?</p>
","python, nlp, pytorch, huggingface-transformers, transformer-model",
How to create a langchain doc from an str?,"<p>I've searched all over langchain documentation on their official website but I didn't find how to create a langchain doc from a str variable in python so I searched in their GitHub code and I found this :</p>
<pre><code>  doc=Document(
                page_content=&quot;text&quot;,
                metadata={&quot;source&quot;: &quot;local&quot;}
            )

</code></pre>
<p>PS: I added the metadata attribute<br>
then I tried using that doc with my chain:<br>
Memory and Chain:</p>
<pre><code>memory = ConversationBufferMemory(memory_key=&quot;chat_history&quot;, input_key=&quot;human_input&quot;)
chain = load_qa_chain(
    llm, chain_type=&quot;stuff&quot;, memory=memory, prompt=prompt
)

</code></pre>
<p>the call method:</p>
<pre><code>  chain({&quot;input_documents&quot;: doc, &quot;human_input&quot;: query})
</code></pre>
<p>prompt template:</p>
<pre><code>template = &quot;&quot;&quot;You are a senior financial analyst analyzing the below document and having a conversation with a human.
{context}
{chat_history}
Human: {human_input}
senior financial analyst:&quot;&quot;&quot;

prompt = PromptTemplate(
    input_variables=[&quot;chat_history&quot;, &quot;human_input&quot;, &quot;context&quot;], template=template
)
</code></pre>
<p>but I am  getting the following error:</p>
<pre><code>AttributeError: 'tuple' object has no attribute 'page_content'

</code></pre>
<p>when I tried to check the type and the page content of the Document object before using it with the chain I got this</p>
<pre><code>print(type(doc))
&lt;class 'langchain.schema.Document'&gt;
print(doc.page_content)
&quot;text&quot;


</code></pre>
","python, nlp, langchain, large-language-model","<p>I had a similar issue and I noticed the API calls for a list, so try</p>
<pre><code>    doc = Document(page_content=input,
            metatdata={
                &quot;source&quot;: &quot;userinput&quot;
            }
        )
    #db.add_documents(doc)
    db.add_documents([doc])
</code></pre>
"
How to Train GloVe algorithm on my own corpus,"<p>I tried to follow <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">this.</a><br>
But some how I wasted a lot of time ending up with nothing useful.<br>
I just want to train a <code>GloVe</code> model on my own corpus (~900Mb corpus.txt file).
I downloaded the files provided in the link above and compiled it using <code>cygwin</code> (after editing the demo.sh file and changed it to <code>VOCAB_FILE=corpus.txt</code> . should I leave <code>CORPUS=text8</code> unchanged?)
the output was:  </p>

<ol>
<li>cooccurrence.bin </li>
<li>cooccurrence.shuf.bin  </li>
<li>text8</li>
<li>corpus.txt</li>
<li>vectors.txt</li>
</ol>

<p>How can I used those files to load it as a <code>GloVe</code> model on python?</p>
","nlp, stanford-nlp, gensim, word2vec, glove",
